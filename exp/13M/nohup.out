2023-11-10 21:21:44.150019: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2023-11-10 21:21:44.185549: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: SSE4.1 SSE4.2 AVX AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-11-10 21:21:54.155754: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 46200 MB memory:  -> device: 0, name: NVIDIA RTX A6000, pci bus id: 0000:17:00.0, compute capability: 8.6
2023-11-10 21:21:54.156364: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:1 with 46200 MB memory:  -> device: 1, name: NVIDIA RTX A6000, pci bus id: 0000:65:00.0, compute capability: 8.6
W1110 21:21:54.594038 140264174335808 utils.py:10] No checkpoint found at ./exp/13M/workdir/checkpoints-meta/checkpoint.pth. Returned the same state as input
I1110 21:21:54.610749 140264174335808 xla_bridge.py:627] Unable to initialize backend 'rocm': NOT_FOUND: Could not find registered platform with name: "rocm". Available platform names are: Interpreter CUDA
I1110 21:21:54.611630 140264174335808 xla_bridge.py:627] Unable to initialize backend 'tpu': module 'jaxlib.xla_extension' has no attribute 'get_tpu_client'
I1110 21:21:54.629448 140264174335808 dataset_info.py:565] Load dataset info from /home/mmorafah@AD.UCSD.EDU/tensorflow_datasets/cifar10/3.0.2
W1110 21:21:54.636364 140264174335808 options.py:619] options.experimental_threading is deprecated. Use options.threading instead.
W1110 21:21:54.636518 140264174335808 options.py:619] options.experimental_threading is deprecated. Use options.threading instead.
I1110 21:21:54.636720 140264174335808 dataset_builder.py:522] Reusing dataset cifar10 (/home/mmorafah@AD.UCSD.EDU/tensorflow_datasets/cifar10/3.0.2)
I1110 21:21:54.736062 140264174335808 logging_logger.py:49] Constructing tf.data.Dataset cifar10 for split train, from /home/mmorafah@AD.UCSD.EDU/tensorflow_datasets/cifar10/3.0.2
W1110 21:21:54.852758 140264174335808 options.py:619] options.experimental_threading is deprecated. Use options.threading instead.
W1110 21:21:54.852953 140264174335808 options.py:619] options.experimental_threading is deprecated. Use options.threading instead.
I1110 21:21:54.853152 140264174335808 dataset_builder.py:522] Reusing dataset cifar10 (/home/mmorafah@AD.UCSD.EDU/tensorflow_datasets/cifar10/3.0.2)
I1110 21:21:54.878226 140264174335808 logging_logger.py:49] Constructing tf.data.Dataset cifar10 for split test, from /home/mmorafah@AD.UCSD.EDU/tensorflow_datasets/cifar10/3.0.2
I1110 21:21:54.971148 140264174335808 run_lib.py:143] Starting training loop at step 0.
I1110 21:21:58.140448 140264174335808 run_lib.py:153] step: 0, training_loss: 1.53538e+03
I1110 21:21:58.508189 140264174335808 run_lib.py:166] step: 0, eval_loss: 1.53645e+03
I1110 21:22:09.633087 140264174335808 run_lib.py:153] step: 50, training_loss: 1.52422e+03
I1110 21:22:20.979320 140264174335808 run_lib.py:153] step: 100, training_loss: 1.49058e+03
I1110 21:22:32.162091 140264174335808 run_lib.py:153] step: 150, training_loss: 1.41803e+03
I1110 21:22:42.939542 140264174335808 run_lib.py:153] step: 200, training_loss: 1.30498e+03
I1110 21:22:53.984758 140264174335808 run_lib.py:153] step: 250, training_loss: 1.16326e+03
I1110 21:23:05.240667 140264174335808 run_lib.py:153] step: 300, training_loss: 1.00344e+03
I1110 21:23:15.543745 140264174335808 run_lib.py:153] step: 350, training_loss: 8.31943e+02
I1110 21:23:25.766623 140264174335808 run_lib.py:153] step: 400, training_loss: 6.59687e+02
I1110 21:23:35.948785 140264174335808 run_lib.py:153] step: 450, training_loss: 5.12063e+02
I1110 21:23:46.604139 140264174335808 run_lib.py:153] step: 500, training_loss: 3.55917e+02
I1110 21:23:56.315511 140264174335808 run_lib.py:153] step: 550, training_loss: 2.83000e+02
I1110 21:24:06.605455 140264174335808 run_lib.py:153] step: 600, training_loss: 1.91582e+02
I1110 21:24:16.493193 140264174335808 run_lib.py:153] step: 650, training_loss: 2.19130e+02
I1110 21:24:27.564918 140264174335808 run_lib.py:153] step: 700, training_loss: 1.76569e+02
I1110 21:24:37.859774 140264174335808 run_lib.py:153] step: 750, training_loss: 2.02817e+02
I1110 21:24:48.100334 140264174335808 run_lib.py:153] step: 800, training_loss: 1.98693e+02
I1110 21:24:58.070737 140264174335808 run_lib.py:153] step: 850, training_loss: 1.47167e+02
I1110 21:25:08.259615 140264174335808 run_lib.py:153] step: 900, training_loss: 1.70280e+02
I1110 21:25:18.576384 140264174335808 run_lib.py:153] step: 950, training_loss: 1.61517e+02
I1110 21:25:28.909631 140264174335808 run_lib.py:153] step: 1000, training_loss: 1.72809e+02
I1110 21:25:39.420182 140264174335808 run_lib.py:153] step: 1050, training_loss: 1.68085e+02
I1110 21:25:49.700153 140264174335808 run_lib.py:153] step: 1100, training_loss: 1.82671e+02
I1110 21:26:00.390397 140264174335808 run_lib.py:153] step: 1150, training_loss: 1.87250e+02
I1110 21:26:10.665623 140264174335808 run_lib.py:153] step: 1200, training_loss: 1.70932e+02
I1110 21:26:20.805952 140264174335808 run_lib.py:153] step: 1250, training_loss: 1.50187e+02
I1110 21:26:30.994420 140264174335808 run_lib.py:153] step: 1300, training_loss: 1.39773e+02
I1110 21:26:41.445579 140264174335808 run_lib.py:153] step: 1350, training_loss: 1.82645e+02
I1110 21:26:51.867614 140264174335808 run_lib.py:153] step: 1400, training_loss: 1.63038e+02
I1110 21:27:02.420334 140264174335808 run_lib.py:153] step: 1450, training_loss: 1.43309e+02
I1110 21:27:12.769358 140264174335808 run_lib.py:153] step: 1500, training_loss: 1.27181e+02
I1110 21:27:22.875218 140264174335808 run_lib.py:153] step: 1550, training_loss: 1.58273e+02
I1110 21:27:33.145372 140264174335808 run_lib.py:153] step: 1600, training_loss: 1.39434e+02
I1110 21:27:43.505157 140264174335808 run_lib.py:153] step: 1650, training_loss: 1.57131e+02
I1110 21:27:54.224553 140264174335808 run_lib.py:153] step: 1700, training_loss: 1.93423e+02
I1110 21:28:04.284026 140264174335808 run_lib.py:153] step: 1750, training_loss: 1.69390e+02
I1110 21:28:14.167537 140264174335808 run_lib.py:153] step: 1800, training_loss: 1.35143e+02
I1110 21:28:24.087380 140264174335808 run_lib.py:153] step: 1850, training_loss: 1.65513e+02
I1110 21:28:34.956854 140264174335808 run_lib.py:153] step: 1900, training_loss: 1.51955e+02
I1110 21:28:45.433089 140264174335808 run_lib.py:153] step: 1950, training_loss: 1.65707e+02
I1110 21:28:55.599940 140264174335808 run_lib.py:153] step: 2000, training_loss: 1.49419e+02
I1110 21:29:06.548524 140264174335808 run_lib.py:153] step: 2050, training_loss: 1.43579e+02
I1110 21:29:17.214548 140264174335808 run_lib.py:153] step: 2100, training_loss: 1.29985e+02
I1110 21:29:28.021432 140264174335808 run_lib.py:153] step: 2150, training_loss: 1.53120e+02
I1110 21:29:38.086344 140264174335808 run_lib.py:153] step: 2200, training_loss: 1.48071e+02
I1110 21:29:49.014841 140264174335808 run_lib.py:153] step: 2250, training_loss: 1.22882e+02
I1110 21:29:58.615813 140264174335808 run_lib.py:153] step: 2300, training_loss: 1.55316e+02
I1110 21:30:08.427346 140264174335808 run_lib.py:153] step: 2350, training_loss: 1.39643e+02
I1110 21:30:18.727107 140264174335808 run_lib.py:153] step: 2400, training_loss: 1.44756e+02
I1110 21:30:28.503749 140264174335808 run_lib.py:153] step: 2450, training_loss: 1.62216e+02
I1110 21:30:38.836716 140264174335808 run_lib.py:153] step: 2500, training_loss: 1.35466e+02
I1110 21:30:49.042819 140264174335808 run_lib.py:153] step: 2550, training_loss: 1.44113e+02
I1110 21:30:58.839546 140264174335808 run_lib.py:153] step: 2600, training_loss: 1.37816e+02
I1110 21:31:09.378917 140264174335808 run_lib.py:153] step: 2650, training_loss: 1.49465e+02
I1110 21:31:19.118297 140264174335808 run_lib.py:153] step: 2700, training_loss: 1.35027e+02
I1110 21:31:29.703768 140264174335808 run_lib.py:153] step: 2750, training_loss: 1.31729e+02
I1110 21:31:40.558380 140264174335808 run_lib.py:153] step: 2800, training_loss: 1.75835e+02
I1110 21:31:51.238750 140264174335808 run_lib.py:153] step: 2850, training_loss: 1.27595e+02
I1110 21:32:01.203035 140264174335808 run_lib.py:153] step: 2900, training_loss: 1.40642e+02
I1110 21:32:11.841817 140264174335808 run_lib.py:153] step: 2950, training_loss: 1.59097e+02
I1110 21:32:21.033164 140264174335808 run_lib.py:153] step: 3000, training_loss: 1.63343e+02
I1110 21:32:31.653232 140264174335808 run_lib.py:153] step: 3050, training_loss: 1.52083e+02
I1110 21:32:41.985498 140264174335808 run_lib.py:153] step: 3100, training_loss: 1.53764e+02
I1110 21:32:52.098236 140264174335808 run_lib.py:153] step: 3150, training_loss: 1.41228e+02
I1110 21:33:01.969549 140264174335808 run_lib.py:153] step: 3200, training_loss: 1.40919e+02
I1110 21:33:12.389841 140264174335808 run_lib.py:153] step: 3250, training_loss: 1.40917e+02
I1110 21:33:22.416552 140264174335808 run_lib.py:153] step: 3300, training_loss: 1.38148e+02
I1110 21:33:31.859479 140264174335808 run_lib.py:153] step: 3350, training_loss: 1.40201e+02
I1110 21:33:41.921751 140264174335808 run_lib.py:153] step: 3400, training_loss: 1.85930e+02
I1110 21:33:52.756673 140264174335808 run_lib.py:153] step: 3450, training_loss: 1.44106e+02
I1110 21:34:02.324200 140264174335808 run_lib.py:153] step: 3500, training_loss: 1.49224e+02
I1110 21:34:12.469676 140264174335808 run_lib.py:153] step: 3550, training_loss: 1.34799e+02
I1110 21:34:22.585323 140264174335808 run_lib.py:153] step: 3600, training_loss: 1.53101e+02
I1110 21:34:32.912665 140264174335808 run_lib.py:153] step: 3650, training_loss: 1.59937e+02
I1110 21:34:43.439933 140264174335808 run_lib.py:153] step: 3700, training_loss: 1.41902e+02
I1110 21:34:53.961069 140264174335808 run_lib.py:153] step: 3750, training_loss: 1.32492e+02
I1110 21:35:04.128729 140264174335808 run_lib.py:153] step: 3800, training_loss: 1.32151e+02
I1110 21:35:14.685238 140264174335808 run_lib.py:153] step: 3850, training_loss: 1.26278e+02
I1110 21:35:24.737839 140264174335808 run_lib.py:153] step: 3900, training_loss: 1.38082e+02
I1110 21:35:35.152718 140264174335808 run_lib.py:153] step: 3950, training_loss: 1.46996e+02
I1110 21:35:45.248252 140264174335808 run_lib.py:153] step: 4000, training_loss: 1.39886e+02
I1110 21:35:55.071762 140264174335808 run_lib.py:153] step: 4050, training_loss: 1.30519e+02
I1110 21:36:04.686632 140264174335808 run_lib.py:153] step: 4100, training_loss: 1.75229e+02
I1110 21:36:14.695501 140264174335808 run_lib.py:153] step: 4150, training_loss: 9.11338e+01
I1110 21:36:25.227374 140264174335808 run_lib.py:153] step: 4200, training_loss: 1.46505e+02
I1110 21:36:35.424047 140264174335808 run_lib.py:153] step: 4250, training_loss: 1.12544e+02
I1110 21:36:45.968844 140264174335808 run_lib.py:153] step: 4300, training_loss: 1.69726e+02
I1110 21:36:56.424365 140264174335808 run_lib.py:153] step: 4350, training_loss: 1.40837e+02
I1110 21:37:05.736727 140264174335808 run_lib.py:153] step: 4400, training_loss: 1.38273e+02
I1110 21:37:16.167582 140264174335808 run_lib.py:153] step: 4450, training_loss: 1.22104e+02
I1110 21:37:26.490918 140264174335808 run_lib.py:153] step: 4500, training_loss: 1.36757e+02
I1110 21:37:36.729508 140264174335808 run_lib.py:153] step: 4550, training_loss: 1.35795e+02
I1110 21:37:47.456355 140264174335808 run_lib.py:153] step: 4600, training_loss: 1.37757e+02
I1110 21:37:57.121384 140264174335808 run_lib.py:153] step: 4650, training_loss: 1.40454e+02
I1110 21:38:07.302522 140264174335808 run_lib.py:153] step: 4700, training_loss: 1.59581e+02
I1110 21:38:17.997872 140264174335808 run_lib.py:153] step: 4750, training_loss: 1.21391e+02
I1110 21:38:28.299384 140264174335808 run_lib.py:153] step: 4800, training_loss: 1.78111e+02
I1110 21:38:38.995637 140264174335808 run_lib.py:153] step: 4850, training_loss: 1.32293e+02
I1110 21:38:48.627359 140264174335808 run_lib.py:153] step: 4900, training_loss: 1.37806e+02
I1110 21:38:58.124092 140264174335808 run_lib.py:153] step: 4950, training_loss: 1.23759e+02
I1110 21:39:07.879808 140264174335808 run_lib.py:153] step: 5000, training_loss: 1.21042e+02
I1110 21:39:07.982932 140264174335808 run_lib.py:166] step: 5000, eval_loss: 1.75821e+02
I1110 21:39:17.562856 140264174335808 run_lib.py:153] step: 5050, training_loss: 1.67554e+02
I1110 21:39:28.011067 140264174335808 run_lib.py:153] step: 5100, training_loss: 1.23180e+02
I1110 21:39:38.491754 140264174335808 run_lib.py:153] step: 5150, training_loss: 1.42365e+02
I1110 21:39:48.868712 140264174335808 run_lib.py:153] step: 5200, training_loss: 1.53027e+02
I1110 21:39:58.372033 140264174335808 run_lib.py:153] step: 5250, training_loss: 1.33269e+02
I1110 21:40:08.288950 140264174335808 run_lib.py:153] step: 5300, training_loss: 1.45540e+02
I1110 21:40:17.936980 140264174335808 run_lib.py:153] step: 5350, training_loss: 1.24108e+02
I1110 21:40:27.629896 140264174335808 run_lib.py:153] step: 5400, training_loss: 1.70476e+02
I1110 21:40:37.795601 140264174335808 run_lib.py:153] step: 5450, training_loss: 1.31404e+02
I1110 21:40:47.583804 140264174335808 run_lib.py:153] step: 5500, training_loss: 1.25859e+02
I1110 21:40:57.476916 140264174335808 run_lib.py:153] step: 5550, training_loss: 1.48617e+02
I1110 21:41:07.751073 140264174335808 run_lib.py:153] step: 5600, training_loss: 8.16810e+01
I1110 21:41:17.343157 140264174335808 run_lib.py:153] step: 5650, training_loss: 1.34090e+02
I1110 21:41:27.947010 140264174335808 run_lib.py:153] step: 5700, training_loss: 1.32582e+02
I1110 21:41:38.104096 140264174335808 run_lib.py:153] step: 5750, training_loss: 1.23974e+02
I1110 21:41:48.157518 140264174335808 run_lib.py:153] step: 5800, training_loss: 1.36237e+02
I1110 21:41:58.667763 140264174335808 run_lib.py:153] step: 5850, training_loss: 1.21095e+02
I1110 21:42:08.812144 140264174335808 run_lib.py:153] step: 5900, training_loss: 1.35381e+02
I1110 21:42:19.012775 140264174335808 run_lib.py:153] step: 5950, training_loss: 1.36273e+02
I1110 21:42:29.245328 140264174335808 run_lib.py:153] step: 6000, training_loss: 1.42900e+02
I1110 21:42:39.911409 140264174335808 run_lib.py:153] step: 6050, training_loss: 1.70053e+02
I1110 21:42:49.847772 140264174335808 run_lib.py:153] step: 6100, training_loss: 1.48051e+02
I1110 21:42:59.634881 140264174335808 run_lib.py:153] step: 6150, training_loss: 1.27555e+02
I1110 21:43:09.353681 140264174335808 run_lib.py:153] step: 6200, training_loss: 1.41290e+02
I1110 21:43:19.408912 140264174335808 run_lib.py:153] step: 6250, training_loss: 1.27346e+02
I1110 21:43:29.364347 140264174335808 run_lib.py:153] step: 6300, training_loss: 1.34693e+02
I1110 21:43:39.565985 140264174335808 run_lib.py:153] step: 6350, training_loss: 1.78312e+02
I1110 21:43:50.213881 140264174335808 run_lib.py:153] step: 6400, training_loss: 1.53004e+02
I1110 21:44:00.566555 140264174335808 run_lib.py:153] step: 6450, training_loss: 1.12999e+02
I1110 21:44:10.389275 140264174335808 run_lib.py:153] step: 6500, training_loss: 1.45231e+02
I1110 21:44:20.651354 140264174335808 run_lib.py:153] step: 6550, training_loss: 1.18241e+02
I1110 21:44:31.022445 140264174335808 run_lib.py:153] step: 6600, training_loss: 1.34341e+02
I1110 21:44:41.158508 140264174335808 run_lib.py:153] step: 6650, training_loss: 1.25676e+02
I1110 21:44:50.822286 140264174335808 run_lib.py:153] step: 6700, training_loss: 1.49017e+02
I1110 21:45:00.896774 140264174335808 run_lib.py:153] step: 6750, training_loss: 1.41446e+02
I1110 21:45:11.032423 140264174335808 run_lib.py:153] step: 6800, training_loss: 1.66961e+02
I1110 21:45:20.666301 140264174335808 run_lib.py:153] step: 6850, training_loss: 1.29714e+02
I1110 21:45:31.430029 140264174335808 run_lib.py:153] step: 6900, training_loss: 1.41777e+02
I1110 21:45:41.833824 140264174335808 run_lib.py:153] step: 6950, training_loss: 1.40550e+02
I1110 21:45:52.066063 140264174335808 run_lib.py:153] step: 7000, training_loss: 1.39121e+02
I1110 21:46:01.841231 140264174335808 run_lib.py:153] step: 7050, training_loss: 1.47613e+02
I1110 21:46:12.313553 140264174335808 run_lib.py:153] step: 7100, training_loss: 1.31686e+02
I1110 21:46:22.120068 140264174335808 run_lib.py:153] step: 7150, training_loss: 1.72274e+02
I1110 21:46:31.744136 140264174335808 run_lib.py:153] step: 7200, training_loss: 1.48947e+02
I1110 21:46:41.909544 140264174335808 run_lib.py:153] step: 7250, training_loss: 1.30087e+02
I1110 21:46:52.264758 140264174335808 run_lib.py:153] step: 7300, training_loss: 1.55824e+02
I1110 21:47:02.518319 140264174335808 run_lib.py:153] step: 7350, training_loss: 1.31506e+02
I1110 21:47:11.931632 140264174335808 run_lib.py:153] step: 7400, training_loss: 1.19334e+02
I1110 21:47:21.830732 140264174335808 run_lib.py:153] step: 7450, training_loss: 1.61769e+02
I1110 21:47:32.135812 140264174335808 run_lib.py:153] step: 7500, training_loss: 1.66538e+02
I1110 21:47:42.105425 140264174335808 run_lib.py:153] step: 7550, training_loss: 1.55202e+02
I1110 21:47:51.960457 140264174335808 run_lib.py:153] step: 7600, training_loss: 1.31521e+02
I1110 21:48:01.448687 140264174335808 run_lib.py:153] step: 7650, training_loss: 1.53511e+02
I1110 21:48:11.616783 140264174335808 run_lib.py:153] step: 7700, training_loss: 1.69718e+02
I1110 21:48:22.566926 140264174335808 run_lib.py:153] step: 7750, training_loss: 1.20732e+02
I1110 21:48:32.876478 140264174335808 run_lib.py:153] step: 7800, training_loss: 1.13695e+02
I1110 21:48:42.904388 140264174335808 run_lib.py:153] step: 7850, training_loss: 1.38462e+02
I1110 21:48:53.035231 140264174335808 run_lib.py:153] step: 7900, training_loss: 1.31570e+02
I1110 21:49:02.846157 140264174335808 run_lib.py:153] step: 7950, training_loss: 1.48897e+02
I1110 21:49:12.620971 140264174335808 run_lib.py:153] step: 8000, training_loss: 1.23659e+02
I1110 21:49:23.669409 140264174335808 run_lib.py:153] step: 8050, training_loss: 1.31103e+02
I1110 21:49:33.749450 140264174335808 run_lib.py:153] step: 8100, training_loss: 1.49327e+02
I1110 21:49:44.091692 140264174335808 run_lib.py:153] step: 8150, training_loss: 1.29280e+02
I1110 21:49:54.268641 140264174335808 run_lib.py:153] step: 8200, training_loss: 1.30759e+02
I1110 21:50:04.294440 140264174335808 run_lib.py:153] step: 8250, training_loss: 1.22290e+02
I1110 21:50:14.536191 140264174335808 run_lib.py:153] step: 8300, training_loss: 1.63128e+02
I1110 21:50:24.720085 140264174335808 run_lib.py:153] step: 8350, training_loss: 1.08283e+02
I1110 21:50:34.769808 140264174335808 run_lib.py:153] step: 8400, training_loss: 1.35289e+02
I1110 21:50:44.936698 140264174335808 run_lib.py:153] step: 8450, training_loss: 1.05379e+02
I1110 21:50:55.257563 140264174335808 run_lib.py:153] step: 8500, training_loss: 1.29698e+02
I1110 21:51:05.394045 140264174335808 run_lib.py:153] step: 8550, training_loss: 1.39667e+02
I1110 21:51:15.105100 140264174335808 run_lib.py:153] step: 8600, training_loss: 1.81422e+02
I1110 21:51:25.375222 140264174335808 run_lib.py:153] step: 8650, training_loss: 1.28315e+02
I1110 21:51:35.306080 140264174335808 run_lib.py:153] step: 8700, training_loss: 1.15759e+02
I1110 21:51:45.229313 140264174335808 run_lib.py:153] step: 8750, training_loss: 1.43178e+02
I1110 21:51:55.468803 140264174335808 run_lib.py:153] step: 8800, training_loss: 1.16986e+02
I1110 21:52:05.200896 140264174335808 run_lib.py:153] step: 8850, training_loss: 1.48711e+02
I1110 21:52:15.051156 140264174335808 run_lib.py:153] step: 8900, training_loss: 1.70093e+02
I1110 21:52:24.656587 140264174335808 run_lib.py:153] step: 8950, training_loss: 1.18338e+02
I1110 21:52:34.439197 140264174335808 run_lib.py:153] step: 9000, training_loss: 1.10762e+02
I1110 21:52:44.546861 140264174335808 run_lib.py:153] step: 9050, training_loss: 1.64200e+02
I1110 21:52:54.062475 140264174335808 run_lib.py:153] step: 9100, training_loss: 1.62713e+02
I1110 21:53:03.640154 140264174335808 run_lib.py:153] step: 9150, training_loss: 1.50196e+02
I1110 21:53:13.874907 140264174335808 run_lib.py:153] step: 9200, training_loss: 1.06398e+02
I1110 21:53:24.114634 140264174335808 run_lib.py:153] step: 9250, training_loss: 1.43638e+02
I1110 21:53:33.837409 140264174335808 run_lib.py:153] step: 9300, training_loss: 1.13060e+02
I1110 21:53:44.201458 140264174335808 run_lib.py:153] step: 9350, training_loss: 1.16994e+02
I1110 21:53:53.806333 140264174335808 run_lib.py:153] step: 9400, training_loss: 1.25621e+02
I1110 21:54:04.807108 140264174335808 run_lib.py:153] step: 9450, training_loss: 1.74002e+02
I1110 21:54:14.997595 140264174335808 run_lib.py:153] step: 9500, training_loss: 1.28789e+02
I1110 21:54:24.774928 140264174335808 run_lib.py:153] step: 9550, training_loss: 1.09669e+02
I1110 21:54:34.853866 140264174335808 run_lib.py:153] step: 9600, training_loss: 1.11631e+02
I1110 21:54:44.986824 140264174335808 run_lib.py:153] step: 9650, training_loss: 1.74873e+02
I1110 21:54:55.091500 140264174335808 run_lib.py:153] step: 9700, training_loss: 9.74894e+01
I1110 21:55:05.926608 140264174335808 run_lib.py:153] step: 9750, training_loss: 1.66723e+02
I1110 21:55:15.983787 140264174335808 run_lib.py:153] step: 9800, training_loss: 1.24947e+02
I1110 21:55:25.946596 140264174335808 run_lib.py:153] step: 9850, training_loss: 1.17238e+02
I1110 21:55:35.843331 140264174335808 run_lib.py:153] step: 9900, training_loss: 1.21280e+02
I1110 21:55:45.925297 140264174335808 run_lib.py:153] step: 9950, training_loss: 1.33660e+02
I1110 21:55:56.743668 140264174335808 run_lib.py:153] step: 10000, training_loss: 1.06972e+02
I1110 21:55:57.126684 140264174335808 run_lib.py:166] step: 10000, eval_loss: 1.30254e+02
I1110 21:56:06.715064 140264174335808 run_lib.py:153] step: 10050, training_loss: 1.32287e+02
I1110 21:56:16.750083 140264174335808 run_lib.py:153] step: 10100, training_loss: 1.41186e+02
I1110 21:56:26.089218 140264174335808 run_lib.py:153] step: 10150, training_loss: 1.37624e+02
I1110 21:56:36.101480 140264174335808 run_lib.py:153] step: 10200, training_loss: 1.39749e+02
I1110 21:56:45.843233 140264174335808 run_lib.py:153] step: 10250, training_loss: 1.30813e+02
I1110 21:56:55.823002 140264174335808 run_lib.py:153] step: 10300, training_loss: 1.03752e+02
I1110 21:57:05.555642 140264174335808 run_lib.py:153] step: 10350, training_loss: 1.45464e+02
I1110 21:57:15.888362 140264174335808 run_lib.py:153] step: 10400, training_loss: 1.49665e+02
I1110 21:57:25.826180 140264174335808 run_lib.py:153] step: 10450, training_loss: 1.37259e+02
I1110 21:57:35.654328 140264174335808 run_lib.py:153] step: 10500, training_loss: 1.62911e+02
I1110 21:57:45.299886 140264174335808 run_lib.py:153] step: 10550, training_loss: 1.60382e+02
I1110 21:57:55.468120 140264174335808 run_lib.py:153] step: 10600, training_loss: 1.15886e+02
I1110 21:58:06.054231 140264174335808 run_lib.py:153] step: 10650, training_loss: 1.06076e+02
I1110 21:58:16.545895 140264174335808 run_lib.py:153] step: 10700, training_loss: 8.89416e+01
I1110 21:58:26.496631 140264174335808 run_lib.py:153] step: 10750, training_loss: 1.36526e+02
I1110 21:58:36.911060 140264174335808 run_lib.py:153] step: 10800, training_loss: 1.41660e+02
I1110 21:58:46.761366 140264174335808 run_lib.py:153] step: 10850, training_loss: 1.34892e+02
I1110 21:58:57.094977 140264174335808 run_lib.py:153] step: 10900, training_loss: 1.45450e+02
I1110 21:59:07.115016 140264174335808 run_lib.py:153] step: 10950, training_loss: 1.68807e+02
I1110 21:59:17.315575 140264174335808 run_lib.py:153] step: 11000, training_loss: 9.97463e+01
I1110 21:59:27.528436 140264174335808 run_lib.py:153] step: 11050, training_loss: 1.25787e+02
I1110 21:59:37.863943 140264174335808 run_lib.py:153] step: 11100, training_loss: 1.23255e+02
I1110 21:59:47.996072 140264174335808 run_lib.py:153] step: 11150, training_loss: 1.68646e+02
I1110 21:59:58.604688 140264174335808 run_lib.py:153] step: 11200, training_loss: 1.25388e+02
I1110 22:00:08.914629 140264174335808 run_lib.py:153] step: 11250, training_loss: 1.12672e+02
I1110 22:00:19.304599 140264174335808 run_lib.py:153] step: 11300, training_loss: 1.33118e+02
I1110 22:00:29.214172 140264174335808 run_lib.py:153] step: 11350, training_loss: 1.24547e+02
I1110 22:00:39.137274 140264174335808 run_lib.py:153] step: 11400, training_loss: 1.12323e+02
I1110 22:00:49.288250 140264174335808 run_lib.py:153] step: 11450, training_loss: 1.49316e+02
I1110 22:00:59.516795 140264174335808 run_lib.py:153] step: 11500, training_loss: 1.41320e+02
I1110 22:01:09.063299 140264174335808 run_lib.py:153] step: 11550, training_loss: 1.07412e+02
I1110 22:01:19.222436 140264174335808 run_lib.py:153] step: 11600, training_loss: 1.24057e+02
I1110 22:01:29.383344 140264174335808 run_lib.py:153] step: 11650, training_loss: 1.34039e+02
I1110 22:01:39.123807 140264174335808 run_lib.py:153] step: 11700, training_loss: 1.53178e+02
I1110 22:01:48.450765 140264174335808 run_lib.py:153] step: 11750, training_loss: 1.41818e+02
I1110 22:01:58.873073 140264174335808 run_lib.py:153] step: 11800, training_loss: 1.41150e+02
I1110 22:02:08.921682 140264174335808 run_lib.py:153] step: 11850, training_loss: 1.45304e+02
I1110 22:02:18.683122 140264174335808 run_lib.py:153] step: 11900, training_loss: 1.26714e+02
I1110 22:02:28.891219 140264174335808 run_lib.py:153] step: 11950, training_loss: 1.19897e+02
I1110 22:02:39.274849 140264174335808 run_lib.py:153] step: 12000, training_loss: 1.57532e+02
I1110 22:02:49.986485 140264174335808 run_lib.py:153] step: 12050, training_loss: 1.43115e+02
I1110 22:03:00.083997 140264174335808 run_lib.py:153] step: 12100, training_loss: 1.53602e+02
I1110 22:03:10.682050 140264174335808 run_lib.py:153] step: 12150, training_loss: 1.35746e+02
I1110 22:03:20.040880 140264174335808 run_lib.py:153] step: 12200, training_loss: 1.28281e+02
I1110 22:03:29.389753 140264174335808 run_lib.py:153] step: 12250, training_loss: 1.37679e+02
I1110 22:03:38.902232 140264174335808 run_lib.py:153] step: 12300, training_loss: 1.59797e+02
I1110 22:03:49.549690 140264174335808 run_lib.py:153] step: 12350, training_loss: 1.14188e+02
I1110 22:03:59.716300 140264174335808 run_lib.py:153] step: 12400, training_loss: 1.35319e+02
I1110 22:04:09.708014 140264174335808 run_lib.py:153] step: 12450, training_loss: 1.14855e+02
I1110 22:04:19.910428 140264174335808 run_lib.py:153] step: 12500, training_loss: 1.38081e+02
I1110 22:04:30.383076 140264174335808 run_lib.py:153] step: 12550, training_loss: 1.36109e+02
I1110 22:04:40.483612 140264174335808 run_lib.py:153] step: 12600, training_loss: 1.31366e+02
I1110 22:04:50.342080 140264174335808 run_lib.py:153] step: 12650, training_loss: 1.21472e+02
I1110 22:05:00.410210 140264174335808 run_lib.py:153] step: 12700, training_loss: 1.41206e+02
I1110 22:05:10.244613 140264174335808 run_lib.py:153] step: 12750, training_loss: 1.47623e+02
I1110 22:05:19.823016 140264174335808 run_lib.py:153] step: 12800, training_loss: 1.14828e+02
I1110 22:05:29.632145 140264174335808 run_lib.py:153] step: 12850, training_loss: 1.57558e+02
I1110 22:05:39.874480 140264174335808 run_lib.py:153] step: 12900, training_loss: 1.64667e+02
I1110 22:05:50.142425 140264174335808 run_lib.py:153] step: 12950, training_loss: 1.52007e+02
I1110 22:06:00.085925 140264174335808 run_lib.py:153] step: 13000, training_loss: 1.52718e+02
I1110 22:06:09.888482 140264174335808 run_lib.py:153] step: 13050, training_loss: 1.63061e+02
I1110 22:06:20.407872 140264174335808 run_lib.py:153] step: 13100, training_loss: 1.08389e+02
I1110 22:06:31.459136 140264174335808 run_lib.py:153] step: 13150, training_loss: 1.45202e+02
I1110 22:06:42.070030 140264174335808 run_lib.py:153] step: 13200, training_loss: 1.15736e+02
I1110 22:06:52.493260 140264174335808 run_lib.py:153] step: 13250, training_loss: 1.08664e+02
I1110 22:07:03.164422 140264174335808 run_lib.py:153] step: 13300, training_loss: 1.21749e+02
I1110 22:07:13.419068 140264174335808 run_lib.py:153] step: 13350, training_loss: 1.32141e+02
I1110 22:07:24.052156 140264174335808 run_lib.py:153] step: 13400, training_loss: 1.46612e+02
I1110 22:07:34.974946 140264174335808 run_lib.py:153] step: 13450, training_loss: 1.21441e+02
I1110 22:07:46.387410 140264174335808 run_lib.py:153] step: 13500, training_loss: 1.29317e+02
I1110 22:07:57.031992 140264174335808 run_lib.py:153] step: 13550, training_loss: 1.70703e+02
I1110 22:08:07.364460 140264174335808 run_lib.py:153] step: 13600, training_loss: 1.73966e+02
I1110 22:08:18.005374 140264174335808 run_lib.py:153] step: 13650, training_loss: 1.30024e+02
I1110 22:08:28.069735 140264174335808 run_lib.py:153] step: 13700, training_loss: 1.38152e+02
I1110 22:08:38.420208 140264174335808 run_lib.py:153] step: 13750, training_loss: 1.20286e+02
I1110 22:08:48.177087 140264174335808 run_lib.py:153] step: 13800, training_loss: 1.57615e+02
I1110 22:08:57.501210 140264174335808 run_lib.py:153] step: 13850, training_loss: 1.34947e+02
I1110 22:09:07.691734 140264174335808 run_lib.py:153] step: 13900, training_loss: 1.08666e+02
I1110 22:09:17.662733 140264174335808 run_lib.py:153] step: 13950, training_loss: 1.58077e+02
I1110 22:09:28.089878 140264174335808 run_lib.py:153] step: 14000, training_loss: 1.38120e+02
I1110 22:09:38.458461 140264174335808 run_lib.py:153] step: 14050, training_loss: 1.33008e+02
I1110 22:09:48.071127 140264174335808 run_lib.py:153] step: 14100, training_loss: 1.24564e+02
I1110 22:09:58.026972 140264174335808 run_lib.py:153] step: 14150, training_loss: 1.41416e+02
I1110 22:10:08.089992 140264174335808 run_lib.py:153] step: 14200, training_loss: 1.48539e+02
I1110 22:10:18.084269 140264174335808 run_lib.py:153] step: 14250, training_loss: 1.21746e+02
I1110 22:10:27.764664 140264174335808 run_lib.py:153] step: 14300, training_loss: 1.27339e+02
I1110 22:10:38.785075 140264174335808 run_lib.py:153] step: 14350, training_loss: 1.45817e+02
I1110 22:10:48.874587 140264174335808 run_lib.py:153] step: 14400, training_loss: 1.42078e+02
I1110 22:10:58.863825 140264174335808 run_lib.py:153] step: 14450, training_loss: 1.32500e+02
I1110 22:11:08.476203 140264174335808 run_lib.py:153] step: 14500, training_loss: 1.37569e+02
I1110 22:11:18.298193 140264174335808 run_lib.py:153] step: 14550, training_loss: 1.44069e+02
I1110 22:11:27.969251 140264174335808 run_lib.py:153] step: 14600, training_loss: 1.41962e+02
I1110 22:11:37.526365 140264174335808 run_lib.py:153] step: 14650, training_loss: 1.42373e+02
I1110 22:11:47.974102 140264174335808 run_lib.py:153] step: 14700, training_loss: 1.29106e+02
I1110 22:11:57.679699 140264174335808 run_lib.py:153] step: 14750, training_loss: 1.49084e+02
I1110 22:12:07.848136 140264174335808 run_lib.py:153] step: 14800, training_loss: 1.30995e+02
I1110 22:12:17.819353 140264174335808 run_lib.py:153] step: 14850, training_loss: 1.38293e+02
I1110 22:12:27.641649 140264174335808 run_lib.py:153] step: 14900, training_loss: 1.34936e+02
I1110 22:12:37.356708 140264174335808 run_lib.py:153] step: 14950, training_loss: 1.40977e+02
I1110 22:12:47.824276 140264174335808 run_lib.py:153] step: 15000, training_loss: 1.42972e+02
I1110 22:12:47.964960 140264174335808 run_lib.py:166] step: 15000, eval_loss: 1.31923e+02
I1110 22:12:57.933742 140264174335808 run_lib.py:153] step: 15050, training_loss: 1.15349e+02
I1110 22:13:08.004167 140264174335808 run_lib.py:153] step: 15100, training_loss: 1.45539e+02
I1110 22:13:18.390613 140264174335808 run_lib.py:153] step: 15150, training_loss: 1.60240e+02
I1110 22:13:28.539663 140264174335808 run_lib.py:153] step: 15200, training_loss: 1.41759e+02
I1110 22:13:37.989391 140264174335808 run_lib.py:153] step: 15250, training_loss: 1.23827e+02
I1110 22:13:48.422085 140264174335808 run_lib.py:153] step: 15300, training_loss: 1.32456e+02
I1110 22:13:58.868924 140264174335808 run_lib.py:153] step: 15350, training_loss: 1.20251e+02
I1110 22:14:08.902656 140264174335808 run_lib.py:153] step: 15400, training_loss: 1.25664e+02
I1110 22:14:18.778218 140264174335808 run_lib.py:153] step: 15450, training_loss: 1.22476e+02
I1110 22:14:28.625253 140264174335808 run_lib.py:153] step: 15500, training_loss: 1.28939e+02
I1110 22:14:38.108004 140264174335808 run_lib.py:153] step: 15550, training_loss: 1.61490e+02
I1110 22:14:47.871118 140264174335808 run_lib.py:153] step: 15600, training_loss: 1.30523e+02
I1110 22:14:57.233783 140264174335808 run_lib.py:153] step: 15650, training_loss: 1.69521e+02
I1110 22:15:07.158227 140264174335808 run_lib.py:153] step: 15700, training_loss: 1.55947e+02
I1110 22:15:17.742090 140264174335808 run_lib.py:153] step: 15750, training_loss: 1.51677e+02
I1110 22:15:27.743644 140264174335808 run_lib.py:153] step: 15800, training_loss: 1.18996e+02
I1110 22:15:37.637446 140264174335808 run_lib.py:153] step: 15850, training_loss: 1.25717e+02
I1110 22:15:47.980325 140264174335808 run_lib.py:153] step: 15900, training_loss: 1.44418e+02
I1110 22:15:57.920540 140264174335808 run_lib.py:153] step: 15950, training_loss: 1.38359e+02
I1110 22:16:07.955028 140264174335808 run_lib.py:153] step: 16000, training_loss: 1.57262e+02
I1110 22:16:17.496182 140264174335808 run_lib.py:153] step: 16050, training_loss: 1.61957e+02
I1110 22:16:27.122046 140264174335808 run_lib.py:153] step: 16100, training_loss: 1.34235e+02
I1110 22:16:36.780964 140264174335808 run_lib.py:153] step: 16150, training_loss: 1.21240e+02
I1110 22:16:46.816719 140264174335808 run_lib.py:153] step: 16200, training_loss: 1.58203e+02
I1110 22:16:57.382450 140264174335808 run_lib.py:153] step: 16250, training_loss: 1.41395e+02
I1110 22:17:07.532613 140264174335808 run_lib.py:153] step: 16300, training_loss: 1.36983e+02
I1110 22:17:17.293443 140264174335808 run_lib.py:153] step: 16350, training_loss: 1.58265e+02
I1110 22:17:27.013233 140264174335808 run_lib.py:153] step: 16400, training_loss: 1.36204e+02
I1110 22:17:36.702693 140264174335808 run_lib.py:153] step: 16450, training_loss: 1.51458e+02
I1110 22:17:46.833202 140264174335808 run_lib.py:153] step: 16500, training_loss: 1.47567e+02
I1110 22:17:57.111600 140264174335808 run_lib.py:153] step: 16550, training_loss: 1.03322e+02
I1110 22:18:07.632840 140264174335808 run_lib.py:153] step: 16600, training_loss: 1.39480e+02
I1110 22:18:18.053037 140264174335808 run_lib.py:153] step: 16650, training_loss: 1.21080e+02
I1110 22:18:28.472092 140264174335808 run_lib.py:153] step: 16700, training_loss: 1.01065e+02
I1110 22:18:38.613288 140264174335808 run_lib.py:153] step: 16750, training_loss: 1.48089e+02
I1110 22:18:48.705641 140264174335808 run_lib.py:153] step: 16800, training_loss: 1.32405e+02
I1110 22:18:59.254670 140264174335808 run_lib.py:153] step: 16850, training_loss: 1.20892e+02
I1110 22:19:09.433244 140264174335808 run_lib.py:153] step: 16900, training_loss: 1.61173e+02
I1110 22:19:19.257519 140264174335808 run_lib.py:153] step: 16950, training_loss: 1.15641e+02
I1110 22:19:29.301255 140264174335808 run_lib.py:153] step: 17000, training_loss: 1.56650e+02
I1110 22:19:39.620187 140264174335808 run_lib.py:153] step: 17050, training_loss: 1.27386e+02
I1110 22:19:49.569458 140264174335808 run_lib.py:153] step: 17100, training_loss: 1.54623e+02
I1110 22:19:59.063224 140264174335808 run_lib.py:153] step: 17150, training_loss: 1.36359e+02
I1110 22:20:09.269402 140264174335808 run_lib.py:153] step: 17200, training_loss: 1.52536e+02
I1110 22:20:18.998647 140264174335808 run_lib.py:153] step: 17250, training_loss: 1.43353e+02
I1110 22:20:28.770709 140264174335808 run_lib.py:153] step: 17300, training_loss: 1.33452e+02
I1110 22:20:38.289195 140264174335808 run_lib.py:153] step: 17350, training_loss: 1.10824e+02
I1110 22:20:47.985965 140264174335808 run_lib.py:153] step: 17400, training_loss: 1.28770e+02
I1110 22:20:58.217809 140264174335808 run_lib.py:153] step: 17450, training_loss: 1.22127e+02
I1110 22:21:08.510668 140264174335808 run_lib.py:153] step: 17500, training_loss: 1.39416e+02
I1110 22:21:18.505520 140264174335808 run_lib.py:153] step: 17550, training_loss: 9.57627e+01
I1110 22:21:29.199725 140264174335808 run_lib.py:153] step: 17600, training_loss: 1.49766e+02
I1110 22:21:39.249622 140264174335808 run_lib.py:153] step: 17650, training_loss: 1.48346e+02
I1110 22:21:49.298045 140264174335808 run_lib.py:153] step: 17700, training_loss: 1.25843e+02
I1110 22:21:58.991889 140264174335808 run_lib.py:153] step: 17750, training_loss: 1.45445e+02
I1110 22:22:09.024197 140264174335808 run_lib.py:153] step: 17800, training_loss: 1.42689e+02
I1110 22:22:19.380371 140264174335808 run_lib.py:153] step: 17850, training_loss: 1.15796e+02
I1110 22:22:29.450026 140264174335808 run_lib.py:153] step: 17900, training_loss: 1.46717e+02
I1110 22:22:39.791306 140264174335808 run_lib.py:153] step: 17950, training_loss: 1.31456e+02
I1110 22:22:49.350807 140264174335808 run_lib.py:153] step: 18000, training_loss: 1.27276e+02
I1110 22:22:59.464291 140264174335808 run_lib.py:153] step: 18050, training_loss: 1.44287e+02
I1110 22:23:09.085072 140264174335808 run_lib.py:153] step: 18100, training_loss: 1.76212e+02
I1110 22:23:19.577330 140264174335808 run_lib.py:153] step: 18150, training_loss: 1.56031e+02
I1110 22:23:29.452202 140264174335808 run_lib.py:153] step: 18200, training_loss: 1.19848e+02
I1110 22:23:39.670089 140264174335808 run_lib.py:153] step: 18250, training_loss: 1.24489e+02
I1110 22:23:49.065541 140264174335808 run_lib.py:153] step: 18300, training_loss: 1.58093e+02
I1110 22:23:59.378203 140264174335808 run_lib.py:153] step: 18350, training_loss: 1.54084e+02
I1110 22:24:09.403384 140264174335808 run_lib.py:153] step: 18400, training_loss: 1.25641e+02
I1110 22:24:19.043173 140264174335808 run_lib.py:153] step: 18450, training_loss: 1.09136e+02
I1110 22:24:29.106506 140264174335808 run_lib.py:153] step: 18500, training_loss: 1.43015e+02
I1110 22:24:38.853517 140264174335808 run_lib.py:153] step: 18550, training_loss: 1.27448e+02
I1110 22:24:49.817168 140264174335808 run_lib.py:153] step: 18600, training_loss: 1.19020e+02
I1110 22:24:59.961966 140264174335808 run_lib.py:153] step: 18650, training_loss: 1.38994e+02
I1110 22:25:09.847635 140264174335808 run_lib.py:153] step: 18700, training_loss: 1.35206e+02
I1110 22:25:19.683746 140264174335808 run_lib.py:153] step: 18750, training_loss: 1.36626e+02
I1110 22:25:29.896954 140264174335808 run_lib.py:153] step: 18800, training_loss: 1.43369e+02
I1110 22:25:39.438901 140264174335808 run_lib.py:153] step: 18850, training_loss: 1.04570e+02
I1110 22:25:48.861913 140264174335808 run_lib.py:153] step: 18900, training_loss: 1.41599e+02
I1110 22:25:59.231891 140264174335808 run_lib.py:153] step: 18950, training_loss: 1.40761e+02
I1110 22:26:08.800860 140264174335808 run_lib.py:153] step: 19000, training_loss: 1.13378e+02
I1110 22:26:18.702261 140264174335808 run_lib.py:153] step: 19050, training_loss: 1.29527e+02
I1110 22:26:28.727208 140264174335808 run_lib.py:153] step: 19100, training_loss: 1.47746e+02
I1110 22:26:38.854092 140264174335808 run_lib.py:153] step: 19150, training_loss: 1.39635e+02
I1110 22:26:48.753884 140264174335808 run_lib.py:153] step: 19200, training_loss: 1.39919e+02
I1110 22:26:58.645470 140264174335808 run_lib.py:153] step: 19250, training_loss: 1.27436e+02
I1110 22:27:08.564118 140264174335808 run_lib.py:153] step: 19300, training_loss: 1.20648e+02
I1110 22:27:18.923895 140264174335808 run_lib.py:153] step: 19350, training_loss: 1.27309e+02
I1110 22:27:29.031835 140264174335808 run_lib.py:153] step: 19400, training_loss: 1.17140e+02
I1110 22:27:38.518488 140264174335808 run_lib.py:153] step: 19450, training_loss: 1.04306e+02
I1110 22:27:49.019535 140264174335808 run_lib.py:153] step: 19500, training_loss: 1.37752e+02
I1110 22:27:59.364213 140264174335808 run_lib.py:153] step: 19550, training_loss: 1.34291e+02
I1110 22:28:09.251903 140264174335808 run_lib.py:153] step: 19600, training_loss: 1.09075e+02
I1110 22:28:19.369254 140264174335808 run_lib.py:153] step: 19650, training_loss: 1.30918e+02
I1110 22:28:29.113083 140264174335808 run_lib.py:153] step: 19700, training_loss: 1.07420e+02
I1110 22:28:38.985785 140264174335808 run_lib.py:153] step: 19750, training_loss: 1.09651e+02
I1110 22:28:49.526811 140264174335808 run_lib.py:153] step: 19800, training_loss: 1.39320e+02
I1110 22:28:59.959867 140264174335808 run_lib.py:153] step: 19850, training_loss: 1.31474e+02
I1110 22:29:10.299010 140264174335808 run_lib.py:153] step: 19900, training_loss: 1.48412e+02
I1110 22:29:20.511024 140264174335808 run_lib.py:153] step: 19950, training_loss: 1.46397e+02
I1110 22:29:30.140097 140264174335808 run_lib.py:153] step: 20000, training_loss: 1.20988e+02
I1110 22:29:30.669536 140264174335808 run_lib.py:166] step: 20000, eval_loss: 1.53149e+02
I1110 22:29:40.627246 140264174335808 run_lib.py:153] step: 20050, training_loss: 1.35599e+02
I1110 22:29:50.759059 140264174335808 run_lib.py:153] step: 20100, training_loss: 1.60725e+02
I1110 22:30:01.006374 140264174335808 run_lib.py:153] step: 20150, training_loss: 1.17238e+02
I1110 22:30:11.064074 140264174335808 run_lib.py:153] step: 20200, training_loss: 1.41183e+02
I1110 22:30:21.339415 140264174335808 run_lib.py:153] step: 20250, training_loss: 1.03520e+02
I1110 22:30:31.997468 140264174335808 run_lib.py:153] step: 20300, training_loss: 1.29510e+02
I1110 22:30:42.222574 140264174335808 run_lib.py:153] step: 20350, training_loss: 1.32565e+02
I1110 22:30:52.243731 140264174335808 run_lib.py:153] step: 20400, training_loss: 1.16356e+02
I1110 22:31:03.132274 140264174335808 run_lib.py:153] step: 20450, training_loss: 1.16817e+02
I1110 22:31:13.975836 140264174335808 run_lib.py:153] step: 20500, training_loss: 1.38428e+02
I1110 22:31:24.316899 140264174335808 run_lib.py:153] step: 20550, training_loss: 1.27383e+02
I1110 22:31:35.468334 140264174335808 run_lib.py:153] step: 20600, training_loss: 1.24345e+02
I1110 22:31:45.695924 140264174335808 run_lib.py:153] step: 20650, training_loss: 1.35609e+02
I1110 22:31:56.432062 140264174335808 run_lib.py:153] step: 20700, training_loss: 1.31993e+02
I1110 22:32:07.345201 140264174335808 run_lib.py:153] step: 20750, training_loss: 1.44373e+02
I1110 22:32:17.631982 140264174335808 run_lib.py:153] step: 20800, training_loss: 1.14364e+02
I1110 22:32:28.000743 140264174335808 run_lib.py:153] step: 20850, training_loss: 1.47838e+02
I1110 22:32:38.628135 140264174335808 run_lib.py:153] step: 20900, training_loss: 1.42891e+02
I1110 22:32:48.389320 140264174335808 run_lib.py:153] step: 20950, training_loss: 1.44376e+02
I1110 22:32:57.936084 140264174335808 run_lib.py:153] step: 21000, training_loss: 9.59252e+01
I1110 22:33:08.095315 140264174335808 run_lib.py:153] step: 21050, training_loss: 1.28744e+02
I1110 22:33:18.133777 140264174335808 run_lib.py:153] step: 21100, training_loss: 1.35830e+02
I1110 22:33:28.043420 140264174335808 run_lib.py:153] step: 21150, training_loss: 1.09694e+02
I1110 22:33:38.331689 140264174335808 run_lib.py:153] step: 21200, training_loss: 1.09035e+02
I1110 22:33:48.363961 140264174335808 run_lib.py:153] step: 21250, training_loss: 1.18669e+02
I1110 22:33:58.447960 140264174335808 run_lib.py:153] step: 21300, training_loss: 1.11219e+02
I1110 22:34:08.859300 140264174335808 run_lib.py:153] step: 21350, training_loss: 1.46216e+02
I1110 22:34:19.245898 140264174335808 run_lib.py:153] step: 21400, training_loss: 1.00441e+02
I1110 22:34:29.007330 140264174335808 run_lib.py:153] step: 21450, training_loss: 1.35962e+02
I1110 22:34:39.113389 140264174335808 run_lib.py:153] step: 21500, training_loss: 1.07240e+02
I1110 22:34:48.837738 140264174335808 run_lib.py:153] step: 21550, training_loss: 1.39618e+02
I1110 22:34:58.195927 140264174335808 run_lib.py:153] step: 21600, training_loss: 1.55990e+02
I1110 22:35:08.066244 140264174335808 run_lib.py:153] step: 21650, training_loss: 1.49823e+02
I1110 22:35:18.527413 140264174335808 run_lib.py:153] step: 21700, training_loss: 1.34890e+02
I1110 22:35:28.758949 140264174335808 run_lib.py:153] step: 21750, training_loss: 1.36238e+02
I1110 22:35:39.113818 140264174335808 run_lib.py:153] step: 21800, training_loss: 1.18044e+02
I1110 22:35:48.625065 140264174335808 run_lib.py:153] step: 21850, training_loss: 1.37157e+02
I1110 22:35:59.386281 140264174335808 run_lib.py:153] step: 21900, training_loss: 1.39711e+02
I1110 22:36:08.992253 140264174335808 run_lib.py:153] step: 21950, training_loss: 1.10764e+02
I1110 22:36:18.901816 140264174335808 run_lib.py:153] step: 22000, training_loss: 1.27990e+02
I1110 22:36:29.390959 140264174335808 run_lib.py:153] step: 22050, training_loss: 1.40409e+02
I1110 22:36:39.257932 140264174335808 run_lib.py:153] step: 22100, training_loss: 1.02848e+02
I1110 22:36:49.180006 140264174335808 run_lib.py:153] step: 22150, training_loss: 1.31408e+02
I1110 22:36:59.331931 140264174335808 run_lib.py:153] step: 22200, training_loss: 1.22689e+02
I1110 22:37:09.853074 140264174335808 run_lib.py:153] step: 22250, training_loss: 1.30767e+02
I1110 22:37:20.398710 140264174335808 run_lib.py:153] step: 22300, training_loss: 1.53435e+02
I1110 22:37:30.427100 140264174335808 run_lib.py:153] step: 22350, training_loss: 1.29328e+02
I1110 22:37:41.074343 140264174335808 run_lib.py:153] step: 22400, training_loss: 1.36966e+02
I1110 22:37:51.295334 140264174335808 run_lib.py:153] step: 22450, training_loss: 1.20413e+02
I1110 22:38:01.720868 140264174335808 run_lib.py:153] step: 22500, training_loss: 1.05348e+02
I1110 22:38:12.445374 140264174335808 run_lib.py:153] step: 22550, training_loss: 1.30289e+02
I1110 22:38:22.829680 140264174335808 run_lib.py:153] step: 22600, training_loss: 1.26668e+02
I1110 22:38:33.138474 140264174335808 run_lib.py:153] step: 22650, training_loss: 1.60327e+02
I1110 22:38:42.768519 140264174335808 run_lib.py:153] step: 22700, training_loss: 1.24754e+02
I1110 22:38:52.257591 140264174335808 run_lib.py:153] step: 22750, training_loss: 1.29097e+02
I1110 22:39:02.955217 140264174335808 run_lib.py:153] step: 22800, training_loss: 1.10691e+02
I1110 22:39:12.714106 140264174335808 run_lib.py:153] step: 22850, training_loss: 1.31434e+02
I1110 22:39:22.377290 140264174335808 run_lib.py:153] step: 22900, training_loss: 1.24922e+02
I1110 22:39:31.989421 140264174335808 run_lib.py:153] step: 22950, training_loss: 1.41910e+02
I1110 22:39:41.857243 140264174335808 run_lib.py:153] step: 23000, training_loss: 1.32927e+02
I1110 22:39:51.904463 140264174335808 run_lib.py:153] step: 23050, training_loss: 1.46145e+02
I1110 22:40:01.922702 140264174335808 run_lib.py:153] step: 23100, training_loss: 1.24976e+02
I1110 22:40:12.074264 140264174335808 run_lib.py:153] step: 23150, training_loss: 1.23589e+02
I1110 22:40:21.750343 140264174335808 run_lib.py:153] step: 23200, training_loss: 1.33978e+02
I1110 22:40:31.611330 140264174335808 run_lib.py:153] step: 23250, training_loss: 1.47061e+02
I1110 22:40:41.847204 140264174335808 run_lib.py:153] step: 23300, training_loss: 1.55111e+02
I1110 22:40:52.245845 140264174335808 run_lib.py:153] step: 23350, training_loss: 1.36407e+02
I1110 22:41:02.278867 140264174335808 run_lib.py:153] step: 23400, training_loss: 1.17189e+02
I1110 22:41:12.090053 140264174335808 run_lib.py:153] step: 23450, training_loss: 9.61504e+01
I1110 22:41:22.700798 140264174335808 run_lib.py:153] step: 23500, training_loss: 1.36692e+02
I1110 22:41:32.829369 140264174335808 run_lib.py:153] step: 23550, training_loss: 1.48534e+02
I1110 22:41:43.299715 140264174335808 run_lib.py:153] step: 23600, training_loss: 1.27100e+02
I1110 22:41:53.751309 140264174335808 run_lib.py:153] step: 23650, training_loss: 1.36356e+02
I1110 22:42:04.403201 140264174335808 run_lib.py:153] step: 23700, training_loss: 1.34965e+02
I1110 22:42:13.883478 140264174335808 run_lib.py:153] step: 23750, training_loss: 1.43469e+02
I1110 22:42:24.030092 140264174335808 run_lib.py:153] step: 23800, training_loss: 1.31551e+02
I1110 22:42:33.928424 140264174335808 run_lib.py:153] step: 23850, training_loss: 1.55811e+02
I1110 22:42:44.675070 140264174335808 run_lib.py:153] step: 23900, training_loss: 1.18664e+02
I1110 22:42:54.507003 140264174335808 run_lib.py:153] step: 23950, training_loss: 1.26137e+02
I1110 22:43:05.281067 140264174335808 run_lib.py:153] step: 24000, training_loss: 1.36660e+02
I1110 22:43:15.460934 140264174335808 run_lib.py:153] step: 24050, training_loss: 1.52629e+02
I1110 22:43:26.040199 140264174335808 run_lib.py:153] step: 24100, training_loss: 1.18111e+02
I1110 22:43:36.204464 140264174335808 run_lib.py:153] step: 24150, training_loss: 1.17932e+02
I1110 22:43:46.121091 140264174335808 run_lib.py:153] step: 24200, training_loss: 1.18904e+02
I1110 22:43:56.329165 140264174335808 run_lib.py:153] step: 24250, training_loss: 1.40881e+02
I1110 22:44:06.558107 140264174335808 run_lib.py:153] step: 24300, training_loss: 1.58330e+02
I1110 22:44:16.464371 140264174335808 run_lib.py:153] step: 24350, training_loss: 1.32732e+02
I1110 22:44:26.018221 140264174335808 run_lib.py:153] step: 24400, training_loss: 1.20842e+02
I1110 22:44:36.279389 140264174335808 run_lib.py:153] step: 24450, training_loss: 1.60204e+02
I1110 22:44:46.839683 140264174335808 run_lib.py:153] step: 24500, training_loss: 1.32915e+02
I1110 22:44:56.477274 140264174335808 run_lib.py:153] step: 24550, training_loss: 1.18588e+02
I1110 22:45:06.135461 140264174335808 run_lib.py:153] step: 24600, training_loss: 1.48718e+02
I1110 22:45:16.042546 140264174335808 run_lib.py:153] step: 24650, training_loss: 1.45648e+02
I1110 22:45:26.140445 140264174335808 run_lib.py:153] step: 24700, training_loss: 1.17701e+02
I1110 22:45:36.064889 140264174335808 run_lib.py:153] step: 24750, training_loss: 1.08531e+02
I1110 22:45:46.214737 140264174335808 run_lib.py:153] step: 24800, training_loss: 1.06881e+02
I1110 22:45:56.017719 140264174335808 run_lib.py:153] step: 24850, training_loss: 1.23937e+02
I1110 22:46:06.207971 140264174335808 run_lib.py:153] step: 24900, training_loss: 1.10370e+02
I1110 22:46:16.265103 140264174335808 run_lib.py:153] step: 24950, training_loss: 1.43417e+02
I1110 22:46:26.225291 140264174335808 run_lib.py:153] step: 25000, training_loss: 1.38901e+02
I1110 22:46:26.363585 140264174335808 run_lib.py:166] step: 25000, eval_loss: 1.27347e+02
I1110 22:46:35.981725 140264174335808 run_lib.py:153] step: 25050, training_loss: 1.51865e+02
I1110 22:46:45.988598 140264174335808 run_lib.py:153] step: 25100, training_loss: 1.20198e+02
I1110 22:46:55.729724 140264174335808 run_lib.py:153] step: 25150, training_loss: 1.41921e+02
I1110 22:47:05.903251 140264174335808 run_lib.py:153] step: 25200, training_loss: 1.66416e+02
I1110 22:47:15.498753 140264174335808 run_lib.py:153] step: 25250, training_loss: 1.50396e+02
I1110 22:47:25.767542 140264174335808 run_lib.py:153] step: 25300, training_loss: 1.15856e+02
I1110 22:47:36.246206 140264174335808 run_lib.py:153] step: 25350, training_loss: 1.44289e+02
I1110 22:47:46.094030 140264174335808 run_lib.py:153] step: 25400, training_loss: 1.15565e+02
I1110 22:47:55.651075 140264174335808 run_lib.py:153] step: 25450, training_loss: 1.28252e+02
I1110 22:48:05.831389 140264174335808 run_lib.py:153] step: 25500, training_loss: 1.63590e+02
I1110 22:48:15.755234 140264174335808 run_lib.py:153] step: 25550, training_loss: 1.44500e+02
I1110 22:48:26.348093 140264174335808 run_lib.py:153] step: 25600, training_loss: 1.28963e+02
I1110 22:48:36.557332 140264174335808 run_lib.py:153] step: 25650, training_loss: 1.45061e+02
I1110 22:48:46.582996 140264174335808 run_lib.py:153] step: 25700, training_loss: 1.05019e+02
I1110 22:48:56.300230 140264174335808 run_lib.py:153] step: 25750, training_loss: 1.52051e+02
I1110 22:49:05.815728 140264174335808 run_lib.py:153] step: 25800, training_loss: 1.28219e+02
I1110 22:49:15.689338 140264174335808 run_lib.py:153] step: 25850, training_loss: 1.30966e+02
I1110 22:49:25.780218 140264174335808 run_lib.py:153] step: 25900, training_loss: 1.25262e+02
I1110 22:49:36.101187 140264174335808 run_lib.py:153] step: 25950, training_loss: 1.73294e+02
I1110 22:49:46.445597 140264174335808 run_lib.py:153] step: 26000, training_loss: 1.71595e+02
I1110 22:49:56.913509 140264174335808 run_lib.py:153] step: 26050, training_loss: 1.12444e+02
I1110 22:50:06.779490 140264174335808 run_lib.py:153] step: 26100, training_loss: 1.30377e+02
I1110 22:50:17.140907 140264174335808 run_lib.py:153] step: 26150, training_loss: 1.21255e+02
I1110 22:50:27.898986 140264174335808 run_lib.py:153] step: 26200, training_loss: 1.27644e+02
I1110 22:50:38.421298 140264174335808 run_lib.py:153] step: 26250, training_loss: 1.43378e+02
I1110 22:50:48.268326 140264174335808 run_lib.py:153] step: 26300, training_loss: 1.30233e+02
I1110 22:50:57.741852 140264174335808 run_lib.py:153] step: 26350, training_loss: 1.19772e+02
I1110 22:51:07.580152 140264174335808 run_lib.py:153] step: 26400, training_loss: 1.32729e+02
I1110 22:51:17.708245 140264174335808 run_lib.py:153] step: 26450, training_loss: 1.24094e+02
I1110 22:51:28.208482 140264174335808 run_lib.py:153] step: 26500, training_loss: 1.10626e+02
I1110 22:51:38.377966 140264174335808 run_lib.py:153] step: 26550, training_loss: 9.77235e+01
I1110 22:51:49.253684 140264174335808 run_lib.py:153] step: 26600, training_loss: 1.49206e+02
I1110 22:51:59.955722 140264174335808 run_lib.py:153] step: 26650, training_loss: 1.28675e+02
I1110 22:52:10.173867 140264174335808 run_lib.py:153] step: 26700, training_loss: 1.32904e+02
I1110 22:52:20.269954 140264174335808 run_lib.py:153] step: 26750, training_loss: 1.26580e+02
I1110 22:52:30.365856 140264174335808 run_lib.py:153] step: 26800, training_loss: 1.27002e+02
I1110 22:52:40.488856 140264174335808 run_lib.py:153] step: 26850, training_loss: 1.41255e+02
I1110 22:52:50.311660 140264174335808 run_lib.py:153] step: 26900, training_loss: 1.01965e+02
I1110 22:53:00.206794 140264174335808 run_lib.py:153] step: 26950, training_loss: 1.76618e+02
I1110 22:53:10.245507 140264174335808 run_lib.py:153] step: 27000, training_loss: 1.33891e+02
I1110 22:53:20.505289 140264174335808 run_lib.py:153] step: 27050, training_loss: 1.26293e+02
I1110 22:53:30.600357 140264174335808 run_lib.py:153] step: 27100, training_loss: 1.15654e+02
I1110 22:53:40.286206 140264174335808 run_lib.py:153] step: 27150, training_loss: 1.63315e+02
I1110 22:53:50.429790 140264174335808 run_lib.py:153] step: 27200, training_loss: 1.37001e+02
I1110 22:54:00.179787 140264174335808 run_lib.py:153] step: 27250, training_loss: 1.05864e+02
I1110 22:54:10.088192 140264174335808 run_lib.py:153] step: 27300, training_loss: 1.29449e+02
I1110 22:54:19.990317 140264174335808 run_lib.py:153] step: 27350, training_loss: 1.33278e+02
I1110 22:54:30.082629 140264174335808 run_lib.py:153] step: 27400, training_loss: 1.47111e+02
I1110 22:54:40.106514 140264174335808 run_lib.py:153] step: 27450, training_loss: 1.28815e+02
I1110 22:54:50.054525 140264174335808 run_lib.py:153] step: 27500, training_loss: 1.27261e+02
I1110 22:54:59.542913 140264174335808 run_lib.py:153] step: 27550, training_loss: 1.18320e+02
I1110 22:55:08.871696 140264174335808 run_lib.py:153] step: 27600, training_loss: 1.40918e+02
I1110 22:55:18.597522 140264174335808 run_lib.py:153] step: 27650, training_loss: 1.24067e+02
I1110 22:55:28.541545 140264174335808 run_lib.py:153] step: 27700, training_loss: 1.38193e+02
I1110 22:55:38.726639 140264174335808 run_lib.py:153] step: 27750, training_loss: 9.83513e+01
I1110 22:55:48.427676 140264174335808 run_lib.py:153] step: 27800, training_loss: 1.58012e+02
I1110 22:55:57.968960 140264174335808 run_lib.py:153] step: 27850, training_loss: 1.00837e+02
I1110 22:56:07.392368 140264174335808 run_lib.py:153] step: 27900, training_loss: 1.28762e+02
I1110 22:56:17.216740 140264174335808 run_lib.py:153] step: 27950, training_loss: 1.39654e+02
I1110 22:56:26.762917 140264174335808 run_lib.py:153] step: 28000, training_loss: 1.23696e+02
I1110 22:56:36.386466 140264174335808 run_lib.py:153] step: 28050, training_loss: 1.12702e+02
I1110 22:56:45.836086 140264174335808 run_lib.py:153] step: 28100, training_loss: 1.23060e+02
I1110 22:56:55.348394 140264174335808 run_lib.py:153] step: 28150, training_loss: 1.33044e+02
I1110 22:57:05.518270 140264174335808 run_lib.py:153] step: 28200, training_loss: 1.09181e+02
I1110 22:57:16.024558 140264174335808 run_lib.py:153] step: 28250, training_loss: 1.38874e+02
I1110 22:57:26.274863 140264174335808 run_lib.py:153] step: 28300, training_loss: 1.43657e+02
I1110 22:57:36.434725 140264174335808 run_lib.py:153] step: 28350, training_loss: 1.22664e+02
I1110 22:57:46.908959 140264174335808 run_lib.py:153] step: 28400, training_loss: 1.08983e+02
I1110 22:57:57.262538 140264174335808 run_lib.py:153] step: 28450, training_loss: 1.28904e+02
I1110 22:58:07.726133 140264174335808 run_lib.py:153] step: 28500, training_loss: 1.30896e+02
I1110 22:58:17.384527 140264174335808 run_lib.py:153] step: 28550, training_loss: 1.44823e+02
I1110 22:58:27.587750 140264174335808 run_lib.py:153] step: 28600, training_loss: 1.38730e+02
I1110 22:58:37.450560 140264174335808 run_lib.py:153] step: 28650, training_loss: 1.58765e+02
I1110 22:58:48.275487 140264174335808 run_lib.py:153] step: 28700, training_loss: 1.43505e+02
I1110 22:58:58.719924 140264174335808 run_lib.py:153] step: 28750, training_loss: 1.44363e+02
I1110 22:59:08.353815 140264174335808 run_lib.py:153] step: 28800, training_loss: 1.55958e+02
I1110 22:59:18.648876 140264174335808 run_lib.py:153] step: 28850, training_loss: 1.31182e+02
I1110 22:59:29.011306 140264174335808 run_lib.py:153] step: 28900, training_loss: 1.63585e+02
I1110 22:59:39.047613 140264174335808 run_lib.py:153] step: 28950, training_loss: 1.65792e+02
I1110 22:59:49.512331 140264174335808 run_lib.py:153] step: 29000, training_loss: 1.23689e+02
I1110 22:59:59.416233 140264174335808 run_lib.py:153] step: 29050, training_loss: 1.59983e+02
I1110 23:00:09.210081 140264174335808 run_lib.py:153] step: 29100, training_loss: 1.49241e+02
I1110 23:00:19.609153 140264174335808 run_lib.py:153] step: 29150, training_loss: 1.36396e+02
I1110 23:00:30.217226 140264174335808 run_lib.py:153] step: 29200, training_loss: 1.55251e+02
I1110 23:00:40.276882 140264174335808 run_lib.py:153] step: 29250, training_loss: 1.58834e+02
I1110 23:00:51.009971 140264174335808 run_lib.py:153] step: 29300, training_loss: 1.23834e+02
I1110 23:01:01.078680 140264174335808 run_lib.py:153] step: 29350, training_loss: 1.53553e+02
I1110 23:01:11.712261 140264174335808 run_lib.py:153] step: 29400, training_loss: 1.18914e+02
I1110 23:01:21.860374 140264174335808 run_lib.py:153] step: 29450, training_loss: 1.49814e+02
I1110 23:01:31.616870 140264174335808 run_lib.py:153] step: 29500, training_loss: 1.36261e+02
I1110 23:01:42.015414 140264174335808 run_lib.py:153] step: 29550, training_loss: 1.20117e+02
I1110 23:01:52.242772 140264174335808 run_lib.py:153] step: 29600, training_loss: 1.39804e+02
I1110 23:02:02.758693 140264174335808 run_lib.py:153] step: 29650, training_loss: 1.60881e+02
I1110 23:02:13.369843 140264174335808 run_lib.py:153] step: 29700, training_loss: 1.11585e+02
I1110 23:02:23.458170 140264174335808 run_lib.py:153] step: 29750, training_loss: 1.20827e+02
I1110 23:02:33.722871 140264174335808 run_lib.py:153] step: 29800, training_loss: 1.25059e+02
I1110 23:02:43.751255 140264174335808 run_lib.py:153] step: 29850, training_loss: 1.22757e+02
I1110 23:02:53.787794 140264174335808 run_lib.py:153] step: 29900, training_loss: 1.04835e+02
I1110 23:03:03.944587 140264174335808 run_lib.py:153] step: 29950, training_loss: 1.42532e+02
I1110 23:03:14.452669 140264174335808 run_lib.py:153] step: 30000, training_loss: 1.37295e+02
I1110 23:03:15.021528 140264174335808 run_lib.py:166] step: 30000, eval_loss: 1.36777e+02
I1110 23:03:24.830687 140264174335808 run_lib.py:153] step: 30050, training_loss: 1.16186e+02
I1110 23:03:35.314861 140264174335808 run_lib.py:153] step: 30100, training_loss: 1.66562e+02
I1110 23:03:45.924324 140264174335808 run_lib.py:153] step: 30150, training_loss: 1.21197e+02
I1110 23:03:56.168108 140264174335808 run_lib.py:153] step: 30200, training_loss: 1.23779e+02
I1110 23:04:06.935296 140264174335808 run_lib.py:153] step: 30250, training_loss: 1.34710e+02
I1110 23:04:17.272037 140264174335808 run_lib.py:153] step: 30300, training_loss: 1.17933e+02
I1110 23:04:26.934914 140264174335808 run_lib.py:153] step: 30350, training_loss: 1.62783e+02
I1110 23:04:36.652747 140264174335808 run_lib.py:153] step: 30400, training_loss: 1.30244e+02
I1110 23:04:46.787172 140264174335808 run_lib.py:153] step: 30450, training_loss: 1.09225e+02
I1110 23:04:56.469010 140264174335808 run_lib.py:153] step: 30500, training_loss: 1.40363e+02
I1110 23:05:07.049476 140264174335808 run_lib.py:153] step: 30550, training_loss: 1.04737e+02
I1110 23:05:17.397324 140264174335808 run_lib.py:153] step: 30600, training_loss: 1.41267e+02
I1110 23:05:27.468788 140264174335808 run_lib.py:153] step: 30650, training_loss: 1.49592e+02
I1110 23:05:37.537337 140264174335808 run_lib.py:153] step: 30700, training_loss: 1.40087e+02
I1110 23:05:48.079741 140264174335808 run_lib.py:153] step: 30750, training_loss: 1.47046e+02
I1110 23:05:58.776698 140264174335808 run_lib.py:153] step: 30800, training_loss: 1.26752e+02
I1110 23:06:09.006024 140264174335808 run_lib.py:153] step: 30850, training_loss: 1.56971e+02
I1110 23:06:19.052314 140264174335808 run_lib.py:153] step: 30900, training_loss: 1.39796e+02
I1110 23:06:28.934396 140264174335808 run_lib.py:153] step: 30950, training_loss: 1.26360e+02
I1110 23:06:39.609521 140264174335808 run_lib.py:153] step: 31000, training_loss: 1.12266e+02
I1110 23:06:49.377027 140264174335808 run_lib.py:153] step: 31050, training_loss: 1.49454e+02
I1110 23:06:59.339686 140264174335808 run_lib.py:153] step: 31100, training_loss: 1.71218e+02
I1110 23:07:09.759707 140264174335808 run_lib.py:153] step: 31150, training_loss: 1.23350e+02
I1110 23:07:19.854193 140264174335808 run_lib.py:153] step: 31200, training_loss: 1.43637e+02
I1110 23:07:29.994850 140264174335808 run_lib.py:153] step: 31250, training_loss: 1.37893e+02
I1110 23:07:40.201404 140264174335808 run_lib.py:153] step: 31300, training_loss: 1.24462e+02
I1110 23:07:50.603198 140264174335808 run_lib.py:153] step: 31350, training_loss: 1.36958e+02
I1110 23:08:01.035951 140264174335808 run_lib.py:153] step: 31400, training_loss: 1.27804e+02
I1110 23:08:10.916174 140264174335808 run_lib.py:153] step: 31450, training_loss: 1.37339e+02
I1110 23:08:20.987282 140264174335808 run_lib.py:153] step: 31500, training_loss: 1.09232e+02
I1110 23:08:30.614456 140264174335808 run_lib.py:153] step: 31550, training_loss: 1.22177e+02
I1110 23:08:40.393121 140264174335808 run_lib.py:153] step: 31600, training_loss: 1.64391e+02
I1110 23:08:50.313680 140264174335808 run_lib.py:153] step: 31650, training_loss: 1.37698e+02
I1110 23:09:00.139478 140264174335808 run_lib.py:153] step: 31700, training_loss: 1.31484e+02
I1110 23:09:10.890725 140264174335808 run_lib.py:153] step: 31750, training_loss: 1.33318e+02
I1110 23:09:21.258056 140264174335808 run_lib.py:153] step: 31800, training_loss: 1.34423e+02
I1110 23:09:31.889315 140264174335808 run_lib.py:153] step: 31850, training_loss: 1.09258e+02
I1110 23:09:41.756044 140264174335808 run_lib.py:153] step: 31900, training_loss: 1.21373e+02
I1110 23:09:51.656507 140264174335808 run_lib.py:153] step: 31950, training_loss: 1.22008e+02
I1110 23:10:01.724549 140264174335808 run_lib.py:153] step: 32000, training_loss: 1.46891e+02
I1110 23:10:11.586511 140264174335808 run_lib.py:153] step: 32050, training_loss: 1.24808e+02
I1110 23:10:22.234409 140264174335808 run_lib.py:153] step: 32100, training_loss: 1.40377e+02
I1110 23:10:32.643599 140264174335808 run_lib.py:153] step: 32150, training_loss: 1.18515e+02
I1110 23:10:43.032675 140264174335808 run_lib.py:153] step: 32200, training_loss: 1.07642e+02
I1110 23:10:53.426952 140264174335808 run_lib.py:153] step: 32250, training_loss: 1.44327e+02
I1110 23:11:04.682729 140264174335808 run_lib.py:153] step: 32300, training_loss: 1.29811e+02
I1110 23:11:15.306933 140264174335808 run_lib.py:153] step: 32350, training_loss: 1.15192e+02
I1110 23:11:25.245833 140264174335808 run_lib.py:153] step: 32400, training_loss: 1.23511e+02
I1110 23:11:35.034673 140264174335808 run_lib.py:153] step: 32450, training_loss: 1.07015e+02
I1110 23:11:45.803665 140264174335808 run_lib.py:153] step: 32500, training_loss: 1.64843e+02
I1110 23:11:55.977575 140264174335808 run_lib.py:153] step: 32550, training_loss: 1.48622e+02
I1110 23:12:06.751161 140264174335808 run_lib.py:153] step: 32600, training_loss: 1.45788e+02
I1110 23:12:16.487479 140264174335808 run_lib.py:153] step: 32650, training_loss: 1.59915e+02
I1110 23:12:27.068135 140264174335808 run_lib.py:153] step: 32700, training_loss: 1.40606e+02
I1110 23:12:36.968794 140264174335808 run_lib.py:153] step: 32750, training_loss: 1.35263e+02
I1110 23:12:46.262791 140264174335808 run_lib.py:153] step: 32800, training_loss: 1.33819e+02
I1110 23:12:56.436363 140264174335808 run_lib.py:153] step: 32850, training_loss: 1.17656e+02
I1110 23:13:06.800470 140264174335808 run_lib.py:153] step: 32900, training_loss: 1.62356e+02
I1110 23:13:16.386666 140264174335808 run_lib.py:153] step: 32950, training_loss: 1.59027e+02
I1110 23:13:26.281485 140264174335808 run_lib.py:153] step: 33000, training_loss: 1.55994e+02
I1110 23:13:36.002884 140264174335808 run_lib.py:153] step: 33050, training_loss: 1.20025e+02
I1110 23:13:46.872499 140264174335808 run_lib.py:153] step: 33100, training_loss: 1.40032e+02
I1110 23:13:57.680492 140264174335808 run_lib.py:153] step: 33150, training_loss: 1.52259e+02
I1110 23:14:08.341928 140264174335808 run_lib.py:153] step: 33200, training_loss: 1.11157e+02
I1110 23:14:17.967305 140264174335808 run_lib.py:153] step: 33250, training_loss: 1.20568e+02
I1110 23:14:27.752218 140264174335808 run_lib.py:153] step: 33300, training_loss: 1.40041e+02
I1110 23:14:37.886751 140264174335808 run_lib.py:153] step: 33350, training_loss: 1.33866e+02
I1110 23:14:48.168884 140264174335808 run_lib.py:153] step: 33400, training_loss: 1.45021e+02
I1110 23:14:58.198282 140264174335808 run_lib.py:153] step: 33450, training_loss: 1.29012e+02
I1110 23:15:08.491744 140264174335808 run_lib.py:153] step: 33500, training_loss: 1.45918e+02
I1110 23:15:18.036644 140264174335808 run_lib.py:153] step: 33550, training_loss: 1.50503e+02
I1110 23:15:28.249489 140264174335808 run_lib.py:153] step: 33600, training_loss: 1.53222e+02
I1110 23:15:37.809904 140264174335808 run_lib.py:153] step: 33650, training_loss: 1.46744e+02
I1110 23:15:47.076464 140264174335808 run_lib.py:153] step: 33700, training_loss: 1.63000e+02
I1110 23:15:57.029421 140264174335808 run_lib.py:153] step: 33750, training_loss: 1.24426e+02
I1110 23:16:06.866321 140264174335808 run_lib.py:153] step: 33800, training_loss: 1.51349e+02
I1110 23:16:17.008092 140264174335808 run_lib.py:153] step: 33850, training_loss: 1.34712e+02
I1110 23:16:27.020906 140264174335808 run_lib.py:153] step: 33900, training_loss: 1.24519e+02
I1110 23:16:36.790938 140264174335808 run_lib.py:153] step: 33950, training_loss: 1.17290e+02
I1110 23:16:47.323596 140264174335808 run_lib.py:153] step: 34000, training_loss: 1.35127e+02
I1110 23:16:58.343506 140264174335808 run_lib.py:153] step: 34050, training_loss: 1.09840e+02
I1110 23:17:08.425711 140264174335808 run_lib.py:153] step: 34100, training_loss: 1.41012e+02
I1110 23:17:18.411158 140264174335808 run_lib.py:153] step: 34150, training_loss: 1.52744e+02
I1110 23:17:28.809880 140264174335808 run_lib.py:153] step: 34200, training_loss: 1.17620e+02
I1110 23:17:38.415956 140264174335808 run_lib.py:153] step: 34250, training_loss: 1.40625e+02
I1110 23:17:47.708329 140264174335808 run_lib.py:153] step: 34300, training_loss: 1.45891e+02
I1110 23:17:58.114725 140264174335808 run_lib.py:153] step: 34350, training_loss: 1.33411e+02
I1110 23:18:08.326289 140264174335808 run_lib.py:153] step: 34400, training_loss: 1.44419e+02
I1110 23:18:19.033534 140264174335808 run_lib.py:153] step: 34450, training_loss: 1.53369e+02
I1110 23:18:30.014791 140264174335808 run_lib.py:153] step: 34500, training_loss: 1.22965e+02
I1110 23:18:40.608623 140264174335808 run_lib.py:153] step: 34550, training_loss: 1.23448e+02
I1110 23:18:50.330884 140264174335808 run_lib.py:153] step: 34600, training_loss: 1.40353e+02
I1110 23:19:00.155384 140264174335808 run_lib.py:153] step: 34650, training_loss: 1.35678e+02
I1110 23:19:09.776090 140264174335808 run_lib.py:153] step: 34700, training_loss: 1.36051e+02
I1110 23:19:19.924163 140264174335808 run_lib.py:153] step: 34750, training_loss: 1.48727e+02
I1110 23:19:29.654133 140264174335808 run_lib.py:153] step: 34800, training_loss: 1.54554e+02
I1110 23:19:39.865585 140264174335808 run_lib.py:153] step: 34850, training_loss: 1.00230e+02
I1110 23:19:49.980577 140264174335808 run_lib.py:153] step: 34900, training_loss: 1.32610e+02
I1110 23:20:00.395391 140264174335808 run_lib.py:153] step: 34950, training_loss: 1.30923e+02
I1110 23:20:10.404068 140264174335808 run_lib.py:153] step: 35000, training_loss: 1.25795e+02
I1110 23:20:10.512862 140264174335808 run_lib.py:166] step: 35000, eval_loss: 1.42682e+02
I1110 23:20:21.497659 140264174335808 run_lib.py:153] step: 35050, training_loss: 1.40366e+02
I1110 23:20:32.020942 140264174335808 run_lib.py:153] step: 35100, training_loss: 1.30490e+02
I1110 23:20:42.104139 140264174335808 run_lib.py:153] step: 35150, training_loss: 1.04175e+02
I1110 23:20:52.383093 140264174335808 run_lib.py:153] step: 35200, training_loss: 1.19956e+02
I1110 23:21:02.139380 140264174335808 run_lib.py:153] step: 35250, training_loss: 1.28905e+02
I1110 23:21:11.634333 140264174335808 run_lib.py:153] step: 35300, training_loss: 1.21321e+02
I1110 23:21:22.295090 140264174335808 run_lib.py:153] step: 35350, training_loss: 1.41587e+02
I1110 23:21:32.927452 140264174335808 run_lib.py:153] step: 35400, training_loss: 1.51188e+02
I1110 23:21:43.351809 140264174335808 run_lib.py:153] step: 35450, training_loss: 1.22429e+02
I1110 23:21:53.681740 140264174335808 run_lib.py:153] step: 35500, training_loss: 1.11443e+02
I1110 23:22:03.895576 140264174335808 run_lib.py:153] step: 35550, training_loss: 1.69935e+02
I1110 23:22:13.883730 140264174335808 run_lib.py:153] step: 35600, training_loss: 1.59364e+02
I1110 23:22:24.233460 140264174335808 run_lib.py:153] step: 35650, training_loss: 1.26480e+02
I1110 23:22:34.518368 140264174335808 run_lib.py:153] step: 35700, training_loss: 1.39881e+02
I1110 23:22:44.606765 140264174335808 run_lib.py:153] step: 35750, training_loss: 1.08337e+02
I1110 23:22:54.868552 140264174335808 run_lib.py:153] step: 35800, training_loss: 1.29693e+02
I1110 23:23:05.217176 140264174335808 run_lib.py:153] step: 35850, training_loss: 1.51437e+02
I1110 23:23:15.967146 140264174335808 run_lib.py:153] step: 35900, training_loss: 1.59267e+02
I1110 23:23:26.369781 140264174335808 run_lib.py:153] step: 35950, training_loss: 1.36886e+02
I1110 23:23:37.281827 140264174335808 run_lib.py:153] step: 36000, training_loss: 1.50631e+02
I1110 23:23:47.115644 140264174335808 run_lib.py:153] step: 36050, training_loss: 1.12805e+02
I1110 23:23:57.267703 140264174335808 run_lib.py:153] step: 36100, training_loss: 1.40025e+02
I1110 23:24:07.458312 140264174335808 run_lib.py:153] step: 36150, training_loss: 1.59825e+02
I1110 23:24:17.945765 140264174335808 run_lib.py:153] step: 36200, training_loss: 1.05655e+02
I1110 23:24:28.313395 140264174335808 run_lib.py:153] step: 36250, training_loss: 1.39859e+02
I1110 23:24:38.570370 140264174335808 run_lib.py:153] step: 36300, training_loss: 1.10539e+02
I1110 23:24:48.825355 140264174335808 run_lib.py:153] step: 36350, training_loss: 1.08060e+02
I1110 23:24:59.257001 140264174335808 run_lib.py:153] step: 36400, training_loss: 1.33010e+02
I1110 23:25:09.806937 140264174335808 run_lib.py:153] step: 36450, training_loss: 1.44535e+02
I1110 23:25:19.897505 140264174335808 run_lib.py:153] step: 36500, training_loss: 1.18193e+02
I1110 23:25:29.901071 140264174335808 run_lib.py:153] step: 36550, training_loss: 1.72559e+02
I1110 23:25:39.889727 140264174335808 run_lib.py:153] step: 36600, training_loss: 1.31564e+02
I1110 23:25:49.747603 140264174335808 run_lib.py:153] step: 36650, training_loss: 1.34332e+02
I1110 23:25:59.751782 140264174335808 run_lib.py:153] step: 36700, training_loss: 1.17425e+02
I1110 23:26:09.535497 140264174335808 run_lib.py:153] step: 36750, training_loss: 1.24842e+02
I1110 23:26:19.514078 140264174335808 run_lib.py:153] step: 36800, training_loss: 1.55118e+02
I1110 23:26:29.385115 140264174335808 run_lib.py:153] step: 36850, training_loss: 1.27091e+02
I1110 23:26:39.078094 140264174335808 run_lib.py:153] step: 36900, training_loss: 1.32231e+02
I1110 23:26:48.861104 140264174335808 run_lib.py:153] step: 36950, training_loss: 1.10740e+02
I1110 23:26:58.137377 140264174335808 run_lib.py:153] step: 37000, training_loss: 1.48313e+02
I1110 23:27:07.456504 140264174335808 run_lib.py:153] step: 37050, training_loss: 1.33558e+02
I1110 23:27:17.135119 140264174335808 run_lib.py:153] step: 37100, training_loss: 1.15134e+02
I1110 23:27:26.607394 140264174335808 run_lib.py:153] step: 37150, training_loss: 1.54114e+02
I1110 23:27:36.728560 140264174335808 run_lib.py:153] step: 37200, training_loss: 1.46594e+02
I1110 23:27:46.239462 140264174335808 run_lib.py:153] step: 37250, training_loss: 1.22330e+02
I1110 23:27:56.002331 140264174335808 run_lib.py:153] step: 37300, training_loss: 1.39818e+02
I1110 23:28:05.724018 140264174335808 run_lib.py:153] step: 37350, training_loss: 1.24356e+02
I1110 23:28:15.633028 140264174335808 run_lib.py:153] step: 37400, training_loss: 1.02394e+02
I1110 23:28:25.279455 140264174335808 run_lib.py:153] step: 37450, training_loss: 1.24679e+02
I1110 23:28:35.980627 140264174335808 run_lib.py:153] step: 37500, training_loss: 1.35336e+02
I1110 23:28:46.298617 140264174335808 run_lib.py:153] step: 37550, training_loss: 1.37432e+02
I1110 23:28:57.109357 140264174335808 run_lib.py:153] step: 37600, training_loss: 1.33242e+02
I1110 23:29:07.124849 140264174335808 run_lib.py:153] step: 37650, training_loss: 1.55472e+02
I1110 23:29:17.656918 140264174335808 run_lib.py:153] step: 37700, training_loss: 1.39957e+02
I1110 23:29:27.564998 140264174335808 run_lib.py:153] step: 37750, training_loss: 1.14893e+02
I1110 23:29:37.711482 140264174335808 run_lib.py:153] step: 37800, training_loss: 1.51079e+02
I1110 23:29:47.241971 140264174335808 run_lib.py:153] step: 37850, training_loss: 1.54666e+02
I1110 23:29:58.149649 140264174335808 run_lib.py:153] step: 37900, training_loss: 1.39939e+02
I1110 23:30:08.278302 140264174335808 run_lib.py:153] step: 37950, training_loss: 1.23677e+02
I1110 23:30:18.401445 140264174335808 run_lib.py:153] step: 38000, training_loss: 1.26123e+02
I1110 23:30:28.043268 140264174335808 run_lib.py:153] step: 38050, training_loss: 1.45375e+02
I1110 23:30:37.928651 140264174335808 run_lib.py:153] step: 38100, training_loss: 1.40958e+02
I1110 23:30:47.473950 140264174335808 run_lib.py:153] step: 38150, training_loss: 1.16773e+02
I1110 23:30:56.976757 140264174335808 run_lib.py:153] step: 38200, training_loss: 1.50064e+02
I1110 23:31:07.205454 140264174335808 run_lib.py:153] step: 38250, training_loss: 1.47406e+02
I1110 23:31:18.094686 140264174335808 run_lib.py:153] step: 38300, training_loss: 1.52584e+02
I1110 23:31:28.282634 140264174335808 run_lib.py:153] step: 38350, training_loss: 1.14626e+02
I1110 23:31:39.069810 140264174335808 run_lib.py:153] step: 38400, training_loss: 1.09587e+02
I1110 23:31:49.756602 140264174335808 run_lib.py:153] step: 38450, training_loss: 1.23182e+02
I1110 23:32:00.235596 140264174335808 run_lib.py:153] step: 38500, training_loss: 1.39458e+02
I1110 23:32:10.385676 140264174335808 run_lib.py:153] step: 38550, training_loss: 1.13832e+02
I1110 23:32:20.635884 140264174335808 run_lib.py:153] step: 38600, training_loss: 1.43265e+02
I1110 23:32:31.346828 140264174335808 run_lib.py:153] step: 38650, training_loss: 1.16529e+02
I1110 23:32:41.811754 140264174335808 run_lib.py:153] step: 38700, training_loss: 1.24374e+02
I1110 23:32:52.050626 140264174335808 run_lib.py:153] step: 38750, training_loss: 1.31675e+02
I1110 23:33:02.102759 140264174335808 run_lib.py:153] step: 38800, training_loss: 1.41870e+02
I1110 23:33:12.398286 140264174335808 run_lib.py:153] step: 38850, training_loss: 1.63955e+02
I1110 23:33:21.743771 140264174335808 run_lib.py:153] step: 38900, training_loss: 1.25958e+02
I1110 23:33:31.350419 140264174335808 run_lib.py:153] step: 38950, training_loss: 1.52122e+02
I1110 23:33:42.076268 140264174335808 run_lib.py:153] step: 39000, training_loss: 1.00328e+02
I1110 23:33:52.114193 140264174335808 run_lib.py:153] step: 39050, training_loss: 1.26121e+02
I1110 23:34:02.435648 140264174335808 run_lib.py:153] step: 39100, training_loss: 1.26635e+02
I1110 23:34:12.801288 140264174335808 run_lib.py:153] step: 39150, training_loss: 1.68411e+02
I1110 23:34:22.529641 140264174335808 run_lib.py:153] step: 39200, training_loss: 1.39175e+02
I1110 23:34:33.386325 140264174335808 run_lib.py:153] step: 39250, training_loss: 1.26399e+02
I1110 23:34:43.571446 140264174335808 run_lib.py:153] step: 39300, training_loss: 1.32480e+02
I1110 23:34:53.464212 140264174335808 run_lib.py:153] step: 39350, training_loss: 1.59338e+02
I1110 23:35:03.059448 140264174335808 run_lib.py:153] step: 39400, training_loss: 1.60417e+02
I1110 23:35:13.372909 140264174335808 run_lib.py:153] step: 39450, training_loss: 1.60573e+02
I1110 23:35:23.612919 140264174335808 run_lib.py:153] step: 39500, training_loss: 1.36296e+02
I1110 23:35:33.404557 140264174335808 run_lib.py:153] step: 39550, training_loss: 1.31196e+02
I1110 23:35:43.352678 140264174335808 run_lib.py:153] step: 39600, training_loss: 1.50714e+02
I1110 23:35:53.235422 140264174335808 run_lib.py:153] step: 39650, training_loss: 1.45871e+02
I1110 23:36:02.931076 140264174335808 run_lib.py:153] step: 39700, training_loss: 1.32288e+02
I1110 23:36:12.910184 140264174335808 run_lib.py:153] step: 39750, training_loss: 1.19321e+02
I1110 23:36:22.990416 140264174335808 run_lib.py:153] step: 39800, training_loss: 1.06860e+02
I1110 23:36:33.231511 140264174335808 run_lib.py:153] step: 39850, training_loss: 1.17538e+02
I1110 23:36:43.557272 140264174335808 run_lib.py:153] step: 39900, training_loss: 1.23030e+02
I1110 23:36:53.532461 140264174335808 run_lib.py:153] step: 39950, training_loss: 1.40191e+02
I1110 23:37:03.131704 140264174335808 run_lib.py:153] step: 40000, training_loss: 1.64498e+02
I1110 23:37:03.678586 140264174335808 run_lib.py:166] step: 40000, eval_loss: 1.59065e+02
I1110 23:37:13.586703 140264174335808 run_lib.py:153] step: 40050, training_loss: 1.60249e+02
I1110 23:37:23.335738 140264174335808 run_lib.py:153] step: 40100, training_loss: 1.29653e+02
I1110 23:37:33.593439 140264174335808 run_lib.py:153] step: 40150, training_loss: 1.39592e+02
I1110 23:37:43.423856 140264174335808 run_lib.py:153] step: 40200, training_loss: 1.50778e+02
I1110 23:37:53.532665 140264174335808 run_lib.py:153] step: 40250, training_loss: 1.36415e+02
I1110 23:38:04.064198 140264174335808 run_lib.py:153] step: 40300, training_loss: 1.21321e+02
I1110 23:38:14.085360 140264174335808 run_lib.py:153] step: 40350, training_loss: 1.33640e+02
I1110 23:38:23.355992 140264174335808 run_lib.py:153] step: 40400, training_loss: 1.15723e+02
I1110 23:38:33.857941 140264174335808 run_lib.py:153] step: 40450, training_loss: 1.17484e+02
I1110 23:38:43.796769 140264174335808 run_lib.py:153] step: 40500, training_loss: 1.40392e+02
I1110 23:38:54.448440 140264174335808 run_lib.py:153] step: 40550, training_loss: 1.31668e+02
I1110 23:39:04.023151 140264174335808 run_lib.py:153] step: 40600, training_loss: 1.39738e+02
I1110 23:39:14.066897 140264174335808 run_lib.py:153] step: 40650, training_loss: 1.20777e+02
I1110 23:39:24.067174 140264174335808 run_lib.py:153] step: 40700, training_loss: 1.09268e+02
I1110 23:39:33.711326 140264174335808 run_lib.py:153] step: 40750, training_loss: 1.39374e+02
I1110 23:39:43.623494 140264174335808 run_lib.py:153] step: 40800, training_loss: 1.19406e+02
I1110 23:39:53.380066 140264174335808 run_lib.py:153] step: 40850, training_loss: 1.53731e+02
I1110 23:40:03.125476 140264174335808 run_lib.py:153] step: 40900, training_loss: 1.36382e+02
I1110 23:40:12.575867 140264174335808 run_lib.py:153] step: 40950, training_loss: 1.48578e+02
I1110 23:40:22.428096 140264174335808 run_lib.py:153] step: 41000, training_loss: 1.42749e+02
I1110 23:40:32.324975 140264174335808 run_lib.py:153] step: 41050, training_loss: 1.33351e+02
I1110 23:40:42.172857 140264174335808 run_lib.py:153] step: 41100, training_loss: 1.31735e+02
I1110 23:40:52.667912 140264174335808 run_lib.py:153] step: 41150, training_loss: 1.21003e+02
I1110 23:41:02.813840 140264174335808 run_lib.py:153] step: 41200, training_loss: 1.22544e+02
I1110 23:41:13.035874 140264174335808 run_lib.py:153] step: 41250, training_loss: 1.36405e+02
I1110 23:41:22.624017 140264174335808 run_lib.py:153] step: 41300, training_loss: 1.68448e+02
I1110 23:41:32.215341 140264174335808 run_lib.py:153] step: 41350, training_loss: 1.52490e+02
I1110 23:41:41.874382 140264174335808 run_lib.py:153] step: 41400, training_loss: 1.05139e+02
I1110 23:41:52.229654 140264174335808 run_lib.py:153] step: 41450, training_loss: 1.30174e+02
I1110 23:42:01.774068 140264174335808 run_lib.py:153] step: 41500, training_loss: 1.35619e+02
I1110 23:42:11.782804 140264174335808 run_lib.py:153] step: 41550, training_loss: 1.41730e+02
I1110 23:42:21.932764 140264174335808 run_lib.py:153] step: 41600, training_loss: 1.40254e+02
I1110 23:42:31.732512 140264174335808 run_lib.py:153] step: 41650, training_loss: 1.28728e+02
I1110 23:42:41.320419 140264174335808 run_lib.py:153] step: 41700, training_loss: 1.26847e+02
I1110 23:42:50.684093 140264174335808 run_lib.py:153] step: 41750, training_loss: 1.29061e+02
I1110 23:43:00.822961 140264174335808 run_lib.py:153] step: 41800, training_loss: 1.55480e+02
I1110 23:43:10.604192 140264174335808 run_lib.py:153] step: 41850, training_loss: 1.35412e+02
I1110 23:43:20.527425 140264174335808 run_lib.py:153] step: 41900, training_loss: 1.35227e+02
I1110 23:43:30.100465 140264174335808 run_lib.py:153] step: 41950, training_loss: 1.32594e+02
I1110 23:43:40.191892 140264174335808 run_lib.py:153] step: 42000, training_loss: 1.36869e+02
I1110 23:43:50.343398 140264174335808 run_lib.py:153] step: 42050, training_loss: 1.45232e+02
I1110 23:44:00.197294 140264174335808 run_lib.py:153] step: 42100, training_loss: 1.46541e+02
I1110 23:44:10.264339 140264174335808 run_lib.py:153] step: 42150, training_loss: 9.75565e+01
I1110 23:44:20.798493 140264174335808 run_lib.py:153] step: 42200, training_loss: 1.28825e+02
I1110 23:44:30.200376 140264174335808 run_lib.py:153] step: 42250, training_loss: 1.50919e+02
I1110 23:44:40.248968 140264174335808 run_lib.py:153] step: 42300, training_loss: 1.50040e+02
I1110 23:44:50.992305 140264174335808 run_lib.py:153] step: 42350, training_loss: 1.32762e+02
I1110 23:45:01.936269 140264174335808 run_lib.py:153] step: 42400, training_loss: 1.34581e+02
I1110 23:45:13.246598 140264174335808 run_lib.py:153] step: 42450, training_loss: 1.70114e+02
I1110 23:45:23.884268 140264174335808 run_lib.py:153] step: 42500, training_loss: 1.41299e+02
I1110 23:45:34.196129 140264174335808 run_lib.py:153] step: 42550, training_loss: 9.87711e+01
I1110 23:45:43.920739 140264174335808 run_lib.py:153] step: 42600, training_loss: 1.45476e+02
I1110 23:45:54.297190 140264174335808 run_lib.py:153] step: 42650, training_loss: 1.71748e+02
I1110 23:46:04.261652 140264174335808 run_lib.py:153] step: 42700, training_loss: 1.53210e+02
I1110 23:46:13.846721 140264174335808 run_lib.py:153] step: 42750, training_loss: 1.74196e+02
I1110 23:46:24.248879 140264174335808 run_lib.py:153] step: 42800, training_loss: 1.19584e+02
I1110 23:46:34.902750 140264174335808 run_lib.py:153] step: 42850, training_loss: 1.22432e+02
I1110 23:46:45.655792 140264174335808 run_lib.py:153] step: 42900, training_loss: 1.14545e+02
I1110 23:46:56.223394 140264174335808 run_lib.py:153] step: 42950, training_loss: 1.02559e+02
I1110 23:47:06.301579 140264174335808 run_lib.py:153] step: 43000, training_loss: 1.20368e+02
I1110 23:47:16.119305 140264174335808 run_lib.py:153] step: 43050, training_loss: 1.29262e+02
I1110 23:47:25.845128 140264174335808 run_lib.py:153] step: 43100, training_loss: 1.40796e+02
I1110 23:47:35.342426 140264174335808 run_lib.py:153] step: 43150, training_loss: 1.26115e+02
I1110 23:47:44.791590 140264174335808 run_lib.py:153] step: 43200, training_loss: 1.29619e+02
I1110 23:47:55.043899 140264174335808 run_lib.py:153] step: 43250, training_loss: 1.30162e+02
I1110 23:48:05.831789 140264174335808 run_lib.py:153] step: 43300, training_loss: 1.25994e+02
I1110 23:48:16.900936 140264174335808 run_lib.py:153] step: 43350, training_loss: 1.35887e+02
I1110 23:48:26.952286 140264174335808 run_lib.py:153] step: 43400, training_loss: 1.44307e+02
I1110 23:48:36.454147 140264174335808 run_lib.py:153] step: 43450, training_loss: 1.14937e+02
I1110 23:48:46.022983 140264174335808 run_lib.py:153] step: 43500, training_loss: 1.30049e+02
I1110 23:48:55.812305 140264174335808 run_lib.py:153] step: 43550, training_loss: 1.40353e+02
I1110 23:49:05.840216 140264174335808 run_lib.py:153] step: 43600, training_loss: 1.06691e+02
I1110 23:49:15.115728 140264174335808 run_lib.py:153] step: 43650, training_loss: 1.65946e+02
I1110 23:49:24.835846 140264174335808 run_lib.py:153] step: 43700, training_loss: 1.62260e+02
I1110 23:49:34.535696 140264174335808 run_lib.py:153] step: 43750, training_loss: 1.65390e+02
I1110 23:49:44.706532 140264174335808 run_lib.py:153] step: 43800, training_loss: 1.19261e+02
I1110 23:49:54.685579 140264174335808 run_lib.py:153] step: 43850, training_loss: 1.19362e+02
I1110 23:50:04.331837 140264174335808 run_lib.py:153] step: 43900, training_loss: 1.22182e+02
I1110 23:50:14.236505 140264174335808 run_lib.py:153] step: 43950, training_loss: 1.06078e+02
I1110 23:50:23.631800 140264174335808 run_lib.py:153] step: 44000, training_loss: 1.25272e+02
I1110 23:50:33.626940 140264174335808 run_lib.py:153] step: 44050, training_loss: 1.26873e+02
I1110 23:50:43.598441 140264174335808 run_lib.py:153] step: 44100, training_loss: 1.19469e+02
I1110 23:50:53.762236 140264174335808 run_lib.py:153] step: 44150, training_loss: 1.46426e+02
I1110 23:51:03.140108 140264174335808 run_lib.py:153] step: 44200, training_loss: 1.42410e+02
I1110 23:51:12.728197 140264174335808 run_lib.py:153] step: 44250, training_loss: 1.25914e+02
I1110 23:51:22.774659 140264174335808 run_lib.py:153] step: 44300, training_loss: 1.36277e+02
I1110 23:51:33.627003 140264174335808 run_lib.py:153] step: 44350, training_loss: 1.13476e+02
I1110 23:51:43.049076 140264174335808 run_lib.py:153] step: 44400, training_loss: 1.26360e+02
I1110 23:51:53.142225 140264174335808 run_lib.py:153] step: 44450, training_loss: 1.49876e+02
I1110 23:52:02.571496 140264174335808 run_lib.py:153] step: 44500, training_loss: 1.00441e+02
I1110 23:52:12.686250 140264174335808 run_lib.py:153] step: 44550, training_loss: 1.25658e+02
I1110 23:52:22.713464 140264174335808 run_lib.py:153] step: 44600, training_loss: 1.22976e+02
I1110 23:52:33.289129 140264174335808 run_lib.py:153] step: 44650, training_loss: 1.60334e+02
I1110 23:52:42.665419 140264174335808 run_lib.py:153] step: 44700, training_loss: 1.32141e+02
I1110 23:52:52.162963 140264174335808 run_lib.py:153] step: 44750, training_loss: 1.14202e+02
I1110 23:53:01.782617 140264174335808 run_lib.py:153] step: 44800, training_loss: 1.47675e+02
I1110 23:53:11.327973 140264174335808 run_lib.py:153] step: 44850, training_loss: 1.16759e+02
I1110 23:53:21.196709 140264174335808 run_lib.py:153] step: 44900, training_loss: 1.38755e+02
I1110 23:53:31.041796 140264174335808 run_lib.py:153] step: 44950, training_loss: 1.59193e+02
I1110 23:53:41.995958 140264174335808 run_lib.py:153] step: 45000, training_loss: 1.20378e+02
I1110 23:53:42.097720 140264174335808 run_lib.py:166] step: 45000, eval_loss: 1.46814e+02
I1110 23:53:51.913683 140264174335808 run_lib.py:153] step: 45050, training_loss: 1.43262e+02
I1110 23:54:02.109346 140264174335808 run_lib.py:153] step: 45100, training_loss: 1.29734e+02
I1110 23:54:12.518962 140264174335808 run_lib.py:153] step: 45150, training_loss: 1.35065e+02
I1110 23:54:22.199042 140264174335808 run_lib.py:153] step: 45200, training_loss: 1.58234e+02
I1110 23:54:31.963062 140264174335808 run_lib.py:153] step: 45250, training_loss: 1.11786e+02
I1110 23:54:42.204856 140264174335808 run_lib.py:153] step: 45300, training_loss: 1.17728e+02
I1110 23:54:52.411963 140264174335808 run_lib.py:153] step: 45350, training_loss: 1.39225e+02
I1110 23:55:02.881993 140264174335808 run_lib.py:153] step: 45400, training_loss: 1.50584e+02
I1110 23:55:13.113436 140264174335808 run_lib.py:153] step: 45450, training_loss: 1.39228e+02
I1110 23:55:22.800304 140264174335808 run_lib.py:153] step: 45500, training_loss: 1.51602e+02
I1110 23:55:33.581557 140264174335808 run_lib.py:153] step: 45550, training_loss: 1.19796e+02
I1110 23:55:44.233620 140264174335808 run_lib.py:153] step: 45600, training_loss: 1.53861e+02
I1110 23:55:54.749248 140264174335808 run_lib.py:153] step: 45650, training_loss: 1.38471e+02
I1110 23:56:05.218413 140264174335808 run_lib.py:153] step: 45700, training_loss: 1.08750e+02
I1110 23:56:15.164875 140264174335808 run_lib.py:153] step: 45750, training_loss: 1.11285e+02
I1110 23:56:25.683357 140264174335808 run_lib.py:153] step: 45800, training_loss: 1.11955e+02
I1110 23:56:35.515049 140264174335808 run_lib.py:153] step: 45850, training_loss: 1.33688e+02
I1110 23:56:45.718035 140264174335808 run_lib.py:153] step: 45900, training_loss: 1.13438e+02
I1110 23:56:55.685129 140264174335808 run_lib.py:153] step: 45950, training_loss: 1.50301e+02
I1110 23:57:05.645922 140264174335808 run_lib.py:153] step: 46000, training_loss: 1.23839e+02
I1110 23:57:15.694216 140264174335808 run_lib.py:153] step: 46050, training_loss: 1.26890e+02
I1110 23:57:25.804965 140264174335808 run_lib.py:153] step: 46100, training_loss: 1.48209e+02
I1110 23:57:36.570159 140264174335808 run_lib.py:153] step: 46150, training_loss: 1.62176e+02
I1110 23:57:46.906425 140264174335808 run_lib.py:153] step: 46200, training_loss: 1.20601e+02
I1110 23:57:56.590888 140264174335808 run_lib.py:153] step: 46250, training_loss: 1.23313e+02
I1110 23:58:06.875390 140264174335808 run_lib.py:153] step: 46300, training_loss: 1.07734e+02
I1110 23:58:16.705500 140264174335808 run_lib.py:153] step: 46350, training_loss: 1.10156e+02
I1110 23:58:26.839919 140264174335808 run_lib.py:153] step: 46400, training_loss: 1.23214e+02
I1110 23:58:36.678081 140264174335808 run_lib.py:153] step: 46450, training_loss: 1.29169e+02
I1110 23:58:46.303259 140264174335808 run_lib.py:153] step: 46500, training_loss: 1.31480e+02
I1110 23:58:56.727128 140264174335808 run_lib.py:153] step: 46550, training_loss: 1.54368e+02
I1110 23:59:07.328829 140264174335808 run_lib.py:153] step: 46600, training_loss: 1.43115e+02
I1110 23:59:17.571935 140264174335808 run_lib.py:153] step: 46650, training_loss: 1.27065e+02
I1110 23:59:27.610282 140264174335808 run_lib.py:153] step: 46700, training_loss: 1.15484e+02
I1110 23:59:38.083990 140264174335808 run_lib.py:153] step: 46750, training_loss: 1.23462e+02
I1110 23:59:48.287660 140264174335808 run_lib.py:153] step: 46800, training_loss: 1.33802e+02
I1110 23:59:58.023901 140264174335808 run_lib.py:153] step: 46850, training_loss: 1.33794e+02
I1111 00:00:08.841098 140264174335808 run_lib.py:153] step: 46900, training_loss: 1.20197e+02
I1111 00:00:18.805037 140264174335808 run_lib.py:153] step: 46950, training_loss: 1.09363e+02
I1111 00:00:28.454127 140264174335808 run_lib.py:153] step: 47000, training_loss: 1.37349e+02
I1111 00:00:38.737032 140264174335808 run_lib.py:153] step: 47050, training_loss: 9.04657e+01
I1111 00:00:48.755063 140264174335808 run_lib.py:153] step: 47100, training_loss: 1.21663e+02
I1111 00:00:58.824424 140264174335808 run_lib.py:153] step: 47150, training_loss: 1.44706e+02
I1111 00:01:08.902809 140264174335808 run_lib.py:153] step: 47200, training_loss: 1.59793e+02
I1111 00:01:19.378814 140264174335808 run_lib.py:153] step: 47250, training_loss: 1.34391e+02
I1111 00:01:29.553012 140264174335808 run_lib.py:153] step: 47300, training_loss: 1.39443e+02
I1111 00:01:40.150450 140264174335808 run_lib.py:153] step: 47350, training_loss: 1.07229e+02
I1111 00:01:50.697643 140264174335808 run_lib.py:153] step: 47400, training_loss: 1.56006e+02
I1111 00:02:01.171853 140264174335808 run_lib.py:153] step: 47450, training_loss: 1.54084e+02
I1111 00:02:12.331864 140264174335808 run_lib.py:153] step: 47500, training_loss: 1.30179e+02
I1111 00:02:22.660309 140264174335808 run_lib.py:153] step: 47550, training_loss: 1.34480e+02
I1111 00:02:33.003913 140264174335808 run_lib.py:153] step: 47600, training_loss: 1.39023e+02
I1111 00:02:43.015650 140264174335808 run_lib.py:153] step: 47650, training_loss: 1.27370e+02
I1111 00:02:52.915188 140264174335808 run_lib.py:153] step: 47700, training_loss: 1.37625e+02
I1111 00:03:02.457661 140264174335808 run_lib.py:153] step: 47750, training_loss: 1.31990e+02
I1111 00:03:12.398622 140264174335808 run_lib.py:153] step: 47800, training_loss: 1.28470e+02
I1111 00:03:21.961445 140264174335808 run_lib.py:153] step: 47850, training_loss: 1.27788e+02
I1111 00:03:31.620758 140264174335808 run_lib.py:153] step: 47900, training_loss: 1.02292e+02
I1111 00:03:41.572360 140264174335808 run_lib.py:153] step: 47950, training_loss: 1.31619e+02
I1111 00:03:51.662968 140264174335808 run_lib.py:153] step: 48000, training_loss: 1.33471e+02
I1111 00:04:01.079549 140264174335808 run_lib.py:153] step: 48050, training_loss: 1.23671e+02
I1111 00:04:11.163869 140264174335808 run_lib.py:153] step: 48100, training_loss: 1.32482e+02
I1111 00:04:21.196150 140264174335808 run_lib.py:153] step: 48150, training_loss: 1.11531e+02
I1111 00:04:31.383306 140264174335808 run_lib.py:153] step: 48200, training_loss: 1.41257e+02
I1111 00:04:41.101376 140264174335808 run_lib.py:153] step: 48250, training_loss: 1.04894e+02
I1111 00:04:50.996807 140264174335808 run_lib.py:153] step: 48300, training_loss: 1.13948e+02
I1111 00:05:02.123745 140264174335808 run_lib.py:153] step: 48350, training_loss: 1.22133e+02
I1111 00:05:13.143966 140264174335808 run_lib.py:153] step: 48400, training_loss: 1.42923e+02
I1111 00:05:23.076485 140264174335808 run_lib.py:153] step: 48450, training_loss: 1.36616e+02
I1111 00:05:33.304109 140264174335808 run_lib.py:153] step: 48500, training_loss: 1.21264e+02
I1111 00:05:44.334497 140264174335808 run_lib.py:153] step: 48550, training_loss: 1.48766e+02
I1111 00:05:54.234396 140264174335808 run_lib.py:153] step: 48600, training_loss: 1.26628e+02
I1111 00:06:04.947043 140264174335808 run_lib.py:153] step: 48650, training_loss: 1.52154e+02
I1111 00:06:14.700147 140264174335808 run_lib.py:153] step: 48700, training_loss: 1.37282e+02
I1111 00:06:24.671011 140264174335808 run_lib.py:153] step: 48750, training_loss: 1.31261e+02
I1111 00:06:34.686908 140264174335808 run_lib.py:153] step: 48800, training_loss: 1.17390e+02
I1111 00:06:45.761464 140264174335808 run_lib.py:153] step: 48850, training_loss: 1.14575e+02
I1111 00:06:55.895043 140264174335808 run_lib.py:153] step: 48900, training_loss: 1.25857e+02
I1111 00:07:05.494217 140264174335808 run_lib.py:153] step: 48950, training_loss: 1.27566e+02
I1111 00:07:15.501582 140264174335808 run_lib.py:153] step: 49000, training_loss: 1.31062e+02
I1111 00:07:25.569471 140264174335808 run_lib.py:153] step: 49050, training_loss: 1.55540e+02
I1111 00:07:35.109161 140264174335808 run_lib.py:153] step: 49100, training_loss: 1.20430e+02
I1111 00:07:45.118001 140264174335808 run_lib.py:153] step: 49150, training_loss: 1.36579e+02
I1111 00:07:55.302023 140264174335808 run_lib.py:153] step: 49200, training_loss: 1.37936e+02
I1111 00:08:05.433044 140264174335808 run_lib.py:153] step: 49250, training_loss: 1.15086e+02
I1111 00:08:16.536481 140264174335808 run_lib.py:153] step: 49300, training_loss: 1.18413e+02
I1111 00:08:26.701132 140264174335808 run_lib.py:153] step: 49350, training_loss: 1.15007e+02
I1111 00:08:36.602282 140264174335808 run_lib.py:153] step: 49400, training_loss: 1.26579e+02
I1111 00:08:47.075283 140264174335808 run_lib.py:153] step: 49450, training_loss: 1.69285e+02
I1111 00:08:57.063430 140264174335808 run_lib.py:153] step: 49500, training_loss: 1.36931e+02
I1111 00:09:07.225631 140264174335808 run_lib.py:153] step: 49550, training_loss: 1.20679e+02
I1111 00:09:16.872121 140264174335808 run_lib.py:153] step: 49600, training_loss: 1.35617e+02
I1111 00:09:27.585497 140264174335808 run_lib.py:153] step: 49650, training_loss: 1.58342e+02
I1111 00:09:38.167099 140264174335808 run_lib.py:153] step: 49700, training_loss: 1.11381e+02
I1111 00:09:49.002676 140264174335808 run_lib.py:153] step: 49750, training_loss: 1.27385e+02
I1111 00:09:59.678751 140264174335808 run_lib.py:153] step: 49800, training_loss: 1.59161e+02
I1111 00:10:10.168592 140264174335808 run_lib.py:153] step: 49850, training_loss: 1.45096e+02
I1111 00:10:21.048209 140264174335808 run_lib.py:153] step: 49900, training_loss: 1.24068e+02
I1111 00:10:32.283856 140264174335808 run_lib.py:153] step: 49950, training_loss: 1.05136e+02
I1111 00:10:42.777425 140264174335808 run_lib.py:153] step: 50000, training_loss: 1.40581e+02
I1111 00:10:43.369018 140264174335808 run_lib.py:166] step: 50000, eval_loss: 1.14058e+02
I1111 00:10:54.077856 140264174335808 run_lib.py:153] step: 50050, training_loss: 1.31839e+02
I1111 00:11:05.040009 140264174335808 run_lib.py:153] step: 50100, training_loss: 1.45695e+02
I1111 00:11:15.666649 140264174335808 run_lib.py:153] step: 50150, training_loss: 1.10800e+02
I1111 00:11:26.070683 140264174335808 run_lib.py:153] step: 50200, training_loss: 1.02087e+02
I1111 00:11:35.820330 140264174335808 run_lib.py:153] step: 50250, training_loss: 1.17338e+02
I1111 00:11:46.583501 140264174335808 run_lib.py:153] step: 50300, training_loss: 1.33430e+02
I1111 00:11:56.689554 140264174335808 run_lib.py:153] step: 50350, training_loss: 1.73958e+02
I1111 00:12:06.921064 140264174335808 run_lib.py:153] step: 50400, training_loss: 1.30189e+02
I1111 00:12:17.134824 140264174335808 run_lib.py:153] step: 50450, training_loss: 1.46582e+02
I1111 00:12:26.991740 140264174335808 run_lib.py:153] step: 50500, training_loss: 1.40595e+02
I1111 00:12:37.215790 140264174335808 run_lib.py:153] step: 50550, training_loss: 1.11625e+02
I1111 00:12:47.618197 140264174335808 run_lib.py:153] step: 50600, training_loss: 1.46826e+02
I1111 00:12:57.138579 140264174335808 run_lib.py:153] step: 50650, training_loss: 1.07669e+02
I1111 00:13:06.922033 140264174335808 run_lib.py:153] step: 50700, training_loss: 1.35072e+02
I1111 00:13:16.820614 140264174335808 run_lib.py:153] step: 50750, training_loss: 1.34659e+02
I1111 00:13:26.426173 140264174335808 run_lib.py:153] step: 50800, training_loss: 1.24977e+02
I1111 00:13:36.502346 140264174335808 run_lib.py:153] step: 50850, training_loss: 1.14115e+02
I1111 00:13:46.942416 140264174335808 run_lib.py:153] step: 50900, training_loss: 1.43273e+02
I1111 00:13:57.373699 140264174335808 run_lib.py:153] step: 50950, training_loss: 1.26251e+02
I1111 00:14:07.820160 140264174335808 run_lib.py:153] step: 51000, training_loss: 1.36422e+02
I1111 00:14:18.051344 140264174335808 run_lib.py:153] step: 51050, training_loss: 1.37503e+02
I1111 00:14:28.985203 140264174335808 run_lib.py:153] step: 51100, training_loss: 1.23064e+02
I1111 00:14:39.729816 140264174335808 run_lib.py:153] step: 51150, training_loss: 1.35469e+02
I1111 00:14:50.083383 140264174335808 run_lib.py:153] step: 51200, training_loss: 1.09986e+02
I1111 00:15:00.645408 140264174335808 run_lib.py:153] step: 51250, training_loss: 1.75582e+02
I1111 00:15:10.832531 140264174335808 run_lib.py:153] step: 51300, training_loss: 1.43461e+02
I1111 00:15:20.636520 140264174335808 run_lib.py:153] step: 51350, training_loss: 1.39583e+02
I1111 00:15:30.500482 140264174335808 run_lib.py:153] step: 51400, training_loss: 1.14281e+02
I1111 00:15:40.407807 140264174335808 run_lib.py:153] step: 51450, training_loss: 1.21892e+02
I1111 00:15:49.925771 140264174335808 run_lib.py:153] step: 51500, training_loss: 9.74207e+01
I1111 00:15:59.320563 140264174335808 run_lib.py:153] step: 51550, training_loss: 1.08695e+02
I1111 00:16:09.229469 140264174335808 run_lib.py:153] step: 51600, training_loss: 1.38934e+02
I1111 00:16:19.173033 140264174335808 run_lib.py:153] step: 51650, training_loss: 1.26434e+02
I1111 00:16:29.063756 140264174335808 run_lib.py:153] step: 51700, training_loss: 1.07815e+02
I1111 00:16:39.636710 140264174335808 run_lib.py:153] step: 51750, training_loss: 1.28140e+02
I1111 00:16:50.193265 140264174335808 run_lib.py:153] step: 51800, training_loss: 1.37156e+02
I1111 00:17:00.372541 140264174335808 run_lib.py:153] step: 51850, training_loss: 1.57728e+02
I1111 00:17:11.216125 140264174335808 run_lib.py:153] step: 51900, training_loss: 1.57020e+02
I1111 00:17:21.459759 140264174335808 run_lib.py:153] step: 51950, training_loss: 1.39685e+02
I1111 00:17:31.181551 140264174335808 run_lib.py:153] step: 52000, training_loss: 1.43574e+02
I1111 00:17:40.901675 140264174335808 run_lib.py:153] step: 52050, training_loss: 1.42818e+02
I1111 00:17:50.321654 140264174335808 run_lib.py:153] step: 52100, training_loss: 1.17464e+02
I1111 00:18:00.135899 140264174335808 run_lib.py:153] step: 52150, training_loss: 1.25814e+02
I1111 00:18:09.675292 140264174335808 run_lib.py:153] step: 52200, training_loss: 1.31116e+02
I1111 00:18:19.459481 140264174335808 run_lib.py:153] step: 52250, training_loss: 1.66131e+02
I1111 00:18:29.703433 140264174335808 run_lib.py:153] step: 52300, training_loss: 1.31193e+02
I1111 00:18:39.701628 140264174335808 run_lib.py:153] step: 52350, training_loss: 1.35585e+02
I1111 00:18:50.040272 140264174335808 run_lib.py:153] step: 52400, training_loss: 1.15042e+02
I1111 00:19:00.301277 140264174335808 run_lib.py:153] step: 52450, training_loss: 1.29797e+02
I1111 00:19:09.770166 140264174335808 run_lib.py:153] step: 52500, training_loss: 1.13580e+02
I1111 00:19:19.598968 140264174335808 run_lib.py:153] step: 52550, training_loss: 1.46840e+02
I1111 00:19:29.387856 140264174335808 run_lib.py:153] step: 52600, training_loss: 1.55839e+02
I1111 00:19:39.011399 140264174335808 run_lib.py:153] step: 52650, training_loss: 1.39562e+02
I1111 00:19:49.450700 140264174335808 run_lib.py:153] step: 52700, training_loss: 1.05553e+02
I1111 00:19:59.232168 140264174335808 run_lib.py:153] step: 52750, training_loss: 1.55097e+02
I1111 00:20:09.248334 140264174335808 run_lib.py:153] step: 52800, training_loss: 1.30724e+02
I1111 00:20:19.679520 140264174335808 run_lib.py:153] step: 52850, training_loss: 1.43015e+02
I1111 00:20:29.915491 140264174335808 run_lib.py:153] step: 52900, training_loss: 1.13995e+02
I1111 00:20:39.406044 140264174335808 run_lib.py:153] step: 52950, training_loss: 1.47363e+02
I1111 00:20:48.833859 140264174335808 run_lib.py:153] step: 53000, training_loss: 1.15542e+02
I1111 00:20:58.777694 140264174335808 run_lib.py:153] step: 53050, training_loss: 1.43720e+02
I1111 00:21:08.407459 140264174335808 run_lib.py:153] step: 53100, training_loss: 1.44702e+02
I1111 00:21:18.434209 140264174335808 run_lib.py:153] step: 53150, training_loss: 1.15319e+02
I1111 00:21:28.527860 140264174335808 run_lib.py:153] step: 53200, training_loss: 1.47985e+02
I1111 00:21:38.496836 140264174335808 run_lib.py:153] step: 53250, training_loss: 1.44947e+02
I1111 00:21:48.416703 140264174335808 run_lib.py:153] step: 53300, training_loss: 1.57790e+02
I1111 00:21:57.999737 140264174335808 run_lib.py:153] step: 53350, training_loss: 1.35536e+02
I1111 00:22:07.540673 140264174335808 run_lib.py:153] step: 53400, training_loss: 1.03766e+02
I1111 00:22:17.085785 140264174335808 run_lib.py:153] step: 53450, training_loss: 1.10901e+02
I1111 00:22:27.881622 140264174335808 run_lib.py:153] step: 53500, training_loss: 1.34644e+02
I1111 00:22:37.894338 140264174335808 run_lib.py:153] step: 53550, training_loss: 1.52369e+02
I1111 00:22:47.801103 140264174335808 run_lib.py:153] step: 53600, training_loss: 1.19177e+02
I1111 00:22:57.547187 140264174335808 run_lib.py:153] step: 53650, training_loss: 1.33052e+02
I1111 00:23:07.211036 140264174335808 run_lib.py:153] step: 53700, training_loss: 1.28781e+02
I1111 00:23:16.710623 140264174335808 run_lib.py:153] step: 53750, training_loss: 1.09817e+02
I1111 00:23:26.362308 140264174335808 run_lib.py:153] step: 53800, training_loss: 1.43539e+02
I1111 00:23:36.090031 140264174335808 run_lib.py:153] step: 53850, training_loss: 1.46418e+02
I1111 00:23:46.327956 140264174335808 run_lib.py:153] step: 53900, training_loss: 1.15879e+02
I1111 00:23:56.996504 140264174335808 run_lib.py:153] step: 53950, training_loss: 1.01747e+02
I1111 00:24:06.780613 140264174335808 run_lib.py:153] step: 54000, training_loss: 1.63102e+02
I1111 00:24:16.460628 140264174335808 run_lib.py:153] step: 54050, training_loss: 1.66824e+02
I1111 00:24:26.228356 140264174335808 run_lib.py:153] step: 54100, training_loss: 1.13811e+02
I1111 00:24:36.011842 140264174335808 run_lib.py:153] step: 54150, training_loss: 1.27736e+02
I1111 00:24:45.527722 140264174335808 run_lib.py:153] step: 54200, training_loss: 1.18528e+02
I1111 00:24:55.473208 140264174335808 run_lib.py:153] step: 54250, training_loss: 1.25172e+02
I1111 00:25:04.826914 140264174335808 run_lib.py:153] step: 54300, training_loss: 1.28870e+02
I1111 00:25:14.717657 140264174335808 run_lib.py:153] step: 54350, training_loss: 1.20103e+02
I1111 00:25:24.627469 140264174335808 run_lib.py:153] step: 54400, training_loss: 1.20503e+02
I1111 00:25:34.032298 140264174335808 run_lib.py:153] step: 54450, training_loss: 1.37591e+02
I1111 00:25:43.667347 140264174335808 run_lib.py:153] step: 54500, training_loss: 1.50814e+02
I1111 00:25:53.816761 140264174335808 run_lib.py:153] step: 54550, training_loss: 1.34645e+02
I1111 00:26:03.643528 140264174335808 run_lib.py:153] step: 54600, training_loss: 1.18310e+02
I1111 00:26:13.754378 140264174335808 run_lib.py:153] step: 54650, training_loss: 1.56769e+02
I1111 00:26:23.730578 140264174335808 run_lib.py:153] step: 54700, training_loss: 1.35442e+02
I1111 00:26:33.099239 140264174335808 run_lib.py:153] step: 54750, training_loss: 1.13172e+02
I1111 00:26:42.703632 140264174335808 run_lib.py:153] step: 54800, training_loss: 1.26753e+02
I1111 00:26:53.259107 140264174335808 run_lib.py:153] step: 54850, training_loss: 1.47626e+02
I1111 00:27:02.833191 140264174335808 run_lib.py:153] step: 54900, training_loss: 1.17619e+02
I1111 00:27:12.541893 140264174335808 run_lib.py:153] step: 54950, training_loss: 1.36357e+02
I1111 00:27:21.801304 140264174335808 run_lib.py:153] step: 55000, training_loss: 1.14803e+02
I1111 00:27:21.905887 140264174335808 run_lib.py:166] step: 55000, eval_loss: 1.48470e+02
I1111 00:27:31.785520 140264174335808 run_lib.py:153] step: 55050, training_loss: 1.04569e+02
I1111 00:27:41.411135 140264174335808 run_lib.py:153] step: 55100, training_loss: 1.26757e+02
I1111 00:27:51.190783 140264174335808 run_lib.py:153] step: 55150, training_loss: 1.24802e+02
I1111 00:28:01.443842 140264174335808 run_lib.py:153] step: 55200, training_loss: 1.14259e+02
I1111 00:28:11.241099 140264174335808 run_lib.py:153] step: 55250, training_loss: 1.22499e+02
I1111 00:28:21.173573 140264174335808 run_lib.py:153] step: 55300, training_loss: 1.10634e+02
I1111 00:28:30.543453 140264174335808 run_lib.py:153] step: 55350, training_loss: 1.57518e+02
I1111 00:28:40.130057 140264174335808 run_lib.py:153] step: 55400, training_loss: 1.15082e+02
I1111 00:28:50.181772 140264174335808 run_lib.py:153] step: 55450, training_loss: 1.45863e+02
I1111 00:29:00.061461 140264174335808 run_lib.py:153] step: 55500, training_loss: 1.20279e+02
I1111 00:29:09.655601 140264174335808 run_lib.py:153] step: 55550, training_loss: 1.36706e+02
I1111 00:29:19.302189 140264174335808 run_lib.py:153] step: 55600, training_loss: 1.31989e+02
I1111 00:29:30.180573 140264174335808 run_lib.py:153] step: 55650, training_loss: 1.56302e+02
I1111 00:29:39.728749 140264174335808 run_lib.py:153] step: 55700, training_loss: 1.06277e+02
I1111 00:29:49.853729 140264174335808 run_lib.py:153] step: 55750, training_loss: 1.19225e+02
I1111 00:30:00.016913 140264174335808 run_lib.py:153] step: 55800, training_loss: 1.11297e+02
I1111 00:30:10.015863 140264174335808 run_lib.py:153] step: 55850, training_loss: 1.02658e+02
I1111 00:30:19.428864 140264174335808 run_lib.py:153] step: 55900, training_loss: 1.40785e+02
I1111 00:30:29.132282 140264174335808 run_lib.py:153] step: 55950, training_loss: 1.31412e+02
I1111 00:30:39.097126 140264174335808 run_lib.py:153] step: 56000, training_loss: 1.16349e+02
I1111 00:30:49.879155 140264174335808 run_lib.py:153] step: 56050, training_loss: 1.65695e+02
I1111 00:30:59.413430 140264174335808 run_lib.py:153] step: 56100, training_loss: 1.28373e+02
I1111 00:31:09.003660 140264174335808 run_lib.py:153] step: 56150, training_loss: 1.46851e+02
I1111 00:31:18.810765 140264174335808 run_lib.py:153] step: 56200, training_loss: 1.66771e+02
I1111 00:31:28.509911 140264174335808 run_lib.py:153] step: 56250, training_loss: 1.08155e+02
I1111 00:31:39.463321 140264174335808 run_lib.py:153] step: 56300, training_loss: 1.27746e+02
I1111 00:31:49.743926 140264174335808 run_lib.py:153] step: 56350, training_loss: 1.48868e+02
I1111 00:32:00.040354 140264174335808 run_lib.py:153] step: 56400, training_loss: 1.40856e+02
I1111 00:32:10.210445 140264174335808 run_lib.py:153] step: 56450, training_loss: 1.42203e+02
I1111 00:32:19.788550 140264174335808 run_lib.py:153] step: 56500, training_loss: 1.45813e+02
I1111 00:32:29.489884 140264174335808 run_lib.py:153] step: 56550, training_loss: 1.57414e+02
I1111 00:32:39.387263 140264174335808 run_lib.py:153] step: 56600, training_loss: 1.27058e+02
I1111 00:32:48.820260 140264174335808 run_lib.py:153] step: 56650, training_loss: 1.64621e+02
I1111 00:32:58.798344 140264174335808 run_lib.py:153] step: 56700, training_loss: 1.29132e+02
I1111 00:33:08.267897 140264174335808 run_lib.py:153] step: 56750, training_loss: 1.60058e+02
I1111 00:33:18.720869 140264174335808 run_lib.py:153] step: 56800, training_loss: 1.40353e+02
I1111 00:33:29.906431 140264174335808 run_lib.py:153] step: 56850, training_loss: 1.18456e+02
I1111 00:33:40.091400 140264174335808 run_lib.py:153] step: 56900, training_loss: 1.37068e+02
I1111 00:33:49.924143 140264174335808 run_lib.py:153] step: 56950, training_loss: 1.52877e+02
I1111 00:33:59.937890 140264174335808 run_lib.py:153] step: 57000, training_loss: 1.48820e+02
I1111 00:34:10.448373 140264174335808 run_lib.py:153] step: 57050, training_loss: 1.18567e+02
I1111 00:34:21.248023 140264174335808 run_lib.py:153] step: 57100, training_loss: 1.27854e+02
I1111 00:34:31.758646 140264174335808 run_lib.py:153] step: 57150, training_loss: 1.25623e+02
I1111 00:34:41.895615 140264174335808 run_lib.py:153] step: 57200, training_loss: 1.30737e+02
I1111 00:34:52.164776 140264174335808 run_lib.py:153] step: 57250, training_loss: 1.25429e+02
I1111 00:35:02.901860 140264174335808 run_lib.py:153] step: 57300, training_loss: 1.23509e+02
I1111 00:35:13.364077 140264174335808 run_lib.py:153] step: 57350, training_loss: 1.47452e+02
I1111 00:35:23.180844 140264174335808 run_lib.py:153] step: 57400, training_loss: 1.30173e+02
I1111 00:35:33.751216 140264174335808 run_lib.py:153] step: 57450, training_loss: 1.37731e+02
I1111 00:35:44.298876 140264174335808 run_lib.py:153] step: 57500, training_loss: 1.15111e+02
I1111 00:35:54.765187 140264174335808 run_lib.py:153] step: 57550, training_loss: 1.09497e+02
I1111 00:36:04.614068 140264174335808 run_lib.py:153] step: 57600, training_loss: 1.68886e+02
I1111 00:36:14.650459 140264174335808 run_lib.py:153] step: 57650, training_loss: 1.08614e+02
I1111 00:36:24.808004 140264174335808 run_lib.py:153] step: 57700, training_loss: 1.52776e+02
I1111 00:36:35.655605 140264174335808 run_lib.py:153] step: 57750, training_loss: 1.18273e+02
I1111 00:36:45.556641 140264174335808 run_lib.py:153] step: 57800, training_loss: 1.27018e+02
I1111 00:36:56.342091 140264174335808 run_lib.py:153] step: 57850, training_loss: 1.16885e+02
I1111 00:37:06.001351 140264174335808 run_lib.py:153] step: 57900, training_loss: 1.19910e+02
I1111 00:37:15.561214 140264174335808 run_lib.py:153] step: 57950, training_loss: 1.47259e+02
I1111 00:37:25.035319 140264174335808 run_lib.py:153] step: 58000, training_loss: 1.52922e+02
I1111 00:37:34.596510 140264174335808 run_lib.py:153] step: 58050, training_loss: 1.26098e+02
I1111 00:37:44.957302 140264174335808 run_lib.py:153] step: 58100, training_loss: 1.31774e+02
I1111 00:37:55.440527 140264174335808 run_lib.py:153] step: 58150, training_loss: 1.21755e+02
I1111 00:38:06.169061 140264174335808 run_lib.py:153] step: 58200, training_loss: 1.59151e+02
I1111 00:38:16.072293 140264174335808 run_lib.py:153] step: 58250, training_loss: 1.25363e+02
I1111 00:38:25.797044 140264174335808 run_lib.py:153] step: 58300, training_loss: 1.46933e+02
I1111 00:38:37.104333 140264174335808 run_lib.py:153] step: 58350, training_loss: 1.20409e+02
I1111 00:38:47.759879 140264174335808 run_lib.py:153] step: 58400, training_loss: 9.67602e+01
I1111 00:38:58.038931 140264174335808 run_lib.py:153] step: 58450, training_loss: 1.42443e+02
I1111 00:39:07.848865 140264174335808 run_lib.py:153] step: 58500, training_loss: 1.29035e+02
I1111 00:39:17.879618 140264174335808 run_lib.py:153] step: 58550, training_loss: 1.08383e+02
I1111 00:39:27.702204 140264174335808 run_lib.py:153] step: 58600, training_loss: 1.55359e+02
I1111 00:39:37.405846 140264174335808 run_lib.py:153] step: 58650, training_loss: 1.31194e+02
I1111 00:39:48.137797 140264174335808 run_lib.py:153] step: 58700, training_loss: 1.15194e+02
I1111 00:39:58.003116 140264174335808 run_lib.py:153] step: 58750, training_loss: 1.70026e+02
I1111 00:40:07.610017 140264174335808 run_lib.py:153] step: 58800, training_loss: 1.06961e+02
I1111 00:40:16.881639 140264174335808 run_lib.py:153] step: 58850, training_loss: 1.46107e+02
I1111 00:40:27.321781 140264174335808 run_lib.py:153] step: 58900, training_loss: 1.25219e+02
I1111 00:40:37.061959 140264174335808 run_lib.py:153] step: 58950, training_loss: 1.53903e+02
I1111 00:40:47.533900 140264174335808 run_lib.py:153] step: 59000, training_loss: 1.47832e+02
I1111 00:40:57.374852 140264174335808 run_lib.py:153] step: 59050, training_loss: 1.44725e+02
I1111 00:41:07.184455 140264174335808 run_lib.py:153] step: 59100, training_loss: 1.20032e+02
I1111 00:41:17.073026 140264174335808 run_lib.py:153] step: 59150, training_loss: 1.70244e+02
I1111 00:41:27.434189 140264174335808 run_lib.py:153] step: 59200, training_loss: 1.32894e+02
I1111 00:41:38.142831 140264174335808 run_lib.py:153] step: 59250, training_loss: 1.28665e+02
I1111 00:41:47.871671 140264174335808 run_lib.py:153] step: 59300, training_loss: 1.50641e+02
I1111 00:41:57.666845 140264174335808 run_lib.py:153] step: 59350, training_loss: 1.32323e+02
I1111 00:42:07.373973 140264174335808 run_lib.py:153] step: 59400, training_loss: 1.24720e+02
I1111 00:42:17.836947 140264174335808 run_lib.py:153] step: 59450, training_loss: 1.18106e+02
I1111 00:42:27.969765 140264174335808 run_lib.py:153] step: 59500, training_loss: 1.22244e+02
I1111 00:42:37.491224 140264174335808 run_lib.py:153] step: 59550, training_loss: 1.33776e+02
I1111 00:42:47.672900 140264174335808 run_lib.py:153] step: 59600, training_loss: 1.51227e+02
I1111 00:42:58.046571 140264174335808 run_lib.py:153] step: 59650, training_loss: 1.25817e+02
I1111 00:43:08.126108 140264174335808 run_lib.py:153] step: 59700, training_loss: 1.07477e+02
I1111 00:43:17.909698 140264174335808 run_lib.py:153] step: 59750, training_loss: 1.31238e+02
I1111 00:43:27.587462 140264174335808 run_lib.py:153] step: 59800, training_loss: 1.26316e+02
I1111 00:43:37.202862 140264174335808 run_lib.py:153] step: 59850, training_loss: 1.58929e+02
I1111 00:43:46.824372 140264174335808 run_lib.py:153] step: 59900, training_loss: 1.10922e+02
I1111 00:43:56.722632 140264174335808 run_lib.py:153] step: 59950, training_loss: 1.21142e+02
I1111 00:44:06.416308 140264174335808 run_lib.py:153] step: 60000, training_loss: 1.18035e+02
I1111 00:44:06.942761 140264174335808 run_lib.py:166] step: 60000, eval_loss: 1.13959e+02
I1111 00:44:17.283324 140264174335808 run_lib.py:153] step: 60050, training_loss: 1.39822e+02
I1111 00:44:27.621442 140264174335808 run_lib.py:153] step: 60100, training_loss: 1.41540e+02
I1111 00:44:37.804559 140264174335808 run_lib.py:153] step: 60150, training_loss: 1.31403e+02
I1111 00:44:48.337120 140264174335808 run_lib.py:153] step: 60200, training_loss: 1.30562e+02
I1111 00:44:58.900946 140264174335808 run_lib.py:153] step: 60250, training_loss: 1.52118e+02
I1111 00:45:08.971798 140264174335808 run_lib.py:153] step: 60300, training_loss: 1.27138e+02
I1111 00:45:19.273963 140264174335808 run_lib.py:153] step: 60350, training_loss: 1.20787e+02
I1111 00:45:29.705777 140264174335808 run_lib.py:153] step: 60400, training_loss: 1.20210e+02
I1111 00:45:39.611652 140264174335808 run_lib.py:153] step: 60450, training_loss: 1.01632e+02
I1111 00:45:49.554526 140264174335808 run_lib.py:153] step: 60500, training_loss: 1.64698e+02
I1111 00:45:59.316666 140264174335808 run_lib.py:153] step: 60550, training_loss: 1.01864e+02
I1111 00:46:09.216410 140264174335808 run_lib.py:153] step: 60600, training_loss: 1.54710e+02
I1111 00:46:19.590834 140264174335808 run_lib.py:153] step: 60650, training_loss: 1.41803e+02
I1111 00:46:29.623310 140264174335808 run_lib.py:153] step: 60700, training_loss: 1.16666e+02
I1111 00:46:39.630276 140264174335808 run_lib.py:153] step: 60750, training_loss: 1.26001e+02
I1111 00:46:49.365213 140264174335808 run_lib.py:153] step: 60800, training_loss: 1.14855e+02
I1111 00:46:59.264847 140264174335808 run_lib.py:153] step: 60850, training_loss: 1.51185e+02
I1111 00:47:08.674520 140264174335808 run_lib.py:153] step: 60900, training_loss: 1.01459e+02
I1111 00:47:18.505005 140264174335808 run_lib.py:153] step: 60950, training_loss: 1.37854e+02
I1111 00:47:28.190778 140264174335808 run_lib.py:153] step: 61000, training_loss: 1.01075e+02
I1111 00:47:37.867860 140264174335808 run_lib.py:153] step: 61050, training_loss: 1.54822e+02
I1111 00:47:47.493781 140264174335808 run_lib.py:153] step: 61100, training_loss: 1.15093e+02
I1111 00:47:57.014032 140264174335808 run_lib.py:153] step: 61150, training_loss: 1.27824e+02
I1111 00:48:07.227671 140264174335808 run_lib.py:153] step: 61200, training_loss: 1.24488e+02
I1111 00:48:17.768017 140264174335808 run_lib.py:153] step: 61250, training_loss: 1.47749e+02
I1111 00:48:27.799891 140264174335808 run_lib.py:153] step: 61300, training_loss: 1.25332e+02
I1111 00:48:38.378262 140264174335808 run_lib.py:153] step: 61350, training_loss: 1.55332e+02
I1111 00:48:48.606699 140264174335808 run_lib.py:153] step: 61400, training_loss: 1.17324e+02
I1111 00:48:58.371321 140264174335808 run_lib.py:153] step: 61450, training_loss: 1.29010e+02
I1111 00:49:09.063047 140264174335808 run_lib.py:153] step: 61500, training_loss: 1.60717e+02
I1111 00:49:18.830947 140264174335808 run_lib.py:153] step: 61550, training_loss: 1.31222e+02
I1111 00:49:29.028146 140264174335808 run_lib.py:153] step: 61600, training_loss: 1.29132e+02
I1111 00:49:39.316619 140264174335808 run_lib.py:153] step: 61650, training_loss: 1.82562e+02
I1111 00:49:49.393342 140264174335808 run_lib.py:153] step: 61700, training_loss: 1.46879e+02
I1111 00:49:59.512990 140264174335808 run_lib.py:153] step: 61750, training_loss: 1.13411e+02
I1111 00:50:10.542722 140264174335808 run_lib.py:153] step: 61800, training_loss: 1.22585e+02
I1111 00:50:20.003864 140264174335808 run_lib.py:153] step: 61850, training_loss: 1.51330e+02
I1111 00:50:29.811254 140264174335808 run_lib.py:153] step: 61900, training_loss: 1.47873e+02
I1111 00:50:39.482700 140264174335808 run_lib.py:153] step: 61950, training_loss: 1.16545e+02
I1111 00:50:49.068220 140264174335808 run_lib.py:153] step: 62000, training_loss: 1.48544e+02
I1111 00:50:58.872707 140264174335808 run_lib.py:153] step: 62050, training_loss: 1.44117e+02
I1111 00:51:09.407797 140264174335808 run_lib.py:153] step: 62100, training_loss: 1.36156e+02
I1111 00:51:19.928649 140264174335808 run_lib.py:153] step: 62150, training_loss: 1.24449e+02
I1111 00:51:30.139525 140264174335808 run_lib.py:153] step: 62200, training_loss: 1.17193e+02
I1111 00:51:39.621123 140264174335808 run_lib.py:153] step: 62250, training_loss: 1.11004e+02
I1111 00:51:49.691695 140264174335808 run_lib.py:153] step: 62300, training_loss: 1.39802e+02
I1111 00:52:00.372158 140264174335808 run_lib.py:153] step: 62350, training_loss: 1.20144e+02
I1111 00:52:10.503534 140264174335808 run_lib.py:153] step: 62400, training_loss: 1.21831e+02
I1111 00:52:20.037815 140264174335808 run_lib.py:153] step: 62450, training_loss: 1.46309e+02
I1111 00:52:29.288928 140264174335808 run_lib.py:153] step: 62500, training_loss: 1.46051e+02
I1111 00:52:38.747775 140264174335808 run_lib.py:153] step: 62550, training_loss: 1.32658e+02
I1111 00:52:48.983022 140264174335808 run_lib.py:153] step: 62600, training_loss: 1.37291e+02
I1111 00:52:58.928311 140264174335808 run_lib.py:153] step: 62650, training_loss: 1.23793e+02
I1111 00:53:08.756767 140264174335808 run_lib.py:153] step: 62700, training_loss: 1.39816e+02
I1111 00:53:18.963689 140264174335808 run_lib.py:153] step: 62750, training_loss: 1.31714e+02
I1111 00:53:28.658731 140264174335808 run_lib.py:153] step: 62800, training_loss: 1.22839e+02
I1111 00:53:39.531858 140264174335808 run_lib.py:153] step: 62850, training_loss: 1.30699e+02
I1111 00:53:49.462450 140264174335808 run_lib.py:153] step: 62900, training_loss: 1.33614e+02
I1111 00:53:59.842494 140264174335808 run_lib.py:153] step: 62950, training_loss: 1.45528e+02
I1111 00:54:09.791983 140264174335808 run_lib.py:153] step: 63000, training_loss: 1.59270e+02
I1111 00:54:19.625537 140264174335808 run_lib.py:153] step: 63050, training_loss: 1.34083e+02
I1111 00:54:29.881825 140264174335808 run_lib.py:153] step: 63100, training_loss: 9.29863e+01
I1111 00:54:40.237617 140264174335808 run_lib.py:153] step: 63150, training_loss: 1.22248e+02
I1111 00:54:50.399768 140264174335808 run_lib.py:153] step: 63200, training_loss: 1.20395e+02
I1111 00:55:00.069892 140264174335808 run_lib.py:153] step: 63250, training_loss: 1.20933e+02
I1111 00:55:10.500306 140264174335808 run_lib.py:153] step: 63300, training_loss: 1.35241e+02
I1111 00:55:20.511915 140264174335808 run_lib.py:153] step: 63350, training_loss: 1.58661e+02
I1111 00:55:31.391694 140264174335808 run_lib.py:153] step: 63400, training_loss: 1.43514e+02
I1111 00:55:42.065759 140264174335808 run_lib.py:153] step: 63450, training_loss: 1.20029e+02
I1111 00:55:52.235677 140264174335808 run_lib.py:153] step: 63500, training_loss: 1.21201e+02
I1111 00:56:02.879428 140264174335808 run_lib.py:153] step: 63550, training_loss: 1.29522e+02
I1111 00:56:13.728430 140264174335808 run_lib.py:153] step: 63600, training_loss: 1.18862e+02
I1111 00:56:23.865350 140264174335808 run_lib.py:153] step: 63650, training_loss: 1.23969e+02
I1111 00:56:34.636063 140264174335808 run_lib.py:153] step: 63700, training_loss: 1.35685e+02
I1111 00:56:44.781335 140264174335808 run_lib.py:153] step: 63750, training_loss: 1.03116e+02
I1111 00:56:54.515361 140264174335808 run_lib.py:153] step: 63800, training_loss: 1.15529e+02
I1111 00:57:04.419509 140264174335808 run_lib.py:153] step: 63850, training_loss: 1.32189e+02
I1111 00:57:14.699612 140264174335808 run_lib.py:153] step: 63900, training_loss: 1.01678e+02
I1111 00:57:25.462119 140264174335808 run_lib.py:153] step: 63950, training_loss: 1.19010e+02
I1111 00:57:35.644530 140264174335808 run_lib.py:153] step: 64000, training_loss: 1.30280e+02
I1111 00:57:46.001027 140264174335808 run_lib.py:153] step: 64050, training_loss: 1.52594e+02
I1111 00:57:56.759583 140264174335808 run_lib.py:153] step: 64100, training_loss: 1.58063e+02
I1111 00:58:07.042180 140264174335808 run_lib.py:153] step: 64150, training_loss: 1.30745e+02
I1111 00:58:17.244757 140264174335808 run_lib.py:153] step: 64200, training_loss: 1.18811e+02
I1111 00:58:27.041427 140264174335808 run_lib.py:153] step: 64250, training_loss: 1.47121e+02
I1111 00:58:37.160660 140264174335808 run_lib.py:153] step: 64300, training_loss: 1.61415e+02
I1111 00:58:46.792065 140264174335808 run_lib.py:153] step: 64350, training_loss: 1.64624e+02
I1111 00:58:56.425246 140264174335808 run_lib.py:153] step: 64400, training_loss: 1.24167e+02
I1111 00:59:06.162830 140264174335808 run_lib.py:153] step: 64450, training_loss: 1.16160e+02
I1111 00:59:15.592076 140264174335808 run_lib.py:153] step: 64500, training_loss: 1.23389e+02
I1111 00:59:25.152753 140264174335808 run_lib.py:153] step: 64550, training_loss: 1.24924e+02
I1111 00:59:34.584989 140264174335808 run_lib.py:153] step: 64600, training_loss: 1.56779e+02
I1111 00:59:44.346848 140264174335808 run_lib.py:153] step: 64650, training_loss: 1.31938e+02
I1111 00:59:54.741513 140264174335808 run_lib.py:153] step: 64700, training_loss: 1.25753e+02
I1111 01:00:05.355241 140264174335808 run_lib.py:153] step: 64750, training_loss: 1.28413e+02
I1111 01:00:15.687348 140264174335808 run_lib.py:153] step: 64800, training_loss: 1.34421e+02
I1111 01:00:26.130312 140264174335808 run_lib.py:153] step: 64850, training_loss: 1.18668e+02
I1111 01:00:36.592645 140264174335808 run_lib.py:153] step: 64900, training_loss: 1.26105e+02
I1111 01:00:46.916121 140264174335808 run_lib.py:153] step: 64950, training_loss: 1.10762e+02
I1111 01:00:57.565030 140264174335808 run_lib.py:153] step: 65000, training_loss: 1.31768e+02
I1111 01:00:57.667851 140264174335808 run_lib.py:166] step: 65000, eval_loss: 1.23564e+02
I1111 01:01:07.469405 140264174335808 run_lib.py:153] step: 65050, training_loss: 1.24952e+02
I1111 01:01:17.915317 140264174335808 run_lib.py:153] step: 65100, training_loss: 1.45318e+02
I1111 01:01:28.671854 140264174335808 run_lib.py:153] step: 65150, training_loss: 1.52285e+02
I1111 01:01:39.227112 140264174335808 run_lib.py:153] step: 65200, training_loss: 1.35225e+02
I1111 01:01:48.800748 140264174335808 run_lib.py:153] step: 65250, training_loss: 1.14060e+02
I1111 01:01:58.676434 140264174335808 run_lib.py:153] step: 65300, training_loss: 1.19372e+02
I1111 01:02:08.540936 140264174335808 run_lib.py:153] step: 65350, training_loss: 1.57590e+02
I1111 01:02:19.092395 140264174335808 run_lib.py:153] step: 65400, training_loss: 1.01523e+02
I1111 01:02:29.663715 140264174335808 run_lib.py:153] step: 65450, training_loss: 1.35497e+02
I1111 01:02:39.799153 140264174335808 run_lib.py:153] step: 65500, training_loss: 1.49396e+02
I1111 01:02:49.799701 140264174335808 run_lib.py:153] step: 65550, training_loss: 1.64672e+02
I1111 01:02:59.969999 140264174335808 run_lib.py:153] step: 65600, training_loss: 1.35848e+02
I1111 01:03:09.774095 140264174335808 run_lib.py:153] step: 65650, training_loss: 1.19350e+02
I1111 01:03:19.971263 140264174335808 run_lib.py:153] step: 65700, training_loss: 1.15258e+02
I1111 01:03:29.926804 140264174335808 run_lib.py:153] step: 65750, training_loss: 1.03217e+02
I1111 01:03:39.543727 140264174335808 run_lib.py:153] step: 65800, training_loss: 1.11354e+02
I1111 01:03:49.814795 140264174335808 run_lib.py:153] step: 65850, training_loss: 7.87174e+01
I1111 01:03:59.439004 140264174335808 run_lib.py:153] step: 65900, training_loss: 1.21387e+02
I1111 01:04:08.893259 140264174335808 run_lib.py:153] step: 65950, training_loss: 1.26206e+02
I1111 01:04:19.408566 140264174335808 run_lib.py:153] step: 66000, training_loss: 1.40854e+02
I1111 01:04:29.285100 140264174335808 run_lib.py:153] step: 66050, training_loss: 1.69337e+02
I1111 01:04:38.879760 140264174335808 run_lib.py:153] step: 66100, training_loss: 1.29118e+02
I1111 01:04:48.341181 140264174335808 run_lib.py:153] step: 66150, training_loss: 1.22123e+02
I1111 01:04:57.888244 140264174335808 run_lib.py:153] step: 66200, training_loss: 1.18531e+02
I1111 01:05:08.213995 140264174335808 run_lib.py:153] step: 66250, training_loss: 1.07614e+02
I1111 01:05:18.464039 140264174335808 run_lib.py:153] step: 66300, training_loss: 1.37101e+02
I1111 01:05:29.122123 140264174335808 run_lib.py:153] step: 66350, training_loss: 1.26807e+02
I1111 01:05:39.514190 140264174335808 run_lib.py:153] step: 66400, training_loss: 1.26664e+02
I1111 01:05:49.340564 140264174335808 run_lib.py:153] step: 66450, training_loss: 1.37198e+02
I1111 01:05:59.291801 140264174335808 run_lib.py:153] step: 66500, training_loss: 1.04444e+02
I1111 01:06:09.166212 140264174335808 run_lib.py:153] step: 66550, training_loss: 1.24705e+02
I1111 01:06:19.320849 140264174335808 run_lib.py:153] step: 66600, training_loss: 1.31388e+02
I1111 01:06:29.557767 140264174335808 run_lib.py:153] step: 66650, training_loss: 1.24482e+02
I1111 01:06:39.191142 140264174335808 run_lib.py:153] step: 66700, training_loss: 1.27981e+02
I1111 01:06:49.660046 140264174335808 run_lib.py:153] step: 66750, training_loss: 1.51444e+02
I1111 01:06:59.690898 140264174335808 run_lib.py:153] step: 66800, training_loss: 1.06822e+02
I1111 01:07:09.820930 140264174335808 run_lib.py:153] step: 66850, training_loss: 1.52276e+02
I1111 01:07:19.991823 140264174335808 run_lib.py:153] step: 66900, training_loss: 1.21772e+02
I1111 01:07:30.691751 140264174335808 run_lib.py:153] step: 66950, training_loss: 1.50039e+02
I1111 01:07:40.626953 140264174335808 run_lib.py:153] step: 67000, training_loss: 1.38454e+02
I1111 01:07:51.833454 140264174335808 run_lib.py:153] step: 67050, training_loss: 1.39817e+02
I1111 01:08:01.896615 140264174335808 run_lib.py:153] step: 67100, training_loss: 1.07948e+02
I1111 01:08:12.695796 140264174335808 run_lib.py:153] step: 67150, training_loss: 1.27424e+02
I1111 01:08:22.437170 140264174335808 run_lib.py:153] step: 67200, training_loss: 1.34475e+02
I1111 01:08:33.076589 140264174335808 run_lib.py:153] step: 67250, training_loss: 1.28376e+02
I1111 01:08:43.007763 140264174335808 run_lib.py:153] step: 67300, training_loss: 1.24603e+02
I1111 01:08:52.528351 140264174335808 run_lib.py:153] step: 67350, training_loss: 1.56302e+02
I1111 01:09:02.898994 140264174335808 run_lib.py:153] step: 67400, training_loss: 1.23321e+02
I1111 01:09:12.780147 140264174335808 run_lib.py:153] step: 67450, training_loss: 1.56801e+02
I1111 01:09:23.449698 140264174335808 run_lib.py:153] step: 67500, training_loss: 1.19628e+02
I1111 01:09:34.357811 140264174335808 run_lib.py:153] step: 67550, training_loss: 1.20533e+02
I1111 01:09:44.286901 140264174335808 run_lib.py:153] step: 67600, training_loss: 1.25472e+02
I1111 01:09:54.427142 140264174335808 run_lib.py:153] step: 67650, training_loss: 1.27054e+02
I1111 01:10:04.175695 140264174335808 run_lib.py:153] step: 67700, training_loss: 9.93089e+01
I1111 01:10:14.444233 140264174335808 run_lib.py:153] step: 67750, training_loss: 1.46010e+02
I1111 01:10:24.806092 140264174335808 run_lib.py:153] step: 67800, training_loss: 1.24427e+02
I1111 01:10:34.387546 140264174335808 run_lib.py:153] step: 67850, training_loss: 1.30604e+02
I1111 01:10:43.814121 140264174335808 run_lib.py:153] step: 67900, training_loss: 1.55565e+02
I1111 01:10:53.980168 140264174335808 run_lib.py:153] step: 67950, training_loss: 1.09737e+02
I1111 01:11:03.999464 140264174335808 run_lib.py:153] step: 68000, training_loss: 1.20216e+02
I1111 01:11:14.079899 140264174335808 run_lib.py:153] step: 68050, training_loss: 1.06429e+02
I1111 01:11:24.336766 140264174335808 run_lib.py:153] step: 68100, training_loss: 1.28178e+02
I1111 01:11:34.924291 140264174335808 run_lib.py:153] step: 68150, training_loss: 1.34153e+02
I1111 01:11:44.618388 140264174335808 run_lib.py:153] step: 68200, training_loss: 1.33803e+02
I1111 01:11:54.547073 140264174335808 run_lib.py:153] step: 68250, training_loss: 1.25631e+02
I1111 01:12:05.064157 140264174335808 run_lib.py:153] step: 68300, training_loss: 1.42129e+02
I1111 01:12:15.388543 140264174335808 run_lib.py:153] step: 68350, training_loss: 1.38124e+02
I1111 01:12:25.455707 140264174335808 run_lib.py:153] step: 68400, training_loss: 1.92784e+02
I1111 01:12:35.699493 140264174335808 run_lib.py:153] step: 68450, training_loss: 1.42381e+02
I1111 01:12:46.180011 140264174335808 run_lib.py:153] step: 68500, training_loss: 1.35675e+02
I1111 01:12:56.738786 140264174335808 run_lib.py:153] step: 68550, training_loss: 1.51004e+02
I1111 01:13:06.853433 140264174335808 run_lib.py:153] step: 68600, training_loss: 1.33986e+02
I1111 01:13:17.155840 140264174335808 run_lib.py:153] step: 68650, training_loss: 1.31525e+02
I1111 01:13:27.709933 140264174335808 run_lib.py:153] step: 68700, training_loss: 1.24745e+02
I1111 01:13:38.000058 140264174335808 run_lib.py:153] step: 68750, training_loss: 9.95378e+01
I1111 01:13:48.082366 140264174335808 run_lib.py:153] step: 68800, training_loss: 1.12225e+02
I1111 01:13:57.700650 140264174335808 run_lib.py:153] step: 68850, training_loss: 1.16790e+02
I1111 01:14:07.382113 140264174335808 run_lib.py:153] step: 68900, training_loss: 1.45199e+02
I1111 01:14:17.132714 140264174335808 run_lib.py:153] step: 68950, training_loss: 1.24428e+02
I1111 01:14:27.378252 140264174335808 run_lib.py:153] step: 69000, training_loss: 1.55460e+02
I1111 01:14:37.880118 140264174335808 run_lib.py:153] step: 69050, training_loss: 1.01776e+02
I1111 01:14:47.815510 140264174335808 run_lib.py:153] step: 69100, training_loss: 9.25411e+01
I1111 01:14:57.869327 140264174335808 run_lib.py:153] step: 69150, training_loss: 9.45184e+01
I1111 01:15:08.313245 140264174335808 run_lib.py:153] step: 69200, training_loss: 1.46266e+02
I1111 01:15:18.175886 140264174335808 run_lib.py:153] step: 69250, training_loss: 1.49677e+02
I1111 01:15:28.468084 140264174335808 run_lib.py:153] step: 69300, training_loss: 1.32766e+02
I1111 01:15:38.708846 140264174335808 run_lib.py:153] step: 69350, training_loss: 1.55643e+02
I1111 01:15:48.405205 140264174335808 run_lib.py:153] step: 69400, training_loss: 1.41650e+02
I1111 01:15:58.435688 140264174335808 run_lib.py:153] step: 69450, training_loss: 1.39680e+02
I1111 01:16:08.790092 140264174335808 run_lib.py:153] step: 69500, training_loss: 1.44757e+02
I1111 01:16:18.380084 140264174335808 run_lib.py:153] step: 69550, training_loss: 1.36856e+02
I1111 01:16:28.329057 140264174335808 run_lib.py:153] step: 69600, training_loss: 1.22187e+02
I1111 01:16:37.869925 140264174335808 run_lib.py:153] step: 69650, training_loss: 1.10717e+02
I1111 01:16:47.701383 140264174335808 run_lib.py:153] step: 69700, training_loss: 1.61039e+02
I1111 01:16:57.829565 140264174335808 run_lib.py:153] step: 69750, training_loss: 1.01568e+02
I1111 01:17:08.141841 140264174335808 run_lib.py:153] step: 69800, training_loss: 1.43607e+02
I1111 01:17:17.900711 140264174335808 run_lib.py:153] step: 69850, training_loss: 1.39771e+02
I1111 01:17:27.958297 140264174335808 run_lib.py:153] step: 69900, training_loss: 1.21399e+02
I1111 01:17:38.002569 140264174335808 run_lib.py:153] step: 69950, training_loss: 1.25353e+02
I1111 01:17:48.113036 140264174335808 run_lib.py:153] step: 70000, training_loss: 1.08730e+02
I1111 01:17:48.682096 140264174335808 run_lib.py:166] step: 70000, eval_loss: 1.50584e+02
I1111 01:17:58.718569 140264174335808 run_lib.py:153] step: 70050, training_loss: 1.24355e+02
I1111 01:18:09.243336 140264174335808 run_lib.py:153] step: 70100, training_loss: 1.49099e+02
I1111 01:18:19.084163 140264174335808 run_lib.py:153] step: 70150, training_loss: 1.54396e+02
I1111 01:18:28.841877 140264174335808 run_lib.py:153] step: 70200, training_loss: 1.12354e+02
I1111 01:18:38.673105 140264174335808 run_lib.py:153] step: 70250, training_loss: 1.22477e+02
I1111 01:18:48.352898 140264174335808 run_lib.py:153] step: 70300, training_loss: 1.22113e+02
I1111 01:18:58.468926 140264174335808 run_lib.py:153] step: 70350, training_loss: 1.24385e+02
I1111 01:19:08.481188 140264174335808 run_lib.py:153] step: 70400, training_loss: 1.43826e+02
I1111 01:19:18.699132 140264174335808 run_lib.py:153] step: 70450, training_loss: 1.37283e+02
I1111 01:19:28.099960 140264174335808 run_lib.py:153] step: 70500, training_loss: 1.39398e+02
I1111 01:19:37.973234 140264174335808 run_lib.py:153] step: 70550, training_loss: 8.35452e+01
I1111 01:19:47.680274 140264174335808 run_lib.py:153] step: 70600, training_loss: 1.51444e+02
I1111 01:19:58.095503 140264174335808 run_lib.py:153] step: 70650, training_loss: 1.22542e+02
I1111 01:20:07.808964 140264174335808 run_lib.py:153] step: 70700, training_loss: 1.16978e+02
I1111 01:20:17.527984 140264174335808 run_lib.py:153] step: 70750, training_loss: 1.37693e+02
I1111 01:20:27.821732 140264174335808 run_lib.py:153] step: 70800, training_loss: 1.19531e+02
I1111 01:20:37.971319 140264174335808 run_lib.py:153] step: 70850, training_loss: 1.30837e+02
I1111 01:20:48.641070 140264174335808 run_lib.py:153] step: 70900, training_loss: 1.24241e+02
I1111 01:20:59.146407 140264174335808 run_lib.py:153] step: 70950, training_loss: 1.03768e+02
I1111 01:21:10.181493 140264174335808 run_lib.py:153] step: 71000, training_loss: 1.34833e+02
I1111 01:21:20.274175 140264174335808 run_lib.py:153] step: 71050, training_loss: 1.39025e+02
I1111 01:21:30.132741 140264174335808 run_lib.py:153] step: 71100, training_loss: 1.32530e+02
I1111 01:21:39.735589 140264174335808 run_lib.py:153] step: 71150, training_loss: 1.13809e+02
I1111 01:21:49.475076 140264174335808 run_lib.py:153] step: 71200, training_loss: 1.19217e+02
I1111 01:21:58.941008 140264174335808 run_lib.py:153] step: 71250, training_loss: 1.54771e+02
I1111 01:22:08.619477 140264174335808 run_lib.py:153] step: 71300, training_loss: 1.45698e+02
I1111 01:22:18.965987 140264174335808 run_lib.py:153] step: 71350, training_loss: 1.10932e+02
I1111 01:22:28.944676 140264174335808 run_lib.py:153] step: 71400, training_loss: 1.30076e+02
I1111 01:22:39.088205 140264174335808 run_lib.py:153] step: 71450, training_loss: 9.90345e+01
I1111 01:22:48.881681 140264174335808 run_lib.py:153] step: 71500, training_loss: 1.47948e+02
I1111 01:22:58.311739 140264174335808 run_lib.py:153] step: 71550, training_loss: 1.30735e+02
I1111 01:23:08.278131 140264174335808 run_lib.py:153] step: 71600, training_loss: 1.55455e+02
I1111 01:23:18.944112 140264174335808 run_lib.py:153] step: 71650, training_loss: 1.26536e+02
I1111 01:23:29.475533 140264174335808 run_lib.py:153] step: 71700, training_loss: 1.58477e+02
I1111 01:23:40.308975 140264174335808 run_lib.py:153] step: 71750, training_loss: 1.35796e+02
I1111 01:23:50.425211 140264174335808 run_lib.py:153] step: 71800, training_loss: 1.61763e+02
I1111 01:24:00.126561 140264174335808 run_lib.py:153] step: 71850, training_loss: 1.25952e+02
I1111 01:24:09.914116 140264174335808 run_lib.py:153] step: 71900, training_loss: 1.40223e+02
I1111 01:24:20.494232 140264174335808 run_lib.py:153] step: 71950, training_loss: 1.45414e+02
I1111 01:24:30.539148 140264174335808 run_lib.py:153] step: 72000, training_loss: 1.29757e+02
I1111 01:24:41.031403 140264174335808 run_lib.py:153] step: 72050, training_loss: 1.58492e+02
I1111 01:24:51.026891 140264174335808 run_lib.py:153] step: 72100, training_loss: 1.30722e+02
I1111 01:25:01.676118 140264174335808 run_lib.py:153] step: 72150, training_loss: 1.26314e+02
I1111 01:25:12.425360 140264174335808 run_lib.py:153] step: 72200, training_loss: 1.34618e+02
I1111 01:25:22.959747 140264174335808 run_lib.py:153] step: 72250, training_loss: 1.67723e+02
I1111 01:25:33.438051 140264174335808 run_lib.py:153] step: 72300, training_loss: 1.21187e+02
I1111 01:25:43.829485 140264174335808 run_lib.py:153] step: 72350, training_loss: 1.31863e+02
I1111 01:25:54.419732 140264174335808 run_lib.py:153] step: 72400, training_loss: 1.47117e+02
I1111 01:26:04.562937 140264174335808 run_lib.py:153] step: 72450, training_loss: 1.28389e+02
I1111 01:26:14.773472 140264174335808 run_lib.py:153] step: 72500, training_loss: 1.50227e+02
I1111 01:26:24.727639 140264174335808 run_lib.py:153] step: 72550, training_loss: 1.20719e+02
I1111 01:26:35.012710 140264174335808 run_lib.py:153] step: 72600, training_loss: 1.24721e+02
I1111 01:26:45.122832 140264174335808 run_lib.py:153] step: 72650, training_loss: 1.38949e+02
I1111 01:26:55.105223 140264174335808 run_lib.py:153] step: 72700, training_loss: 1.60052e+02
I1111 01:27:04.995738 140264174335808 run_lib.py:153] step: 72750, training_loss: 1.26489e+02
I1111 01:27:14.848912 140264174335808 run_lib.py:153] step: 72800, training_loss: 1.23449e+02
I1111 01:27:25.021569 140264174335808 run_lib.py:153] step: 72850, training_loss: 1.69874e+02
I1111 01:27:34.672838 140264174335808 run_lib.py:153] step: 72900, training_loss: 1.18461e+02
I1111 01:27:45.505596 140264174335808 run_lib.py:153] step: 72950, training_loss: 1.79072e+02
I1111 01:27:55.782049 140264174335808 run_lib.py:153] step: 73000, training_loss: 9.61601e+01
I1111 01:28:06.289532 140264174335808 run_lib.py:153] step: 73050, training_loss: 1.32444e+02
I1111 01:28:16.773350 140264174335808 run_lib.py:153] step: 73100, training_loss: 1.58258e+02
I1111 01:28:26.858763 140264174335808 run_lib.py:153] step: 73150, training_loss: 1.20151e+02
I1111 01:28:36.665002 140264174335808 run_lib.py:153] step: 73200, training_loss: 1.33499e+02
I1111 01:28:46.297375 140264174335808 run_lib.py:153] step: 73250, training_loss: 1.33666e+02
I1111 01:28:56.074879 140264174335808 run_lib.py:153] step: 73300, training_loss: 1.52322e+02
I1111 01:29:06.418002 140264174335808 run_lib.py:153] step: 73350, training_loss: 1.09869e+02
I1111 01:29:16.721549 140264174335808 run_lib.py:153] step: 73400, training_loss: 1.11522e+02
I1111 01:29:27.313524 140264174335808 run_lib.py:153] step: 73450, training_loss: 9.53542e+01
I1111 01:29:37.596490 140264174335808 run_lib.py:153] step: 73500, training_loss: 1.40983e+02
I1111 01:29:47.277773 140264174335808 run_lib.py:153] step: 73550, training_loss: 1.21951e+02
I1111 01:29:57.870349 140264174335808 run_lib.py:153] step: 73600, training_loss: 1.37545e+02
I1111 01:30:08.366618 140264174335808 run_lib.py:153] step: 73650, training_loss: 1.33160e+02
I1111 01:30:18.473777 140264174335808 run_lib.py:153] step: 73700, training_loss: 1.48208e+02
I1111 01:30:28.829786 140264174335808 run_lib.py:153] step: 73750, training_loss: 1.36534e+02
I1111 01:30:39.344535 140264174335808 run_lib.py:153] step: 73800, training_loss: 1.32010e+02
I1111 01:30:49.057693 140264174335808 run_lib.py:153] step: 73850, training_loss: 1.48857e+02
I1111 01:30:58.764514 140264174335808 run_lib.py:153] step: 73900, training_loss: 1.60468e+02
I1111 01:31:08.321933 140264174335808 run_lib.py:153] step: 73950, training_loss: 1.57126e+02
I1111 01:31:18.263746 140264174335808 run_lib.py:153] step: 74000, training_loss: 1.34988e+02
I1111 01:31:28.335122 140264174335808 run_lib.py:153] step: 74050, training_loss: 1.30615e+02
I1111 01:31:38.994426 140264174335808 run_lib.py:153] step: 74100, training_loss: 1.38532e+02
I1111 01:31:48.893545 140264174335808 run_lib.py:153] step: 74150, training_loss: 1.25519e+02
I1111 01:31:59.258996 140264174335808 run_lib.py:153] step: 74200, training_loss: 1.47052e+02
I1111 01:32:08.596221 140264174335808 run_lib.py:153] step: 74250, training_loss: 1.37875e+02
I1111 01:32:18.057231 140264174335808 run_lib.py:153] step: 74300, training_loss: 1.25654e+02
I1111 01:32:27.715107 140264174335808 run_lib.py:153] step: 74350, training_loss: 1.29893e+02
I1111 01:32:38.396769 140264174335808 run_lib.py:153] step: 74400, training_loss: 1.35628e+02
I1111 01:32:48.455586 140264174335808 run_lib.py:153] step: 74450, training_loss: 1.28309e+02
I1111 01:32:58.643833 140264174335808 run_lib.py:153] step: 74500, training_loss: 1.21169e+02
I1111 01:33:08.624438 140264174335808 run_lib.py:153] step: 74550, training_loss: 1.29045e+02
I1111 01:33:18.653263 140264174335808 run_lib.py:153] step: 74600, training_loss: 1.46910e+02
I1111 01:33:28.575947 140264174335808 run_lib.py:153] step: 74650, training_loss: 1.18121e+02
I1111 01:33:38.581048 140264174335808 run_lib.py:153] step: 74700, training_loss: 1.10359e+02
I1111 01:33:49.158264 140264174335808 run_lib.py:153] step: 74750, training_loss: 1.51198e+02
I1111 01:33:59.515226 140264174335808 run_lib.py:153] step: 74800, training_loss: 1.20688e+02
I1111 01:34:09.468640 140264174335808 run_lib.py:153] step: 74850, training_loss: 1.44392e+02
I1111 01:34:19.171252 140264174335808 run_lib.py:153] step: 74900, training_loss: 1.58217e+02
I1111 01:34:29.148721 140264174335808 run_lib.py:153] step: 74950, training_loss: 1.42508e+02
I1111 01:34:39.255199 140264174335808 run_lib.py:153] step: 75000, training_loss: 8.33186e+01
I1111 01:34:39.368508 140264174335808 run_lib.py:166] step: 75000, eval_loss: 1.05007e+02
I1111 01:34:50.040535 140264174335808 run_lib.py:153] step: 75050, training_loss: 1.44541e+02
I1111 01:35:00.335612 140264174335808 run_lib.py:153] step: 75100, training_loss: 1.18182e+02
I1111 01:35:10.336406 140264174335808 run_lib.py:153] step: 75150, training_loss: 1.37409e+02
I1111 01:35:20.284893 140264174335808 run_lib.py:153] step: 75200, training_loss: 1.59281e+02
I1111 01:35:30.900166 140264174335808 run_lib.py:153] step: 75250, training_loss: 8.96465e+01
I1111 01:35:40.982913 140264174335808 run_lib.py:153] step: 75300, training_loss: 1.44463e+02
I1111 01:35:50.899693 140264174335808 run_lib.py:153] step: 75350, training_loss: 1.26990e+02
I1111 01:36:00.809254 140264174335808 run_lib.py:153] step: 75400, training_loss: 1.34395e+02
I1111 01:36:10.090476 140264174335808 run_lib.py:153] step: 75450, training_loss: 1.79221e+02
I1111 01:36:19.943201 140264174335808 run_lib.py:153] step: 75500, training_loss: 1.15131e+02
I1111 01:36:29.786310 140264174335808 run_lib.py:153] step: 75550, training_loss: 1.56633e+02
I1111 01:36:39.567760 140264174335808 run_lib.py:153] step: 75600, training_loss: 1.04595e+02
I1111 01:36:48.898254 140264174335808 run_lib.py:153] step: 75650, training_loss: 1.20322e+02
I1111 01:36:59.209627 140264174335808 run_lib.py:153] step: 75700, training_loss: 1.32440e+02
I1111 01:37:09.967892 140264174335808 run_lib.py:153] step: 75750, training_loss: 1.26046e+02
I1111 01:37:19.930107 140264174335808 run_lib.py:153] step: 75800, training_loss: 1.39088e+02
I1111 01:37:29.951183 140264174335808 run_lib.py:153] step: 75850, training_loss: 1.51408e+02
I1111 01:37:40.268419 140264174335808 run_lib.py:153] step: 75900, training_loss: 1.05033e+02
I1111 01:37:50.690325 140264174335808 run_lib.py:153] step: 75950, training_loss: 1.26118e+02
I1111 01:38:01.676600 140264174335808 run_lib.py:153] step: 76000, training_loss: 1.43153e+02
I1111 01:38:11.116950 140264174335808 run_lib.py:153] step: 76050, training_loss: 1.43139e+02
I1111 01:38:21.176984 140264174335808 run_lib.py:153] step: 76100, training_loss: 1.59710e+02
I1111 01:38:31.267048 140264174335808 run_lib.py:153] step: 76150, training_loss: 1.38297e+02
I1111 01:38:41.026866 140264174335808 run_lib.py:153] step: 76200, training_loss: 1.41391e+02
I1111 01:38:50.681474 140264174335808 run_lib.py:153] step: 76250, training_loss: 1.13950e+02
I1111 01:39:00.737411 140264174335808 run_lib.py:153] step: 76300, training_loss: 1.19006e+02
I1111 01:39:11.096228 140264174335808 run_lib.py:153] step: 76350, training_loss: 9.30236e+01
I1111 01:39:20.730191 140264174335808 run_lib.py:153] step: 76400, training_loss: 1.26261e+02
I1111 01:39:30.411337 140264174335808 run_lib.py:153] step: 76450, training_loss: 1.05010e+02
I1111 01:39:40.998807 140264174335808 run_lib.py:153] step: 76500, training_loss: 1.41264e+02
I1111 01:39:50.752168 140264174335808 run_lib.py:153] step: 76550, training_loss: 1.26112e+02
I1111 01:40:01.018525 140264174335808 run_lib.py:153] step: 76600, training_loss: 1.41595e+02
I1111 01:40:10.789845 140264174335808 run_lib.py:153] step: 76650, training_loss: 1.24542e+02
I1111 01:40:20.762729 140264174335808 run_lib.py:153] step: 76700, training_loss: 1.00316e+02
I1111 01:40:30.764015 140264174335808 run_lib.py:153] step: 76750, training_loss: 1.57922e+02
I1111 01:40:40.643365 140264174335808 run_lib.py:153] step: 76800, training_loss: 1.24914e+02
I1111 01:40:50.989747 140264174335808 run_lib.py:153] step: 76850, training_loss: 1.54403e+02
I1111 01:41:01.552694 140264174335808 run_lib.py:153] step: 76900, training_loss: 1.39048e+02
I1111 01:41:11.735119 140264174335808 run_lib.py:153] step: 76950, training_loss: 1.08288e+02
I1111 01:41:21.699761 140264174335808 run_lib.py:153] step: 77000, training_loss: 1.07767e+02
I1111 01:41:31.820422 140264174335808 run_lib.py:153] step: 77050, training_loss: 1.44120e+02
I1111 01:41:41.815581 140264174335808 run_lib.py:153] step: 77100, training_loss: 1.21430e+02
I1111 01:41:51.576946 140264174335808 run_lib.py:153] step: 77150, training_loss: 1.31191e+02
I1111 01:42:00.928759 140264174335808 run_lib.py:153] step: 77200, training_loss: 1.19806e+02
I1111 01:42:10.547200 140264174335808 run_lib.py:153] step: 77250, training_loss: 1.56073e+02
I1111 01:42:20.486078 140264174335808 run_lib.py:153] step: 77300, training_loss: 1.18184e+02
I1111 01:42:30.693591 140264174335808 run_lib.py:153] step: 77350, training_loss: 1.22480e+02
I1111 01:42:40.387452 140264174335808 run_lib.py:153] step: 77400, training_loss: 1.26615e+02
I1111 01:42:50.443766 140264174335808 run_lib.py:153] step: 77450, training_loss: 1.50537e+02
I1111 01:42:59.980126 140264174335808 run_lib.py:153] step: 77500, training_loss: 1.19347e+02
I1111 01:43:09.766323 140264174335808 run_lib.py:153] step: 77550, training_loss: 1.27475e+02
I1111 01:43:19.193578 140264174335808 run_lib.py:153] step: 77600, training_loss: 1.31367e+02
I1111 01:43:28.533077 140264174335808 run_lib.py:153] step: 77650, training_loss: 1.43234e+02
I1111 01:43:38.444724 140264174335808 run_lib.py:153] step: 77700, training_loss: 1.25059e+02
I1111 01:43:48.482273 140264174335808 run_lib.py:153] step: 77750, training_loss: 1.11550e+02
I1111 01:43:58.107334 140264174335808 run_lib.py:153] step: 77800, training_loss: 1.38147e+02
I1111 01:44:07.510234 140264174335808 run_lib.py:153] step: 77850, training_loss: 1.18042e+02
I1111 01:44:16.798065 140264174335808 run_lib.py:153] step: 77900, training_loss: 1.36654e+02
I1111 01:44:26.977829 140264174335808 run_lib.py:153] step: 77950, training_loss: 1.30845e+02
I1111 01:44:36.581257 140264174335808 run_lib.py:153] step: 78000, training_loss: 1.18855e+02
I1111 01:44:46.571608 140264174335808 run_lib.py:153] step: 78050, training_loss: 1.17245e+02
I1111 01:44:56.468719 140264174335808 run_lib.py:153] step: 78100, training_loss: 1.25385e+02
I1111 01:45:06.029749 140264174335808 run_lib.py:153] step: 78150, training_loss: 1.24427e+02
I1111 01:45:15.739860 140264174335808 run_lib.py:153] step: 78200, training_loss: 9.64790e+01
I1111 01:45:25.340412 140264174335808 run_lib.py:153] step: 78250, training_loss: 1.51844e+02
I1111 01:45:35.585059 140264174335808 run_lib.py:153] step: 78300, training_loss: 1.63326e+02
I1111 01:45:46.482145 140264174335808 run_lib.py:153] step: 78350, training_loss: 1.01195e+02
I1111 01:45:56.011688 140264174335808 run_lib.py:153] step: 78400, training_loss: 1.53882e+02
I1111 01:46:05.707298 140264174335808 run_lib.py:153] step: 78450, training_loss: 9.70193e+01
I1111 01:46:15.792734 140264174335808 run_lib.py:153] step: 78500, training_loss: 1.28707e+02
I1111 01:46:26.259867 140264174335808 run_lib.py:153] step: 78550, training_loss: 1.15811e+02
I1111 01:46:35.684993 140264174335808 run_lib.py:153] step: 78600, training_loss: 1.58406e+02
I1111 01:46:45.107346 140264174335808 run_lib.py:153] step: 78650, training_loss: 1.24036e+02
I1111 01:46:55.481852 140264174335808 run_lib.py:153] step: 78700, training_loss: 1.24731e+02
I1111 01:47:04.982497 140264174335808 run_lib.py:153] step: 78750, training_loss: 1.24606e+02
I1111 01:47:14.512682 140264174335808 run_lib.py:153] step: 78800, training_loss: 1.41420e+02
I1111 01:47:24.442475 140264174335808 run_lib.py:153] step: 78850, training_loss: 1.42075e+02
I1111 01:47:34.523427 140264174335808 run_lib.py:153] step: 78900, training_loss: 1.23609e+02
I1111 01:47:45.101474 140264174335808 run_lib.py:153] step: 78950, training_loss: 1.38108e+02
I1111 01:47:55.201457 140264174335808 run_lib.py:153] step: 79000, training_loss: 1.20236e+02
I1111 01:48:05.025166 140264174335808 run_lib.py:153] step: 79050, training_loss: 1.52816e+02
I1111 01:48:14.913376 140264174335808 run_lib.py:153] step: 79100, training_loss: 1.43530e+02
I1111 01:48:24.319244 140264174335808 run_lib.py:153] step: 79150, training_loss: 1.13429e+02
I1111 01:48:33.843914 140264174335808 run_lib.py:153] step: 79200, training_loss: 1.44085e+02
I1111 01:48:43.706678 140264174335808 run_lib.py:153] step: 79250, training_loss: 1.38632e+02
I1111 01:48:53.453647 140264174335808 run_lib.py:153] step: 79300, training_loss: 1.19584e+02
I1111 01:49:02.685053 140264174335808 run_lib.py:153] step: 79350, training_loss: 1.63797e+02
I1111 01:49:13.152562 140264174335808 run_lib.py:153] step: 79400, training_loss: 1.25391e+02
I1111 01:49:23.644329 140264174335808 run_lib.py:153] step: 79450, training_loss: 1.42490e+02
I1111 01:49:33.854146 140264174335808 run_lib.py:153] step: 79500, training_loss: 1.34156e+02
I1111 01:49:43.763359 140264174335808 run_lib.py:153] step: 79550, training_loss: 1.34770e+02
I1111 01:49:54.324308 140264174335808 run_lib.py:153] step: 79600, training_loss: 1.51356e+02
I1111 01:50:04.620057 140264174335808 run_lib.py:153] step: 79650, training_loss: 1.17435e+02
I1111 01:50:14.065814 140264174335808 run_lib.py:153] step: 79700, training_loss: 1.20098e+02
I1111 01:50:23.331636 140264174335808 run_lib.py:153] step: 79750, training_loss: 1.65442e+02
I1111 01:50:34.169229 140264174335808 run_lib.py:153] step: 79800, training_loss: 1.16537e+02
I1111 01:50:44.060617 140264174335808 run_lib.py:153] step: 79850, training_loss: 1.28889e+02
I1111 01:50:53.866302 140264174335808 run_lib.py:153] step: 79900, training_loss: 1.12634e+02
I1111 01:51:03.897154 140264174335808 run_lib.py:153] step: 79950, training_loss: 1.46085e+02
I1111 01:51:13.610386 140264174335808 run_lib.py:153] step: 80000, training_loss: 1.22900e+02
I1111 01:51:14.152074 140264174335808 run_lib.py:166] step: 80000, eval_loss: 1.09406e+02
I1111 01:51:24.780664 140264174335808 run_lib.py:153] step: 80050, training_loss: 1.39251e+02
I1111 01:51:36.235129 140264174335808 run_lib.py:153] step: 80100, training_loss: 1.24789e+02
I1111 01:51:47.066280 140264174335808 run_lib.py:153] step: 80150, training_loss: 1.31973e+02
I1111 01:51:57.104517 140264174335808 run_lib.py:153] step: 80200, training_loss: 1.31782e+02
I1111 01:52:07.393435 140264174335808 run_lib.py:153] step: 80250, training_loss: 1.25091e+02
I1111 01:52:18.259752 140264174335808 run_lib.py:153] step: 80300, training_loss: 1.27595e+02
I1111 01:52:28.451987 140264174335808 run_lib.py:153] step: 80350, training_loss: 1.05740e+02
I1111 01:52:38.805036 140264174335808 run_lib.py:153] step: 80400, training_loss: 1.55246e+02
I1111 01:52:48.611902 140264174335808 run_lib.py:153] step: 80450, training_loss: 1.25928e+02
I1111 01:52:58.968209 140264174335808 run_lib.py:153] step: 80500, training_loss: 1.27204e+02
I1111 01:53:08.999320 140264174335808 run_lib.py:153] step: 80550, training_loss: 1.75002e+02
I1111 01:53:18.838564 140264174335808 run_lib.py:153] step: 80600, training_loss: 1.63270e+02
I1111 01:53:28.977039 140264174335808 run_lib.py:153] step: 80650, training_loss: 1.51349e+02
I1111 01:53:39.026640 140264174335808 run_lib.py:153] step: 80700, training_loss: 1.30669e+02
I1111 01:53:49.727663 140264174335808 run_lib.py:153] step: 80750, training_loss: 1.24402e+02
I1111 01:53:59.384939 140264174335808 run_lib.py:153] step: 80800, training_loss: 1.41359e+02
I1111 01:54:09.623879 140264174335808 run_lib.py:153] step: 80850, training_loss: 1.52052e+02
I1111 01:54:20.055688 140264174335808 run_lib.py:153] step: 80900, training_loss: 1.31313e+02
I1111 01:54:30.414209 140264174335808 run_lib.py:153] step: 80950, training_loss: 1.04737e+02
I1111 01:54:40.759434 140264174335808 run_lib.py:153] step: 81000, training_loss: 1.16219e+02
I1111 01:54:50.212079 140264174335808 run_lib.py:153] step: 81050, training_loss: 1.24670e+02
I1111 01:55:00.544835 140264174335808 run_lib.py:153] step: 81100, training_loss: 1.59244e+02
I1111 01:55:11.272146 140264174335808 run_lib.py:153] step: 81150, training_loss: 1.44297e+02
I1111 01:55:21.524477 140264174335808 run_lib.py:153] step: 81200, training_loss: 1.52573e+02
I1111 01:55:31.049836 140264174335808 run_lib.py:153] step: 81250, training_loss: 1.20141e+02
I1111 01:55:41.313566 140264174335808 run_lib.py:153] step: 81300, training_loss: 1.46266e+02
I1111 01:55:51.619136 140264174335808 run_lib.py:153] step: 81350, training_loss: 1.86817e+02
I1111 01:56:01.698195 140264174335808 run_lib.py:153] step: 81400, training_loss: 1.25280e+02
I1111 01:56:12.162856 140264174335808 run_lib.py:153] step: 81450, training_loss: 1.41635e+02
I1111 01:56:22.121201 140264174335808 run_lib.py:153] step: 81500, training_loss: 1.18251e+02
I1111 01:56:31.796252 140264174335808 run_lib.py:153] step: 81550, training_loss: 1.34517e+02
I1111 01:56:42.289246 140264174335808 run_lib.py:153] step: 81600, training_loss: 1.16600e+02
I1111 01:56:53.011728 140264174335808 run_lib.py:153] step: 81650, training_loss: 1.28478e+02
I1111 01:57:02.763630 140264174335808 run_lib.py:153] step: 81700, training_loss: 1.10006e+02
I1111 01:57:13.267411 140264174335808 run_lib.py:153] step: 81750, training_loss: 1.53440e+02
I1111 01:57:23.032047 140264174335808 run_lib.py:153] step: 81800, training_loss: 1.34065e+02
I1111 01:57:33.437990 140264174335808 run_lib.py:153] step: 81850, training_loss: 1.18076e+02
I1111 01:57:44.054580 140264174335808 run_lib.py:153] step: 81900, training_loss: 1.20397e+02
I1111 01:57:53.754876 140264174335808 run_lib.py:153] step: 81950, training_loss: 1.35873e+02
I1111 01:58:03.881288 140264174335808 run_lib.py:153] step: 82000, training_loss: 1.25523e+02
I1111 01:58:13.602507 140264174335808 run_lib.py:153] step: 82050, training_loss: 1.24884e+02
I1111 01:58:23.236815 140264174335808 run_lib.py:153] step: 82100, training_loss: 1.31467e+02
I1111 01:58:34.243066 140264174335808 run_lib.py:153] step: 82150, training_loss: 1.47883e+02
I1111 01:58:44.954171 140264174335808 run_lib.py:153] step: 82200, training_loss: 1.19096e+02
I1111 01:58:54.968857 140264174335808 run_lib.py:153] step: 82250, training_loss: 1.40874e+02
I1111 01:59:05.092072 140264174335808 run_lib.py:153] step: 82300, training_loss: 1.30478e+02
I1111 01:59:15.740126 140264174335808 run_lib.py:153] step: 82350, training_loss: 1.50499e+02
I1111 01:59:26.089065 140264174335808 run_lib.py:153] step: 82400, training_loss: 1.19704e+02
I1111 01:59:36.154696 140264174335808 run_lib.py:153] step: 82450, training_loss: 1.40730e+02
I1111 01:59:46.084245 140264174335808 run_lib.py:153] step: 82500, training_loss: 1.18800e+02
I1111 01:59:56.299611 140264174335808 run_lib.py:153] step: 82550, training_loss: 1.14444e+02
I1111 02:00:06.472426 140264174335808 run_lib.py:153] step: 82600, training_loss: 1.47327e+02
I1111 02:00:16.830842 140264174335808 run_lib.py:153] step: 82650, training_loss: 1.22712e+02
I1111 02:00:27.027670 140264174335808 run_lib.py:153] step: 82700, training_loss: 1.42608e+02
I1111 02:00:36.730552 140264174335808 run_lib.py:153] step: 82750, training_loss: 1.43543e+02
I1111 02:00:47.338121 140264174335808 run_lib.py:153] step: 82800, training_loss: 1.45326e+02
I1111 02:00:57.677617 140264174335808 run_lib.py:153] step: 82850, training_loss: 1.65287e+02
I1111 02:01:07.380342 140264174335808 run_lib.py:153] step: 82900, training_loss: 1.34927e+02
I1111 02:01:17.085306 140264174335808 run_lib.py:153] step: 82950, training_loss: 1.31736e+02
I1111 02:01:27.249882 140264174335808 run_lib.py:153] step: 83000, training_loss: 1.47705e+02
I1111 02:01:37.646345 140264174335808 run_lib.py:153] step: 83050, training_loss: 1.11443e+02
I1111 02:01:47.787832 140264174335808 run_lib.py:153] step: 83100, training_loss: 1.46833e+02
I1111 02:01:57.691502 140264174335808 run_lib.py:153] step: 83150, training_loss: 1.18917e+02
I1111 02:02:07.316328 140264174335808 run_lib.py:153] step: 83200, training_loss: 1.36627e+02
I1111 02:02:17.574501 140264174335808 run_lib.py:153] step: 83250, training_loss: 1.17066e+02
I1111 02:02:28.400339 140264174335808 run_lib.py:153] step: 83300, training_loss: 1.27341e+02
I1111 02:02:38.563688 140264174335808 run_lib.py:153] step: 83350, training_loss: 8.57972e+01
I1111 02:02:48.268643 140264174335808 run_lib.py:153] step: 83400, training_loss: 1.48103e+02
I1111 02:02:58.323749 140264174335808 run_lib.py:153] step: 83450, training_loss: 1.36096e+02
I1111 02:03:08.884614 140264174335808 run_lib.py:153] step: 83500, training_loss: 1.31272e+02
I1111 02:03:19.430945 140264174335808 run_lib.py:153] step: 83550, training_loss: 1.11610e+02
I1111 02:03:30.289762 140264174335808 run_lib.py:153] step: 83600, training_loss: 1.09390e+02
I1111 02:03:39.803987 140264174335808 run_lib.py:153] step: 83650, training_loss: 1.33625e+02
I1111 02:03:49.880720 140264174335808 run_lib.py:153] step: 83700, training_loss: 1.14926e+02
I1111 02:04:00.351747 140264174335808 run_lib.py:153] step: 83750, training_loss: 1.23917e+02
I1111 02:04:10.899891 140264174335808 run_lib.py:153] step: 83800, training_loss: 1.25691e+02
I1111 02:04:20.680337 140264174335808 run_lib.py:153] step: 83850, training_loss: 1.36325e+02
I1111 02:04:31.316079 140264174335808 run_lib.py:153] step: 83900, training_loss: 1.78210e+02
I1111 02:04:41.747296 140264174335808 run_lib.py:153] step: 83950, training_loss: 1.23319e+02
I1111 02:04:51.745107 140264174335808 run_lib.py:153] step: 84000, training_loss: 1.37780e+02
I1111 02:05:01.960568 140264174335808 run_lib.py:153] step: 84050, training_loss: 1.46159e+02
I1111 02:05:12.331866 140264174335808 run_lib.py:153] step: 84100, training_loss: 1.70107e+02
I1111 02:05:22.215330 140264174335808 run_lib.py:153] step: 84150, training_loss: 1.14732e+02
I1111 02:05:31.885455 140264174335808 run_lib.py:153] step: 84200, training_loss: 1.18895e+02
I1111 02:05:41.968272 140264174335808 run_lib.py:153] step: 84250, training_loss: 1.20783e+02
I1111 02:05:52.036742 140264174335808 run_lib.py:153] step: 84300, training_loss: 1.06127e+02
I1111 02:06:01.508168 140264174335808 run_lib.py:153] step: 84350, training_loss: 1.41215e+02
I1111 02:06:11.593331 140264174335808 run_lib.py:153] step: 84400, training_loss: 1.03031e+02
I1111 02:06:21.508557 140264174335808 run_lib.py:153] step: 84450, training_loss: 1.05721e+02
I1111 02:06:31.876765 140264174335808 run_lib.py:153] step: 84500, training_loss: 1.33075e+02
I1111 02:06:42.137809 140264174335808 run_lib.py:153] step: 84550, training_loss: 1.37002e+02
I1111 02:06:51.723591 140264174335808 run_lib.py:153] step: 84600, training_loss: 1.41823e+02
I1111 02:07:02.686118 140264174335808 run_lib.py:153] step: 84650, training_loss: 1.16759e+02
I1111 02:07:13.881352 140264174335808 run_lib.py:153] step: 84700, training_loss: 1.31413e+02
I1111 02:07:23.878654 140264174335808 run_lib.py:153] step: 84750, training_loss: 1.23603e+02
I1111 02:07:34.052546 140264174335808 run_lib.py:153] step: 84800, training_loss: 1.30340e+02
I1111 02:07:44.143894 140264174335808 run_lib.py:153] step: 84850, training_loss: 1.54281e+02
I1111 02:07:54.184885 140264174335808 run_lib.py:153] step: 84900, training_loss: 1.37998e+02
I1111 02:08:04.022391 140264174335808 run_lib.py:153] step: 84950, training_loss: 1.43907e+02
I1111 02:08:13.801106 140264174335808 run_lib.py:153] step: 85000, training_loss: 1.10854e+02
I1111 02:08:13.898557 140264174335808 run_lib.py:166] step: 85000, eval_loss: 9.60558e+01
I1111 02:08:24.357290 140264174335808 run_lib.py:153] step: 85050, training_loss: 1.89053e+02
I1111 02:08:34.611332 140264174335808 run_lib.py:153] step: 85100, training_loss: 1.18828e+02
I1111 02:08:44.871733 140264174335808 run_lib.py:153] step: 85150, training_loss: 1.44366e+02
I1111 02:08:55.089580 140264174335808 run_lib.py:153] step: 85200, training_loss: 1.27574e+02
I1111 02:09:05.072306 140264174335808 run_lib.py:153] step: 85250, training_loss: 1.25804e+02
I1111 02:09:15.301860 140264174335808 run_lib.py:153] step: 85300, training_loss: 1.27112e+02
I1111 02:09:25.349301 140264174335808 run_lib.py:153] step: 85350, training_loss: 1.55484e+02
I1111 02:09:35.737863 140264174335808 run_lib.py:153] step: 85400, training_loss: 1.50486e+02
I1111 02:09:45.547000 140264174335808 run_lib.py:153] step: 85450, training_loss: 1.03564e+02
I1111 02:09:56.048695 140264174335808 run_lib.py:153] step: 85500, training_loss: 1.13223e+02
I1111 02:10:06.216324 140264174335808 run_lib.py:153] step: 85550, training_loss: 1.30025e+02
I1111 02:10:16.054571 140264174335808 run_lib.py:153] step: 85600, training_loss: 1.31179e+02
I1111 02:10:25.813715 140264174335808 run_lib.py:153] step: 85650, training_loss: 1.29114e+02
I1111 02:10:35.368141 140264174335808 run_lib.py:153] step: 85700, training_loss: 1.55096e+02
I1111 02:10:46.076833 140264174335808 run_lib.py:153] step: 85750, training_loss: 1.19188e+02
I1111 02:10:56.454157 140264174335808 run_lib.py:153] step: 85800, training_loss: 1.42305e+02
I1111 02:11:06.475450 140264174335808 run_lib.py:153] step: 85850, training_loss: 1.19625e+02
I1111 02:11:16.841852 140264174335808 run_lib.py:153] step: 85900, training_loss: 1.22836e+02
I1111 02:11:26.204042 140264174335808 run_lib.py:153] step: 85950, training_loss: 1.15517e+02
I1111 02:11:37.130509 140264174335808 run_lib.py:153] step: 86000, training_loss: 1.26214e+02
I1111 02:11:47.283764 140264174335808 run_lib.py:153] step: 86050, training_loss: 1.36089e+02
I1111 02:11:57.335427 140264174335808 run_lib.py:153] step: 86100, training_loss: 1.05188e+02
I1111 02:12:07.461423 140264174335808 run_lib.py:153] step: 86150, training_loss: 1.32898e+02
I1111 02:12:17.533751 140264174335808 run_lib.py:153] step: 86200, training_loss: 1.35262e+02
I1111 02:12:27.399521 140264174335808 run_lib.py:153] step: 86250, training_loss: 9.86351e+01
I1111 02:12:37.496859 140264174335808 run_lib.py:153] step: 86300, training_loss: 1.75324e+02
I1111 02:12:47.656587 140264174335808 run_lib.py:153] step: 86350, training_loss: 1.13649e+02
I1111 02:12:58.362359 140264174335808 run_lib.py:153] step: 86400, training_loss: 1.13388e+02
I1111 02:13:08.771810 140264174335808 run_lib.py:153] step: 86450, training_loss: 1.31512e+02
I1111 02:13:19.347143 140264174335808 run_lib.py:153] step: 86500, training_loss: 1.58972e+02
I1111 02:13:30.276240 140264174335808 run_lib.py:153] step: 86550, training_loss: 1.43686e+02
I1111 02:13:40.590844 140264174335808 run_lib.py:153] step: 86600, training_loss: 1.24199e+02
I1111 02:13:50.914578 140264174335808 run_lib.py:153] step: 86650, training_loss: 1.68830e+02
I1111 02:14:01.547520 140264174335808 run_lib.py:153] step: 86700, training_loss: 1.29426e+02
I1111 02:14:12.215653 140264174335808 run_lib.py:153] step: 86750, training_loss: 1.35556e+02
I1111 02:14:22.695249 140264174335808 run_lib.py:153] step: 86800, training_loss: 1.25828e+02
I1111 02:14:32.793154 140264174335808 run_lib.py:153] step: 86850, training_loss: 1.22922e+02
I1111 02:14:42.818935 140264174335808 run_lib.py:153] step: 86900, training_loss: 1.64432e+02
I1111 02:14:52.918623 140264174335808 run_lib.py:153] step: 86950, training_loss: 1.33510e+02
I1111 02:15:02.959730 140264174335808 run_lib.py:153] step: 87000, training_loss: 1.22671e+02
I1111 02:15:13.852837 140264174335808 run_lib.py:153] step: 87050, training_loss: 9.02523e+01
I1111 02:15:24.543735 140264174335808 run_lib.py:153] step: 87100, training_loss: 1.38808e+02
I1111 02:15:34.818847 140264174335808 run_lib.py:153] step: 87150, training_loss: 1.28344e+02
I1111 02:15:45.021656 140264174335808 run_lib.py:153] step: 87200, training_loss: 1.38310e+02
I1111 02:15:54.352625 140264174335808 run_lib.py:153] step: 87250, training_loss: 1.30410e+02
I1111 02:16:04.605872 140264174335808 run_lib.py:153] step: 87300, training_loss: 1.05411e+02
I1111 02:16:15.085212 140264174335808 run_lib.py:153] step: 87350, training_loss: 1.25539e+02
I1111 02:16:24.870432 140264174335808 run_lib.py:153] step: 87400, training_loss: 1.34946e+02
I1111 02:16:35.003949 140264174335808 run_lib.py:153] step: 87450, training_loss: 1.25377e+02
I1111 02:16:44.778037 140264174335808 run_lib.py:153] step: 87500, training_loss: 1.43574e+02
I1111 02:16:54.827703 140264174335808 run_lib.py:153] step: 87550, training_loss: 1.48876e+02
I1111 02:17:04.650405 140264174335808 run_lib.py:153] step: 87600, training_loss: 1.24989e+02
I1111 02:17:14.051811 140264174335808 run_lib.py:153] step: 87650, training_loss: 1.41061e+02
I1111 02:17:24.305841 140264174335808 run_lib.py:153] step: 87700, training_loss: 1.59842e+02
I1111 02:17:34.114460 140264174335808 run_lib.py:153] step: 87750, training_loss: 1.29476e+02
I1111 02:17:43.563218 140264174335808 run_lib.py:153] step: 87800, training_loss: 1.39431e+02
I1111 02:17:53.237451 140264174335808 run_lib.py:153] step: 87850, training_loss: 1.26274e+02
I1111 02:18:03.261740 140264174335808 run_lib.py:153] step: 87900, training_loss: 1.29044e+02
I1111 02:18:13.223721 140264174335808 run_lib.py:153] step: 87950, training_loss: 1.15259e+02
I1111 02:18:23.607101 140264174335808 run_lib.py:153] step: 88000, training_loss: 1.41353e+02
I1111 02:18:34.004091 140264174335808 run_lib.py:153] step: 88050, training_loss: 1.87163e+02
I1111 02:18:43.568248 140264174335808 run_lib.py:153] step: 88100, training_loss: 1.15257e+02
I1111 02:18:53.434480 140264174335808 run_lib.py:153] step: 88150, training_loss: 1.29462e+02
I1111 02:19:03.508844 140264174335808 run_lib.py:153] step: 88200, training_loss: 1.00854e+02
I1111 02:19:13.486839 140264174335808 run_lib.py:153] step: 88250, training_loss: 1.07389e+02
I1111 02:19:24.102695 140264174335808 run_lib.py:153] step: 88300, training_loss: 1.07054e+02
I1111 02:19:34.002610 140264174335808 run_lib.py:153] step: 88350, training_loss: 1.41606e+02
I1111 02:19:43.785057 140264174335808 run_lib.py:153] step: 88400, training_loss: 1.35494e+02
I1111 02:19:53.012752 140264174335808 run_lib.py:153] step: 88450, training_loss: 1.13409e+02
I1111 02:20:02.676804 140264174335808 run_lib.py:153] step: 88500, training_loss: 1.12987e+02
I1111 02:20:12.131796 140264174335808 run_lib.py:153] step: 88550, training_loss: 1.18308e+02
I1111 02:20:22.537348 140264174335808 run_lib.py:153] step: 88600, training_loss: 1.52944e+02
I1111 02:20:32.987600 140264174335808 run_lib.py:153] step: 88650, training_loss: 1.40124e+02
I1111 02:20:42.917895 140264174335808 run_lib.py:153] step: 88700, training_loss: 1.52774e+02
I1111 02:20:52.735664 140264174335808 run_lib.py:153] step: 88750, training_loss: 9.37649e+01
I1111 02:21:02.804015 140264174335808 run_lib.py:153] step: 88800, training_loss: 1.60241e+02
I1111 02:21:12.471243 140264174335808 run_lib.py:153] step: 88850, training_loss: 1.47769e+02
I1111 02:21:22.843660 140264174335808 run_lib.py:153] step: 88900, training_loss: 1.45285e+02
I1111 02:21:32.351173 140264174335808 run_lib.py:153] step: 88950, training_loss: 1.23455e+02
I1111 02:21:41.806717 140264174335808 run_lib.py:153] step: 89000, training_loss: 1.17761e+02
I1111 02:21:51.852653 140264174335808 run_lib.py:153] step: 89050, training_loss: 1.34945e+02
I1111 02:22:02.271807 140264174335808 run_lib.py:153] step: 89100, training_loss: 1.09225e+02
I1111 02:22:12.522056 140264174335808 run_lib.py:153] step: 89150, training_loss: 1.54728e+02
I1111 02:22:22.679951 140264174335808 run_lib.py:153] step: 89200, training_loss: 1.23596e+02
I1111 02:22:33.108213 140264174335808 run_lib.py:153] step: 89250, training_loss: 1.13287e+02
I1111 02:22:43.739337 140264174335808 run_lib.py:153] step: 89300, training_loss: 1.33009e+02
I1111 02:22:54.218479 140264174335808 run_lib.py:153] step: 89350, training_loss: 1.13072e+02
I1111 02:23:04.400535 140264174335808 run_lib.py:153] step: 89400, training_loss: 1.39427e+02
I1111 02:23:15.246697 140264174335808 run_lib.py:153] step: 89450, training_loss: 1.18113e+02
I1111 02:23:25.464624 140264174335808 run_lib.py:153] step: 89500, training_loss: 1.47425e+02
I1111 02:23:36.098750 140264174335808 run_lib.py:153] step: 89550, training_loss: 1.53731e+02
I1111 02:23:45.395805 140264174335808 run_lib.py:153] step: 89600, training_loss: 1.34539e+02
I1111 02:23:55.022988 140264174335808 run_lib.py:153] step: 89650, training_loss: 1.23054e+02
I1111 02:24:04.752357 140264174335808 run_lib.py:153] step: 89700, training_loss: 1.82484e+02
I1111 02:24:14.945903 140264174335808 run_lib.py:153] step: 89750, training_loss: 1.33239e+02
I1111 02:24:25.272906 140264174335808 run_lib.py:153] step: 89800, training_loss: 1.11247e+02
I1111 02:24:35.940121 140264174335808 run_lib.py:153] step: 89850, training_loss: 1.40722e+02
I1111 02:24:46.194768 140264174335808 run_lib.py:153] step: 89900, training_loss: 1.11043e+02
I1111 02:24:56.583235 140264174335808 run_lib.py:153] step: 89950, training_loss: 1.17148e+02
I1111 02:25:06.602450 140264174335808 run_lib.py:153] step: 90000, training_loss: 1.41193e+02
I1111 02:25:07.182024 140264174335808 run_lib.py:166] step: 90000, eval_loss: 1.49238e+02
I1111 02:25:17.569504 140264174335808 run_lib.py:153] step: 90050, training_loss: 1.28179e+02
I1111 02:25:28.153946 140264174335808 run_lib.py:153] step: 90100, training_loss: 1.24198e+02
I1111 02:25:38.480238 140264174335808 run_lib.py:153] step: 90150, training_loss: 9.03117e+01
I1111 02:25:49.157141 140264174335808 run_lib.py:153] step: 90200, training_loss: 1.14484e+02
I1111 02:25:58.706622 140264174335808 run_lib.py:153] step: 90250, training_loss: 1.33067e+02
I1111 02:26:09.038813 140264174335808 run_lib.py:153] step: 90300, training_loss: 1.36211e+02
I1111 02:26:19.047185 140264174335808 run_lib.py:153] step: 90350, training_loss: 1.35219e+02
I1111 02:26:29.693416 140264174335808 run_lib.py:153] step: 90400, training_loss: 1.32989e+02
I1111 02:26:39.689708 140264174335808 run_lib.py:153] step: 90450, training_loss: 1.58985e+02
I1111 02:26:49.365374 140264174335808 run_lib.py:153] step: 90500, training_loss: 1.23216e+02
I1111 02:26:59.163890 140264174335808 run_lib.py:153] step: 90550, training_loss: 1.23563e+02
I1111 02:27:09.360790 140264174335808 run_lib.py:153] step: 90600, training_loss: 1.51649e+02
I1111 02:27:18.955768 140264174335808 run_lib.py:153] step: 90650, training_loss: 1.39170e+02
I1111 02:27:28.850197 140264174335808 run_lib.py:153] step: 90700, training_loss: 1.22569e+02
I1111 02:27:39.365101 140264174335808 run_lib.py:153] step: 90750, training_loss: 1.40560e+02
I1111 02:27:49.590561 140264174335808 run_lib.py:153] step: 90800, training_loss: 1.27726e+02
I1111 02:27:59.216591 140264174335808 run_lib.py:153] step: 90850, training_loss: 1.24559e+02
I1111 02:28:09.497373 140264174335808 run_lib.py:153] step: 90900, training_loss: 1.29468e+02
I1111 02:28:19.341868 140264174335808 run_lib.py:153] step: 90950, training_loss: 1.12352e+02
I1111 02:28:29.527186 140264174335808 run_lib.py:153] step: 91000, training_loss: 1.31116e+02
I1111 02:28:39.638603 140264174335808 run_lib.py:153] step: 91050, training_loss: 1.86417e+02
I1111 02:28:49.752876 140264174335808 run_lib.py:153] step: 91100, training_loss: 1.23222e+02
I1111 02:28:59.494312 140264174335808 run_lib.py:153] step: 91150, training_loss: 1.11809e+02
I1111 02:29:09.425995 140264174335808 run_lib.py:153] step: 91200, training_loss: 1.14984e+02
I1111 02:29:18.657266 140264174335808 run_lib.py:153] step: 91250, training_loss: 1.26153e+02
I1111 02:29:28.499508 140264174335808 run_lib.py:153] step: 91300, training_loss: 1.25217e+02
I1111 02:29:38.689347 140264174335808 run_lib.py:153] step: 91350, training_loss: 1.35392e+02
I1111 02:29:48.593312 140264174335808 run_lib.py:153] step: 91400, training_loss: 1.38376e+02
I1111 02:29:58.453891 140264174335808 run_lib.py:153] step: 91450, training_loss: 1.37871e+02
I1111 02:30:08.839887 140264174335808 run_lib.py:153] step: 91500, training_loss: 1.34752e+02
I1111 02:30:19.086901 140264174335808 run_lib.py:153] step: 91550, training_loss: 1.19955e+02
I1111 02:30:29.438554 140264174335808 run_lib.py:153] step: 91600, training_loss: 1.16441e+02
I1111 02:30:39.885282 140264174335808 run_lib.py:153] step: 91650, training_loss: 1.22497e+02
I1111 02:30:49.883367 140264174335808 run_lib.py:153] step: 91700, training_loss: 1.13800e+02
I1111 02:30:59.670301 140264174335808 run_lib.py:153] step: 91750, training_loss: 1.30602e+02
I1111 02:31:09.737949 140264174335808 run_lib.py:153] step: 91800, training_loss: 1.12229e+02
I1111 02:31:19.506732 140264174335808 run_lib.py:153] step: 91850, training_loss: 1.46116e+02
I1111 02:31:29.266256 140264174335808 run_lib.py:153] step: 91900, training_loss: 1.01859e+02
I1111 02:31:39.633376 140264174335808 run_lib.py:153] step: 91950, training_loss: 1.32226e+02
I1111 02:31:49.948765 140264174335808 run_lib.py:153] step: 92000, training_loss: 1.10611e+02
I1111 02:32:00.140153 140264174335808 run_lib.py:153] step: 92050, training_loss: 1.49200e+02
I1111 02:32:09.809230 140264174335808 run_lib.py:153] step: 92100, training_loss: 1.05659e+02
I1111 02:32:20.222709 140264174335808 run_lib.py:153] step: 92150, training_loss: 9.40979e+01
I1111 02:32:30.365820 140264174335808 run_lib.py:153] step: 92200, training_loss: 1.38110e+02
I1111 02:32:40.115743 140264174335808 run_lib.py:153] step: 92250, training_loss: 1.27076e+02
I1111 02:32:50.133610 140264174335808 run_lib.py:153] step: 92300, training_loss: 1.44588e+02
I1111 02:32:59.905996 140264174335808 run_lib.py:153] step: 92350, training_loss: 1.08403e+02
I1111 02:33:09.722742 140264174335808 run_lib.py:153] step: 92400, training_loss: 9.02936e+01
I1111 02:33:19.195526 140264174335808 run_lib.py:153] step: 92450, training_loss: 1.22763e+02
I1111 02:33:29.210118 140264174335808 run_lib.py:153] step: 92500, training_loss: 1.08577e+02
I1111 02:33:39.192301 140264174335808 run_lib.py:153] step: 92550, training_loss: 1.27264e+02
I1111 02:33:48.783123 140264174335808 run_lib.py:153] step: 92600, training_loss: 1.18378e+02
I1111 02:33:59.264526 140264174335808 run_lib.py:153] step: 92650, training_loss: 1.33944e+02
I1111 02:34:09.839597 140264174335808 run_lib.py:153] step: 92700, training_loss: 1.20978e+02
I1111 02:34:20.262377 140264174335808 run_lib.py:153] step: 92750, training_loss: 1.20385e+02
I1111 02:34:30.967575 140264174335808 run_lib.py:153] step: 92800, training_loss: 1.24146e+02
I1111 02:34:41.308845 140264174335808 run_lib.py:153] step: 92850, training_loss: 1.14293e+02
I1111 02:34:51.726331 140264174335808 run_lib.py:153] step: 92900, training_loss: 1.65850e+02
I1111 02:35:02.703302 140264174335808 run_lib.py:153] step: 92950, training_loss: 1.33578e+02
I1111 02:35:12.851122 140264174335808 run_lib.py:153] step: 93000, training_loss: 1.08631e+02
I1111 02:35:22.774708 140264174335808 run_lib.py:153] step: 93050, training_loss: 1.13097e+02
I1111 02:35:32.803951 140264174335808 run_lib.py:153] step: 93100, training_loss: 1.48461e+02
I1111 02:35:43.226592 140264174335808 run_lib.py:153] step: 93150, training_loss: 1.48025e+02
I1111 02:35:53.181365 140264174335808 run_lib.py:153] step: 93200, training_loss: 1.25931e+02
I1111 02:36:03.834202 140264174335808 run_lib.py:153] step: 93250, training_loss: 1.24944e+02
I1111 02:36:13.873496 140264174335808 run_lib.py:153] step: 93300, training_loss: 1.34653e+02
I1111 02:36:23.840204 140264174335808 run_lib.py:153] step: 93350, training_loss: 1.13421e+02
I1111 02:36:33.702415 140264174335808 run_lib.py:153] step: 93400, training_loss: 1.56493e+02
I1111 02:36:43.427799 140264174335808 run_lib.py:153] step: 93450, training_loss: 1.25201e+02
I1111 02:36:53.741581 140264174335808 run_lib.py:153] step: 93500, training_loss: 1.22540e+02
I1111 02:37:04.017121 140264174335808 run_lib.py:153] step: 93550, training_loss: 1.69632e+02
I1111 02:37:13.670696 140264174335808 run_lib.py:153] step: 93600, training_loss: 1.22201e+02
I1111 02:37:22.926126 140264174335808 run_lib.py:153] step: 93650, training_loss: 1.04485e+02
I1111 02:37:33.192360 140264174335808 run_lib.py:153] step: 93700, training_loss: 1.29354e+02
I1111 02:37:43.104731 140264174335808 run_lib.py:153] step: 93750, training_loss: 1.48833e+02
I1111 02:37:53.063200 140264174335808 run_lib.py:153] step: 93800, training_loss: 1.54045e+02
I1111 02:38:03.676788 140264174335808 run_lib.py:153] step: 93850, training_loss: 1.42848e+02
I1111 02:38:13.902881 140264174335808 run_lib.py:153] step: 93900, training_loss: 1.16667e+02
I1111 02:38:23.383288 140264174335808 run_lib.py:153] step: 93950, training_loss: 1.46898e+02
I1111 02:38:33.405064 140264174335808 run_lib.py:153] step: 94000, training_loss: 1.21369e+02
I1111 02:38:44.194357 140264174335808 run_lib.py:153] step: 94050, training_loss: 1.27565e+02
I1111 02:38:54.370574 140264174335808 run_lib.py:153] step: 94100, training_loss: 1.38482e+02
I1111 02:39:03.760816 140264174335808 run_lib.py:153] step: 94150, training_loss: 1.23748e+02
I1111 02:39:13.828429 140264174335808 run_lib.py:153] step: 94200, training_loss: 1.25126e+02
I1111 02:39:23.593599 140264174335808 run_lib.py:153] step: 94250, training_loss: 1.20568e+02
I1111 02:39:33.237400 140264174335808 run_lib.py:153] step: 94300, training_loss: 1.44662e+02
I1111 02:39:43.197157 140264174335808 run_lib.py:153] step: 94350, training_loss: 1.18822e+02
I1111 02:39:52.954302 140264174335808 run_lib.py:153] step: 94400, training_loss: 1.39231e+02
I1111 02:40:03.651459 140264174335808 run_lib.py:153] step: 94450, training_loss: 1.17056e+02
I1111 02:40:13.941702 140264174335808 run_lib.py:153] step: 94500, training_loss: 1.49291e+02
I1111 02:40:24.014736 140264174335808 run_lib.py:153] step: 94550, training_loss: 1.39670e+02
I1111 02:40:33.674160 140264174335808 run_lib.py:153] step: 94600, training_loss: 1.44517e+02
I1111 02:40:43.199362 140264174335808 run_lib.py:153] step: 94650, training_loss: 1.34070e+02
I1111 02:40:53.084338 140264174335808 run_lib.py:153] step: 94700, training_loss: 1.29503e+02
I1111 02:41:03.438333 140264174335808 run_lib.py:153] step: 94750, training_loss: 1.57284e+02
I1111 02:41:13.579435 140264174335808 run_lib.py:153] step: 94800, training_loss: 1.36788e+02
I1111 02:41:23.497359 140264174335808 run_lib.py:153] step: 94850, training_loss: 1.15031e+02
I1111 02:41:33.539527 140264174335808 run_lib.py:153] step: 94900, training_loss: 1.02014e+02
I1111 02:41:44.403065 140264174335808 run_lib.py:153] step: 94950, training_loss: 1.38460e+02
I1111 02:41:53.782045 140264174335808 run_lib.py:153] step: 95000, training_loss: 1.17223e+02
I1111 02:41:53.884166 140264174335808 run_lib.py:166] step: 95000, eval_loss: 9.73758e+01
I1111 02:42:03.724344 140264174335808 run_lib.py:153] step: 95050, training_loss: 1.36805e+02
I1111 02:42:13.685465 140264174335808 run_lib.py:153] step: 95100, training_loss: 1.04440e+02
I1111 02:42:23.733806 140264174335808 run_lib.py:153] step: 95150, training_loss: 1.25722e+02
I1111 02:42:33.960482 140264174335808 run_lib.py:153] step: 95200, training_loss: 1.08524e+02
I1111 02:42:44.047650 140264174335808 run_lib.py:153] step: 95250, training_loss: 1.06954e+02
I1111 02:42:54.143237 140264174335808 run_lib.py:153] step: 95300, training_loss: 1.35897e+02
I1111 02:43:04.261960 140264174335808 run_lib.py:153] step: 95350, training_loss: 1.37890e+02
I1111 02:43:15.172601 140264174335808 run_lib.py:153] step: 95400, training_loss: 1.37027e+02
I1111 02:43:25.380712 140264174335808 run_lib.py:153] step: 95450, training_loss: 1.04363e+02
I1111 02:43:35.803038 140264174335808 run_lib.py:153] step: 95500, training_loss: 9.62570e+01
I1111 02:43:46.215110 140264174335808 run_lib.py:153] step: 95550, training_loss: 1.40919e+02
I1111 02:43:56.540616 140264174335808 run_lib.py:153] step: 95600, training_loss: 1.04067e+02
I1111 02:44:06.390925 140264174335808 run_lib.py:153] step: 95650, training_loss: 1.13318e+02
I1111 02:44:16.557893 140264174335808 run_lib.py:153] step: 95700, training_loss: 1.22085e+02
I1111 02:44:26.381441 140264174335808 run_lib.py:153] step: 95750, training_loss: 1.11511e+02
I1111 02:44:36.438099 140264174335808 run_lib.py:153] step: 95800, training_loss: 1.52819e+02
I1111 02:44:47.178400 140264174335808 run_lib.py:153] step: 95850, training_loss: 1.45058e+02
I1111 02:44:57.493352 140264174335808 run_lib.py:153] step: 95900, training_loss: 1.33653e+02
I1111 02:45:07.464395 140264174335808 run_lib.py:153] step: 95950, training_loss: 1.28110e+02
I1111 02:45:17.949436 140264174335808 run_lib.py:153] step: 96000, training_loss: 1.52908e+02
I1111 02:45:28.135847 140264174335808 run_lib.py:153] step: 96050, training_loss: 1.46175e+02
I1111 02:45:38.028842 140264174335808 run_lib.py:153] step: 96100, training_loss: 1.41775e+02
I1111 02:45:48.037015 140264174335808 run_lib.py:153] step: 96150, training_loss: 1.20035e+02
I1111 02:45:57.857021 140264174335808 run_lib.py:153] step: 96200, training_loss: 1.33001e+02
I1111 02:46:08.245238 140264174335808 run_lib.py:153] step: 96250, training_loss: 1.50873e+02
I1111 02:46:18.246878 140264174335808 run_lib.py:153] step: 96300, training_loss: 1.13045e+02
I1111 02:46:28.317694 140264174335808 run_lib.py:153] step: 96350, training_loss: 1.37194e+02
I1111 02:46:38.036337 140264174335808 run_lib.py:153] step: 96400, training_loss: 1.11735e+02
I1111 02:46:48.095008 140264174335808 run_lib.py:153] step: 96450, training_loss: 1.19123e+02
I1111 02:46:57.718066 140264174335808 run_lib.py:153] step: 96500, training_loss: 1.29244e+02
I1111 02:47:08.577870 140264174335808 run_lib.py:153] step: 96550, training_loss: 1.17720e+02
I1111 02:47:19.199581 140264174335808 run_lib.py:153] step: 96600, training_loss: 1.30080e+02
I1111 02:47:29.319945 140264174335808 run_lib.py:153] step: 96650, training_loss: 1.11914e+02
I1111 02:47:40.410579 140264174335808 run_lib.py:153] step: 96700, training_loss: 1.29319e+02
I1111 02:47:50.269500 140264174335808 run_lib.py:153] step: 96750, training_loss: 1.16455e+02
I1111 02:47:59.983866 140264174335808 run_lib.py:153] step: 96800, training_loss: 1.81780e+02
I1111 02:48:10.274484 140264174335808 run_lib.py:153] step: 96850, training_loss: 1.33637e+02
I1111 02:48:20.382422 140264174335808 run_lib.py:153] step: 96900, training_loss: 1.33939e+02
I1111 02:48:30.233525 140264174335808 run_lib.py:153] step: 96950, training_loss: 1.53203e+02
I1111 02:48:40.447750 140264174335808 run_lib.py:153] step: 97000, training_loss: 9.84225e+01
I1111 02:48:51.027072 140264174335808 run_lib.py:153] step: 97050, training_loss: 1.14799e+02
I1111 02:49:01.361140 140264174335808 run_lib.py:153] step: 97100, training_loss: 1.24095e+02
I1111 02:49:12.070577 140264174335808 run_lib.py:153] step: 97150, training_loss: 1.59792e+02
I1111 02:49:22.232538 140264174335808 run_lib.py:153] step: 97200, training_loss: 1.31958e+02
I1111 02:49:32.779221 140264174335808 run_lib.py:153] step: 97250, training_loss: 1.25236e+02
I1111 02:49:43.209478 140264174335808 run_lib.py:153] step: 97300, training_loss: 1.30800e+02
I1111 02:49:53.683819 140264174335808 run_lib.py:153] step: 97350, training_loss: 1.35834e+02
I1111 02:50:03.961590 140264174335808 run_lib.py:153] step: 97400, training_loss: 1.00851e+02
I1111 02:50:13.838051 140264174335808 run_lib.py:153] step: 97450, training_loss: 1.24797e+02
I1111 02:50:24.031888 140264174335808 run_lib.py:153] step: 97500, training_loss: 1.47785e+02
I1111 02:50:34.038591 140264174335808 run_lib.py:153] step: 97550, training_loss: 1.17967e+02
I1111 02:50:44.800412 140264174335808 run_lib.py:153] step: 97600, training_loss: 1.12329e+02
I1111 02:50:54.818381 140264174335808 run_lib.py:153] step: 97650, training_loss: 1.22478e+02
I1111 02:51:04.400256 140264174335808 run_lib.py:153] step: 97700, training_loss: 1.27283e+02
I1111 02:51:15.606715 140264174335808 run_lib.py:153] step: 97750, training_loss: 1.73963e+02
I1111 02:51:26.036153 140264174335808 run_lib.py:153] step: 97800, training_loss: 1.22868e+02
I1111 02:51:35.905283 140264174335808 run_lib.py:153] step: 97850, training_loss: 1.39990e+02
I1111 02:51:46.273239 140264174335808 run_lib.py:153] step: 97900, training_loss: 1.33079e+02
I1111 02:51:56.340207 140264174335808 run_lib.py:153] step: 97950, training_loss: 1.18023e+02
I1111 02:52:06.774247 140264174335808 run_lib.py:153] step: 98000, training_loss: 1.40388e+02
I1111 02:52:17.194913 140264174335808 run_lib.py:153] step: 98050, training_loss: 1.14881e+02
I1111 02:52:27.624089 140264174335808 run_lib.py:153] step: 98100, training_loss: 1.20306e+02
I1111 02:52:37.667320 140264174335808 run_lib.py:153] step: 98150, training_loss: 1.14258e+02
I1111 02:52:47.382170 140264174335808 run_lib.py:153] step: 98200, training_loss: 1.37927e+02
I1111 02:52:57.616992 140264174335808 run_lib.py:153] step: 98250, training_loss: 1.34585e+02
I1111 02:53:07.696680 140264174335808 run_lib.py:153] step: 98300, training_loss: 1.36312e+02
I1111 02:53:17.712965 140264174335808 run_lib.py:153] step: 98350, training_loss: 1.03487e+02
I1111 02:53:27.516473 140264174335808 run_lib.py:153] step: 98400, training_loss: 1.65763e+02
I1111 02:53:36.767682 140264174335808 run_lib.py:153] step: 98450, training_loss: 1.23161e+02
I1111 02:53:47.515711 140264174335808 run_lib.py:153] step: 98500, training_loss: 1.18056e+02
I1111 02:53:58.231757 140264174335808 run_lib.py:153] step: 98550, training_loss: 1.46298e+02
I1111 02:54:08.929523 140264174335808 run_lib.py:153] step: 98600, training_loss: 1.31301e+02
I1111 02:54:19.298483 140264174335808 run_lib.py:153] step: 98650, training_loss: 1.70658e+02
I1111 02:54:29.788668 140264174335808 run_lib.py:153] step: 98700, training_loss: 1.36118e+02
I1111 02:54:39.842051 140264174335808 run_lib.py:153] step: 98750, training_loss: 1.24524e+02
I1111 02:54:49.442230 140264174335808 run_lib.py:153] step: 98800, training_loss: 1.30489e+02
I1111 02:54:59.851782 140264174335808 run_lib.py:153] step: 98850, training_loss: 1.22151e+02
I1111 02:55:10.894027 140264174335808 run_lib.py:153] step: 98900, training_loss: 1.37689e+02
I1111 02:55:21.262962 140264174335808 run_lib.py:153] step: 98950, training_loss: 1.70210e+02
I1111 02:55:31.120833 140264174335808 run_lib.py:153] step: 99000, training_loss: 1.23445e+02
I1111 02:55:41.421226 140264174335808 run_lib.py:153] step: 99050, training_loss: 1.09979e+02
I1111 02:55:50.798363 140264174335808 run_lib.py:153] step: 99100, training_loss: 1.52972e+02
I1111 02:56:00.629669 140264174335808 run_lib.py:153] step: 99150, training_loss: 1.29845e+02
I1111 02:56:10.736963 140264174335808 run_lib.py:153] step: 99200, training_loss: 1.36603e+02
I1111 02:56:21.535645 140264174335808 run_lib.py:153] step: 99250, training_loss: 1.22315e+02
I1111 02:56:32.151926 140264174335808 run_lib.py:153] step: 99300, training_loss: 1.25302e+02
I1111 02:56:43.093477 140264174335808 run_lib.py:153] step: 99350, training_loss: 1.44807e+02
I1111 02:56:53.329607 140264174335808 run_lib.py:153] step: 99400, training_loss: 1.46097e+02
I1111 02:57:03.522939 140264174335808 run_lib.py:153] step: 99450, training_loss: 1.00516e+02
I1111 02:57:13.260316 140264174335808 run_lib.py:153] step: 99500, training_loss: 1.30745e+02
I1111 02:57:23.093217 140264174335808 run_lib.py:153] step: 99550, training_loss: 1.42474e+02
I1111 02:57:32.575041 140264174335808 run_lib.py:153] step: 99600, training_loss: 1.10050e+02
I1111 02:57:42.698543 140264174335808 run_lib.py:153] step: 99650, training_loss: 1.37980e+02
I1111 02:57:53.243294 140264174335808 run_lib.py:153] step: 99700, training_loss: 1.36429e+02
I1111 02:58:03.262444 140264174335808 run_lib.py:153] step: 99750, training_loss: 1.03265e+02
I1111 02:58:14.072661 140264174335808 run_lib.py:153] step: 99800, training_loss: 1.17837e+02
I1111 02:58:24.499365 140264174335808 run_lib.py:153] step: 99850, training_loss: 1.45536e+02
I1111 02:58:34.695547 140264174335808 run_lib.py:153] step: 99900, training_loss: 8.98625e+01
I1111 02:58:45.277522 140264174335808 run_lib.py:153] step: 99950, training_loss: 1.27984e+02
I1111 02:58:55.885234 140264174335808 run_lib.py:153] step: 100000, training_loss: 1.15574e+02
I1111 02:58:56.544013 140264174335808 run_lib.py:166] step: 100000, eval_loss: 1.40829e+02
I1111 02:59:07.555014 140264174335808 run_lib.py:153] step: 100050, training_loss: 1.49831e+02
I1111 02:59:17.602186 140264174335808 run_lib.py:153] step: 100100, training_loss: 1.63271e+02
I1111 02:59:28.049179 140264174335808 run_lib.py:153] step: 100150, training_loss: 1.49400e+02
I1111 02:59:38.347978 140264174335808 run_lib.py:153] step: 100200, training_loss: 1.02190e+02
I1111 02:59:48.642001 140264174335808 run_lib.py:153] step: 100250, training_loss: 1.37824e+02
I1111 02:59:59.108373 140264174335808 run_lib.py:153] step: 100300, training_loss: 1.29869e+02
I1111 03:00:09.749038 140264174335808 run_lib.py:153] step: 100350, training_loss: 1.71668e+02
I1111 03:00:20.234890 140264174335808 run_lib.py:153] step: 100400, training_loss: 9.34432e+01
I1111 03:00:29.961677 140264174335808 run_lib.py:153] step: 100450, training_loss: 1.49270e+02
I1111 03:00:40.637919 140264174335808 run_lib.py:153] step: 100500, training_loss: 1.43667e+02
I1111 03:00:51.132599 140264174335808 run_lib.py:153] step: 100550, training_loss: 1.29747e+02
I1111 03:01:00.852658 140264174335808 run_lib.py:153] step: 100600, training_loss: 1.35652e+02
I1111 03:01:10.901237 140264174335808 run_lib.py:153] step: 100650, training_loss: 1.39983e+02
I1111 03:01:20.544049 140264174335808 run_lib.py:153] step: 100700, training_loss: 1.26161e+02
I1111 03:01:31.071187 140264174335808 run_lib.py:153] step: 100750, training_loss: 1.29989e+02
I1111 03:01:41.120082 140264174335808 run_lib.py:153] step: 100800, training_loss: 1.54427e+02
I1111 03:01:52.100523 140264174335808 run_lib.py:153] step: 100850, training_loss: 1.10104e+02
I1111 03:02:02.764604 140264174335808 run_lib.py:153] step: 100900, training_loss: 1.41542e+02
I1111 03:02:12.527101 140264174335808 run_lib.py:153] step: 100950, training_loss: 1.35191e+02
I1111 03:02:22.582020 140264174335808 run_lib.py:153] step: 101000, training_loss: 1.23054e+02
I1111 03:02:32.905255 140264174335808 run_lib.py:153] step: 101050, training_loss: 1.35107e+02
I1111 03:02:42.463342 140264174335808 run_lib.py:153] step: 101100, training_loss: 8.91542e+01
I1111 03:02:53.233640 140264174335808 run_lib.py:153] step: 101150, training_loss: 1.33700e+02
I1111 03:03:03.727947 140264174335808 run_lib.py:153] step: 101200, training_loss: 1.54088e+02
I1111 03:03:13.825718 140264174335808 run_lib.py:153] step: 101250, training_loss: 1.41363e+02
I1111 03:03:23.625272 140264174335808 run_lib.py:153] step: 101300, training_loss: 1.38281e+02
I1111 03:03:33.016479 140264174335808 run_lib.py:153] step: 101350, training_loss: 1.41584e+02
I1111 03:03:42.601990 140264174335808 run_lib.py:153] step: 101400, training_loss: 1.31578e+02
I1111 03:03:52.944977 140264174335808 run_lib.py:153] step: 101450, training_loss: 1.15491e+02
I1111 03:04:03.458669 140264174335808 run_lib.py:153] step: 101500, training_loss: 1.23561e+02
I1111 03:04:13.126470 140264174335808 run_lib.py:153] step: 101550, training_loss: 1.28780e+02
I1111 03:04:23.168946 140264174335808 run_lib.py:153] step: 101600, training_loss: 1.20308e+02
I1111 03:04:33.715067 140264174335808 run_lib.py:153] step: 101650, training_loss: 1.29996e+02
I1111 03:04:44.234762 140264174335808 run_lib.py:153] step: 101700, training_loss: 1.03679e+02
I1111 03:04:54.078846 140264174335808 run_lib.py:153] step: 101750, training_loss: 1.49224e+02
I1111 03:05:03.534999 140264174335808 run_lib.py:153] step: 101800, training_loss: 1.31374e+02
I1111 03:05:13.324282 140264174335808 run_lib.py:153] step: 101850, training_loss: 1.28131e+02
I1111 03:05:23.235993 140264174335808 run_lib.py:153] step: 101900, training_loss: 1.17646e+02
I1111 03:05:33.242472 140264174335808 run_lib.py:153] step: 101950, training_loss: 1.30552e+02
I1111 03:05:42.979408 140264174335808 run_lib.py:153] step: 102000, training_loss: 1.24032e+02
I1111 03:05:53.188804 140264174335808 run_lib.py:153] step: 102050, training_loss: 1.46507e+02
I1111 03:06:03.542994 140264174335808 run_lib.py:153] step: 102100, training_loss: 1.15911e+02
I1111 03:06:14.048734 140264174335808 run_lib.py:153] step: 102150, training_loss: 1.18144e+02
I1111 03:06:24.289841 140264174335808 run_lib.py:153] step: 102200, training_loss: 1.47968e+02
I1111 03:06:34.278336 140264174335808 run_lib.py:153] step: 102250, training_loss: 1.52651e+02
I1111 03:06:44.554085 140264174335808 run_lib.py:153] step: 102300, training_loss: 1.27674e+02
I1111 03:06:54.039400 140264174335808 run_lib.py:153] step: 102350, training_loss: 1.37053e+02
I1111 03:07:04.336125 140264174335808 run_lib.py:153] step: 102400, training_loss: 1.63045e+02
I1111 03:07:13.862421 140264174335808 run_lib.py:153] step: 102450, training_loss: 1.07174e+02
I1111 03:07:23.661520 140264174335808 run_lib.py:153] step: 102500, training_loss: 1.29074e+02
I1111 03:07:33.326458 140264174335808 run_lib.py:153] step: 102550, training_loss: 1.48879e+02
I1111 03:07:42.624713 140264174335808 run_lib.py:153] step: 102600, training_loss: 1.46756e+02
I1111 03:07:52.397210 140264174335808 run_lib.py:153] step: 102650, training_loss: 1.25490e+02
I1111 03:08:02.253876 140264174335808 run_lib.py:153] step: 102700, training_loss: 1.27576e+02
I1111 03:08:11.831960 140264174335808 run_lib.py:153] step: 102750, training_loss: 1.03348e+02
I1111 03:08:21.652740 140264174335808 run_lib.py:153] step: 102800, training_loss: 1.41778e+02
I1111 03:08:31.712259 140264174335808 run_lib.py:153] step: 102850, training_loss: 1.07989e+02
I1111 03:08:41.756056 140264174335808 run_lib.py:153] step: 102900, training_loss: 1.19890e+02
I1111 03:08:51.955095 140264174335808 run_lib.py:153] step: 102950, training_loss: 1.30813e+02
I1111 03:09:02.447218 140264174335808 run_lib.py:153] step: 103000, training_loss: 1.25196e+02
I1111 03:09:11.967885 140264174335808 run_lib.py:153] step: 103050, training_loss: 1.44848e+02
I1111 03:09:21.353516 140264174335808 run_lib.py:153] step: 103100, training_loss: 1.15848e+02
I1111 03:09:31.581266 140264174335808 run_lib.py:153] step: 103150, training_loss: 1.23929e+02
I1111 03:09:41.496858 140264174335808 run_lib.py:153] step: 103200, training_loss: 1.27108e+02
I1111 03:09:52.432405 140264174335808 run_lib.py:153] step: 103250, training_loss: 1.25189e+02
I1111 03:10:02.585571 140264174335808 run_lib.py:153] step: 103300, training_loss: 1.33929e+02
I1111 03:10:13.222476 140264174335808 run_lib.py:153] step: 103350, training_loss: 1.08538e+02
I1111 03:10:22.773647 140264174335808 run_lib.py:153] step: 103400, training_loss: 1.37025e+02
I1111 03:10:32.258996 140264174335808 run_lib.py:153] step: 103450, training_loss: 1.36646e+02
I1111 03:10:42.301900 140264174335808 run_lib.py:153] step: 103500, training_loss: 1.46126e+02
I1111 03:10:51.727375 140264174335808 run_lib.py:153] step: 103550, training_loss: 1.32079e+02
I1111 03:11:01.362763 140264174335808 run_lib.py:153] step: 103600, training_loss: 1.30703e+02
I1111 03:11:11.471258 140264174335808 run_lib.py:153] step: 103650, training_loss: 1.46081e+02
I1111 03:11:21.934086 140264174335808 run_lib.py:153] step: 103700, training_loss: 1.14845e+02
I1111 03:11:32.484078 140264174335808 run_lib.py:153] step: 103750, training_loss: 1.23296e+02
I1111 03:11:43.209617 140264174335808 run_lib.py:153] step: 103800, training_loss: 1.26367e+02
I1111 03:11:53.405139 140264174335808 run_lib.py:153] step: 103850, training_loss: 1.34561e+02
I1111 03:12:03.647009 140264174335808 run_lib.py:153] step: 103900, training_loss: 1.25167e+02
I1111 03:12:13.553443 140264174335808 run_lib.py:153] step: 103950, training_loss: 1.22322e+02
I1111 03:12:23.329362 140264174335808 run_lib.py:153] step: 104000, training_loss: 1.60491e+02
I1111 03:12:33.261760 140264174335808 run_lib.py:153] step: 104050, training_loss: 1.70247e+02
I1111 03:12:43.523707 140264174335808 run_lib.py:153] step: 104100, training_loss: 1.04492e+02
I1111 03:12:53.311383 140264174335808 run_lib.py:153] step: 104150, training_loss: 1.29591e+02
I1111 03:13:03.397786 140264174335808 run_lib.py:153] step: 104200, training_loss: 1.54751e+02
I1111 03:13:13.107974 140264174335808 run_lib.py:153] step: 104250, training_loss: 1.24835e+02
I1111 03:13:23.558621 140264174335808 run_lib.py:153] step: 104300, training_loss: 1.32435e+02
I1111 03:13:33.040816 140264174335808 run_lib.py:153] step: 104350, training_loss: 1.37009e+02
I1111 03:13:43.441223 140264174335808 run_lib.py:153] step: 104400, training_loss: 9.96235e+01
I1111 03:13:53.544993 140264174335808 run_lib.py:153] step: 104450, training_loss: 1.37143e+02
I1111 03:14:03.749872 140264174335808 run_lib.py:153] step: 104500, training_loss: 1.21638e+02
I1111 03:14:13.094696 140264174335808 run_lib.py:153] step: 104550, training_loss: 1.56030e+02
I1111 03:14:22.401260 140264174335808 run_lib.py:153] step: 104600, training_loss: 1.40273e+02
I1111 03:14:32.952381 140264174335808 run_lib.py:153] step: 104650, training_loss: 1.71908e+02
I1111 03:14:42.878169 140264174335808 run_lib.py:153] step: 104700, training_loss: 1.50317e+02
I1111 03:14:52.585536 140264174335808 run_lib.py:153] step: 104750, training_loss: 9.63703e+01
I1111 03:15:02.901441 140264174335808 run_lib.py:153] step: 104800, training_loss: 1.08031e+02
I1111 03:15:12.813064 140264174335808 run_lib.py:153] step: 104850, training_loss: 1.54372e+02
I1111 03:15:23.235736 140264174335808 run_lib.py:153] step: 104900, training_loss: 1.19743e+02
I1111 03:15:32.997213 140264174335808 run_lib.py:153] step: 104950, training_loss: 1.22950e+02
I1111 03:15:43.567854 140264174335808 run_lib.py:153] step: 105000, training_loss: 1.03708e+02
I1111 03:15:43.709280 140264174335808 run_lib.py:166] step: 105000, eval_loss: 1.63077e+02
I1111 03:15:53.356149 140264174335808 run_lib.py:153] step: 105050, training_loss: 1.17337e+02
I1111 03:16:03.155498 140264174335808 run_lib.py:153] step: 105100, training_loss: 1.43368e+02
I1111 03:16:14.099606 140264174335808 run_lib.py:153] step: 105150, training_loss: 1.27060e+02
I1111 03:16:24.352649 140264174335808 run_lib.py:153] step: 105200, training_loss: 1.28284e+02
I1111 03:16:35.397901 140264174335808 run_lib.py:153] step: 105250, training_loss: 1.16422e+02
I1111 03:16:45.550474 140264174335808 run_lib.py:153] step: 105300, training_loss: 1.39526e+02
I1111 03:16:56.010122 140264174335808 run_lib.py:153] step: 105350, training_loss: 1.41555e+02
I1111 03:17:05.904092 140264174335808 run_lib.py:153] step: 105400, training_loss: 1.16662e+02
I1111 03:17:15.945992 140264174335808 run_lib.py:153] step: 105450, training_loss: 1.61195e+02
I1111 03:17:25.884887 140264174335808 run_lib.py:153] step: 105500, training_loss: 1.33431e+02
I1111 03:17:35.816858 140264174335808 run_lib.py:153] step: 105550, training_loss: 1.43344e+02
I1111 03:17:46.084186 140264174335808 run_lib.py:153] step: 105600, training_loss: 1.23613e+02
I1111 03:17:55.917859 140264174335808 run_lib.py:153] step: 105650, training_loss: 1.35297e+02
I1111 03:18:05.252776 140264174335808 run_lib.py:153] step: 105700, training_loss: 1.31523e+02
I1111 03:18:15.253354 140264174335808 run_lib.py:153] step: 105750, training_loss: 1.62549e+02
I1111 03:18:25.175196 140264174335808 run_lib.py:153] step: 105800, training_loss: 1.02427e+02
I1111 03:18:34.568832 140264174335808 run_lib.py:153] step: 105850, training_loss: 1.43648e+02
I1111 03:18:43.854396 140264174335808 run_lib.py:153] step: 105900, training_loss: 1.47127e+02
I1111 03:18:53.223987 140264174335808 run_lib.py:153] step: 105950, training_loss: 1.37303e+02
I1111 03:19:03.388736 140264174335808 run_lib.py:153] step: 106000, training_loss: 1.24373e+02
I1111 03:19:13.380144 140264174335808 run_lib.py:153] step: 106050, training_loss: 1.29782e+02
I1111 03:19:23.640099 140264174335808 run_lib.py:153] step: 106100, training_loss: 1.20170e+02
I1111 03:19:33.377958 140264174335808 run_lib.py:153] step: 106150, training_loss: 1.23928e+02
I1111 03:19:43.605378 140264174335808 run_lib.py:153] step: 106200, training_loss: 1.49407e+02
I1111 03:19:53.670357 140264174335808 run_lib.py:153] step: 106250, training_loss: 1.45295e+02
I1111 03:20:04.018211 140264174335808 run_lib.py:153] step: 106300, training_loss: 1.14245e+02
I1111 03:20:13.851250 140264174335808 run_lib.py:153] step: 106350, training_loss: 1.56636e+02
I1111 03:20:24.066963 140264174335808 run_lib.py:153] step: 106400, training_loss: 1.07478e+02
I1111 03:20:34.275918 140264174335808 run_lib.py:153] step: 106450, training_loss: 1.23274e+02
I1111 03:20:43.813727 140264174335808 run_lib.py:153] step: 106500, training_loss: 1.18026e+02
I1111 03:20:54.108461 140264174335808 run_lib.py:153] step: 106550, training_loss: 1.47693e+02
I1111 03:21:04.468478 140264174335808 run_lib.py:153] step: 106600, training_loss: 1.24340e+02
I1111 03:21:14.347833 140264174335808 run_lib.py:153] step: 106650, training_loss: 1.17944e+02
I1111 03:21:24.974923 140264174335808 run_lib.py:153] step: 106700, training_loss: 1.66794e+02
I1111 03:21:34.811046 140264174335808 run_lib.py:153] step: 106750, training_loss: 1.21657e+02
I1111 03:21:44.716282 140264174335808 run_lib.py:153] step: 106800, training_loss: 1.31662e+02
I1111 03:21:54.799740 140264174335808 run_lib.py:153] step: 106850, training_loss: 1.39621e+02
I1111 03:22:04.413543 140264174335808 run_lib.py:153] step: 106900, training_loss: 1.30971e+02
I1111 03:22:14.597822 140264174335808 run_lib.py:153] step: 106950, training_loss: 1.68030e+02
I1111 03:22:24.859356 140264174335808 run_lib.py:153] step: 107000, training_loss: 1.45113e+02
I1111 03:22:34.806725 140264174335808 run_lib.py:153] step: 107050, training_loss: 1.43739e+02
I1111 03:22:44.969470 140264174335808 run_lib.py:153] step: 107100, training_loss: 1.13017e+02
I1111 03:22:54.697115 140264174335808 run_lib.py:153] step: 107150, training_loss: 1.34141e+02
I1111 03:23:04.598883 140264174335808 run_lib.py:153] step: 107200, training_loss: 1.32832e+02
I1111 03:23:15.149496 140264174335808 run_lib.py:153] step: 107250, training_loss: 1.29642e+02
I1111 03:23:24.734259 140264174335808 run_lib.py:153] step: 107300, training_loss: 1.30688e+02
I1111 03:23:34.425802 140264174335808 run_lib.py:153] step: 107350, training_loss: 1.02270e+02
I1111 03:23:43.990661 140264174335808 run_lib.py:153] step: 107400, training_loss: 1.21206e+02
I1111 03:23:53.952763 140264174335808 run_lib.py:153] step: 107450, training_loss: 1.28421e+02
I1111 03:24:03.871706 140264174335808 run_lib.py:153] step: 107500, training_loss: 1.23999e+02
I1111 03:24:13.407302 140264174335808 run_lib.py:153] step: 107550, training_loss: 1.61248e+02
I1111 03:24:23.151000 140264174335808 run_lib.py:153] step: 107600, training_loss: 1.38295e+02
I1111 03:24:33.050510 140264174335808 run_lib.py:153] step: 107650, training_loss: 1.29371e+02
I1111 03:24:42.422254 140264174335808 run_lib.py:153] step: 107700, training_loss: 1.65658e+02
I1111 03:24:52.061990 140264174335808 run_lib.py:153] step: 107750, training_loss: 1.16369e+02
I1111 03:25:01.902474 140264174335808 run_lib.py:153] step: 107800, training_loss: 9.71463e+01
I1111 03:25:11.832599 140264174335808 run_lib.py:153] step: 107850, training_loss: 1.08095e+02
I1111 03:25:21.694601 140264174335808 run_lib.py:153] step: 107900, training_loss: 1.23559e+02
I1111 03:25:31.486145 140264174335808 run_lib.py:153] step: 107950, training_loss: 1.34726e+02
I1111 03:25:41.247984 140264174335808 run_lib.py:153] step: 108000, training_loss: 1.57597e+02
I1111 03:25:51.526879 140264174335808 run_lib.py:153] step: 108050, training_loss: 1.21932e+02
I1111 03:26:01.261037 140264174335808 run_lib.py:153] step: 108100, training_loss: 1.44397e+02
I1111 03:26:11.350023 140264174335808 run_lib.py:153] step: 108150, training_loss: 1.24468e+02
I1111 03:26:21.155035 140264174335808 run_lib.py:153] step: 108200, training_loss: 1.52490e+02
I1111 03:26:31.163417 140264174335808 run_lib.py:153] step: 108250, training_loss: 1.51382e+02
I1111 03:26:41.204731 140264174335808 run_lib.py:153] step: 108300, training_loss: 1.45891e+02
I1111 03:26:51.682399 140264174335808 run_lib.py:153] step: 108350, training_loss: 1.36959e+02
I1111 03:27:01.590167 140264174335808 run_lib.py:153] step: 108400, training_loss: 1.14277e+02
I1111 03:27:11.435039 140264174335808 run_lib.py:153] step: 108450, training_loss: 1.29231e+02
I1111 03:27:21.051958 140264174335808 run_lib.py:153] step: 108500, training_loss: 1.49317e+02
I1111 03:27:30.801408 140264174335808 run_lib.py:153] step: 108550, training_loss: 1.23868e+02
I1111 03:27:40.756553 140264174335808 run_lib.py:153] step: 108600, training_loss: 1.36788e+02
I1111 03:27:50.497326 140264174335808 run_lib.py:153] step: 108650, training_loss: 1.40781e+02
I1111 03:28:00.702284 140264174335808 run_lib.py:153] step: 108700, training_loss: 1.43425e+02
I1111 03:28:11.460358 140264174335808 run_lib.py:153] step: 108750, training_loss: 1.13882e+02
I1111 03:28:21.311676 140264174335808 run_lib.py:153] step: 108800, training_loss: 1.51075e+02
I1111 03:28:30.926208 140264174335808 run_lib.py:153] step: 108850, training_loss: 1.15379e+02
I1111 03:28:40.910106 140264174335808 run_lib.py:153] step: 108900, training_loss: 1.37320e+02
I1111 03:28:51.039592 140264174335808 run_lib.py:153] step: 108950, training_loss: 1.22995e+02
I1111 03:29:01.068031 140264174335808 run_lib.py:153] step: 109000, training_loss: 1.21269e+02
I1111 03:29:11.238790 140264174335808 run_lib.py:153] step: 109050, training_loss: 1.26807e+02
I1111 03:29:22.013056 140264174335808 run_lib.py:153] step: 109100, training_loss: 1.20527e+02
I1111 03:29:31.703876 140264174335808 run_lib.py:153] step: 109150, training_loss: 1.37165e+02
I1111 03:29:41.642303 140264174335808 run_lib.py:153] step: 109200, training_loss: 1.35464e+02
I1111 03:29:51.735919 140264174335808 run_lib.py:153] step: 109250, training_loss: 1.49062e+02
I1111 03:30:01.552159 140264174335808 run_lib.py:153] step: 109300, training_loss: 1.24974e+02
I1111 03:30:11.877394 140264174335808 run_lib.py:153] step: 109350, training_loss: 1.45914e+02
I1111 03:30:21.997736 140264174335808 run_lib.py:153] step: 109400, training_loss: 1.14186e+02
I1111 03:30:31.987367 140264174335808 run_lib.py:153] step: 109450, training_loss: 1.41343e+02
I1111 03:30:42.663031 140264174335808 run_lib.py:153] step: 109500, training_loss: 1.42767e+02
I1111 03:30:52.533668 140264174335808 run_lib.py:153] step: 109550, training_loss: 1.41797e+02
I1111 03:31:02.578539 140264174335808 run_lib.py:153] step: 109600, training_loss: 1.10869e+02
I1111 03:31:12.341463 140264174335808 run_lib.py:153] step: 109650, training_loss: 1.09032e+02
I1111 03:31:21.695682 140264174335808 run_lib.py:153] step: 109700, training_loss: 1.34230e+02
I1111 03:31:31.859486 140264174335808 run_lib.py:153] step: 109750, training_loss: 1.22388e+02
I1111 03:31:41.429600 140264174335808 run_lib.py:153] step: 109800, training_loss: 1.14646e+02
I1111 03:31:51.053240 140264174335808 run_lib.py:153] step: 109850, training_loss: 1.15875e+02
I1111 03:32:00.972799 140264174335808 run_lib.py:153] step: 109900, training_loss: 1.36130e+02
I1111 03:32:11.151088 140264174335808 run_lib.py:153] step: 109950, training_loss: 1.42815e+02
I1111 03:32:21.567959 140264174335808 run_lib.py:153] step: 110000, training_loss: 1.20990e+02
I1111 03:32:22.150008 140264174335808 run_lib.py:166] step: 110000, eval_loss: 1.13336e+02
I1111 03:32:31.596582 140264174335808 run_lib.py:153] step: 110050, training_loss: 1.36355e+02
I1111 03:32:42.119118 140264174335808 run_lib.py:153] step: 110100, training_loss: 1.18436e+02
I1111 03:32:52.071184 140264174335808 run_lib.py:153] step: 110150, training_loss: 1.07148e+02
I1111 03:33:02.928808 140264174335808 run_lib.py:153] step: 110200, training_loss: 1.25039e+02
I1111 03:33:12.646442 140264174335808 run_lib.py:153] step: 110250, training_loss: 1.01185e+02
I1111 03:33:23.237104 140264174335808 run_lib.py:153] step: 110300, training_loss: 1.30560e+02
I1111 03:33:33.467184 140264174335808 run_lib.py:153] step: 110350, training_loss: 1.27258e+02
I1111 03:33:43.545300 140264174335808 run_lib.py:153] step: 110400, training_loss: 1.37626e+02
I1111 03:33:53.475773 140264174335808 run_lib.py:153] step: 110450, training_loss: 1.75111e+02
I1111 03:34:03.788420 140264174335808 run_lib.py:153] step: 110500, training_loss: 1.27805e+02
I1111 03:34:14.243064 140264174335808 run_lib.py:153] step: 110550, training_loss: 1.32273e+02
I1111 03:34:25.057540 140264174335808 run_lib.py:153] step: 110600, training_loss: 1.23038e+02
I1111 03:34:34.920146 140264174335808 run_lib.py:153] step: 110650, training_loss: 1.26735e+02
I1111 03:34:44.518239 140264174335808 run_lib.py:153] step: 110700, training_loss: 1.35139e+02
I1111 03:34:54.900744 140264174335808 run_lib.py:153] step: 110750, training_loss: 1.08046e+02
I1111 03:35:05.633389 140264174335808 run_lib.py:153] step: 110800, training_loss: 1.14749e+02
I1111 03:35:16.166967 140264174335808 run_lib.py:153] step: 110850, training_loss: 1.49369e+02
I1111 03:35:26.084601 140264174335808 run_lib.py:153] step: 110900, training_loss: 1.34657e+02
I1111 03:35:36.093989 140264174335808 run_lib.py:153] step: 110950, training_loss: 1.22707e+02
I1111 03:35:46.346466 140264174335808 run_lib.py:153] step: 111000, training_loss: 1.23325e+02
I1111 03:35:56.279291 140264174335808 run_lib.py:153] step: 111050, training_loss: 9.82103e+01
I1111 03:36:06.798077 140264174335808 run_lib.py:153] step: 111100, training_loss: 1.45921e+02
I1111 03:36:17.156217 140264174335808 run_lib.py:153] step: 111150, training_loss: 1.22977e+02
I1111 03:36:27.641808 140264174335808 run_lib.py:153] step: 111200, training_loss: 1.30318e+02
I1111 03:36:37.704070 140264174335808 run_lib.py:153] step: 111250, training_loss: 1.44636e+02
I1111 03:36:47.527485 140264174335808 run_lib.py:153] step: 111300, training_loss: 1.42325e+02
I1111 03:36:57.185818 140264174335808 run_lib.py:153] step: 111350, training_loss: 1.30293e+02
I1111 03:37:06.774213 140264174335808 run_lib.py:153] step: 111400, training_loss: 1.06202e+02
I1111 03:37:16.999221 140264174335808 run_lib.py:153] step: 111450, training_loss: 1.14412e+02
I1111 03:37:27.549674 140264174335808 run_lib.py:153] step: 111500, training_loss: 1.34805e+02
I1111 03:37:38.306567 140264174335808 run_lib.py:153] step: 111550, training_loss: 1.16749e+02
I1111 03:37:48.376847 140264174335808 run_lib.py:153] step: 111600, training_loss: 1.08774e+02
I1111 03:37:58.761488 140264174335808 run_lib.py:153] step: 111650, training_loss: 1.56900e+02
I1111 03:38:09.446347 140264174335808 run_lib.py:153] step: 111700, training_loss: 1.51396e+02
I1111 03:38:19.778855 140264174335808 run_lib.py:153] step: 111750, training_loss: 1.08397e+02
I1111 03:38:29.658227 140264174335808 run_lib.py:153] step: 111800, training_loss: 1.32903e+02
I1111 03:38:40.193169 140264174335808 run_lib.py:153] step: 111850, training_loss: 1.39365e+02
I1111 03:38:50.538220 140264174335808 run_lib.py:153] step: 111900, training_loss: 1.58839e+02
I1111 03:39:01.318749 140264174335808 run_lib.py:153] step: 111950, training_loss: 1.27794e+02
I1111 03:39:11.643005 140264174335808 run_lib.py:153] step: 112000, training_loss: 1.28422e+02
I1111 03:39:21.701349 140264174335808 run_lib.py:153] step: 112050, training_loss: 1.59875e+02
I1111 03:39:31.617037 140264174335808 run_lib.py:153] step: 112100, training_loss: 1.21647e+02
I1111 03:39:41.997089 140264174335808 run_lib.py:153] step: 112150, training_loss: 1.37832e+02
I1111 03:39:52.632971 140264174335808 run_lib.py:153] step: 112200, training_loss: 1.48721e+02
I1111 03:40:03.285645 140264174335808 run_lib.py:153] step: 112250, training_loss: 1.42186e+02
I1111 03:40:13.825654 140264174335808 run_lib.py:153] step: 112300, training_loss: 1.51675e+02
I1111 03:40:24.172956 140264174335808 run_lib.py:153] step: 112350, training_loss: 1.40344e+02
I1111 03:40:34.115246 140264174335808 run_lib.py:153] step: 112400, training_loss: 1.32077e+02
I1111 03:40:44.222070 140264174335808 run_lib.py:153] step: 112450, training_loss: 1.29847e+02
I1111 03:40:54.678670 140264174335808 run_lib.py:153] step: 112500, training_loss: 1.76693e+02
I1111 03:41:05.323202 140264174335808 run_lib.py:153] step: 112550, training_loss: 1.25992e+02
I1111 03:41:15.330909 140264174335808 run_lib.py:153] step: 112600, training_loss: 1.16141e+02
I1111 03:41:25.391941 140264174335808 run_lib.py:153] step: 112650, training_loss: 1.14034e+02
I1111 03:41:35.343201 140264174335808 run_lib.py:153] step: 112700, training_loss: 1.55763e+02
I1111 03:41:45.749754 140264174335808 run_lib.py:153] step: 112750, training_loss: 1.52389e+02
I1111 03:41:55.225364 140264174335808 run_lib.py:153] step: 112800, training_loss: 1.27418e+02
I1111 03:42:05.615447 140264174335808 run_lib.py:153] step: 112850, training_loss: 1.28385e+02
I1111 03:42:15.332160 140264174335808 run_lib.py:153] step: 112900, training_loss: 1.59382e+02
I1111 03:42:25.146361 140264174335808 run_lib.py:153] step: 112950, training_loss: 1.18286e+02
I1111 03:42:34.965856 140264174335808 run_lib.py:153] step: 113000, training_loss: 1.42897e+02
I1111 03:42:44.939230 140264174335808 run_lib.py:153] step: 113050, training_loss: 1.30832e+02
I1111 03:42:54.361980 140264174335808 run_lib.py:153] step: 113100, training_loss: 1.09383e+02
I1111 03:43:04.073446 140264174335808 run_lib.py:153] step: 113150, training_loss: 1.09240e+02
I1111 03:43:14.779866 140264174335808 run_lib.py:153] step: 113200, training_loss: 1.51468e+02
I1111 03:43:24.756648 140264174335808 run_lib.py:153] step: 113250, training_loss: 1.21003e+02
I1111 03:43:34.271417 140264174335808 run_lib.py:153] step: 113300, training_loss: 1.46786e+02
I1111 03:43:44.527673 140264174335808 run_lib.py:153] step: 113350, training_loss: 1.18844e+02
I1111 03:43:54.664847 140264174335808 run_lib.py:153] step: 113400, training_loss: 1.21361e+02
I1111 03:44:04.645212 140264174335808 run_lib.py:153] step: 113450, training_loss: 1.18838e+02
I1111 03:44:15.380854 140264174335808 run_lib.py:153] step: 113500, training_loss: 1.17739e+02
I1111 03:44:25.238881 140264174335808 run_lib.py:153] step: 113550, training_loss: 1.41492e+02
I1111 03:44:34.815870 140264174335808 run_lib.py:153] step: 113600, training_loss: 1.12234e+02
I1111 03:44:45.154335 140264174335808 run_lib.py:153] step: 113650, training_loss: 1.31590e+02
I1111 03:44:54.834004 140264174335808 run_lib.py:153] step: 113700, training_loss: 1.47369e+02
I1111 03:45:05.550981 140264174335808 run_lib.py:153] step: 113750, training_loss: 9.94667e+01
I1111 03:45:16.363468 140264174335808 run_lib.py:153] step: 113800, training_loss: 1.31703e+02
I1111 03:45:26.170099 140264174335808 run_lib.py:153] step: 113850, training_loss: 1.23226e+02
I1111 03:45:37.147252 140264174335808 run_lib.py:153] step: 113900, training_loss: 1.54523e+02
I1111 03:45:47.929951 140264174335808 run_lib.py:153] step: 113950, training_loss: 1.27569e+02
I1111 03:45:57.845708 140264174335808 run_lib.py:153] step: 114000, training_loss: 1.37446e+02
I1111 03:46:07.834083 140264174335808 run_lib.py:153] step: 114050, training_loss: 1.42758e+02
I1111 03:46:18.219319 140264174335808 run_lib.py:153] step: 114100, training_loss: 1.24076e+02
I1111 03:46:27.735809 140264174335808 run_lib.py:153] step: 114150, training_loss: 1.41985e+02
I1111 03:46:37.082006 140264174335808 run_lib.py:153] step: 114200, training_loss: 1.34904e+02
I1111 03:46:46.952255 140264174335808 run_lib.py:153] step: 114250, training_loss: 1.50719e+02
I1111 03:46:57.594478 140264174335808 run_lib.py:153] step: 114300, training_loss: 1.47877e+02
I1111 03:47:07.356679 140264174335808 run_lib.py:153] step: 114350, training_loss: 1.61500e+02
I1111 03:47:17.279587 140264174335808 run_lib.py:153] step: 114400, training_loss: 1.18826e+02
I1111 03:47:26.601528 140264174335808 run_lib.py:153] step: 114450, training_loss: 1.37609e+02
I1111 03:47:36.064774 140264174335808 run_lib.py:153] step: 114500, training_loss: 1.45094e+02
I1111 03:47:45.787456 140264174335808 run_lib.py:153] step: 114550, training_loss: 1.19178e+02
I1111 03:47:56.254467 140264174335808 run_lib.py:153] step: 114600, training_loss: 1.12308e+02
I1111 03:48:06.595287 140264174335808 run_lib.py:153] step: 114650, training_loss: 1.40122e+02
I1111 03:48:17.034607 140264174335808 run_lib.py:153] step: 114700, training_loss: 1.30628e+02
I1111 03:48:26.942093 140264174335808 run_lib.py:153] step: 114750, training_loss: 1.37258e+02
I1111 03:48:36.650177 140264174335808 run_lib.py:153] step: 114800, training_loss: 1.44549e+02
I1111 03:48:46.597557 140264174335808 run_lib.py:153] step: 114850, training_loss: 1.19155e+02
I1111 03:48:56.779166 140264174335808 run_lib.py:153] step: 114900, training_loss: 1.30620e+02
I1111 03:49:07.271306 140264174335808 run_lib.py:153] step: 114950, training_loss: 1.26126e+02
I1111 03:49:16.974225 140264174335808 run_lib.py:153] step: 115000, training_loss: 1.19181e+02
I1111 03:49:17.076232 140264174335808 run_lib.py:166] step: 115000, eval_loss: 1.47589e+02
I1111 03:49:27.627387 140264174335808 run_lib.py:153] step: 115050, training_loss: 1.59585e+02
I1111 03:49:37.476777 140264174335808 run_lib.py:153] step: 115100, training_loss: 1.38929e+02
I1111 03:49:48.116088 140264174335808 run_lib.py:153] step: 115150, training_loss: 1.45418e+02
I1111 03:49:58.857186 140264174335808 run_lib.py:153] step: 115200, training_loss: 1.41656e+02
I1111 03:50:09.362620 140264174335808 run_lib.py:153] step: 115250, training_loss: 9.20103e+01
I1111 03:50:19.918393 140264174335808 run_lib.py:153] step: 115300, training_loss: 1.36370e+02
I1111 03:50:30.194036 140264174335808 run_lib.py:153] step: 115350, training_loss: 1.10712e+02
I1111 03:50:40.535097 140264174335808 run_lib.py:153] step: 115400, training_loss: 1.43181e+02
I1111 03:50:50.718453 140264174335808 run_lib.py:153] step: 115450, training_loss: 1.11322e+02
I1111 03:51:00.605333 140264174335808 run_lib.py:153] step: 115500, training_loss: 1.22167e+02
I1111 03:51:10.563994 140264174335808 run_lib.py:153] step: 115550, training_loss: 1.63037e+02
I1111 03:51:20.309934 140264174335808 run_lib.py:153] step: 115600, training_loss: 1.32141e+02
I1111 03:51:31.169209 140264174335808 run_lib.py:153] step: 115650, training_loss: 1.48096e+02
I1111 03:51:40.837252 140264174335808 run_lib.py:153] step: 115700, training_loss: 1.16206e+02
I1111 03:51:51.360918 140264174335808 run_lib.py:153] step: 115750, training_loss: 1.08012e+02
I1111 03:52:00.770654 140264174335808 run_lib.py:153] step: 115800, training_loss: 1.83584e+02
I1111 03:52:10.205859 140264174335808 run_lib.py:153] step: 115850, training_loss: 1.23295e+02
I1111 03:52:19.935414 140264174335808 run_lib.py:153] step: 115900, training_loss: 1.22745e+02
I1111 03:52:30.154867 140264174335808 run_lib.py:153] step: 115950, training_loss: 1.27613e+02
I1111 03:52:40.763040 140264174335808 run_lib.py:153] step: 116000, training_loss: 1.17378e+02
I1111 03:52:51.282967 140264174335808 run_lib.py:153] step: 116050, training_loss: 1.29703e+02
I1111 03:53:01.846243 140264174335808 run_lib.py:153] step: 116100, training_loss: 1.23268e+02
I1111 03:53:11.616596 140264174335808 run_lib.py:153] step: 116150, training_loss: 1.12279e+02
I1111 03:53:21.455307 140264174335808 run_lib.py:153] step: 116200, training_loss: 1.27689e+02
I1111 03:53:31.983285 140264174335808 run_lib.py:153] step: 116250, training_loss: 1.07189e+02
I1111 03:53:41.992489 140264174335808 run_lib.py:153] step: 116300, training_loss: 1.22009e+02
I1111 03:53:51.982170 140264174335808 run_lib.py:153] step: 116350, training_loss: 1.67088e+02
I1111 03:54:02.054819 140264174335808 run_lib.py:153] step: 116400, training_loss: 1.57660e+02
I1111 03:54:11.635177 140264174335808 run_lib.py:153] step: 116450, training_loss: 9.99929e+01
I1111 03:54:21.575104 140264174335808 run_lib.py:153] step: 116500, training_loss: 1.01337e+02
I1111 03:54:31.571986 140264174335808 run_lib.py:153] step: 116550, training_loss: 1.20416e+02
I1111 03:54:42.243065 140264174335808 run_lib.py:153] step: 116600, training_loss: 1.23056e+02
I1111 03:54:51.981912 140264174335808 run_lib.py:153] step: 116650, training_loss: 1.35539e+02
I1111 03:55:01.526489 140264174335808 run_lib.py:153] step: 116700, training_loss: 1.30372e+02
I1111 03:55:11.834219 140264174335808 run_lib.py:153] step: 116750, training_loss: 1.47695e+02
I1111 03:55:21.678129 140264174335808 run_lib.py:153] step: 116800, training_loss: 9.87322e+01
I1111 03:55:31.945746 140264174335808 run_lib.py:153] step: 116850, training_loss: 1.66355e+02
I1111 03:55:42.359700 140264174335808 run_lib.py:153] step: 116900, training_loss: 1.20547e+02
I1111 03:55:52.750049 140264174335808 run_lib.py:153] step: 116950, training_loss: 1.38965e+02
I1111 03:56:03.951820 140264174335808 run_lib.py:153] step: 117000, training_loss: 1.25894e+02
I1111 03:56:13.995625 140264174335808 run_lib.py:153] step: 117050, training_loss: 1.30623e+02
I1111 03:56:24.231403 140264174335808 run_lib.py:153] step: 117100, training_loss: 1.26033e+02
I1111 03:56:34.425555 140264174335808 run_lib.py:153] step: 117150, training_loss: 1.40125e+02
I1111 03:56:44.541314 140264174335808 run_lib.py:153] step: 117200, training_loss: 1.42544e+02
I1111 03:56:54.730132 140264174335808 run_lib.py:153] step: 117250, training_loss: 1.21924e+02
I1111 03:57:05.008308 140264174335808 run_lib.py:153] step: 117300, training_loss: 1.05398e+02
I1111 03:57:15.427046 140264174335808 run_lib.py:153] step: 117350, training_loss: 1.23258e+02
I1111 03:57:25.771176 140264174335808 run_lib.py:153] step: 117400, training_loss: 1.23589e+02
I1111 03:57:35.711731 140264174335808 run_lib.py:153] step: 117450, training_loss: 1.33235e+02
I1111 03:57:45.632352 140264174335808 run_lib.py:153] step: 117500, training_loss: 1.71188e+02
I1111 03:57:55.759934 140264174335808 run_lib.py:153] step: 117550, training_loss: 1.34067e+02
I1111 03:58:05.912439 140264174335808 run_lib.py:153] step: 117600, training_loss: 1.22940e+02
I1111 03:58:16.099011 140264174335808 run_lib.py:153] step: 117650, training_loss: 1.11522e+02
I1111 03:58:26.229305 140264174335808 run_lib.py:153] step: 117700, training_loss: 9.50586e+01
I1111 03:58:36.438140 140264174335808 run_lib.py:153] step: 117750, training_loss: 1.05704e+02
I1111 03:58:46.330516 140264174335808 run_lib.py:153] step: 117800, training_loss: 1.30816e+02
I1111 03:58:56.079427 140264174335808 run_lib.py:153] step: 117850, training_loss: 1.48506e+02
I1111 03:59:05.794033 140264174335808 run_lib.py:153] step: 117900, training_loss: 9.80872e+01
I1111 03:59:16.574009 140264174335808 run_lib.py:153] step: 117950, training_loss: 1.52783e+02
I1111 03:59:26.635558 140264174335808 run_lib.py:153] step: 118000, training_loss: 1.36137e+02
I1111 03:59:36.689583 140264174335808 run_lib.py:153] step: 118050, training_loss: 1.20221e+02
I1111 03:59:46.345901 140264174335808 run_lib.py:153] step: 118100, training_loss: 1.35353e+02
I1111 03:59:56.116174 140264174335808 run_lib.py:153] step: 118150, training_loss: 1.32787e+02
I1111 04:00:05.563615 140264174335808 run_lib.py:153] step: 118200, training_loss: 1.12976e+02
I1111 04:00:15.479952 140264174335808 run_lib.py:153] step: 118250, training_loss: 1.18655e+02
I1111 04:00:25.592999 140264174335808 run_lib.py:153] step: 118300, training_loss: 1.36029e+02
I1111 04:00:35.513773 140264174335808 run_lib.py:153] step: 118350, training_loss: 1.22763e+02
I1111 04:00:44.907504 140264174335808 run_lib.py:153] step: 118400, training_loss: 1.23558e+02
I1111 04:00:55.299820 140264174335808 run_lib.py:153] step: 118450, training_loss: 1.11495e+02
I1111 04:01:06.029365 140264174335808 run_lib.py:153] step: 118500, training_loss: 1.29055e+02
I1111 04:01:16.037729 140264174335808 run_lib.py:153] step: 118550, training_loss: 1.30163e+02
I1111 04:01:25.698030 140264174335808 run_lib.py:153] step: 118600, training_loss: 1.21595e+02
I1111 04:01:36.232678 140264174335808 run_lib.py:153] step: 118650, training_loss: 1.09765e+02
I1111 04:01:46.746089 140264174335808 run_lib.py:153] step: 118700, training_loss: 1.53683e+02
I1111 04:01:56.833599 140264174335808 run_lib.py:153] step: 118750, training_loss: 1.22686e+02
I1111 04:02:07.567957 140264174335808 run_lib.py:153] step: 118800, training_loss: 1.17035e+02
I1111 04:02:18.118891 140264174335808 run_lib.py:153] step: 118850, training_loss: 1.13900e+02
I1111 04:02:28.567978 140264174335808 run_lib.py:153] step: 118900, training_loss: 1.18327e+02
I1111 04:02:39.284391 140264174335808 run_lib.py:153] step: 118950, training_loss: 1.08711e+02
I1111 04:02:49.692345 140264174335808 run_lib.py:153] step: 119000, training_loss: 1.07832e+02
I1111 04:02:59.193244 140264174335808 run_lib.py:153] step: 119050, training_loss: 1.17446e+02
I1111 04:03:08.742937 140264174335808 run_lib.py:153] step: 119100, training_loss: 1.56363e+02
I1111 04:03:18.650806 140264174335808 run_lib.py:153] step: 119150, training_loss: 1.35465e+02
I1111 04:03:28.496036 140264174335808 run_lib.py:153] step: 119200, training_loss: 1.66848e+02
I1111 04:03:38.716784 140264174335808 run_lib.py:153] step: 119250, training_loss: 1.26349e+02
I1111 04:03:48.275226 140264174335808 run_lib.py:153] step: 119300, training_loss: 1.10781e+02
I1111 04:03:58.139560 140264174335808 run_lib.py:153] step: 119350, training_loss: 1.01592e+02
I1111 04:04:07.794309 140264174335808 run_lib.py:153] step: 119400, training_loss: 1.41337e+02
I1111 04:04:17.931810 140264174335808 run_lib.py:153] step: 119450, training_loss: 1.30376e+02
I1111 04:04:28.559074 140264174335808 run_lib.py:153] step: 119500, training_loss: 1.34253e+02
I1111 04:04:38.729963 140264174335808 run_lib.py:153] step: 119550, training_loss: 1.14204e+02
I1111 04:04:48.985163 140264174335808 run_lib.py:153] step: 119600, training_loss: 1.38010e+02
I1111 04:04:59.147166 140264174335808 run_lib.py:153] step: 119650, training_loss: 1.44413e+02
I1111 04:05:09.149633 140264174335808 run_lib.py:153] step: 119700, training_loss: 1.50532e+02
I1111 04:05:18.785394 140264174335808 run_lib.py:153] step: 119750, training_loss: 1.42629e+02
I1111 04:05:28.415196 140264174335808 run_lib.py:153] step: 119800, training_loss: 1.46149e+02
I1111 04:05:38.566969 140264174335808 run_lib.py:153] step: 119850, training_loss: 1.42380e+02
I1111 04:05:48.063117 140264174335808 run_lib.py:153] step: 119900, training_loss: 1.44087e+02
I1111 04:05:58.313039 140264174335808 run_lib.py:153] step: 119950, training_loss: 1.52175e+02
I1111 04:06:08.650587 140264174335808 run_lib.py:153] step: 120000, training_loss: 1.43780e+02
I1111 04:06:09.283661 140264174335808 run_lib.py:166] step: 120000, eval_loss: 1.28149e+02
I1111 04:06:19.422411 140264174335808 run_lib.py:153] step: 120050, training_loss: 1.37053e+02
I1111 04:06:30.110310 140264174335808 run_lib.py:153] step: 120100, training_loss: 1.37383e+02
I1111 04:06:40.033602 140264174335808 run_lib.py:153] step: 120150, training_loss: 1.20206e+02
I1111 04:06:49.431901 140264174335808 run_lib.py:153] step: 120200, training_loss: 1.22122e+02
I1111 04:06:59.767265 140264174335808 run_lib.py:153] step: 120250, training_loss: 1.42581e+02
I1111 04:07:09.760693 140264174335808 run_lib.py:153] step: 120300, training_loss: 1.12844e+02
I1111 04:07:19.342522 140264174335808 run_lib.py:153] step: 120350, training_loss: 1.11049e+02
I1111 04:07:29.215591 140264174335808 run_lib.py:153] step: 120400, training_loss: 1.20113e+02
I1111 04:07:39.691992 140264174335808 run_lib.py:153] step: 120450, training_loss: 1.24014e+02
I1111 04:07:49.323055 140264174335808 run_lib.py:153] step: 120500, training_loss: 1.22196e+02
I1111 04:07:59.302605 140264174335808 run_lib.py:153] step: 120550, training_loss: 1.28342e+02
I1111 04:08:08.882038 140264174335808 run_lib.py:153] step: 120600, training_loss: 1.61286e+02
I1111 04:08:18.556019 140264174335808 run_lib.py:153] step: 120650, training_loss: 1.07409e+02
I1111 04:08:28.416389 140264174335808 run_lib.py:153] step: 120700, training_loss: 1.25124e+02
I1111 04:08:38.523377 140264174335808 run_lib.py:153] step: 120750, training_loss: 1.19213e+02
I1111 04:08:49.139440 140264174335808 run_lib.py:153] step: 120800, training_loss: 1.23691e+02
I1111 04:08:58.532979 140264174335808 run_lib.py:153] step: 120850, training_loss: 1.59607e+02
I1111 04:09:08.158771 140264174335808 run_lib.py:153] step: 120900, training_loss: 1.46475e+02
I1111 04:09:18.127213 140264174335808 run_lib.py:153] step: 120950, training_loss: 1.22136e+02
I1111 04:09:28.199742 140264174335808 run_lib.py:153] step: 121000, training_loss: 1.33393e+02
I1111 04:09:38.572574 140264174335808 run_lib.py:153] step: 121050, training_loss: 1.07448e+02
I1111 04:09:48.911029 140264174335808 run_lib.py:153] step: 121100, training_loss: 1.04639e+02
I1111 04:09:59.055919 140264174335808 run_lib.py:153] step: 121150, training_loss: 1.21544e+02
I1111 04:10:09.097682 140264174335808 run_lib.py:153] step: 121200, training_loss: 1.27019e+02
I1111 04:10:19.190553 140264174335808 run_lib.py:153] step: 121250, training_loss: 1.40243e+02
I1111 04:10:28.633864 140264174335808 run_lib.py:153] step: 121300, training_loss: 1.37163e+02
I1111 04:10:38.320436 140264174335808 run_lib.py:153] step: 121350, training_loss: 1.48102e+02
I1111 04:10:47.768909 140264174335808 run_lib.py:153] step: 121400, training_loss: 1.11685e+02
I1111 04:10:57.668610 140264174335808 run_lib.py:153] step: 121450, training_loss: 1.40605e+02
I1111 04:11:07.051459 140264174335808 run_lib.py:153] step: 121500, training_loss: 1.31276e+02
I1111 04:11:16.715922 140264174335808 run_lib.py:153] step: 121550, training_loss: 1.06561e+02
I1111 04:11:26.481975 140264174335808 run_lib.py:153] step: 121600, training_loss: 1.15459e+02
I1111 04:11:36.429915 140264174335808 run_lib.py:153] step: 121650, training_loss: 1.26433e+02
I1111 04:11:46.603229 140264174335808 run_lib.py:153] step: 121700, training_loss: 1.31174e+02
I1111 04:11:56.142040 140264174335808 run_lib.py:153] step: 121750, training_loss: 1.55411e+02
I1111 04:12:06.718453 140264174335808 run_lib.py:153] step: 121800, training_loss: 1.19493e+02
I1111 04:12:17.690911 140264174335808 run_lib.py:153] step: 121850, training_loss: 1.31321e+02
I1111 04:12:27.801842 140264174335808 run_lib.py:153] step: 121900, training_loss: 1.34135e+02
I1111 04:12:37.983997 140264174335808 run_lib.py:153] step: 121950, training_loss: 1.18230e+02
I1111 04:12:47.827784 140264174335808 run_lib.py:153] step: 122000, training_loss: 1.19892e+02
I1111 04:12:58.151499 140264174335808 run_lib.py:153] step: 122050, training_loss: 1.14688e+02
I1111 04:13:07.938523 140264174335808 run_lib.py:153] step: 122100, training_loss: 1.03392e+02
I1111 04:13:17.827786 140264174335808 run_lib.py:153] step: 122150, training_loss: 1.23354e+02
I1111 04:13:27.625679 140264174335808 run_lib.py:153] step: 122200, training_loss: 1.12568e+02
I1111 04:13:37.359702 140264174335808 run_lib.py:153] step: 122250, training_loss: 1.06033e+02
I1111 04:13:46.864848 140264174335808 run_lib.py:153] step: 122300, training_loss: 1.15142e+02
I1111 04:13:56.333808 140264174335808 run_lib.py:153] step: 122350, training_loss: 1.12604e+02
I1111 04:14:06.102809 140264174335808 run_lib.py:153] step: 122400, training_loss: 1.38735e+02
I1111 04:14:15.850680 140264174335808 run_lib.py:153] step: 122450, training_loss: 1.44021e+02
I1111 04:14:26.160633 140264174335808 run_lib.py:153] step: 122500, training_loss: 1.67515e+02
I1111 04:14:36.743991 140264174335808 run_lib.py:153] step: 122550, training_loss: 1.44885e+02
I1111 04:14:46.602551 140264174335808 run_lib.py:153] step: 122600, training_loss: 1.09075e+02
I1111 04:14:56.459792 140264174335808 run_lib.py:153] step: 122650, training_loss: 1.48255e+02
I1111 04:15:06.478880 140264174335808 run_lib.py:153] step: 122700, training_loss: 1.20066e+02
I1111 04:15:16.476935 140264174335808 run_lib.py:153] step: 122750, training_loss: 8.92909e+01
I1111 04:15:26.535545 140264174335808 run_lib.py:153] step: 122800, training_loss: 1.21720e+02
I1111 04:15:36.472623 140264174335808 run_lib.py:153] step: 122850, training_loss: 1.15327e+02
I1111 04:15:45.844300 140264174335808 run_lib.py:153] step: 122900, training_loss: 1.21994e+02
I1111 04:15:56.066149 140264174335808 run_lib.py:153] step: 122950, training_loss: 1.56018e+02
I1111 04:16:06.510126 140264174335808 run_lib.py:153] step: 123000, training_loss: 1.58473e+02
I1111 04:16:16.801876 140264174335808 run_lib.py:153] step: 123050, training_loss: 1.37212e+02
I1111 04:16:27.451484 140264174335808 run_lib.py:153] step: 123100, training_loss: 1.12693e+02
I1111 04:16:37.624637 140264174335808 run_lib.py:153] step: 123150, training_loss: 1.13004e+02
I1111 04:16:47.680600 140264174335808 run_lib.py:153] step: 123200, training_loss: 1.38610e+02
I1111 04:16:57.742574 140264174335808 run_lib.py:153] step: 123250, training_loss: 1.48625e+02
I1111 04:17:07.164710 140264174335808 run_lib.py:153] step: 123300, training_loss: 1.25611e+02
I1111 04:17:17.334724 140264174335808 run_lib.py:153] step: 123350, training_loss: 1.27086e+02
I1111 04:17:26.908189 140264174335808 run_lib.py:153] step: 123400, training_loss: 1.12971e+02
I1111 04:17:37.456151 140264174335808 run_lib.py:153] step: 123450, training_loss: 1.26799e+02
I1111 04:17:47.413257 140264174335808 run_lib.py:153] step: 123500, training_loss: 1.47203e+02
I1111 04:17:57.875065 140264174335808 run_lib.py:153] step: 123550, training_loss: 1.50949e+02
I1111 04:18:08.099942 140264174335808 run_lib.py:153] step: 123600, training_loss: 1.38798e+02
I1111 04:18:18.267270 140264174335808 run_lib.py:153] step: 123650, training_loss: 1.50732e+02
I1111 04:18:28.323507 140264174335808 run_lib.py:153] step: 123700, training_loss: 1.13043e+02
I1111 04:18:38.447332 140264174335808 run_lib.py:153] step: 123750, training_loss: 1.49298e+02
I1111 04:18:48.575865 140264174335808 run_lib.py:153] step: 123800, training_loss: 1.29561e+02
I1111 04:18:58.602186 140264174335808 run_lib.py:153] step: 123850, training_loss: 1.54336e+02
I1111 04:19:09.065659 140264174335808 run_lib.py:153] step: 123900, training_loss: 1.17691e+02
I1111 04:19:19.403863 140264174335808 run_lib.py:153] step: 123950, training_loss: 1.47877e+02
I1111 04:19:29.543420 140264174335808 run_lib.py:153] step: 124000, training_loss: 1.33996e+02
I1111 04:19:39.504658 140264174335808 run_lib.py:153] step: 124050, training_loss: 1.06501e+02
I1111 04:19:49.324847 140264174335808 run_lib.py:153] step: 124100, training_loss: 1.04207e+02
I1111 04:19:59.419280 140264174335808 run_lib.py:153] step: 124150, training_loss: 1.34946e+02
I1111 04:20:09.598173 140264174335808 run_lib.py:153] step: 124200, training_loss: 1.20254e+02
I1111 04:20:20.263607 140264174335808 run_lib.py:153] step: 124250, training_loss: 1.21430e+02
I1111 04:20:31.125594 140264174335808 run_lib.py:153] step: 124300, training_loss: 1.44807e+02
I1111 04:20:41.432860 140264174335808 run_lib.py:153] step: 124350, training_loss: 1.47158e+02
I1111 04:20:51.648913 140264174335808 run_lib.py:153] step: 124400, training_loss: 1.42083e+02
I1111 04:21:02.114659 140264174335808 run_lib.py:153] step: 124450, training_loss: 1.38675e+02
I1111 04:21:12.530957 140264174335808 run_lib.py:153] step: 124500, training_loss: 1.60113e+02
I1111 04:21:22.677935 140264174335808 run_lib.py:153] step: 124550, training_loss: 1.21886e+02
I1111 04:21:32.983119 140264174335808 run_lib.py:153] step: 124600, training_loss: 1.26307e+02
I1111 04:21:43.292066 140264174335808 run_lib.py:153] step: 124650, training_loss: 1.56924e+02
I1111 04:21:54.062989 140264174335808 run_lib.py:153] step: 124700, training_loss: 1.16551e+02
I1111 04:22:03.714486 140264174335808 run_lib.py:153] step: 124750, training_loss: 1.47520e+02
I1111 04:22:13.599728 140264174335808 run_lib.py:153] step: 124800, training_loss: 1.17592e+02
I1111 04:22:23.097277 140264174335808 run_lib.py:153] step: 124850, training_loss: 1.28774e+02
I1111 04:22:32.861716 140264174335808 run_lib.py:153] step: 124900, training_loss: 1.39010e+02
I1111 04:22:43.237948 140264174335808 run_lib.py:153] step: 124950, training_loss: 1.55273e+02
I1111 04:22:52.950096 140264174335808 run_lib.py:153] step: 125000, training_loss: 1.16280e+02
I1111 04:22:53.070264 140264174335808 run_lib.py:166] step: 125000, eval_loss: 1.29265e+02
I1111 04:23:02.359808 140264174335808 run_lib.py:153] step: 125050, training_loss: 1.31934e+02
I1111 04:23:11.633996 140264174335808 run_lib.py:153] step: 125100, training_loss: 1.10632e+02
I1111 04:23:21.522805 140264174335808 run_lib.py:153] step: 125150, training_loss: 1.41631e+02
I1111 04:23:31.149531 140264174335808 run_lib.py:153] step: 125200, training_loss: 1.29790e+02
I1111 04:23:40.886300 140264174335808 run_lib.py:153] step: 125250, training_loss: 1.45741e+02
I1111 04:23:50.136293 140264174335808 run_lib.py:153] step: 125300, training_loss: 1.41152e+02
I1111 04:23:59.901799 140264174335808 run_lib.py:153] step: 125350, training_loss: 1.08996e+02
I1111 04:24:10.443424 140264174335808 run_lib.py:153] step: 125400, training_loss: 1.17597e+02
I1111 04:24:21.149895 140264174335808 run_lib.py:153] step: 125450, training_loss: 1.20107e+02
I1111 04:24:31.086107 140264174335808 run_lib.py:153] step: 125500, training_loss: 1.59919e+02
I1111 04:24:40.958610 140264174335808 run_lib.py:153] step: 125550, training_loss: 1.23485e+02
I1111 04:24:51.007128 140264174335808 run_lib.py:153] step: 125600, training_loss: 1.52119e+02
I1111 04:25:01.581532 140264174335808 run_lib.py:153] step: 125650, training_loss: 1.20127e+02
I1111 04:25:11.734754 140264174335808 run_lib.py:153] step: 125700, training_loss: 1.22716e+02
I1111 04:25:22.437220 140264174335808 run_lib.py:153] step: 125750, training_loss: 9.94646e+01
I1111 04:25:32.226227 140264174335808 run_lib.py:153] step: 125800, training_loss: 1.24390e+02
I1111 04:25:42.358508 140264174335808 run_lib.py:153] step: 125850, training_loss: 1.15047e+02
I1111 04:25:52.830897 140264174335808 run_lib.py:153] step: 125900, training_loss: 1.04921e+02
I1111 04:26:03.367017 140264174335808 run_lib.py:153] step: 125950, training_loss: 1.35221e+02
I1111 04:26:13.797979 140264174335808 run_lib.py:153] step: 126000, training_loss: 1.20588e+02
I1111 04:26:24.281958 140264174335808 run_lib.py:153] step: 126050, training_loss: 1.31259e+02
I1111 04:26:34.101461 140264174335808 run_lib.py:153] step: 126100, training_loss: 1.39151e+02
I1111 04:26:43.896856 140264174335808 run_lib.py:153] step: 126150, training_loss: 1.38895e+02
I1111 04:26:53.905871 140264174335808 run_lib.py:153] step: 126200, training_loss: 1.41861e+02
I1111 04:27:03.367482 140264174335808 run_lib.py:153] step: 126250, training_loss: 1.08831e+02
I1111 04:27:13.263751 140264174335808 run_lib.py:153] step: 126300, training_loss: 1.33347e+02
I1111 04:27:22.877410 140264174335808 run_lib.py:153] step: 126350, training_loss: 1.44705e+02
I1111 04:27:33.154891 140264174335808 run_lib.py:153] step: 126400, training_loss: 1.18861e+02
I1111 04:27:43.135821 140264174335808 run_lib.py:153] step: 126450, training_loss: 1.08736e+02
I1111 04:27:53.100858 140264174335808 run_lib.py:153] step: 126500, training_loss: 1.47489e+02
I1111 04:28:03.461701 140264174335808 run_lib.py:153] step: 126550, training_loss: 1.45674e+02
I1111 04:28:13.752917 140264174335808 run_lib.py:153] step: 126600, training_loss: 1.30661e+02
I1111 04:28:23.430974 140264174335808 run_lib.py:153] step: 126650, training_loss: 1.48780e+02
I1111 04:28:33.909250 140264174335808 run_lib.py:153] step: 126700, training_loss: 1.31459e+02
I1111 04:28:44.044680 140264174335808 run_lib.py:153] step: 126750, training_loss: 1.35650e+02
I1111 04:28:54.095558 140264174335808 run_lib.py:153] step: 126800, training_loss: 1.11952e+02
I1111 04:29:03.857020 140264174335808 run_lib.py:153] step: 126850, training_loss: 9.93968e+01
I1111 04:29:13.475091 140264174335808 run_lib.py:153] step: 126900, training_loss: 1.21003e+02
I1111 04:29:23.619464 140264174335808 run_lib.py:153] step: 126950, training_loss: 1.11995e+02
I1111 04:29:33.352901 140264174335808 run_lib.py:153] step: 127000, training_loss: 1.45977e+02
I1111 04:29:43.260496 140264174335808 run_lib.py:153] step: 127050, training_loss: 1.31112e+02
I1111 04:29:53.371270 140264174335808 run_lib.py:153] step: 127100, training_loss: 1.51547e+02
I1111 04:30:03.495352 140264174335808 run_lib.py:153] step: 127150, training_loss: 1.13095e+02
I1111 04:30:13.582263 140264174335808 run_lib.py:153] step: 127200, training_loss: 1.44833e+02
I1111 04:30:23.709237 140264174335808 run_lib.py:153] step: 127250, training_loss: 1.41898e+02
I1111 04:30:33.602618 140264174335808 run_lib.py:153] step: 127300, training_loss: 1.03029e+02
I1111 04:30:43.325264 140264174335808 run_lib.py:153] step: 127350, training_loss: 1.50071e+02
I1111 04:30:53.548830 140264174335808 run_lib.py:153] step: 127400, training_loss: 1.15619e+02
I1111 04:31:03.194760 140264174335808 run_lib.py:153] step: 127450, training_loss: 1.12049e+02
I1111 04:31:13.185972 140264174335808 run_lib.py:153] step: 127500, training_loss: 1.12816e+02
I1111 04:31:23.183532 140264174335808 run_lib.py:153] step: 127550, training_loss: 9.01134e+01
I1111 04:31:33.311830 140264174335808 run_lib.py:153] step: 127600, training_loss: 1.17815e+02
I1111 04:31:43.330954 140264174335808 run_lib.py:153] step: 127650, training_loss: 9.53654e+01
I1111 04:31:53.386173 140264174335808 run_lib.py:153] step: 127700, training_loss: 1.27178e+02
I1111 04:32:03.553327 140264174335808 run_lib.py:153] step: 127750, training_loss: 1.14000e+02
I1111 04:32:13.222903 140264174335808 run_lib.py:153] step: 127800, training_loss: 1.72242e+02
I1111 04:32:22.941853 140264174335808 run_lib.py:153] step: 127850, training_loss: 1.15659e+02
I1111 04:32:32.522314 140264174335808 run_lib.py:153] step: 127900, training_loss: 1.28963e+02
I1111 04:32:42.742381 140264174335808 run_lib.py:153] step: 127950, training_loss: 1.36235e+02
I1111 04:32:52.612529 140264174335808 run_lib.py:153] step: 128000, training_loss: 1.31892e+02
I1111 04:33:02.759463 140264174335808 run_lib.py:153] step: 128050, training_loss: 1.32861e+02
I1111 04:33:12.563626 140264174335808 run_lib.py:153] step: 128100, training_loss: 1.09842e+02
I1111 04:33:22.585910 140264174335808 run_lib.py:153] step: 128150, training_loss: 1.08883e+02
I1111 04:33:32.226983 140264174335808 run_lib.py:153] step: 128200, training_loss: 1.30135e+02
I1111 04:33:41.919090 140264174335808 run_lib.py:153] step: 128250, training_loss: 1.08682e+02
I1111 04:33:51.560515 140264174335808 run_lib.py:153] step: 128300, training_loss: 1.22975e+02
I1111 04:34:02.206203 140264174335808 run_lib.py:153] step: 128350, training_loss: 1.18232e+02
I1111 04:34:12.017472 140264174335808 run_lib.py:153] step: 128400, training_loss: 1.23221e+02
I1111 04:34:21.735728 140264174335808 run_lib.py:153] step: 128450, training_loss: 7.97110e+01
I1111 04:34:31.315332 140264174335808 run_lib.py:153] step: 128500, training_loss: 1.35093e+02
I1111 04:34:41.247706 140264174335808 run_lib.py:153] step: 128550, training_loss: 1.41165e+02
I1111 04:34:51.260612 140264174335808 run_lib.py:153] step: 128600, training_loss: 1.21503e+02
I1111 04:35:01.199191 140264174335808 run_lib.py:153] step: 128650, training_loss: 1.35773e+02
I1111 04:35:11.441353 140264174335808 run_lib.py:153] step: 128700, training_loss: 1.51770e+02
I1111 04:35:21.301170 140264174335808 run_lib.py:153] step: 128750, training_loss: 1.68807e+02
I1111 04:35:31.196422 140264174335808 run_lib.py:153] step: 128800, training_loss: 9.85452e+01
I1111 04:35:41.057612 140264174335808 run_lib.py:153] step: 128850, training_loss: 1.23098e+02
I1111 04:35:51.167312 140264174335808 run_lib.py:153] step: 128900, training_loss: 1.13810e+02
I1111 04:36:01.346063 140264174335808 run_lib.py:153] step: 128950, training_loss: 1.33825e+02
I1111 04:36:11.051064 140264174335808 run_lib.py:153] step: 129000, training_loss: 1.18706e+02
I1111 04:36:21.474797 140264174335808 run_lib.py:153] step: 129050, training_loss: 1.29910e+02
I1111 04:36:31.647409 140264174335808 run_lib.py:153] step: 129100, training_loss: 1.26389e+02
I1111 04:36:41.388006 140264174335808 run_lib.py:153] step: 129150, training_loss: 1.49442e+02
I1111 04:36:51.738332 140264174335808 run_lib.py:153] step: 129200, training_loss: 1.31793e+02
I1111 04:37:02.406089 140264174335808 run_lib.py:153] step: 129250, training_loss: 1.42022e+02
I1111 04:37:13.262018 140264174335808 run_lib.py:153] step: 129300, training_loss: 1.36496e+02
I1111 04:37:24.290109 140264174335808 run_lib.py:153] step: 129350, training_loss: 1.31601e+02
I1111 04:37:34.736703 140264174335808 run_lib.py:153] step: 129400, training_loss: 1.18082e+02
I1111 04:37:44.356357 140264174335808 run_lib.py:153] step: 129450, training_loss: 1.21872e+02
I1111 04:37:55.341959 140264174335808 run_lib.py:153] step: 129500, training_loss: 1.25751e+02
I1111 04:38:04.976913 140264174335808 run_lib.py:153] step: 129550, training_loss: 1.32072e+02
I1111 04:38:15.207613 140264174335808 run_lib.py:153] step: 129600, training_loss: 9.40447e+01
I1111 04:38:25.613585 140264174335808 run_lib.py:153] step: 129650, training_loss: 1.59193e+02
I1111 04:38:35.750334 140264174335808 run_lib.py:153] step: 129700, training_loss: 1.24635e+02
I1111 04:38:45.920999 140264174335808 run_lib.py:153] step: 129750, training_loss: 1.08097e+02
I1111 04:38:55.631204 140264174335808 run_lib.py:153] step: 129800, training_loss: 1.48123e+02
I1111 04:39:05.878557 140264174335808 run_lib.py:153] step: 129850, training_loss: 1.17716e+02
I1111 04:39:16.098228 140264174335808 run_lib.py:153] step: 129900, training_loss: 1.08883e+02
I1111 04:39:25.664406 140264174335808 run_lib.py:153] step: 129950, training_loss: 1.42428e+02
I1111 04:39:35.291661 140264174335808 run_lib.py:153] step: 130000, training_loss: 1.52831e+02
I1111 04:39:35.861244 140264174335808 run_lib.py:166] step: 130000, eval_loss: 1.26104e+02
I1111 04:39:45.454359 140264174335808 run_lib.py:153] step: 130050, training_loss: 1.29934e+02
I1111 04:39:56.896426 140264174335808 run_lib.py:153] step: 130100, training_loss: 1.23071e+02
I1111 04:40:06.876111 140264174335808 run_lib.py:153] step: 130150, training_loss: 9.59614e+01
I1111 04:40:16.566018 140264174335808 run_lib.py:153] step: 130200, training_loss: 1.31100e+02
I1111 04:40:26.443231 140264174335808 run_lib.py:153] step: 130250, training_loss: 1.42368e+02
I1111 04:40:36.625820 140264174335808 run_lib.py:153] step: 130300, training_loss: 1.35647e+02
I1111 04:40:46.568666 140264174335808 run_lib.py:153] step: 130350, training_loss: 1.10002e+02
I1111 04:40:56.926140 140264174335808 run_lib.py:153] step: 130400, training_loss: 1.43144e+02
I1111 04:41:06.527689 140264174335808 run_lib.py:153] step: 130450, training_loss: 1.43946e+02
I1111 04:41:16.651041 140264174335808 run_lib.py:153] step: 130500, training_loss: 1.47575e+02
I1111 04:41:26.614127 140264174335808 run_lib.py:153] step: 130550, training_loss: 1.24762e+02
I1111 04:41:37.132371 140264174335808 run_lib.py:153] step: 130600, training_loss: 1.45603e+02
I1111 04:41:47.986626 140264174335808 run_lib.py:153] step: 130650, training_loss: 1.36837e+02
I1111 04:41:57.725173 140264174335808 run_lib.py:153] step: 130700, training_loss: 1.24328e+02
I1111 04:42:07.627620 140264174335808 run_lib.py:153] step: 130750, training_loss: 1.52103e+02
I1111 04:42:17.753228 140264174335808 run_lib.py:153] step: 130800, training_loss: 1.68541e+02
I1111 04:42:28.004036 140264174335808 run_lib.py:153] step: 130850, training_loss: 1.31136e+02
I1111 04:42:38.369883 140264174335808 run_lib.py:153] step: 130900, training_loss: 1.47449e+02
I1111 04:42:48.452638 140264174335808 run_lib.py:153] step: 130950, training_loss: 1.28540e+02
I1111 04:42:58.355767 140264174335808 run_lib.py:153] step: 131000, training_loss: 1.26425e+02
I1111 04:43:07.634072 140264174335808 run_lib.py:153] step: 131050, training_loss: 1.19541e+02
I1111 04:43:17.459615 140264174335808 run_lib.py:153] step: 131100, training_loss: 1.48311e+02
I1111 04:43:27.912286 140264174335808 run_lib.py:153] step: 131150, training_loss: 1.35310e+02
I1111 04:43:38.321984 140264174335808 run_lib.py:153] step: 131200, training_loss: 1.16142e+02
I1111 04:43:48.796930 140264174335808 run_lib.py:153] step: 131250, training_loss: 1.49960e+02
I1111 04:43:58.851910 140264174335808 run_lib.py:153] step: 131300, training_loss: 1.19771e+02
I1111 04:44:08.764377 140264174335808 run_lib.py:153] step: 131350, training_loss: 9.55869e+01
I1111 04:44:18.805676 140264174335808 run_lib.py:153] step: 131400, training_loss: 1.30613e+02
I1111 04:44:28.752868 140264174335808 run_lib.py:153] step: 131450, training_loss: 1.16943e+02
I1111 04:44:38.648829 140264174335808 run_lib.py:153] step: 131500, training_loss: 1.23587e+02
I1111 04:44:48.692852 140264174335808 run_lib.py:153] step: 131550, training_loss: 1.55714e+02
I1111 04:44:58.883570 140264174335808 run_lib.py:153] step: 131600, training_loss: 1.38363e+02
I1111 04:45:09.463150 140264174335808 run_lib.py:153] step: 131650, training_loss: 1.57624e+02
I1111 04:45:19.768841 140264174335808 run_lib.py:153] step: 131700, training_loss: 1.48718e+02
I1111 04:45:29.422365 140264174335808 run_lib.py:153] step: 131750, training_loss: 1.28298e+02
I1111 04:45:39.363626 140264174335808 run_lib.py:153] step: 131800, training_loss: 1.32437e+02
I1111 04:45:49.471558 140264174335808 run_lib.py:153] step: 131850, training_loss: 1.46685e+02
I1111 04:45:59.843763 140264174335808 run_lib.py:153] step: 131900, training_loss: 9.92919e+01
I1111 04:46:09.569210 140264174335808 run_lib.py:153] step: 131950, training_loss: 1.33778e+02
I1111 04:46:19.016249 140264174335808 run_lib.py:153] step: 132000, training_loss: 1.17002e+02
I1111 04:46:28.796127 140264174335808 run_lib.py:153] step: 132050, training_loss: 1.39153e+02
I1111 04:46:38.909859 140264174335808 run_lib.py:153] step: 132100, training_loss: 1.26726e+02
I1111 04:46:49.120244 140264174335808 run_lib.py:153] step: 132150, training_loss: 1.05856e+02
I1111 04:46:59.219800 140264174335808 run_lib.py:153] step: 132200, training_loss: 1.34604e+02
I1111 04:47:09.815629 140264174335808 run_lib.py:153] step: 132250, training_loss: 1.27598e+02
I1111 04:47:20.296578 140264174335808 run_lib.py:153] step: 132300, training_loss: 1.32172e+02
I1111 04:47:30.180440 140264174335808 run_lib.py:153] step: 132350, training_loss: 1.65350e+02
I1111 04:47:40.400337 140264174335808 run_lib.py:153] step: 132400, training_loss: 1.16844e+02
I1111 04:47:50.757850 140264174335808 run_lib.py:153] step: 132450, training_loss: 1.07490e+02
I1111 04:48:00.457510 140264174335808 run_lib.py:153] step: 132500, training_loss: 1.17401e+02
I1111 04:48:10.894589 140264174335808 run_lib.py:153] step: 132550, training_loss: 1.56663e+02
I1111 04:48:21.500363 140264174335808 run_lib.py:153] step: 132600, training_loss: 1.19782e+02
I1111 04:48:31.438346 140264174335808 run_lib.py:153] step: 132650, training_loss: 1.31398e+02
I1111 04:48:41.224476 140264174335808 run_lib.py:153] step: 132700, training_loss: 1.43061e+02
I1111 04:48:52.155223 140264174335808 run_lib.py:153] step: 132750, training_loss: 1.33557e+02
I1111 04:49:02.488118 140264174335808 run_lib.py:153] step: 132800, training_loss: 1.48263e+02
I1111 04:49:12.285471 140264174335808 run_lib.py:153] step: 132850, training_loss: 1.74124e+02
I1111 04:49:21.586207 140264174335808 run_lib.py:153] step: 132900, training_loss: 1.20032e+02
I1111 04:49:31.849046 140264174335808 run_lib.py:153] step: 132950, training_loss: 1.43994e+02
I1111 04:49:42.181095 140264174335808 run_lib.py:153] step: 133000, training_loss: 1.10347e+02
I1111 04:49:52.494978 140264174335808 run_lib.py:153] step: 133050, training_loss: 1.01971e+02
I1111 04:50:02.445505 140264174335808 run_lib.py:153] step: 133100, training_loss: 1.31168e+02
I1111 04:50:12.272576 140264174335808 run_lib.py:153] step: 133150, training_loss: 1.31293e+02
I1111 04:50:21.907051 140264174335808 run_lib.py:153] step: 133200, training_loss: 1.44188e+02
I1111 04:50:31.468887 140264174335808 run_lib.py:153] step: 133250, training_loss: 9.00968e+01
I1111 04:50:41.265918 140264174335808 run_lib.py:153] step: 133300, training_loss: 1.27042e+02
I1111 04:50:51.526543 140264174335808 run_lib.py:153] step: 133350, training_loss: 1.36015e+02
I1111 04:51:02.332055 140264174335808 run_lib.py:153] step: 133400, training_loss: 1.33775e+02
I1111 04:51:12.656952 140264174335808 run_lib.py:153] step: 133450, training_loss: 1.82616e+02
I1111 04:51:22.592395 140264174335808 run_lib.py:153] step: 133500, training_loss: 1.23243e+02
I1111 04:51:32.794800 140264174335808 run_lib.py:153] step: 133550, training_loss: 1.47658e+02
I1111 04:51:43.710054 140264174335808 run_lib.py:153] step: 133600, training_loss: 1.09265e+02
I1111 04:51:54.013687 140264174335808 run_lib.py:153] step: 133650, training_loss: 1.57490e+02
I1111 04:52:03.924753 140264174335808 run_lib.py:153] step: 133700, training_loss: 1.19276e+02
I1111 04:52:14.206706 140264174335808 run_lib.py:153] step: 133750, training_loss: 1.24602e+02
I1111 04:52:25.197167 140264174335808 run_lib.py:153] step: 133800, training_loss: 1.54051e+02
I1111 04:52:35.350356 140264174335808 run_lib.py:153] step: 133850, training_loss: 1.25142e+02
I1111 04:52:45.716797 140264174335808 run_lib.py:153] step: 133900, training_loss: 1.49869e+02
I1111 04:52:56.371783 140264174335808 run_lib.py:153] step: 133950, training_loss: 1.56602e+02
I1111 04:53:06.407776 140264174335808 run_lib.py:153] step: 134000, training_loss: 1.13411e+02
I1111 04:53:16.264709 140264174335808 run_lib.py:153] step: 134050, training_loss: 1.14937e+02
I1111 04:53:26.661209 140264174335808 run_lib.py:153] step: 134100, training_loss: 1.10672e+02
I1111 04:53:36.737234 140264174335808 run_lib.py:153] step: 134150, training_loss: 1.14245e+02
I1111 04:53:46.541173 140264174335808 run_lib.py:153] step: 134200, training_loss: 1.34147e+02
I1111 04:53:56.507734 140264174335808 run_lib.py:153] step: 134250, training_loss: 1.18788e+02
I1111 04:54:06.223492 140264174335808 run_lib.py:153] step: 134300, training_loss: 9.41029e+01
I1111 04:54:16.303894 140264174335808 run_lib.py:153] step: 134350, training_loss: 1.41696e+02
I1111 04:54:26.041448 140264174335808 run_lib.py:153] step: 134400, training_loss: 1.14108e+02
I1111 04:54:35.316159 140264174335808 run_lib.py:153] step: 134450, training_loss: 1.17696e+02
I1111 04:54:45.497781 140264174335808 run_lib.py:153] step: 134500, training_loss: 1.02616e+02
I1111 04:54:55.262665 140264174335808 run_lib.py:153] step: 134550, training_loss: 1.46142e+02
I1111 04:55:04.763266 140264174335808 run_lib.py:153] step: 134600, training_loss: 1.39965e+02
I1111 04:55:15.838404 140264174335808 run_lib.py:153] step: 134650, training_loss: 9.37606e+01
I1111 04:55:25.737102 140264174335808 run_lib.py:153] step: 134700, training_loss: 1.09428e+02
I1111 04:55:35.250793 140264174335808 run_lib.py:153] step: 134750, training_loss: 1.41444e+02
I1111 04:55:45.121444 140264174335808 run_lib.py:153] step: 134800, training_loss: 1.18232e+02
I1111 04:55:55.442070 140264174335808 run_lib.py:153] step: 134850, training_loss: 1.46797e+02
I1111 04:56:05.830609 140264174335808 run_lib.py:153] step: 134900, training_loss: 1.54022e+02
I1111 04:56:15.816077 140264174335808 run_lib.py:153] step: 134950, training_loss: 1.55098e+02
I1111 04:56:25.513055 140264174335808 run_lib.py:153] step: 135000, training_loss: 1.16022e+02
I1111 04:56:25.616143 140264174335808 run_lib.py:166] step: 135000, eval_loss: 1.24305e+02
I1111 04:56:35.675446 140264174335808 run_lib.py:153] step: 135050, training_loss: 1.48702e+02
I1111 04:56:46.052051 140264174335808 run_lib.py:153] step: 135100, training_loss: 1.39961e+02
I1111 04:56:55.933745 140264174335808 run_lib.py:153] step: 135150, training_loss: 1.29787e+02
I1111 04:57:05.809026 140264174335808 run_lib.py:153] step: 135200, training_loss: 1.07960e+02
I1111 04:57:15.443014 140264174335808 run_lib.py:153] step: 135250, training_loss: 1.14843e+02
I1111 04:57:25.404917 140264174335808 run_lib.py:153] step: 135300, training_loss: 1.17519e+02
I1111 04:57:35.981178 140264174335808 run_lib.py:153] step: 135350, training_loss: 1.36308e+02
I1111 04:57:46.061353 140264174335808 run_lib.py:153] step: 135400, training_loss: 1.49705e+02
I1111 04:57:56.142211 140264174335808 run_lib.py:153] step: 135450, training_loss: 1.15734e+02
I1111 04:58:06.231043 140264174335808 run_lib.py:153] step: 135500, training_loss: 9.75698e+01
I1111 04:58:16.179564 140264174335808 run_lib.py:153] step: 135550, training_loss: 1.11238e+02
I1111 04:58:26.620615 140264174335808 run_lib.py:153] step: 135600, training_loss: 1.42445e+02
I1111 04:58:36.385002 140264174335808 run_lib.py:153] step: 135650, training_loss: 1.31966e+02
I1111 04:58:46.339167 140264174335808 run_lib.py:153] step: 135700, training_loss: 1.08516e+02
I1111 04:58:56.240392 140264174335808 run_lib.py:153] step: 135750, training_loss: 1.15234e+02
I1111 04:59:06.756087 140264174335808 run_lib.py:153] step: 135800, training_loss: 1.34108e+02
I1111 04:59:16.770524 140264174335808 run_lib.py:153] step: 135850, training_loss: 1.21940e+02
I1111 04:59:27.195450 140264174335808 run_lib.py:153] step: 135900, training_loss: 1.24923e+02
I1111 04:59:37.400104 140264174335808 run_lib.py:153] step: 135950, training_loss: 1.16887e+02
I1111 04:59:48.196386 140264174335808 run_lib.py:153] step: 136000, training_loss: 1.19380e+02
I1111 04:59:58.268132 140264174335808 run_lib.py:153] step: 136050, training_loss: 1.32953e+02
I1111 05:00:08.473604 140264174335808 run_lib.py:153] step: 136100, training_loss: 1.13660e+02
I1111 05:00:17.681721 140264174335808 run_lib.py:153] step: 136150, training_loss: 1.25655e+02
I1111 05:00:27.250796 140264174335808 run_lib.py:153] step: 136200, training_loss: 1.70804e+02
I1111 05:00:36.997423 140264174335808 run_lib.py:153] step: 136250, training_loss: 1.27879e+02
I1111 05:00:47.199132 140264174335808 run_lib.py:153] step: 136300, training_loss: 1.38075e+02
I1111 05:00:56.731836 140264174335808 run_lib.py:153] step: 136350, training_loss: 1.21213e+02
I1111 05:01:06.683114 140264174335808 run_lib.py:153] step: 136400, training_loss: 1.47604e+02
I1111 05:01:16.561551 140264174335808 run_lib.py:153] step: 136450, training_loss: 1.36400e+02
I1111 05:01:26.447594 140264174335808 run_lib.py:153] step: 136500, training_loss: 1.04560e+02
I1111 05:01:37.026875 140264174335808 run_lib.py:153] step: 136550, training_loss: 1.34001e+02
I1111 05:01:47.890551 140264174335808 run_lib.py:153] step: 136600, training_loss: 1.20011e+02
I1111 05:01:58.166125 140264174335808 run_lib.py:153] step: 136650, training_loss: 1.61439e+02
I1111 05:02:08.316416 140264174335808 run_lib.py:153] step: 136700, training_loss: 1.15073e+02
I1111 05:02:18.135973 140264174335808 run_lib.py:153] step: 136750, training_loss: 1.42072e+02
I1111 05:02:28.067768 140264174335808 run_lib.py:153] step: 136800, training_loss: 1.23032e+02
I1111 05:02:38.018965 140264174335808 run_lib.py:153] step: 136850, training_loss: 1.15164e+02
I1111 05:02:48.767822 140264174335808 run_lib.py:153] step: 136900, training_loss: 1.28641e+02
I1111 05:02:58.598603 140264174335808 run_lib.py:153] step: 136950, training_loss: 1.44718e+02
I1111 05:03:08.112493 140264174335808 run_lib.py:153] step: 137000, training_loss: 1.42063e+02
I1111 05:03:17.888058 140264174335808 run_lib.py:153] step: 137050, training_loss: 1.43774e+02
I1111 05:03:27.313180 140264174335808 run_lib.py:153] step: 137100, training_loss: 1.31880e+02
I1111 05:03:36.956979 140264174335808 run_lib.py:153] step: 137150, training_loss: 1.41125e+02
I1111 05:03:47.028890 140264174335808 run_lib.py:153] step: 137200, training_loss: 1.30531e+02
I1111 05:03:56.745055 140264174335808 run_lib.py:153] step: 137250, training_loss: 1.21934e+02
I1111 05:04:06.601828 140264174335808 run_lib.py:153] step: 137300, training_loss: 1.14030e+02
I1111 05:04:16.480119 140264174335808 run_lib.py:153] step: 137350, training_loss: 1.28552e+02
I1111 05:04:26.160908 140264174335808 run_lib.py:153] step: 137400, training_loss: 1.29881e+02
I1111 05:04:35.980244 140264174335808 run_lib.py:153] step: 137450, training_loss: 1.30731e+02
I1111 05:04:45.708528 140264174335808 run_lib.py:153] step: 137500, training_loss: 1.35343e+02
I1111 05:04:56.043927 140264174335808 run_lib.py:153] step: 137550, training_loss: 1.26608e+02
I1111 05:05:06.625962 140264174335808 run_lib.py:153] step: 137600, training_loss: 1.55906e+02
I1111 05:05:16.461704 140264174335808 run_lib.py:153] step: 137650, training_loss: 1.60822e+02
I1111 05:05:26.620979 140264174335808 run_lib.py:153] step: 137700, training_loss: 1.25496e+02
I1111 05:05:37.133573 140264174335808 run_lib.py:153] step: 137750, training_loss: 1.13332e+02
I1111 05:05:47.294184 140264174335808 run_lib.py:153] step: 137800, training_loss: 1.32066e+02
I1111 05:05:57.275335 140264174335808 run_lib.py:153] step: 137850, training_loss: 1.28790e+02
I1111 05:06:06.657703 140264174335808 run_lib.py:153] step: 137900, training_loss: 1.25669e+02
I1111 05:06:16.163270 140264174335808 run_lib.py:153] step: 137950, training_loss: 1.20778e+02
I1111 05:06:26.581376 140264174335808 run_lib.py:153] step: 138000, training_loss: 1.31690e+02
I1111 05:06:36.214427 140264174335808 run_lib.py:153] step: 138050, training_loss: 1.58007e+02
I1111 05:06:46.319082 140264174335808 run_lib.py:153] step: 138100, training_loss: 1.39338e+02
I1111 05:06:56.506695 140264174335808 run_lib.py:153] step: 138150, training_loss: 1.26089e+02
I1111 05:07:06.769782 140264174335808 run_lib.py:153] step: 138200, training_loss: 1.22655e+02
I1111 05:07:16.986189 140264174335808 run_lib.py:153] step: 138250, training_loss: 1.11117e+02
I1111 05:07:26.909267 140264174335808 run_lib.py:153] step: 138300, training_loss: 1.20873e+02
I1111 05:07:36.906218 140264174335808 run_lib.py:153] step: 138350, training_loss: 1.22451e+02
I1111 05:07:46.887000 140264174335808 run_lib.py:153] step: 138400, training_loss: 1.13400e+02
I1111 05:07:56.970651 140264174335808 run_lib.py:153] step: 138450, training_loss: 1.27963e+02
I1111 05:08:06.473482 140264174335808 run_lib.py:153] step: 138500, training_loss: 1.34634e+02
I1111 05:08:17.095915 140264174335808 run_lib.py:153] step: 138550, training_loss: 1.46795e+02
I1111 05:08:26.651839 140264174335808 run_lib.py:153] step: 138600, training_loss: 1.09987e+02
I1111 05:08:36.341425 140264174335808 run_lib.py:153] step: 138650, training_loss: 1.12361e+02
I1111 05:08:45.815717 140264174335808 run_lib.py:153] step: 138700, training_loss: 1.15977e+02
I1111 05:08:55.636697 140264174335808 run_lib.py:153] step: 138750, training_loss: 1.51708e+02
I1111 05:09:05.276456 140264174335808 run_lib.py:153] step: 138800, training_loss: 1.29761e+02
I1111 05:09:14.695921 140264174335808 run_lib.py:153] step: 138850, training_loss: 1.33083e+02
I1111 05:09:24.707049 140264174335808 run_lib.py:153] step: 138900, training_loss: 1.16486e+02
I1111 05:09:34.880270 140264174335808 run_lib.py:153] step: 138950, training_loss: 1.25915e+02
I1111 05:09:44.739222 140264174335808 run_lib.py:153] step: 139000, training_loss: 1.66142e+02
I1111 05:09:54.741026 140264174335808 run_lib.py:153] step: 139050, training_loss: 1.29585e+02
I1111 05:10:04.257938 140264174335808 run_lib.py:153] step: 139100, training_loss: 1.33057e+02
I1111 05:10:13.834151 140264174335808 run_lib.py:153] step: 139150, training_loss: 1.54100e+02
I1111 05:10:23.311782 140264174335808 run_lib.py:153] step: 139200, training_loss: 1.48743e+02
I1111 05:10:33.273237 140264174335808 run_lib.py:153] step: 139250, training_loss: 1.34731e+02
I1111 05:10:42.926055 140264174335808 run_lib.py:153] step: 139300, training_loss: 1.20421e+02
I1111 05:10:52.474954 140264174335808 run_lib.py:153] step: 139350, training_loss: 1.19134e+02
I1111 05:11:02.199703 140264174335808 run_lib.py:153] step: 139400, training_loss: 1.68738e+02
I1111 05:11:11.710008 140264174335808 run_lib.py:153] step: 139450, training_loss: 1.33527e+02
I1111 05:11:22.353537 140264174335808 run_lib.py:153] step: 139500, training_loss: 1.31720e+02
I1111 05:11:31.894331 140264174335808 run_lib.py:153] step: 139550, training_loss: 1.29945e+02
I1111 05:11:41.589564 140264174335808 run_lib.py:153] step: 139600, training_loss: 1.16027e+02
I1111 05:11:50.843436 140264174335808 run_lib.py:153] step: 139650, training_loss: 1.26662e+02
I1111 05:12:00.554241 140264174335808 run_lib.py:153] step: 139700, training_loss: 1.22950e+02
I1111 05:12:10.428828 140264174335808 run_lib.py:153] step: 139750, training_loss: 1.29089e+02
I1111 05:12:20.764485 140264174335808 run_lib.py:153] step: 139800, training_loss: 1.14805e+02
I1111 05:12:31.998086 140264174335808 run_lib.py:153] step: 139850, training_loss: 1.78981e+02
I1111 05:12:42.504088 140264174335808 run_lib.py:153] step: 139900, training_loss: 1.22643e+02
I1111 05:12:52.863806 140264174335808 run_lib.py:153] step: 139950, training_loss: 1.59014e+02
I1111 05:13:03.790904 140264174335808 run_lib.py:153] step: 140000, training_loss: 1.18184e+02
I1111 05:13:04.428590 140264174335808 run_lib.py:166] step: 140000, eval_loss: 1.57581e+02
I1111 05:13:15.407449 140264174335808 run_lib.py:153] step: 140050, training_loss: 1.01570e+02
I1111 05:13:25.514297 140264174335808 run_lib.py:153] step: 140100, training_loss: 1.54729e+02
I1111 05:13:35.399426 140264174335808 run_lib.py:153] step: 140150, training_loss: 1.22128e+02
I1111 05:13:45.976571 140264174335808 run_lib.py:153] step: 140200, training_loss: 1.00614e+02
I1111 05:13:56.770259 140264174335808 run_lib.py:153] step: 140250, training_loss: 7.82004e+01
I1111 05:14:06.976559 140264174335808 run_lib.py:153] step: 140300, training_loss: 1.38306e+02
I1111 05:14:16.939938 140264174335808 run_lib.py:153] step: 140350, training_loss: 1.49547e+02
I1111 05:14:27.043366 140264174335808 run_lib.py:153] step: 140400, training_loss: 1.38755e+02
I1111 05:14:37.268843 140264174335808 run_lib.py:153] step: 140450, training_loss: 1.26974e+02
I1111 05:14:47.775965 140264174335808 run_lib.py:153] step: 140500, training_loss: 1.28312e+02
I1111 05:14:58.000600 140264174335808 run_lib.py:153] step: 140550, training_loss: 1.32325e+02
I1111 05:15:08.522612 140264174335808 run_lib.py:153] step: 140600, training_loss: 1.50836e+02
I1111 05:15:18.622375 140264174335808 run_lib.py:153] step: 140650, training_loss: 1.27326e+02
I1111 05:15:28.696063 140264174335808 run_lib.py:153] step: 140700, training_loss: 1.26743e+02
I1111 05:15:38.355339 140264174335808 run_lib.py:153] step: 140750, training_loss: 1.26783e+02
I1111 05:15:48.583113 140264174335808 run_lib.py:153] step: 140800, training_loss: 1.33109e+02
I1111 05:15:57.954386 140264174335808 run_lib.py:153] step: 140850, training_loss: 1.35383e+02
I1111 05:16:07.971721 140264174335808 run_lib.py:153] step: 140900, training_loss: 1.24852e+02
I1111 05:16:19.162280 140264174335808 run_lib.py:153] step: 140950, training_loss: 1.15796e+02
I1111 05:16:29.090502 140264174335808 run_lib.py:153] step: 141000, training_loss: 1.42211e+02
I1111 05:16:39.360477 140264174335808 run_lib.py:153] step: 141050, training_loss: 1.15729e+02
I1111 05:16:48.938132 140264174335808 run_lib.py:153] step: 141100, training_loss: 9.72060e+01
I1111 05:16:59.025359 140264174335808 run_lib.py:153] step: 141150, training_loss: 9.22294e+01
I1111 05:17:09.549467 140264174335808 run_lib.py:153] step: 141200, training_loss: 1.18881e+02
I1111 05:17:19.204587 140264174335808 run_lib.py:153] step: 141250, training_loss: 8.85805e+01
I1111 05:17:28.891348 140264174335808 run_lib.py:153] step: 141300, training_loss: 1.19594e+02
I1111 05:17:39.044726 140264174335808 run_lib.py:153] step: 141350, training_loss: 1.29915e+02
I1111 05:17:49.631968 140264174335808 run_lib.py:153] step: 141400, training_loss: 1.44122e+02
I1111 05:17:59.548463 140264174335808 run_lib.py:153] step: 141450, training_loss: 1.04370e+02
I1111 05:18:09.316787 140264174335808 run_lib.py:153] step: 141500, training_loss: 9.48928e+01
I1111 05:18:20.105504 140264174335808 run_lib.py:153] step: 141550, training_loss: 1.31066e+02
I1111 05:18:29.925610 140264174335808 run_lib.py:153] step: 141600, training_loss: 1.10316e+02
I1111 05:18:40.124423 140264174335808 run_lib.py:153] step: 141650, training_loss: 1.28313e+02
I1111 05:18:49.893966 140264174335808 run_lib.py:153] step: 141700, training_loss: 1.24096e+02
I1111 05:19:00.339295 140264174335808 run_lib.py:153] step: 141750, training_loss: 1.08148e+02
I1111 05:19:10.753324 140264174335808 run_lib.py:153] step: 141800, training_loss: 1.38272e+02
I1111 05:19:21.873368 140264174335808 run_lib.py:153] step: 141850, training_loss: 1.55107e+02
I1111 05:19:31.727592 140264174335808 run_lib.py:153] step: 141900, training_loss: 1.21824e+02
I1111 05:19:41.582781 140264174335808 run_lib.py:153] step: 141950, training_loss: 1.13419e+02
I1111 05:19:52.197579 140264174335808 run_lib.py:153] step: 142000, training_loss: 1.56521e+02
I1111 05:20:02.537574 140264174335808 run_lib.py:153] step: 142050, training_loss: 1.13845e+02
I1111 05:20:13.524804 140264174335808 run_lib.py:153] step: 142100, training_loss: 1.30608e+02
I1111 05:20:23.586706 140264174335808 run_lib.py:153] step: 142150, training_loss: 1.13694e+02
I1111 05:20:33.849678 140264174335808 run_lib.py:153] step: 142200, training_loss: 1.31154e+02
I1111 05:20:43.762818 140264174335808 run_lib.py:153] step: 142250, training_loss: 1.35803e+02
I1111 05:20:53.701459 140264174335808 run_lib.py:153] step: 142300, training_loss: 1.16473e+02
I1111 05:21:03.562309 140264174335808 run_lib.py:153] step: 142350, training_loss: 1.29367e+02
I1111 05:21:14.396321 140264174335808 run_lib.py:153] step: 142400, training_loss: 1.32537e+02
I1111 05:21:25.051925 140264174335808 run_lib.py:153] step: 142450, training_loss: 1.10544e+02
I1111 05:21:34.634267 140264174335808 run_lib.py:153] step: 142500, training_loss: 1.06595e+02
I1111 05:21:45.292391 140264174335808 run_lib.py:153] step: 142550, training_loss: 1.15425e+02
I1111 05:21:55.223247 140264174335808 run_lib.py:153] step: 142600, training_loss: 9.92368e+01
I1111 05:22:05.305990 140264174335808 run_lib.py:153] step: 142650, training_loss: 1.44931e+02
I1111 05:22:14.757819 140264174335808 run_lib.py:153] step: 142700, training_loss: 9.65986e+01
I1111 05:22:25.695467 140264174335808 run_lib.py:153] step: 142750, training_loss: 1.20160e+02
I1111 05:22:36.077019 140264174335808 run_lib.py:153] step: 142800, training_loss: 1.20605e+02
I1111 05:22:46.198657 140264174335808 run_lib.py:153] step: 142850, training_loss: 1.42122e+02
I1111 05:22:56.415735 140264174335808 run_lib.py:153] step: 142900, training_loss: 1.30832e+02
I1111 05:23:06.692100 140264174335808 run_lib.py:153] step: 142950, training_loss: 1.48377e+02
I1111 05:23:17.597709 140264174335808 run_lib.py:153] step: 143000, training_loss: 1.16925e+02
I1111 05:23:28.341588 140264174335808 run_lib.py:153] step: 143050, training_loss: 1.42204e+02
I1111 05:23:39.178650 140264174335808 run_lib.py:153] step: 143100, training_loss: 1.40856e+02
I1111 05:23:49.032395 140264174335808 run_lib.py:153] step: 143150, training_loss: 1.68176e+02
I1111 05:23:59.488672 140264174335808 run_lib.py:153] step: 143200, training_loss: 1.23787e+02
I1111 05:24:09.183541 140264174335808 run_lib.py:153] step: 143250, training_loss: 1.20172e+02
I1111 05:24:19.889668 140264174335808 run_lib.py:153] step: 143300, training_loss: 1.33660e+02
I1111 05:24:29.188416 140264174335808 run_lib.py:153] step: 143350, training_loss: 1.50193e+02
I1111 05:24:39.231328 140264174335808 run_lib.py:153] step: 143400, training_loss: 1.16881e+02
I1111 05:24:49.282727 140264174335808 run_lib.py:153] step: 143450, training_loss: 1.14958e+02
I1111 05:24:58.820937 140264174335808 run_lib.py:153] step: 143500, training_loss: 1.09128e+02
I1111 05:25:08.335246 140264174335808 run_lib.py:153] step: 143550, training_loss: 1.08209e+02
I1111 05:25:18.325713 140264174335808 run_lib.py:153] step: 143600, training_loss: 1.18781e+02
I1111 05:25:27.730673 140264174335808 run_lib.py:153] step: 143650, training_loss: 1.62831e+02
I1111 05:25:37.786565 140264174335808 run_lib.py:153] step: 143700, training_loss: 1.39716e+02
I1111 05:25:47.770733 140264174335808 run_lib.py:153] step: 143750, training_loss: 1.52520e+02
I1111 05:25:58.093863 140264174335808 run_lib.py:153] step: 143800, training_loss: 9.54678e+01
I1111 05:26:08.450253 140264174335808 run_lib.py:153] step: 143850, training_loss: 1.28250e+02
I1111 05:26:18.717964 140264174335808 run_lib.py:153] step: 143900, training_loss: 1.17950e+02
I1111 05:26:28.978484 140264174335808 run_lib.py:153] step: 143950, training_loss: 9.42435e+01
I1111 05:26:38.866039 140264174335808 run_lib.py:153] step: 144000, training_loss: 1.42871e+02
I1111 05:26:48.395874 140264174335808 run_lib.py:153] step: 144050, training_loss: 1.85958e+02
I1111 05:26:58.472726 140264174335808 run_lib.py:153] step: 144100, training_loss: 1.32562e+02
I1111 05:27:08.685424 140264174335808 run_lib.py:153] step: 144150, training_loss: 1.34987e+02
I1111 05:27:19.038111 140264174335808 run_lib.py:153] step: 144200, training_loss: 1.14061e+02
I1111 05:27:28.508654 140264174335808 run_lib.py:153] step: 144250, training_loss: 1.03411e+02
I1111 05:27:38.574490 140264174335808 run_lib.py:153] step: 144300, training_loss: 1.50748e+02
I1111 05:27:49.115936 140264174335808 run_lib.py:153] step: 144350, training_loss: 1.20540e+02
I1111 05:27:58.902255 140264174335808 run_lib.py:153] step: 144400, training_loss: 1.16107e+02
I1111 05:28:09.194633 140264174335808 run_lib.py:153] step: 144450, training_loss: 1.43966e+02
I1111 05:28:18.834197 140264174335808 run_lib.py:153] step: 144500, training_loss: 1.37638e+02
I1111 05:28:28.696448 140264174335808 run_lib.py:153] step: 144550, training_loss: 1.52617e+02
I1111 05:28:38.377088 140264174335808 run_lib.py:153] step: 144600, training_loss: 1.29305e+02
I1111 05:28:48.707704 140264174335808 run_lib.py:153] step: 144650, training_loss: 1.33540e+02
I1111 05:28:58.617941 140264174335808 run_lib.py:153] step: 144700, training_loss: 1.18475e+02
I1111 05:29:09.019625 140264174335808 run_lib.py:153] step: 144750, training_loss: 1.42470e+02
I1111 05:29:19.286674 140264174335808 run_lib.py:153] step: 144800, training_loss: 1.27991e+02
I1111 05:29:29.850835 140264174335808 run_lib.py:153] step: 144850, training_loss: 1.23507e+02
I1111 05:29:40.442701 140264174335808 run_lib.py:153] step: 144900, training_loss: 1.08519e+02
I1111 05:29:51.297166 140264174335808 run_lib.py:153] step: 144950, training_loss: 1.46067e+02
I1111 05:30:01.192389 140264174335808 run_lib.py:153] step: 145000, training_loss: 1.37819e+02
I1111 05:30:01.295392 140264174335808 run_lib.py:166] step: 145000, eval_loss: 1.04724e+02
I1111 05:30:11.103760 140264174335808 run_lib.py:153] step: 145050, training_loss: 1.66322e+02
I1111 05:30:20.893710 140264174335808 run_lib.py:153] step: 145100, training_loss: 1.33279e+02
I1111 05:30:30.397721 140264174335808 run_lib.py:153] step: 145150, training_loss: 1.43875e+02
I1111 05:30:40.296415 140264174335808 run_lib.py:153] step: 145200, training_loss: 1.27094e+02
I1111 05:30:50.344145 140264174335808 run_lib.py:153] step: 145250, training_loss: 1.35399e+02
I1111 05:31:00.083997 140264174335808 run_lib.py:153] step: 145300, training_loss: 1.52561e+02
I1111 05:31:10.032085 140264174335808 run_lib.py:153] step: 145350, training_loss: 9.47521e+01
I1111 05:31:20.206905 140264174335808 run_lib.py:153] step: 145400, training_loss: 1.44146e+02
I1111 05:31:29.521711 140264174335808 run_lib.py:153] step: 145450, training_loss: 1.30064e+02
I1111 05:31:38.830561 140264174335808 run_lib.py:153] step: 145500, training_loss: 1.02954e+02
I1111 05:31:48.687043 140264174335808 run_lib.py:153] step: 145550, training_loss: 1.21525e+02
I1111 05:31:58.270741 140264174335808 run_lib.py:153] step: 145600, training_loss: 1.31374e+02
I1111 05:32:08.547254 140264174335808 run_lib.py:153] step: 145650, training_loss: 1.45037e+02
I1111 05:32:18.924598 140264174335808 run_lib.py:153] step: 145700, training_loss: 1.26224e+02
I1111 05:32:29.200628 140264174335808 run_lib.py:153] step: 145750, training_loss: 1.13544e+02
I1111 05:32:39.620775 140264174335808 run_lib.py:153] step: 145800, training_loss: 1.38689e+02
I1111 05:32:49.936650 140264174335808 run_lib.py:153] step: 145850, training_loss: 1.17414e+02
I1111 05:33:00.651693 140264174335808 run_lib.py:153] step: 145900, training_loss: 1.48184e+02
I1111 05:33:10.842367 140264174335808 run_lib.py:153] step: 145950, training_loss: 1.40979e+02
I1111 05:33:20.612724 140264174335808 run_lib.py:153] step: 146000, training_loss: 1.52562e+02
I1111 05:33:31.079347 140264174335808 run_lib.py:153] step: 146050, training_loss: 1.34252e+02
I1111 05:33:41.214607 140264174335808 run_lib.py:153] step: 146100, training_loss: 1.58715e+02
I1111 05:33:51.446670 140264174335808 run_lib.py:153] step: 146150, training_loss: 1.35668e+02
I1111 05:34:01.916188 140264174335808 run_lib.py:153] step: 146200, training_loss: 1.35093e+02
I1111 05:34:12.244702 140264174335808 run_lib.py:153] step: 146250, training_loss: 1.46625e+02
I1111 05:34:21.956932 140264174335808 run_lib.py:153] step: 146300, training_loss: 1.48313e+02
I1111 05:34:31.292355 140264174335808 run_lib.py:153] step: 146350, training_loss: 1.42966e+02
I1111 05:34:41.509940 140264174335808 run_lib.py:153] step: 146400, training_loss: 1.38323e+02
I1111 05:34:51.254041 140264174335808 run_lib.py:153] step: 146450, training_loss: 1.33682e+02
I1111 05:35:02.164798 140264174335808 run_lib.py:153] step: 146500, training_loss: 1.09443e+02
I1111 05:35:12.170559 140264174335808 run_lib.py:153] step: 146550, training_loss: 1.05170e+02
I1111 05:35:22.116538 140264174335808 run_lib.py:153] step: 146600, training_loss: 1.18287e+02
I1111 05:35:32.051747 140264174335808 run_lib.py:153] step: 146650, training_loss: 1.27844e+02
I1111 05:35:42.131241 140264174335808 run_lib.py:153] step: 146700, training_loss: 1.17864e+02
I1111 05:35:51.957810 140264174335808 run_lib.py:153] step: 146750, training_loss: 1.50704e+02
I1111 05:36:01.478544 140264174335808 run_lib.py:153] step: 146800, training_loss: 1.13630e+02
I1111 05:36:11.405703 140264174335808 run_lib.py:153] step: 146850, training_loss: 1.46757e+02
I1111 05:36:21.635779 140264174335808 run_lib.py:153] step: 146900, training_loss: 1.18797e+02
I1111 05:36:32.722256 140264174335808 run_lib.py:153] step: 146950, training_loss: 1.26921e+02
I1111 05:36:43.197677 140264174335808 run_lib.py:153] step: 147000, training_loss: 1.24189e+02
I1111 05:36:54.390096 140264174335808 run_lib.py:153] step: 147050, training_loss: 1.37406e+02
I1111 05:37:05.394078 140264174335808 run_lib.py:153] step: 147100, training_loss: 1.33796e+02
I1111 05:37:15.946303 140264174335808 run_lib.py:153] step: 147150, training_loss: 1.39192e+02
I1111 05:37:25.994230 140264174335808 run_lib.py:153] step: 147200, training_loss: 1.17682e+02
I1111 05:37:35.937772 140264174335808 run_lib.py:153] step: 147250, training_loss: 1.31361e+02
I1111 05:37:46.103665 140264174335808 run_lib.py:153] step: 147300, training_loss: 1.13642e+02
I1111 05:37:56.309540 140264174335808 run_lib.py:153] step: 147350, training_loss: 1.30055e+02
I1111 05:38:06.620997 140264174335808 run_lib.py:153] step: 147400, training_loss: 1.38588e+02
I1111 05:38:16.921092 140264174335808 run_lib.py:153] step: 147450, training_loss: 1.26934e+02
I1111 05:38:26.744689 140264174335808 run_lib.py:153] step: 147500, training_loss: 1.18977e+02
I1111 05:38:36.922110 140264174335808 run_lib.py:153] step: 147550, training_loss: 1.38588e+02
I1111 05:38:47.490054 140264174335808 run_lib.py:153] step: 147600, training_loss: 1.20508e+02
I1111 05:38:57.736366 140264174335808 run_lib.py:153] step: 147650, training_loss: 1.15682e+02
I1111 05:39:07.717907 140264174335808 run_lib.py:153] step: 147700, training_loss: 1.39598e+02
I1111 05:39:18.132377 140264174335808 run_lib.py:153] step: 147750, training_loss: 9.10515e+01
I1111 05:39:28.207940 140264174335808 run_lib.py:153] step: 147800, training_loss: 1.22127e+02
I1111 05:39:38.669318 140264174335808 run_lib.py:153] step: 147850, training_loss: 1.59665e+02
I1111 05:39:49.143445 140264174335808 run_lib.py:153] step: 147900, training_loss: 1.69129e+02
I1111 05:39:59.227751 140264174335808 run_lib.py:153] step: 147950, training_loss: 1.40994e+02
I1111 05:40:08.758326 140264174335808 run_lib.py:153] step: 148000, training_loss: 1.08740e+02
I1111 05:40:18.744019 140264174335808 run_lib.py:153] step: 148050, training_loss: 1.14241e+02
I1111 05:40:29.202907 140264174335808 run_lib.py:153] step: 148100, training_loss: 1.31396e+02
I1111 05:40:39.498720 140264174335808 run_lib.py:153] step: 148150, training_loss: 1.16536e+02
I1111 05:40:49.612239 140264174335808 run_lib.py:153] step: 148200, training_loss: 1.20641e+02
I1111 05:40:59.305765 140264174335808 run_lib.py:153] step: 148250, training_loss: 1.59574e+02
I1111 05:41:09.370347 140264174335808 run_lib.py:153] step: 148300, training_loss: 1.37603e+02
I1111 05:41:19.038548 140264174335808 run_lib.py:153] step: 148350, training_loss: 1.40058e+02
I1111 05:41:28.842593 140264174335808 run_lib.py:153] step: 148400, training_loss: 1.53568e+02
I1111 05:41:39.125768 140264174335808 run_lib.py:153] step: 148450, training_loss: 1.46352e+02
I1111 05:41:48.359242 140264174335808 run_lib.py:153] step: 148500, training_loss: 1.18042e+02
I1111 05:41:58.194407 140264174335808 run_lib.py:153] step: 148550, training_loss: 1.19054e+02
I1111 05:42:07.879998 140264174335808 run_lib.py:153] step: 148600, training_loss: 1.28313e+02
I1111 05:42:18.169738 140264174335808 run_lib.py:153] step: 148650, training_loss: 1.52190e+02
I1111 05:42:27.871027 140264174335808 run_lib.py:153] step: 148700, training_loss: 1.21309e+02
I1111 05:42:38.066889 140264174335808 run_lib.py:153] step: 148750, training_loss: 1.14710e+02
I1111 05:42:48.160678 140264174335808 run_lib.py:153] step: 148800, training_loss: 1.29561e+02
I1111 05:42:58.780072 140264174335808 run_lib.py:153] step: 148850, training_loss: 1.22761e+02
I1111 05:43:09.022439 140264174335808 run_lib.py:153] step: 148900, training_loss: 1.16339e+02
I1111 05:43:19.147643 140264174335808 run_lib.py:153] step: 148950, training_loss: 1.03843e+02
I1111 05:43:29.019327 140264174335808 run_lib.py:153] step: 149000, training_loss: 1.43157e+02
I1111 05:43:38.857123 140264174335808 run_lib.py:153] step: 149050, training_loss: 1.17699e+02
I1111 05:43:49.142421 140264174335808 run_lib.py:153] step: 149100, training_loss: 1.32580e+02
I1111 05:43:59.433750 140264174335808 run_lib.py:153] step: 149150, training_loss: 1.32098e+02
I1111 05:44:09.483370 140264174335808 run_lib.py:153] step: 149200, training_loss: 1.23543e+02
I1111 05:44:19.873568 140264174335808 run_lib.py:153] step: 149250, training_loss: 1.24181e+02
I1111 05:44:29.891554 140264174335808 run_lib.py:153] step: 149300, training_loss: 1.43675e+02
I1111 05:44:39.861365 140264174335808 run_lib.py:153] step: 149350, training_loss: 1.21047e+02
I1111 05:44:50.483616 140264174335808 run_lib.py:153] step: 149400, training_loss: 1.45463e+02
I1111 05:45:00.544559 140264174335808 run_lib.py:153] step: 149450, training_loss: 1.27139e+02
I1111 05:45:10.987742 140264174335808 run_lib.py:153] step: 149500, training_loss: 1.31418e+02
I1111 05:45:21.050091 140264174335808 run_lib.py:153] step: 149550, training_loss: 1.05325e+02
I1111 05:45:31.607340 140264174335808 run_lib.py:153] step: 149600, training_loss: 1.05276e+02
I1111 05:45:41.566575 140264174335808 run_lib.py:153] step: 149650, training_loss: 1.51387e+02
I1111 05:45:51.705888 140264174335808 run_lib.py:153] step: 149700, training_loss: 1.46533e+02
I1111 05:46:01.411453 140264174335808 run_lib.py:153] step: 149750, training_loss: 1.26725e+02
I1111 05:46:11.413876 140264174335808 run_lib.py:153] step: 149800, training_loss: 1.35831e+02
I1111 05:46:20.789850 140264174335808 run_lib.py:153] step: 149850, training_loss: 1.45548e+02
I1111 05:46:30.969636 140264174335808 run_lib.py:153] step: 149900, training_loss: 1.14960e+02
I1111 05:46:41.332477 140264174335808 run_lib.py:153] step: 149950, training_loss: 1.24510e+02
I1111 05:46:51.349262 140264174335808 run_lib.py:153] step: 150000, training_loss: 1.53549e+02
I1111 05:46:51.918382 140264174335808 run_lib.py:166] step: 150000, eval_loss: 1.39667e+02
I1111 05:47:02.076923 140264174335808 run_lib.py:153] step: 150050, training_loss: 1.22166e+02
I1111 05:47:12.288191 140264174335808 run_lib.py:153] step: 150100, training_loss: 1.23444e+02
I1111 05:47:22.183238 140264174335808 run_lib.py:153] step: 150150, training_loss: 1.45425e+02
I1111 05:47:32.258603 140264174335808 run_lib.py:153] step: 150200, training_loss: 1.23253e+02
I1111 05:47:42.241565 140264174335808 run_lib.py:153] step: 150250, training_loss: 1.50730e+02
I1111 05:47:53.432858 140264174335808 run_lib.py:153] step: 150300, training_loss: 1.66043e+02
I1111 05:48:04.256596 140264174335808 run_lib.py:153] step: 150350, training_loss: 1.38239e+02
I1111 05:48:15.200566 140264174335808 run_lib.py:153] step: 150400, training_loss: 1.14230e+02
I1111 05:48:25.024930 140264174335808 run_lib.py:153] step: 150450, training_loss: 1.24042e+02
I1111 05:48:35.086751 140264174335808 run_lib.py:153] step: 150500, training_loss: 1.24261e+02
I1111 05:48:45.053471 140264174335808 run_lib.py:153] step: 150550, training_loss: 1.04434e+02
I1111 05:48:54.704434 140264174335808 run_lib.py:153] step: 150600, training_loss: 1.00035e+02
I1111 05:49:05.511324 140264174335808 run_lib.py:153] step: 150650, training_loss: 1.18614e+02
I1111 05:49:15.192988 140264174335808 run_lib.py:153] step: 150700, training_loss: 1.71892e+02
I1111 05:49:25.049460 140264174335808 run_lib.py:153] step: 150750, training_loss: 1.28877e+02
I1111 05:49:34.743597 140264174335808 run_lib.py:153] step: 150800, training_loss: 1.56892e+02
I1111 05:49:44.038893 140264174335808 run_lib.py:153] step: 150850, training_loss: 1.40865e+02
I1111 05:49:53.782325 140264174335808 run_lib.py:153] step: 150900, training_loss: 1.22361e+02
I1111 05:50:03.756900 140264174335808 run_lib.py:153] step: 150950, training_loss: 1.47354e+02
I1111 05:50:13.564479 140264174335808 run_lib.py:153] step: 151000, training_loss: 1.14143e+02
I1111 05:50:23.768614 140264174335808 run_lib.py:153] step: 151050, training_loss: 1.11073e+02
I1111 05:50:34.023692 140264174335808 run_lib.py:153] step: 151100, training_loss: 9.73337e+01
I1111 05:50:43.878262 140264174335808 run_lib.py:153] step: 151150, training_loss: 1.30560e+02
I1111 05:50:53.736553 140264174335808 run_lib.py:153] step: 151200, training_loss: 1.16978e+02
I1111 05:51:03.620657 140264174335808 run_lib.py:153] step: 151250, training_loss: 1.28191e+02
I1111 05:51:13.443712 140264174335808 run_lib.py:153] step: 151300, training_loss: 1.17120e+02
I1111 05:51:24.278476 140264174335808 run_lib.py:153] step: 151350, training_loss: 1.09530e+02
I1111 05:51:34.642714 140264174335808 run_lib.py:153] step: 151400, training_loss: 1.53685e+02
I1111 05:51:44.703799 140264174335808 run_lib.py:153] step: 151450, training_loss: 1.71752e+02
I1111 05:51:55.570756 140264174335808 run_lib.py:153] step: 151500, training_loss: 1.20274e+02
I1111 05:52:06.590523 140264174335808 run_lib.py:153] step: 151550, training_loss: 1.16687e+02
I1111 05:52:16.238650 140264174335808 run_lib.py:153] step: 151600, training_loss: 1.41750e+02
I1111 05:52:26.654249 140264174335808 run_lib.py:153] step: 151650, training_loss: 1.00741e+02
I1111 05:52:36.771248 140264174335808 run_lib.py:153] step: 151700, training_loss: 1.16363e+02
I1111 05:52:46.743407 140264174335808 run_lib.py:153] step: 151750, training_loss: 1.43597e+02
I1111 05:52:57.011645 140264174335808 run_lib.py:153] step: 151800, training_loss: 1.32792e+02
I1111 05:53:07.560033 140264174335808 run_lib.py:153] step: 151850, training_loss: 1.27572e+02
I1111 05:53:17.147559 140264174335808 run_lib.py:153] step: 151900, training_loss: 1.39585e+02
I1111 05:53:27.579214 140264174335808 run_lib.py:153] step: 151950, training_loss: 1.51135e+02
I1111 05:53:37.642824 140264174335808 run_lib.py:153] step: 152000, training_loss: 1.11851e+02
I1111 05:53:48.076620 140264174335808 run_lib.py:153] step: 152050, training_loss: 9.10376e+01
I1111 05:53:58.510588 140264174335808 run_lib.py:153] step: 152100, training_loss: 1.33059e+02
I1111 05:54:08.512242 140264174335808 run_lib.py:153] step: 152150, training_loss: 1.39862e+02
I1111 05:54:18.577875 140264174335808 run_lib.py:153] step: 152200, training_loss: 1.28338e+02
I1111 05:54:29.347430 140264174335808 run_lib.py:153] step: 152250, training_loss: 1.19921e+02
I1111 05:54:39.352947 140264174335808 run_lib.py:153] step: 152300, training_loss: 1.34468e+02
I1111 05:54:49.818931 140264174335808 run_lib.py:153] step: 152350, training_loss: 1.43083e+02
I1111 05:55:00.583835 140264174335808 run_lib.py:153] step: 152400, training_loss: 1.33092e+02
I1111 05:55:11.196710 140264174335808 run_lib.py:153] step: 152450, training_loss: 1.03142e+02
I1111 05:55:21.691444 140264174335808 run_lib.py:153] step: 152500, training_loss: 1.62392e+02
I1111 05:55:31.907334 140264174335808 run_lib.py:153] step: 152550, training_loss: 1.15294e+02
I1111 05:55:41.446125 140264174335808 run_lib.py:153] step: 152600, training_loss: 1.15464e+02
I1111 05:55:51.205766 140264174335808 run_lib.py:153] step: 152650, training_loss: 1.07208e+02
I1111 05:56:01.294840 140264174335808 run_lib.py:153] step: 152700, training_loss: 1.17551e+02
I1111 05:56:11.381679 140264174335808 run_lib.py:153] step: 152750, training_loss: 1.37304e+02
I1111 05:56:22.004621 140264174335808 run_lib.py:153] step: 152800, training_loss: 1.01456e+02
I1111 05:56:31.892829 140264174335808 run_lib.py:153] step: 152850, training_loss: 1.27807e+02
I1111 05:56:42.582713 140264174335808 run_lib.py:153] step: 152900, training_loss: 1.19961e+02
I1111 05:56:52.975542 140264174335808 run_lib.py:153] step: 152950, training_loss: 1.18685e+02
I1111 05:57:02.780483 140264174335808 run_lib.py:153] step: 153000, training_loss: 1.01995e+02
I1111 05:57:12.694158 140264174335808 run_lib.py:153] step: 153050, training_loss: 1.41236e+02
I1111 05:57:22.321735 140264174335808 run_lib.py:153] step: 153100, training_loss: 1.56954e+02
I1111 05:57:32.083855 140264174335808 run_lib.py:153] step: 153150, training_loss: 1.02455e+02
I1111 05:57:42.166412 140264174335808 run_lib.py:153] step: 153200, training_loss: 1.22847e+02
I1111 05:57:52.315719 140264174335808 run_lib.py:153] step: 153250, training_loss: 1.05097e+02
I1111 05:58:03.260535 140264174335808 run_lib.py:153] step: 153300, training_loss: 1.15376e+02
I1111 05:58:12.982415 140264174335808 run_lib.py:153] step: 153350, training_loss: 1.18256e+02
I1111 05:58:22.653012 140264174335808 run_lib.py:153] step: 153400, training_loss: 1.37789e+02
I1111 05:58:32.254513 140264174335808 run_lib.py:153] step: 153450, training_loss: 1.18566e+02
I1111 05:58:42.365287 140264174335808 run_lib.py:153] step: 153500, training_loss: 1.44037e+02
I1111 05:58:52.177381 140264174335808 run_lib.py:153] step: 153550, training_loss: 1.21278e+02
I1111 05:59:02.508609 140264174335808 run_lib.py:153] step: 153600, training_loss: 1.39788e+02
I1111 05:59:12.249305 140264174335808 run_lib.py:153] step: 153650, training_loss: 1.23771e+02
I1111 05:59:21.793547 140264174335808 run_lib.py:153] step: 153700, training_loss: 1.71032e+02
I1111 05:59:32.284895 140264174335808 run_lib.py:153] step: 153750, training_loss: 1.19180e+02
I1111 05:59:42.652028 140264174335808 run_lib.py:153] step: 153800, training_loss: 1.26292e+02
I1111 05:59:52.626117 140264174335808 run_lib.py:153] step: 153850, training_loss: 1.22241e+02
I1111 06:00:03.085898 140264174335808 run_lib.py:153] step: 153900, training_loss: 1.48989e+02
I1111 06:00:12.605703 140264174335808 run_lib.py:153] step: 153950, training_loss: 1.22380e+02
I1111 06:00:21.867964 140264174335808 run_lib.py:153] step: 154000, training_loss: 1.46882e+02
I1111 06:00:31.487507 140264174335808 run_lib.py:153] step: 154050, training_loss: 1.33199e+02
I1111 06:00:41.655693 140264174335808 run_lib.py:153] step: 154100, training_loss: 1.36874e+02
I1111 06:00:52.344339 140264174335808 run_lib.py:153] step: 154150, training_loss: 1.09948e+02
I1111 06:01:02.066506 140264174335808 run_lib.py:153] step: 154200, training_loss: 1.46134e+02
I1111 06:01:12.260069 140264174335808 run_lib.py:153] step: 154250, training_loss: 1.44704e+02
I1111 06:01:22.422733 140264174335808 run_lib.py:153] step: 154300, training_loss: 1.18466e+02
I1111 06:01:32.080833 140264174335808 run_lib.py:153] step: 154350, training_loss: 1.42787e+02
I1111 06:01:42.372545 140264174335808 run_lib.py:153] step: 154400, training_loss: 1.75919e+02
I1111 06:01:52.851005 140264174335808 run_lib.py:153] step: 154450, training_loss: 1.08426e+02
I1111 06:02:03.304002 140264174335808 run_lib.py:153] step: 154500, training_loss: 1.24094e+02
I1111 06:02:13.462905 140264174335808 run_lib.py:153] step: 154550, training_loss: 1.04944e+02
I1111 06:02:23.387501 140264174335808 run_lib.py:153] step: 154600, training_loss: 1.38400e+02
I1111 06:02:33.188940 140264174335808 run_lib.py:153] step: 154650, training_loss: 1.29173e+02
I1111 06:02:43.255474 140264174335808 run_lib.py:153] step: 154700, training_loss: 1.22671e+02
I1111 06:02:53.040107 140264174335808 run_lib.py:153] step: 154750, training_loss: 1.13849e+02
I1111 06:03:03.007161 140264174335808 run_lib.py:153] step: 154800, training_loss: 1.32631e+02
I1111 06:03:12.911940 140264174335808 run_lib.py:153] step: 154850, training_loss: 1.19224e+02
I1111 06:03:23.599913 140264174335808 run_lib.py:153] step: 154900, training_loss: 1.51024e+02
I1111 06:03:33.067192 140264174335808 run_lib.py:153] step: 154950, training_loss: 1.23654e+02
I1111 06:03:43.096336 140264174335808 run_lib.py:153] step: 155000, training_loss: 1.37956e+02
I1111 06:03:43.197306 140264174335808 run_lib.py:166] step: 155000, eval_loss: 1.28773e+02
I1111 06:03:53.129153 140264174335808 run_lib.py:153] step: 155050, training_loss: 1.14684e+02
I1111 06:04:03.063883 140264174335808 run_lib.py:153] step: 155100, training_loss: 1.18137e+02
I1111 06:04:12.880880 140264174335808 run_lib.py:153] step: 155150, training_loss: 1.27739e+02
I1111 06:04:23.335911 140264174335808 run_lib.py:153] step: 155200, training_loss: 1.30706e+02
I1111 06:04:34.219046 140264174335808 run_lib.py:153] step: 155250, training_loss: 1.24238e+02
I1111 06:04:44.867222 140264174335808 run_lib.py:153] step: 155300, training_loss: 1.39670e+02
I1111 06:04:55.123048 140264174335808 run_lib.py:153] step: 155350, training_loss: 1.25373e+02
I1111 06:05:05.221247 140264174335808 run_lib.py:153] step: 155400, training_loss: 1.35553e+02
I1111 06:05:15.655260 140264174335808 run_lib.py:153] step: 155450, training_loss: 1.28261e+02
I1111 06:05:25.307078 140264174335808 run_lib.py:153] step: 155500, training_loss: 1.27681e+02
I1111 06:05:35.342781 140264174335808 run_lib.py:153] step: 155550, training_loss: 1.15345e+02
I1111 06:05:45.109357 140264174335808 run_lib.py:153] step: 155600, training_loss: 1.28455e+02
I1111 06:05:54.658385 140264174335808 run_lib.py:153] step: 155650, training_loss: 1.42629e+02
I1111 06:06:04.637523 140264174335808 run_lib.py:153] step: 155700, training_loss: 1.17622e+02
I1111 06:06:15.233114 140264174335808 run_lib.py:153] step: 155750, training_loss: 1.34000e+02
I1111 06:06:25.574661 140264174335808 run_lib.py:153] step: 155800, training_loss: 1.48567e+02
I1111 06:06:34.830904 140264174335808 run_lib.py:153] step: 155850, training_loss: 1.39076e+02
I1111 06:06:45.036657 140264174335808 run_lib.py:153] step: 155900, training_loss: 1.50870e+02
I1111 06:06:55.311184 140264174335808 run_lib.py:153] step: 155950, training_loss: 1.18053e+02
I1111 06:07:05.051580 140264174335808 run_lib.py:153] step: 156000, training_loss: 1.32560e+02
I1111 06:07:15.155378 140264174335808 run_lib.py:153] step: 156050, training_loss: 1.64865e+02
I1111 06:07:25.073880 140264174335808 run_lib.py:153] step: 156100, training_loss: 1.57501e+02
I1111 06:07:34.620643 140264174335808 run_lib.py:153] step: 156150, training_loss: 1.71543e+02
I1111 06:07:44.405207 140264174335808 run_lib.py:153] step: 156200, training_loss: 1.17244e+02
I1111 06:07:55.030364 140264174335808 run_lib.py:153] step: 156250, training_loss: 1.60394e+02
I1111 06:08:04.817930 140264174335808 run_lib.py:153] step: 156300, training_loss: 1.06345e+02
I1111 06:08:14.784299 140264174335808 run_lib.py:153] step: 156350, training_loss: 1.37485e+02
I1111 06:08:24.549156 140264174335808 run_lib.py:153] step: 156400, training_loss: 1.15988e+02
I1111 06:08:34.453192 140264174335808 run_lib.py:153] step: 156450, training_loss: 1.41603e+02
I1111 06:08:44.120671 140264174335808 run_lib.py:153] step: 156500, training_loss: 1.15743e+02
I1111 06:08:53.643357 140264174335808 run_lib.py:153] step: 156550, training_loss: 1.30923e+02
I1111 06:09:03.544343 140264174335808 run_lib.py:153] step: 156600, training_loss: 1.47287e+02
I1111 06:09:13.462610 140264174335808 run_lib.py:153] step: 156650, training_loss: 1.41845e+02
I1111 06:09:23.280686 140264174335808 run_lib.py:153] step: 156700, training_loss: 1.13102e+02
I1111 06:09:33.585816 140264174335808 run_lib.py:153] step: 156750, training_loss: 1.16503e+02
I1111 06:09:44.246577 140264174335808 run_lib.py:153] step: 156800, training_loss: 1.20545e+02
I1111 06:09:54.631042 140264174335808 run_lib.py:153] step: 156850, training_loss: 1.34365e+02
I1111 06:10:04.955751 140264174335808 run_lib.py:153] step: 156900, training_loss: 1.46921e+02
I1111 06:10:15.786616 140264174335808 run_lib.py:153] step: 156950, training_loss: 1.34180e+02
I1111 06:10:25.648238 140264174335808 run_lib.py:153] step: 157000, training_loss: 1.44896e+02
I1111 06:10:34.923815 140264174335808 run_lib.py:153] step: 157050, training_loss: 1.37157e+02
I1111 06:10:44.373924 140264174335808 run_lib.py:153] step: 157100, training_loss: 1.28705e+02
I1111 06:10:54.687468 140264174335808 run_lib.py:153] step: 157150, training_loss: 1.07027e+02
I1111 06:11:05.084195 140264174335808 run_lib.py:153] step: 157200, training_loss: 1.36197e+02
I1111 06:11:15.206462 140264174335808 run_lib.py:153] step: 157250, training_loss: 1.33624e+02
I1111 06:11:25.050430 140264174335808 run_lib.py:153] step: 157300, training_loss: 1.16323e+02
I1111 06:11:34.925854 140264174335808 run_lib.py:153] step: 157350, training_loss: 1.06517e+02
I1111 06:11:44.863525 140264174335808 run_lib.py:153] step: 157400, training_loss: 1.36172e+02
I1111 06:11:54.860023 140264174335808 run_lib.py:153] step: 157450, training_loss: 1.39838e+02
I1111 06:12:05.013941 140264174335808 run_lib.py:153] step: 157500, training_loss: 1.47884e+02
I1111 06:12:15.616863 140264174335808 run_lib.py:153] step: 157550, training_loss: 1.52652e+02
I1111 06:12:25.123901 140264174335808 run_lib.py:153] step: 157600, training_loss: 1.40426e+02
I1111 06:12:34.819570 140264174335808 run_lib.py:153] step: 157650, training_loss: 1.30439e+02
I1111 06:12:44.777333 140264174335808 run_lib.py:153] step: 157700, training_loss: 1.35619e+02
I1111 06:12:54.808819 140264174335808 run_lib.py:153] step: 157750, training_loss: 1.17798e+02
I1111 06:13:04.629922 140264174335808 run_lib.py:153] step: 157800, training_loss: 1.39242e+02
I1111 06:13:14.821447 140264174335808 run_lib.py:153] step: 157850, training_loss: 1.45525e+02
I1111 06:13:25.223137 140264174335808 run_lib.py:153] step: 157900, training_loss: 1.48785e+02
I1111 06:13:35.295835 140264174335808 run_lib.py:153] step: 157950, training_loss: 1.58199e+02
I1111 06:13:44.805286 140264174335808 run_lib.py:153] step: 158000, training_loss: 1.31776e+02
I1111 06:13:54.954667 140264174335808 run_lib.py:153] step: 158050, training_loss: 1.07875e+02
I1111 06:14:04.893243 140264174335808 run_lib.py:153] step: 158100, training_loss: 1.65858e+02
I1111 06:14:15.444441 140264174335808 run_lib.py:153] step: 158150, training_loss: 1.45517e+02
I1111 06:14:25.803270 140264174335808 run_lib.py:153] step: 158200, training_loss: 1.18276e+02
I1111 06:14:35.799475 140264174335808 run_lib.py:153] step: 158250, training_loss: 1.45564e+02
I1111 06:14:45.476059 140264174335808 run_lib.py:153] step: 158300, training_loss: 1.45091e+02
I1111 06:14:55.981741 140264174335808 run_lib.py:153] step: 158350, training_loss: 1.41767e+02
I1111 06:15:06.487170 140264174335808 run_lib.py:153] step: 158400, training_loss: 1.23321e+02
I1111 06:15:16.768849 140264174335808 run_lib.py:153] step: 158450, training_loss: 1.34415e+02
I1111 06:15:26.994214 140264174335808 run_lib.py:153] step: 158500, training_loss: 1.30144e+02
I1111 06:15:36.521948 140264174335808 run_lib.py:153] step: 158550, training_loss: 1.49116e+02
I1111 06:15:46.219213 140264174335808 run_lib.py:153] step: 158600, training_loss: 1.18698e+02
I1111 06:15:56.734612 140264174335808 run_lib.py:153] step: 158650, training_loss: 1.57021e+02
I1111 06:16:06.853958 140264174335808 run_lib.py:153] step: 158700, training_loss: 1.33036e+02
I1111 06:16:16.964029 140264174335808 run_lib.py:153] step: 158750, training_loss: 1.30985e+02
I1111 06:16:26.906091 140264174335808 run_lib.py:153] step: 158800, training_loss: 1.17496e+02
I1111 06:16:36.820331 140264174335808 run_lib.py:153] step: 158850, training_loss: 1.39079e+02
I1111 06:16:47.335977 140264174335808 run_lib.py:153] step: 158900, training_loss: 1.12196e+02
I1111 06:16:57.373278 140264174335808 run_lib.py:153] step: 158950, training_loss: 1.49113e+02
I1111 06:17:07.927596 140264174335808 run_lib.py:153] step: 159000, training_loss: 1.26347e+02
I1111 06:17:18.642369 140264174335808 run_lib.py:153] step: 159050, training_loss: 1.46179e+02
I1111 06:17:29.444806 140264174335808 run_lib.py:153] step: 159100, training_loss: 1.18085e+02
I1111 06:17:39.999548 140264174335808 run_lib.py:153] step: 159150, training_loss: 1.08683e+02
I1111 06:17:50.768389 140264174335808 run_lib.py:153] step: 159200, training_loss: 1.23861e+02
I1111 06:18:00.396338 140264174335808 run_lib.py:153] step: 159250, training_loss: 1.30615e+02
I1111 06:18:10.758399 140264174335808 run_lib.py:153] step: 159300, training_loss: 1.44177e+02
I1111 06:18:20.261873 140264174335808 run_lib.py:153] step: 159350, training_loss: 1.49768e+02
I1111 06:18:30.127523 140264174335808 run_lib.py:153] step: 159400, training_loss: 1.28417e+02
I1111 06:18:39.716626 140264174335808 run_lib.py:153] step: 159450, training_loss: 1.29445e+02
I1111 06:18:49.929947 140264174335808 run_lib.py:153] step: 159500, training_loss: 1.18972e+02
I1111 06:19:00.162282 140264174335808 run_lib.py:153] step: 159550, training_loss: 1.35965e+02
I1111 06:19:10.237950 140264174335808 run_lib.py:153] step: 159600, training_loss: 8.71143e+01
I1111 06:19:20.772886 140264174335808 run_lib.py:153] step: 159650, training_loss: 1.40090e+02
I1111 06:19:31.243402 140264174335808 run_lib.py:153] step: 159700, training_loss: 1.26732e+02
I1111 06:19:41.325412 140264174335808 run_lib.py:153] step: 159750, training_loss: 1.38524e+02
I1111 06:19:51.575720 140264174335808 run_lib.py:153] step: 159800, training_loss: 1.23659e+02
I1111 06:20:02.396245 140264174335808 run_lib.py:153] step: 159850, training_loss: 1.65236e+02
I1111 06:20:12.656006 140264174335808 run_lib.py:153] step: 159900, training_loss: 1.15735e+02
I1111 06:20:22.805892 140264174335808 run_lib.py:153] step: 159950, training_loss: 1.10644e+02
I1111 06:20:32.763580 140264174335808 run_lib.py:153] step: 160000, training_loss: 1.35043e+02
I1111 06:20:33.333641 140264174335808 run_lib.py:166] step: 160000, eval_loss: 1.47250e+02
I1111 06:20:43.027601 140264174335808 run_lib.py:153] step: 160050, training_loss: 1.31327e+02
I1111 06:20:52.838226 140264174335808 run_lib.py:153] step: 160100, training_loss: 1.52429e+02
I1111 06:21:02.574335 140264174335808 run_lib.py:153] step: 160150, training_loss: 1.66968e+02
I1111 06:21:11.983330 140264174335808 run_lib.py:153] step: 160200, training_loss: 1.73538e+02
I1111 06:21:21.738208 140264174335808 run_lib.py:153] step: 160250, training_loss: 1.31160e+02
I1111 06:21:31.390035 140264174335808 run_lib.py:153] step: 160300, training_loss: 1.18878e+02
I1111 06:21:41.038001 140264174335808 run_lib.py:153] step: 160350, training_loss: 1.15230e+02
I1111 06:21:50.779622 140264174335808 run_lib.py:153] step: 160400, training_loss: 1.08611e+02
I1111 06:22:00.983910 140264174335808 run_lib.py:153] step: 160450, training_loss: 1.36455e+02
I1111 06:22:10.892862 140264174335808 run_lib.py:153] step: 160500, training_loss: 1.19660e+02
I1111 06:22:20.370073 140264174335808 run_lib.py:153] step: 160550, training_loss: 1.67940e+02
I1111 06:22:29.901685 140264174335808 run_lib.py:153] step: 160600, training_loss: 1.26100e+02
I1111 06:22:39.657279 140264174335808 run_lib.py:153] step: 160650, training_loss: 9.96046e+01
I1111 06:22:49.664140 140264174335808 run_lib.py:153] step: 160700, training_loss: 1.39122e+02
I1111 06:22:59.443524 140264174335808 run_lib.py:153] step: 160750, training_loss: 1.35623e+02
I1111 06:23:08.846879 140264174335808 run_lib.py:153] step: 160800, training_loss: 1.37353e+02
I1111 06:23:18.933974 140264174335808 run_lib.py:153] step: 160850, training_loss: 1.19612e+02
I1111 06:23:28.604697 140264174335808 run_lib.py:153] step: 160900, training_loss: 1.12287e+02
I1111 06:23:38.305987 140264174335808 run_lib.py:153] step: 160950, training_loss: 1.52293e+02
I1111 06:23:48.301625 140264174335808 run_lib.py:153] step: 161000, training_loss: 1.04154e+02
I1111 06:23:58.042554 140264174335808 run_lib.py:153] step: 161050, training_loss: 1.22008e+02
I1111 06:24:08.622779 140264174335808 run_lib.py:153] step: 161100, training_loss: 1.35521e+02
I1111 06:24:18.660039 140264174335808 run_lib.py:153] step: 161150, training_loss: 1.27260e+02
I1111 06:24:28.936007 140264174335808 run_lib.py:153] step: 161200, training_loss: 1.15347e+02
I1111 06:24:39.246326 140264174335808 run_lib.py:153] step: 161250, training_loss: 1.57906e+02
I1111 06:24:49.055295 140264174335808 run_lib.py:153] step: 161300, training_loss: 1.41775e+02
I1111 06:24:58.277537 140264174335808 run_lib.py:153] step: 161350, training_loss: 1.54839e+02
I1111 06:25:08.757475 140264174335808 run_lib.py:153] step: 161400, training_loss: 1.21233e+02
I1111 06:25:19.297336 140264174335808 run_lib.py:153] step: 161450, training_loss: 1.10426e+02
I1111 06:25:29.090758 140264174335808 run_lib.py:153] step: 161500, training_loss: 1.36829e+02
I1111 06:25:38.575722 140264174335808 run_lib.py:153] step: 161550, training_loss: 1.18593e+02
I1111 06:25:48.373920 140264174335808 run_lib.py:153] step: 161600, training_loss: 1.20784e+02
I1111 06:25:57.942166 140264174335808 run_lib.py:153] step: 161650, training_loss: 1.34696e+02
I1111 06:26:07.748156 140264174335808 run_lib.py:153] step: 161700, training_loss: 1.29427e+02
I1111 06:26:18.322711 140264174335808 run_lib.py:153] step: 161750, training_loss: 1.29076e+02
I1111 06:26:28.027659 140264174335808 run_lib.py:153] step: 161800, training_loss: 1.15741e+02
I1111 06:26:38.405068 140264174335808 run_lib.py:153] step: 161850, training_loss: 1.30492e+02
I1111 06:26:48.753537 140264174335808 run_lib.py:153] step: 161900, training_loss: 1.12736e+02
I1111 06:26:59.216520 140264174335808 run_lib.py:153] step: 161950, training_loss: 1.38765e+02
I1111 06:27:09.388154 140264174335808 run_lib.py:153] step: 162000, training_loss: 1.45360e+02
I1111 06:27:19.579683 140264174335808 run_lib.py:153] step: 162050, training_loss: 1.27529e+02
I1111 06:27:29.697423 140264174335808 run_lib.py:153] step: 162100, training_loss: 1.30370e+02
I1111 06:27:40.124425 140264174335808 run_lib.py:153] step: 162150, training_loss: 1.37981e+02
I1111 06:27:50.119614 140264174335808 run_lib.py:153] step: 162200, training_loss: 1.34533e+02
I1111 06:27:59.839807 140264174335808 run_lib.py:153] step: 162250, training_loss: 1.14037e+02
I1111 06:28:09.980304 140264174335808 run_lib.py:153] step: 162300, training_loss: 1.14794e+02
I1111 06:28:20.399607 140264174335808 run_lib.py:153] step: 162350, training_loss: 8.55516e+01
I1111 06:28:30.701739 140264174335808 run_lib.py:153] step: 162400, training_loss: 1.21697e+02
I1111 06:28:41.347497 140264174335808 run_lib.py:153] step: 162450, training_loss: 1.37415e+02
I1111 06:28:51.769568 140264174335808 run_lib.py:153] step: 162500, training_loss: 1.41908e+02
I1111 06:29:01.168438 140264174335808 run_lib.py:153] step: 162550, training_loss: 1.13724e+02
I1111 06:29:11.208239 140264174335808 run_lib.py:153] step: 162600, training_loss: 8.69351e+01
I1111 06:29:21.242665 140264174335808 run_lib.py:153] step: 162650, training_loss: 1.10432e+02
I1111 06:29:30.494060 140264174335808 run_lib.py:153] step: 162700, training_loss: 1.25826e+02
I1111 06:29:41.042727 140264174335808 run_lib.py:153] step: 162750, training_loss: 1.23055e+02
I1111 06:29:51.233178 140264174335808 run_lib.py:153] step: 162800, training_loss: 1.32030e+02
I1111 06:30:01.510675 140264174335808 run_lib.py:153] step: 162850, training_loss: 8.20189e+01
I1111 06:30:11.774094 140264174335808 run_lib.py:153] step: 162900, training_loss: 1.37911e+02
I1111 06:30:22.468896 140264174335808 run_lib.py:153] step: 162950, training_loss: 1.24557e+02
I1111 06:30:32.222067 140264174335808 run_lib.py:153] step: 163000, training_loss: 1.23710e+02
I1111 06:30:41.946836 140264174335808 run_lib.py:153] step: 163050, training_loss: 1.03584e+02
I1111 06:30:51.469810 140264174335808 run_lib.py:153] step: 163100, training_loss: 1.41581e+02
I1111 06:31:00.761454 140264174335808 run_lib.py:153] step: 163150, training_loss: 1.50954e+02
I1111 06:31:10.551435 140264174335808 run_lib.py:153] step: 163200, training_loss: 1.18558e+02
I1111 06:31:20.270247 140264174335808 run_lib.py:153] step: 163250, training_loss: 1.05247e+02
I1111 06:31:29.821707 140264174335808 run_lib.py:153] step: 163300, training_loss: 1.70512e+02
I1111 06:31:39.611932 140264174335808 run_lib.py:153] step: 163350, training_loss: 1.37030e+02
I1111 06:31:49.807227 140264174335808 run_lib.py:153] step: 163400, training_loss: 1.69714e+02
I1111 06:32:00.148734 140264174335808 run_lib.py:153] step: 163450, training_loss: 1.38181e+02
I1111 06:32:10.392816 140264174335808 run_lib.py:153] step: 163500, training_loss: 1.48917e+02
I1111 06:32:20.636329 140264174335808 run_lib.py:153] step: 163550, training_loss: 1.15458e+02
I1111 06:32:30.788800 140264174335808 run_lib.py:153] step: 163600, training_loss: 1.28307e+02
I1111 06:32:40.944791 140264174335808 run_lib.py:153] step: 163650, training_loss: 1.27462e+02
I1111 06:32:50.483048 140264174335808 run_lib.py:153] step: 163700, training_loss: 1.40297e+02
I1111 06:33:00.818734 140264174335808 run_lib.py:153] step: 163750, training_loss: 1.55563e+02
I1111 06:33:10.466346 140264174335808 run_lib.py:153] step: 163800, training_loss: 9.74928e+01
I1111 06:33:20.283277 140264174335808 run_lib.py:153] step: 163850, training_loss: 1.14282e+02
I1111 06:33:30.602052 140264174335808 run_lib.py:153] step: 163900, training_loss: 1.33499e+02
I1111 06:33:40.886018 140264174335808 run_lib.py:153] step: 163950, training_loss: 1.19366e+02
I1111 06:33:50.901102 140264174335808 run_lib.py:153] step: 164000, training_loss: 1.56412e+02
I1111 06:34:00.788490 140264174335808 run_lib.py:153] step: 164050, training_loss: 1.15674e+02
I1111 06:34:10.550449 140264174335808 run_lib.py:153] step: 164100, training_loss: 1.43356e+02
I1111 06:34:20.176689 140264174335808 run_lib.py:153] step: 164150, training_loss: 1.07339e+02
I1111 06:34:29.681487 140264174335808 run_lib.py:153] step: 164200, training_loss: 1.23484e+02
I1111 06:34:39.266851 140264174335808 run_lib.py:153] step: 164250, training_loss: 1.59071e+02
I1111 06:34:49.452481 140264174335808 run_lib.py:153] step: 164300, training_loss: 1.19221e+02
I1111 06:34:59.660048 140264174335808 run_lib.py:153] step: 164350, training_loss: 1.06378e+02
I1111 06:35:10.162842 140264174335808 run_lib.py:153] step: 164400, training_loss: 1.37454e+02
I1111 06:35:20.553113 140264174335808 run_lib.py:153] step: 164450, training_loss: 1.28762e+02
I1111 06:35:30.304783 140264174335808 run_lib.py:153] step: 164500, training_loss: 1.14485e+02
I1111 06:35:40.198374 140264174335808 run_lib.py:153] step: 164550, training_loss: 1.25442e+02
I1111 06:35:50.643860 140264174335808 run_lib.py:153] step: 164600, training_loss: 1.39523e+02
I1111 06:35:59.915770 140264174335808 run_lib.py:153] step: 164650, training_loss: 1.22079e+02
I1111 06:36:10.609439 140264174335808 run_lib.py:153] step: 164700, training_loss: 1.22529e+02
I1111 06:36:19.963317 140264174335808 run_lib.py:153] step: 164750, training_loss: 1.09488e+02
I1111 06:36:29.730323 140264174335808 run_lib.py:153] step: 164800, training_loss: 1.18721e+02
I1111 06:36:39.377338 140264174335808 run_lib.py:153] step: 164850, training_loss: 1.28277e+02
I1111 06:36:49.722839 140264174335808 run_lib.py:153] step: 164900, training_loss: 1.40716e+02
I1111 06:36:59.727888 140264174335808 run_lib.py:153] step: 164950, training_loss: 1.51220e+02
I1111 06:37:09.215369 140264174335808 run_lib.py:153] step: 165000, training_loss: 1.08215e+02
I1111 06:37:09.317735 140264174335808 run_lib.py:166] step: 165000, eval_loss: 1.22020e+02
I1111 06:37:19.034943 140264174335808 run_lib.py:153] step: 165050, training_loss: 1.11701e+02
I1111 06:37:29.110441 140264174335808 run_lib.py:153] step: 165100, training_loss: 1.54434e+02
I1111 06:37:38.483461 140264174335808 run_lib.py:153] step: 165150, training_loss: 1.47282e+02
I1111 06:37:49.333700 140264174335808 run_lib.py:153] step: 165200, training_loss: 1.64931e+02
I1111 06:37:59.477180 140264174335808 run_lib.py:153] step: 165250, training_loss: 1.31294e+02
I1111 06:38:09.020841 140264174335808 run_lib.py:153] step: 165300, training_loss: 1.06013e+02
I1111 06:38:18.604850 140264174335808 run_lib.py:153] step: 165350, training_loss: 1.23100e+02
I1111 06:38:28.571037 140264174335808 run_lib.py:153] step: 165400, training_loss: 1.03052e+02
I1111 06:38:39.029216 140264174335808 run_lib.py:153] step: 165450, training_loss: 1.20730e+02
I1111 06:38:49.071388 140264174335808 run_lib.py:153] step: 165500, training_loss: 1.13871e+02
I1111 06:38:59.129488 140264174335808 run_lib.py:153] step: 165550, training_loss: 1.28158e+02
I1111 06:39:09.542629 140264174335808 run_lib.py:153] step: 165600, training_loss: 1.55097e+02
I1111 06:39:19.791732 140264174335808 run_lib.py:153] step: 165650, training_loss: 1.40796e+02
I1111 06:39:29.691363 140264174335808 run_lib.py:153] step: 165700, training_loss: 1.22388e+02
I1111 06:39:40.061858 140264174335808 run_lib.py:153] step: 165750, training_loss: 1.11749e+02
I1111 06:39:49.818882 140264174335808 run_lib.py:153] step: 165800, training_loss: 9.93900e+01
I1111 06:39:59.567252 140264174335808 run_lib.py:153] step: 165850, training_loss: 1.15023e+02
I1111 06:40:09.173626 140264174335808 run_lib.py:153] step: 165900, training_loss: 1.61213e+02
I1111 06:40:18.821351 140264174335808 run_lib.py:153] step: 165950, training_loss: 1.16741e+02
I1111 06:40:28.720246 140264174335808 run_lib.py:153] step: 166000, training_loss: 1.15443e+02
I1111 06:40:38.821878 140264174335808 run_lib.py:153] step: 166050, training_loss: 1.36235e+02
I1111 06:40:48.750266 140264174335808 run_lib.py:153] step: 166100, training_loss: 1.13109e+02
I1111 06:40:58.960717 140264174335808 run_lib.py:153] step: 166150, training_loss: 1.16164e+02
I1111 06:41:09.238688 140264174335808 run_lib.py:153] step: 166200, training_loss: 1.47403e+02
I1111 06:41:19.106874 140264174335808 run_lib.py:153] step: 166250, training_loss: 1.27629e+02
I1111 06:41:29.640527 140264174335808 run_lib.py:153] step: 166300, training_loss: 9.95741e+01
I1111 06:41:39.665579 140264174335808 run_lib.py:153] step: 166350, training_loss: 1.38057e+02
I1111 06:41:49.652072 140264174335808 run_lib.py:153] step: 166400, training_loss: 1.49688e+02
I1111 06:41:59.613924 140264174335808 run_lib.py:153] step: 166450, training_loss: 1.13472e+02
I1111 06:42:09.741023 140264174335808 run_lib.py:153] step: 166500, training_loss: 1.36139e+02
I1111 06:42:19.738067 140264174335808 run_lib.py:153] step: 166550, training_loss: 1.19782e+02
I1111 06:42:29.651178 140264174335808 run_lib.py:153] step: 166600, training_loss: 1.09462e+02
I1111 06:42:40.217643 140264174335808 run_lib.py:153] step: 166650, training_loss: 1.16728e+02
I1111 06:42:50.110290 140264174335808 run_lib.py:153] step: 166700, training_loss: 1.35443e+02
I1111 06:42:59.619909 140264174335808 run_lib.py:153] step: 166750, training_loss: 1.25180e+02
I1111 06:43:09.855085 140264174335808 run_lib.py:153] step: 166800, training_loss: 1.36526e+02
I1111 06:43:20.138185 140264174335808 run_lib.py:153] step: 166850, training_loss: 1.07864e+02
I1111 06:43:30.292550 140264174335808 run_lib.py:153] step: 166900, training_loss: 9.28842e+01
I1111 06:43:40.803097 140264174335808 run_lib.py:153] step: 166950, training_loss: 1.18847e+02
I1111 06:43:51.410543 140264174335808 run_lib.py:153] step: 167000, training_loss: 1.39958e+02
I1111 06:44:02.522023 140264174335808 run_lib.py:153] step: 167050, training_loss: 1.19549e+02
I1111 06:44:12.906901 140264174335808 run_lib.py:153] step: 167100, training_loss: 1.22572e+02
I1111 06:44:23.799654 140264174335808 run_lib.py:153] step: 167150, training_loss: 1.15688e+02
I1111 06:44:33.636312 140264174335808 run_lib.py:153] step: 167200, training_loss: 1.54610e+02
I1111 06:44:43.314941 140264174335808 run_lib.py:153] step: 167250, training_loss: 1.30876e+02
I1111 06:44:53.656365 140264174335808 run_lib.py:153] step: 167300, training_loss: 1.24412e+02
I1111 06:45:04.248651 140264174335808 run_lib.py:153] step: 167350, training_loss: 1.33059e+02
I1111 06:45:14.612902 140264174335808 run_lib.py:153] step: 167400, training_loss: 1.44701e+02
I1111 06:45:24.815863 140264174335808 run_lib.py:153] step: 167450, training_loss: 1.24898e+02
I1111 06:45:34.636142 140264174335808 run_lib.py:153] step: 167500, training_loss: 1.17271e+02
I1111 06:45:44.249309 140264174335808 run_lib.py:153] step: 167550, training_loss: 1.23708e+02
I1111 06:45:54.068143 140264174335808 run_lib.py:153] step: 167600, training_loss: 1.12915e+02
I1111 06:46:04.304638 140264174335808 run_lib.py:153] step: 167650, training_loss: 1.19731e+02
I1111 06:46:14.224638 140264174335808 run_lib.py:153] step: 167700, training_loss: 1.44154e+02
I1111 06:46:24.469899 140264174335808 run_lib.py:153] step: 167750, training_loss: 1.43411e+02
I1111 06:46:34.091546 140264174335808 run_lib.py:153] step: 167800, training_loss: 1.56686e+02
I1111 06:46:44.027133 140264174335808 run_lib.py:153] step: 167850, training_loss: 1.49327e+02
I1111 06:46:53.958604 140264174335808 run_lib.py:153] step: 167900, training_loss: 1.00303e+02
I1111 06:47:03.874374 140264174335808 run_lib.py:153] step: 167950, training_loss: 1.38934e+02
I1111 06:47:13.651544 140264174335808 run_lib.py:153] step: 168000, training_loss: 1.13658e+02
I1111 06:47:22.922510 140264174335808 run_lib.py:153] step: 168050, training_loss: 1.54292e+02
I1111 06:47:33.057492 140264174335808 run_lib.py:153] step: 168100, training_loss: 1.34145e+02
I1111 06:47:42.325048 140264174335808 run_lib.py:153] step: 168150, training_loss: 1.29270e+02
I1111 06:47:51.714309 140264174335808 run_lib.py:153] step: 168200, training_loss: 1.40662e+02
I1111 06:48:01.280185 140264174335808 run_lib.py:153] step: 168250, training_loss: 1.05413e+02
I1111 06:48:10.889573 140264174335808 run_lib.py:153] step: 168300, training_loss: 1.35794e+02
I1111 06:48:20.352377 140264174335808 run_lib.py:153] step: 168350, training_loss: 1.42312e+02
I1111 06:48:30.044979 140264174335808 run_lib.py:153] step: 168400, training_loss: 9.45179e+01
I1111 06:48:39.580971 140264174335808 run_lib.py:153] step: 168450, training_loss: 1.56820e+02
I1111 06:48:48.972622 140264174335808 run_lib.py:153] step: 168500, training_loss: 1.23740e+02
I1111 06:48:59.300832 140264174335808 run_lib.py:153] step: 168550, training_loss: 1.53112e+02
I1111 06:49:10.222808 140264174335808 run_lib.py:153] step: 168600, training_loss: 1.46228e+02
I1111 06:49:20.716446 140264174335808 run_lib.py:153] step: 168650, training_loss: 9.91331e+01
I1111 06:49:31.453283 140264174335808 run_lib.py:153] step: 168700, training_loss: 1.06079e+02
I1111 06:49:41.839662 140264174335808 run_lib.py:153] step: 168750, training_loss: 1.31228e+02
I1111 06:49:51.891031 140264174335808 run_lib.py:153] step: 168800, training_loss: 1.08699e+02
I1111 06:50:02.095415 140264174335808 run_lib.py:153] step: 168850, training_loss: 1.06082e+02
I1111 06:50:12.821465 140264174335808 run_lib.py:153] step: 168900, training_loss: 1.20844e+02
I1111 06:50:22.522065 140264174335808 run_lib.py:153] step: 168950, training_loss: 1.36698e+02
I1111 06:50:32.161346 140264174335808 run_lib.py:153] step: 169000, training_loss: 1.20698e+02
I1111 06:50:42.958267 140264174335808 run_lib.py:153] step: 169050, training_loss: 1.45886e+02
I1111 06:50:52.717879 140264174335808 run_lib.py:153] step: 169100, training_loss: 1.14968e+02
I1111 06:51:02.687699 140264174335808 run_lib.py:153] step: 169150, training_loss: 1.22411e+02
I1111 06:51:12.200233 140264174335808 run_lib.py:153] step: 169200, training_loss: 1.10117e+02
I1111 06:51:22.781967 140264174335808 run_lib.py:153] step: 169250, training_loss: 1.38523e+02
I1111 06:51:32.538169 140264174335808 run_lib.py:153] step: 169300, training_loss: 1.43084e+02
I1111 06:51:42.394665 140264174335808 run_lib.py:153] step: 169350, training_loss: 1.13433e+02
I1111 06:51:52.300857 140264174335808 run_lib.py:153] step: 169400, training_loss: 1.24235e+02
I1111 06:52:02.529484 140264174335808 run_lib.py:153] step: 169450, training_loss: 1.41313e+02
I1111 06:52:12.146375 140264174335808 run_lib.py:153] step: 169500, training_loss: 1.14597e+02
I1111 06:52:22.085995 140264174335808 run_lib.py:153] step: 169550, training_loss: 1.17818e+02
I1111 06:52:31.691754 140264174335808 run_lib.py:153] step: 169600, training_loss: 1.14691e+02
I1111 06:52:41.789708 140264174335808 run_lib.py:153] step: 169650, training_loss: 1.36922e+02
I1111 06:52:51.236521 140264174335808 run_lib.py:153] step: 169700, training_loss: 1.38098e+02
I1111 06:53:00.774152 140264174335808 run_lib.py:153] step: 169750, training_loss: 1.24496e+02
I1111 06:53:10.395481 140264174335808 run_lib.py:153] step: 169800, training_loss: 1.17228e+02
I1111 06:53:20.339084 140264174335808 run_lib.py:153] step: 169850, training_loss: 1.23746e+02
I1111 06:53:30.565395 140264174335808 run_lib.py:153] step: 169900, training_loss: 1.51973e+02
I1111 06:53:40.386470 140264174335808 run_lib.py:153] step: 169950, training_loss: 1.56626e+02
I1111 06:53:50.270316 140264174335808 run_lib.py:153] step: 170000, training_loss: 1.34804e+02
I1111 06:53:50.840318 140264174335808 run_lib.py:166] step: 170000, eval_loss: 1.20106e+02
I1111 06:54:01.485188 140264174335808 run_lib.py:153] step: 170050, training_loss: 1.23037e+02
I1111 06:54:11.445942 140264174335808 run_lib.py:153] step: 170100, training_loss: 1.34006e+02
I1111 06:54:22.324365 140264174335808 run_lib.py:153] step: 170150, training_loss: 1.42374e+02
I1111 06:54:32.489687 140264174335808 run_lib.py:153] step: 170200, training_loss: 1.41456e+02
I1111 06:54:42.809848 140264174335808 run_lib.py:153] step: 170250, training_loss: 1.35131e+02
I1111 06:54:52.663560 140264174335808 run_lib.py:153] step: 170300, training_loss: 1.30649e+02
I1111 06:55:02.907877 140264174335808 run_lib.py:153] step: 170350, training_loss: 1.28472e+02
I1111 06:55:13.566083 140264174335808 run_lib.py:153] step: 170400, training_loss: 1.12729e+02
I1111 06:55:23.726511 140264174335808 run_lib.py:153] step: 170450, training_loss: 8.45307e+01
I1111 06:55:33.920297 140264174335808 run_lib.py:153] step: 170500, training_loss: 1.46422e+02
I1111 06:55:43.429624 140264174335808 run_lib.py:153] step: 170550, training_loss: 1.34214e+02
I1111 06:55:53.740610 140264174335808 run_lib.py:153] step: 170600, training_loss: 1.21875e+02
I1111 06:56:03.485417 140264174335808 run_lib.py:153] step: 170650, training_loss: 1.43517e+02
I1111 06:56:13.584411 140264174335808 run_lib.py:153] step: 170700, training_loss: 1.20671e+02
I1111 06:56:23.800072 140264174335808 run_lib.py:153] step: 170750, training_loss: 9.22944e+01
I1111 06:56:34.469818 140264174335808 run_lib.py:153] step: 170800, training_loss: 1.53178e+02
I1111 06:56:44.393453 140264174335808 run_lib.py:153] step: 170850, training_loss: 1.12865e+02
I1111 06:56:53.899272 140264174335808 run_lib.py:153] step: 170900, training_loss: 1.26934e+02
I1111 06:57:04.120211 140264174335808 run_lib.py:153] step: 170950, training_loss: 1.07661e+02
I1111 06:57:14.226263 140264174335808 run_lib.py:153] step: 171000, training_loss: 1.15035e+02
I1111 06:57:24.094387 140264174335808 run_lib.py:153] step: 171050, training_loss: 1.22377e+02
I1111 06:57:35.050146 140264174335808 run_lib.py:153] step: 171100, training_loss: 1.08559e+02
I1111 06:57:45.247031 140264174335808 run_lib.py:153] step: 171150, training_loss: 1.27753e+02
I1111 06:57:54.974459 140264174335808 run_lib.py:153] step: 171200, training_loss: 1.16117e+02
I1111 06:58:04.808019 140264174335808 run_lib.py:153] step: 171250, training_loss: 1.23022e+02
I1111 06:58:14.910259 140264174335808 run_lib.py:153] step: 171300, training_loss: 1.21140e+02
I1111 06:58:24.250749 140264174335808 run_lib.py:153] step: 171350, training_loss: 1.29595e+02
I1111 06:58:33.653297 140264174335808 run_lib.py:153] step: 171400, training_loss: 1.49833e+02
I1111 06:58:43.475138 140264174335808 run_lib.py:153] step: 171450, training_loss: 1.40124e+02
I1111 06:58:54.095947 140264174335808 run_lib.py:153] step: 171500, training_loss: 1.07327e+02
I1111 06:59:04.253183 140264174335808 run_lib.py:153] step: 171550, training_loss: 1.38742e+02
I1111 06:59:14.823289 140264174335808 run_lib.py:153] step: 171600, training_loss: 1.35833e+02
I1111 06:59:25.080267 140264174335808 run_lib.py:153] step: 171650, training_loss: 1.30613e+02
I1111 06:59:35.500461 140264174335808 run_lib.py:153] step: 171700, training_loss: 1.26052e+02
I1111 06:59:45.218455 140264174335808 run_lib.py:153] step: 171750, training_loss: 1.49792e+02
I1111 06:59:55.024671 140264174335808 run_lib.py:153] step: 171800, training_loss: 1.13721e+02
I1111 07:00:05.639108 140264174335808 run_lib.py:153] step: 171850, training_loss: 1.25914e+02
I1111 07:00:15.025566 140264174335808 run_lib.py:153] step: 171900, training_loss: 1.21582e+02
I1111 07:00:24.739395 140264174335808 run_lib.py:153] step: 171950, training_loss: 1.09105e+02
I1111 07:00:34.953721 140264174335808 run_lib.py:153] step: 172000, training_loss: 1.41952e+02
I1111 07:00:44.415420 140264174335808 run_lib.py:153] step: 172050, training_loss: 1.24946e+02
I1111 07:00:54.102606 140264174335808 run_lib.py:153] step: 172100, training_loss: 1.50242e+02
I1111 07:01:04.644112 140264174335808 run_lib.py:153] step: 172150, training_loss: 1.13627e+02
I1111 07:01:14.517969 140264174335808 run_lib.py:153] step: 172200, training_loss: 1.34385e+02
I1111 07:01:24.559104 140264174335808 run_lib.py:153] step: 172250, training_loss: 1.10565e+02
I1111 07:01:35.291251 140264174335808 run_lib.py:153] step: 172300, training_loss: 1.35494e+02
I1111 07:01:44.995300 140264174335808 run_lib.py:153] step: 172350, training_loss: 1.27860e+02
I1111 07:01:54.724191 140264174335808 run_lib.py:153] step: 172400, training_loss: 1.44143e+02
I1111 07:02:05.414024 140264174335808 run_lib.py:153] step: 172450, training_loss: 1.26248e+02
I1111 07:02:14.966497 140264174335808 run_lib.py:153] step: 172500, training_loss: 1.17218e+02
I1111 07:02:25.659843 140264174335808 run_lib.py:153] step: 172550, training_loss: 1.02224e+02
I1111 07:02:35.517310 140264174335808 run_lib.py:153] step: 172600, training_loss: 9.65610e+01
I1111 07:02:44.998689 140264174335808 run_lib.py:153] step: 172650, training_loss: 1.46184e+02
I1111 07:02:54.228109 140264174335808 run_lib.py:153] step: 172700, training_loss: 1.23420e+02
I1111 07:03:03.834169 140264174335808 run_lib.py:153] step: 172750, training_loss: 9.97653e+01
I1111 07:03:14.285107 140264174335808 run_lib.py:153] step: 172800, training_loss: 1.47666e+02
I1111 07:03:23.872318 140264174335808 run_lib.py:153] step: 172850, training_loss: 1.21594e+02
I1111 07:03:33.489165 140264174335808 run_lib.py:153] step: 172900, training_loss: 1.15236e+02
I1111 07:03:43.215600 140264174335808 run_lib.py:153] step: 172950, training_loss: 1.35963e+02
I1111 07:03:52.829682 140264174335808 run_lib.py:153] step: 173000, training_loss: 1.76910e+02
I1111 07:04:02.742864 140264174335808 run_lib.py:153] step: 173050, training_loss: 1.01587e+02
I1111 07:04:12.215162 140264174335808 run_lib.py:153] step: 173100, training_loss: 1.49057e+02
I1111 07:04:22.091960 140264174335808 run_lib.py:153] step: 173150, training_loss: 1.19957e+02
I1111 07:04:31.830657 140264174335808 run_lib.py:153] step: 173200, training_loss: 1.25602e+02
I1111 07:04:42.247736 140264174335808 run_lib.py:153] step: 173250, training_loss: 1.47694e+02
I1111 07:04:52.736202 140264174335808 run_lib.py:153] step: 173300, training_loss: 1.24893e+02
I1111 07:05:02.778307 140264174335808 run_lib.py:153] step: 173350, training_loss: 1.46949e+02
I1111 07:05:12.609097 140264174335808 run_lib.py:153] step: 173400, training_loss: 1.23426e+02
I1111 07:05:22.450724 140264174335808 run_lib.py:153] step: 173450, training_loss: 1.11709e+02
I1111 07:05:32.544125 140264174335808 run_lib.py:153] step: 173500, training_loss: 1.26039e+02
I1111 07:05:42.486878 140264174335808 run_lib.py:153] step: 173550, training_loss: 1.47657e+02
I1111 07:05:53.623652 140264174335808 run_lib.py:153] step: 173600, training_loss: 1.12688e+02
I1111 07:06:04.144821 140264174335808 run_lib.py:153] step: 173650, training_loss: 1.31860e+02
I1111 07:06:15.086582 140264174335808 run_lib.py:153] step: 173700, training_loss: 1.14998e+02
I1111 07:06:25.410261 140264174335808 run_lib.py:153] step: 173750, training_loss: 1.21757e+02
I1111 07:06:36.329733 140264174335808 run_lib.py:153] step: 173800, training_loss: 1.31937e+02
I1111 07:06:47.182791 140264174335808 run_lib.py:153] step: 173850, training_loss: 1.59029e+02
I1111 07:06:58.456215 140264174335808 run_lib.py:153] step: 173900, training_loss: 1.23745e+02
I1111 07:07:08.273226 140264174335808 run_lib.py:153] step: 173950, training_loss: 1.06896e+02
I1111 07:07:18.537751 140264174335808 run_lib.py:153] step: 174000, training_loss: 1.11682e+02
I1111 07:07:28.432080 140264174335808 run_lib.py:153] step: 174050, training_loss: 1.05422e+02
I1111 07:07:38.458036 140264174335808 run_lib.py:153] step: 174100, training_loss: 1.26123e+02
I1111 07:07:48.550677 140264174335808 run_lib.py:153] step: 174150, training_loss: 1.42291e+02
I1111 07:07:58.774446 140264174335808 run_lib.py:153] step: 174200, training_loss: 1.39496e+02
I1111 07:08:08.516236 140264174335808 run_lib.py:153] step: 174250, training_loss: 1.29373e+02
I1111 07:08:18.403023 140264174335808 run_lib.py:153] step: 174300, training_loss: 1.42156e+02
I1111 07:08:28.771025 140264174335808 run_lib.py:153] step: 174350, training_loss: 1.28019e+02
I1111 07:08:38.586118 140264174335808 run_lib.py:153] step: 174400, training_loss: 1.28951e+02
I1111 07:08:48.619042 140264174335808 run_lib.py:153] step: 174450, training_loss: 1.20339e+02
I1111 07:08:59.557658 140264174335808 run_lib.py:153] step: 174500, training_loss: 1.15389e+02
I1111 07:09:09.546777 140264174335808 run_lib.py:153] step: 174550, training_loss: 1.19817e+02
I1111 07:09:19.334817 140264174335808 run_lib.py:153] step: 174600, training_loss: 1.34528e+02
I1111 07:09:29.253122 140264174335808 run_lib.py:153] step: 174650, training_loss: 1.35619e+02
I1111 07:09:39.536314 140264174335808 run_lib.py:153] step: 174700, training_loss: 1.31997e+02
I1111 07:09:49.077852 140264174335808 run_lib.py:153] step: 174750, training_loss: 1.20537e+02
I1111 07:09:58.736078 140264174335808 run_lib.py:153] step: 174800, training_loss: 1.30462e+02
I1111 07:10:08.663408 140264174335808 run_lib.py:153] step: 174850, training_loss: 1.08992e+02
I1111 07:10:19.019798 140264174335808 run_lib.py:153] step: 174900, training_loss: 1.30581e+02
I1111 07:10:28.729609 140264174335808 run_lib.py:153] step: 174950, training_loss: 1.35867e+02
I1111 07:10:38.373360 140264174335808 run_lib.py:153] step: 175000, training_loss: 1.02729e+02
I1111 07:10:38.474771 140264174335808 run_lib.py:166] step: 175000, eval_loss: 1.25994e+02
I1111 07:10:48.296401 140264174335808 run_lib.py:153] step: 175050, training_loss: 1.27371e+02
I1111 07:10:58.552534 140264174335808 run_lib.py:153] step: 175100, training_loss: 1.46047e+02
I1111 07:11:08.921654 140264174335808 run_lib.py:153] step: 175150, training_loss: 1.44839e+02
I1111 07:11:18.942835 140264174335808 run_lib.py:153] step: 175200, training_loss: 1.39167e+02
I1111 07:11:29.725590 140264174335808 run_lib.py:153] step: 175250, training_loss: 1.26446e+02
I1111 07:11:40.381149 140264174335808 run_lib.py:153] step: 175300, training_loss: 1.48182e+02
I1111 07:11:50.404752 140264174335808 run_lib.py:153] step: 175350, training_loss: 1.34362e+02
I1111 07:12:00.629922 140264174335808 run_lib.py:153] step: 175400, training_loss: 1.30842e+02
I1111 07:12:10.664921 140264174335808 run_lib.py:153] step: 175450, training_loss: 1.25532e+02
I1111 07:12:20.264802 140264174335808 run_lib.py:153] step: 175500, training_loss: 1.01298e+02
I1111 07:12:30.581410 140264174335808 run_lib.py:153] step: 175550, training_loss: 1.52868e+02
I1111 07:12:40.741214 140264174335808 run_lib.py:153] step: 175600, training_loss: 1.25869e+02
I1111 07:12:50.879245 140264174335808 run_lib.py:153] step: 175650, training_loss: 1.39892e+02
I1111 07:13:01.147166 140264174335808 run_lib.py:153] step: 175700, training_loss: 1.03818e+02
I1111 07:13:10.912021 140264174335808 run_lib.py:153] step: 175750, training_loss: 1.08995e+02
I1111 07:13:20.231923 140264174335808 run_lib.py:153] step: 175800, training_loss: 1.18237e+02
I1111 07:13:30.290000 140264174335808 run_lib.py:153] step: 175850, training_loss: 1.10415e+02
I1111 07:13:40.141959 140264174335808 run_lib.py:153] step: 175900, training_loss: 1.01314e+02
I1111 07:13:50.261323 140264174335808 run_lib.py:153] step: 175950, training_loss: 9.78949e+01
I1111 07:14:00.622276 140264174335808 run_lib.py:153] step: 176000, training_loss: 1.24792e+02
I1111 07:14:11.301222 140264174335808 run_lib.py:153] step: 176050, training_loss: 1.42502e+02
I1111 07:14:21.251044 140264174335808 run_lib.py:153] step: 176100, training_loss: 1.17017e+02
I1111 07:14:31.047089 140264174335808 run_lib.py:153] step: 176150, training_loss: 1.30853e+02
I1111 07:14:40.988005 140264174335808 run_lib.py:153] step: 176200, training_loss: 1.35056e+02
I1111 07:14:51.245290 140264174335808 run_lib.py:153] step: 176250, training_loss: 1.22496e+02
I1111 07:15:01.619624 140264174335808 run_lib.py:153] step: 176300, training_loss: 1.39945e+02
I1111 07:15:11.830763 140264174335808 run_lib.py:153] step: 176350, training_loss: 1.47896e+02
I1111 07:15:21.415441 140264174335808 run_lib.py:153] step: 176400, training_loss: 1.49863e+02
I1111 07:15:31.122969 140264174335808 run_lib.py:153] step: 176450, training_loss: 1.00838e+02
I1111 07:15:40.335661 140264174335808 run_lib.py:153] step: 176500, training_loss: 8.88289e+01
I1111 07:15:50.004102 140264174335808 run_lib.py:153] step: 176550, training_loss: 1.47110e+02
I1111 07:15:59.803297 140264174335808 run_lib.py:153] step: 176600, training_loss: 1.27938e+02
I1111 07:16:09.997423 140264174335808 run_lib.py:153] step: 176650, training_loss: 1.19937e+02
I1111 07:16:20.681840 140264174335808 run_lib.py:153] step: 176700, training_loss: 1.26255e+02
I1111 07:16:30.959262 140264174335808 run_lib.py:153] step: 176750, training_loss: 1.33948e+02
I1111 07:16:41.604814 140264174335808 run_lib.py:153] step: 176800, training_loss: 1.20488e+02
I1111 07:16:52.182058 140264174335808 run_lib.py:153] step: 176850, training_loss: 1.44394e+02
I1111 07:17:03.340151 140264174335808 run_lib.py:153] step: 176900, training_loss: 1.57412e+02
I1111 07:17:13.916981 140264174335808 run_lib.py:153] step: 176950, training_loss: 1.45077e+02
I1111 07:17:23.963979 140264174335808 run_lib.py:153] step: 177000, training_loss: 1.07520e+02
I1111 07:17:34.048500 140264174335808 run_lib.py:153] step: 177050, training_loss: 1.20620e+02
I1111 07:17:44.124747 140264174335808 run_lib.py:153] step: 177100, training_loss: 1.15481e+02
I1111 07:17:53.777375 140264174335808 run_lib.py:153] step: 177150, training_loss: 1.23463e+02
I1111 07:18:03.979726 140264174335808 run_lib.py:153] step: 177200, training_loss: 1.06160e+02
I1111 07:18:13.833122 140264174335808 run_lib.py:153] step: 177250, training_loss: 1.36909e+02
I1111 07:18:23.350926 140264174335808 run_lib.py:153] step: 177300, training_loss: 1.15603e+02
I1111 07:18:33.403029 140264174335808 run_lib.py:153] step: 177350, training_loss: 1.44251e+02
I1111 07:18:42.982072 140264174335808 run_lib.py:153] step: 177400, training_loss: 1.36027e+02
I1111 07:18:53.229781 140264174335808 run_lib.py:153] step: 177450, training_loss: 1.29488e+02
I1111 07:19:03.566449 140264174335808 run_lib.py:153] step: 177500, training_loss: 1.25394e+02
I1111 07:19:13.537156 140264174335808 run_lib.py:153] step: 177550, training_loss: 1.35970e+02
I1111 07:19:23.626671 140264174335808 run_lib.py:153] step: 177600, training_loss: 1.14775e+02
I1111 07:19:34.296889 140264174335808 run_lib.py:153] step: 177650, training_loss: 1.13141e+02
I1111 07:19:45.341804 140264174335808 run_lib.py:153] step: 177700, training_loss: 1.23033e+02
I1111 07:19:55.978131 140264174335808 run_lib.py:153] step: 177750, training_loss: 1.46395e+02
I1111 07:20:06.104046 140264174335808 run_lib.py:153] step: 177800, training_loss: 1.29743e+02
I1111 07:20:16.407774 140264174335808 run_lib.py:153] step: 177850, training_loss: 1.33911e+02
I1111 07:20:25.930347 140264174335808 run_lib.py:153] step: 177900, training_loss: 1.32108e+02
I1111 07:20:36.333114 140264174335808 run_lib.py:153] step: 177950, training_loss: 1.46198e+02
I1111 07:20:46.514291 140264174335808 run_lib.py:153] step: 178000, training_loss: 1.10239e+02
I1111 07:20:56.427598 140264174335808 run_lib.py:153] step: 178050, training_loss: 9.84037e+01
I1111 07:21:06.668927 140264174335808 run_lib.py:153] step: 178100, training_loss: 1.08080e+02
I1111 07:21:17.243371 140264174335808 run_lib.py:153] step: 178150, training_loss: 1.26776e+02
I1111 07:21:27.988595 140264174335808 run_lib.py:153] step: 178200, training_loss: 1.26324e+02
I1111 07:21:37.625000 140264174335808 run_lib.py:153] step: 178250, training_loss: 1.44720e+02
I1111 07:21:47.496113 140264174335808 run_lib.py:153] step: 178300, training_loss: 1.03717e+02
I1111 07:21:57.683718 140264174335808 run_lib.py:153] step: 178350, training_loss: 1.37625e+02
I1111 07:22:07.900866 140264174335808 run_lib.py:153] step: 178400, training_loss: 1.24865e+02
I1111 07:22:17.679869 140264174335808 run_lib.py:153] step: 178450, training_loss: 1.61992e+02
I1111 07:22:28.453422 140264174335808 run_lib.py:153] step: 178500, training_loss: 1.24184e+02
I1111 07:22:38.701867 140264174335808 run_lib.py:153] step: 178550, training_loss: 1.41974e+02
I1111 07:22:48.182692 140264174335808 run_lib.py:153] step: 178600, training_loss: 1.21528e+02
I1111 07:22:58.450381 140264174335808 run_lib.py:153] step: 178650, training_loss: 1.41600e+02
I1111 07:23:08.519612 140264174335808 run_lib.py:153] step: 178700, training_loss: 1.09706e+02
I1111 07:23:18.546863 140264174335808 run_lib.py:153] step: 178750, training_loss: 1.63498e+02
I1111 07:23:28.094512 140264174335808 run_lib.py:153] step: 178800, training_loss: 1.70765e+02
I1111 07:23:37.562032 140264174335808 run_lib.py:153] step: 178850, training_loss: 1.13653e+02
I1111 07:23:47.797634 140264174335808 run_lib.py:153] step: 178900, training_loss: 1.54533e+02
I1111 07:23:57.904636 140264174335808 run_lib.py:153] step: 178950, training_loss: 1.44761e+02
I1111 07:24:07.864078 140264174335808 run_lib.py:153] step: 179000, training_loss: 1.08436e+02
I1111 07:24:18.005129 140264174335808 run_lib.py:153] step: 179050, training_loss: 1.45031e+02
I1111 07:24:27.548938 140264174335808 run_lib.py:153] step: 179100, training_loss: 1.41150e+02
I1111 07:24:37.162817 140264174335808 run_lib.py:153] step: 179150, training_loss: 1.26666e+02
I1111 07:24:47.002764 140264174335808 run_lib.py:153] step: 179200, training_loss: 1.25713e+02
I1111 07:24:57.249076 140264174335808 run_lib.py:153] step: 179250, training_loss: 9.98712e+01
I1111 07:25:07.561631 140264174335808 run_lib.py:153] step: 179300, training_loss: 1.23422e+02
I1111 07:25:17.679085 140264174335808 run_lib.py:153] step: 179350, training_loss: 1.06798e+02
I1111 07:25:27.418923 140264174335808 run_lib.py:153] step: 179400, training_loss: 1.40986e+02
I1111 07:25:36.963610 140264174335808 run_lib.py:153] step: 179450, training_loss: 1.29847e+02
I1111 07:25:46.482335 140264174335808 run_lib.py:153] step: 179500, training_loss: 1.14874e+02
I1111 07:25:56.167007 140264174335808 run_lib.py:153] step: 179550, training_loss: 1.41787e+02
I1111 07:26:06.880404 140264174335808 run_lib.py:153] step: 179600, training_loss: 1.17878e+02
I1111 07:26:17.415512 140264174335808 run_lib.py:153] step: 179650, training_loss: 1.11410e+02
I1111 07:26:27.229005 140264174335808 run_lib.py:153] step: 179700, training_loss: 1.05216e+02
I1111 07:26:37.203160 140264174335808 run_lib.py:153] step: 179750, training_loss: 1.54112e+02
I1111 07:26:47.048858 140264174335808 run_lib.py:153] step: 179800, training_loss: 1.18570e+02
I1111 07:26:57.088996 140264174335808 run_lib.py:153] step: 179850, training_loss: 1.37760e+02
I1111 07:27:07.430644 140264174335808 run_lib.py:153] step: 179900, training_loss: 1.43895e+02
I1111 07:27:17.647937 140264174335808 run_lib.py:153] step: 179950, training_loss: 1.35488e+02
I1111 07:27:27.690196 140264174335808 run_lib.py:153] step: 180000, training_loss: 1.45940e+02
I1111 07:27:28.330977 140264174335808 run_lib.py:166] step: 180000, eval_loss: 1.54467e+02
I1111 07:27:37.920401 140264174335808 run_lib.py:153] step: 180050, training_loss: 1.46399e+02
I1111 07:27:48.680315 140264174335808 run_lib.py:153] step: 180100, training_loss: 1.19371e+02
I1111 07:27:59.117318 140264174335808 run_lib.py:153] step: 180150, training_loss: 1.19089e+02
I1111 07:28:08.964813 140264174335808 run_lib.py:153] step: 180200, training_loss: 1.60618e+02
I1111 07:28:18.423759 140264174335808 run_lib.py:153] step: 180250, training_loss: 1.08799e+02
I1111 07:28:28.914467 140264174335808 run_lib.py:153] step: 180300, training_loss: 1.25020e+02
I1111 07:28:39.418737 140264174335808 run_lib.py:153] step: 180350, training_loss: 1.54757e+02
I1111 07:28:50.052623 140264174335808 run_lib.py:153] step: 180400, training_loss: 1.38686e+02
I1111 07:28:59.813298 140264174335808 run_lib.py:153] step: 180450, training_loss: 1.40667e+02
I1111 07:29:09.979551 140264174335808 run_lib.py:153] step: 180500, training_loss: 1.31637e+02
I1111 07:29:19.681359 140264174335808 run_lib.py:153] step: 180550, training_loss: 1.39718e+02
I1111 07:29:28.989062 140264174335808 run_lib.py:153] step: 180600, training_loss: 1.13096e+02
I1111 07:29:39.513653 140264174335808 run_lib.py:153] step: 180650, training_loss: 1.27972e+02
I1111 07:29:49.323906 140264174335808 run_lib.py:153] step: 180700, training_loss: 1.52607e+02
I1111 07:29:59.794570 140264174335808 run_lib.py:153] step: 180750, training_loss: 1.57160e+02
I1111 07:30:09.678741 140264174335808 run_lib.py:153] step: 180800, training_loss: 1.26773e+02
I1111 07:30:20.517842 140264174335808 run_lib.py:153] step: 180850, training_loss: 1.08232e+02
I1111 07:30:30.436048 140264174335808 run_lib.py:153] step: 180900, training_loss: 1.01519e+02
I1111 07:30:41.350911 140264174335808 run_lib.py:153] step: 180950, training_loss: 1.17042e+02
I1111 07:30:51.547791 140264174335808 run_lib.py:153] step: 181000, training_loss: 9.32331e+01
I1111 07:31:02.204548 140264174335808 run_lib.py:153] step: 181050, training_loss: 1.38623e+02
I1111 07:31:12.447300 140264174335808 run_lib.py:153] step: 181100, training_loss: 1.57361e+02
I1111 07:31:22.717245 140264174335808 run_lib.py:153] step: 181150, training_loss: 1.41046e+02
I1111 07:31:33.586461 140264174335808 run_lib.py:153] step: 181200, training_loss: 1.57002e+02
I1111 07:31:43.223790 140264174335808 run_lib.py:153] step: 181250, training_loss: 1.37928e+02
I1111 07:31:54.314835 140264174335808 run_lib.py:153] step: 181300, training_loss: 1.48520e+02
I1111 07:32:04.691738 140264174335808 run_lib.py:153] step: 181350, training_loss: 1.58989e+02
I1111 07:32:14.895689 140264174335808 run_lib.py:153] step: 181400, training_loss: 1.57895e+02
I1111 07:32:25.422298 140264174335808 run_lib.py:153] step: 181450, training_loss: 1.30504e+02
I1111 07:32:35.330605 140264174335808 run_lib.py:153] step: 181500, training_loss: 1.17465e+02
I1111 07:32:45.936401 140264174335808 run_lib.py:153] step: 181550, training_loss: 1.58593e+02
I1111 07:32:56.587970 140264174335808 run_lib.py:153] step: 181600, training_loss: 1.39015e+02
I1111 07:33:06.260061 140264174335808 run_lib.py:153] step: 181650, training_loss: 1.25335e+02
I1111 07:33:16.393887 140264174335808 run_lib.py:153] step: 181700, training_loss: 1.39717e+02
I1111 07:33:26.605454 140264174335808 run_lib.py:153] step: 181750, training_loss: 1.14170e+02
I1111 07:33:36.981284 140264174335808 run_lib.py:153] step: 181800, training_loss: 1.49461e+02
I1111 07:33:47.497533 140264174335808 run_lib.py:153] step: 181850, training_loss: 1.42415e+02
I1111 07:33:57.839855 140264174335808 run_lib.py:153] step: 181900, training_loss: 1.00259e+02
I1111 07:34:07.648145 140264174335808 run_lib.py:153] step: 181950, training_loss: 1.32752e+02
I1111 07:34:18.102636 140264174335808 run_lib.py:153] step: 182000, training_loss: 1.57313e+02
I1111 07:34:28.073572 140264174335808 run_lib.py:153] step: 182050, training_loss: 1.09917e+02
I1111 07:34:37.968709 140264174335808 run_lib.py:153] step: 182100, training_loss: 1.39283e+02
I1111 07:34:47.973403 140264174335808 run_lib.py:153] step: 182150, training_loss: 1.20015e+02
I1111 07:34:58.178216 140264174335808 run_lib.py:153] step: 182200, training_loss: 9.75468e+01
I1111 07:35:09.233493 140264174335808 run_lib.py:153] step: 182250, training_loss: 1.37128e+02
I1111 07:35:19.971269 140264174335808 run_lib.py:153] step: 182300, training_loss: 1.50712e+02
I1111 07:35:29.944761 140264174335808 run_lib.py:153] step: 182350, training_loss: 1.51534e+02
I1111 07:35:40.130446 140264174335808 run_lib.py:153] step: 182400, training_loss: 1.28915e+02
I1111 07:35:50.660569 140264174335808 run_lib.py:153] step: 182450, training_loss: 1.22964e+02
I1111 07:36:01.201349 140264174335808 run_lib.py:153] step: 182500, training_loss: 1.25254e+02
I1111 07:36:11.978384 140264174335808 run_lib.py:153] step: 182550, training_loss: 1.21540e+02
I1111 07:36:23.047626 140264174335808 run_lib.py:153] step: 182600, training_loss: 1.29830e+02
I1111 07:36:32.520179 140264174335808 run_lib.py:153] step: 182650, training_loss: 1.22439e+02
I1111 07:36:42.917630 140264174335808 run_lib.py:153] step: 182700, training_loss: 1.19499e+02
I1111 07:36:53.646271 140264174335808 run_lib.py:153] step: 182750, training_loss: 1.52163e+02
I1111 07:37:03.786432 140264174335808 run_lib.py:153] step: 182800, training_loss: 1.37095e+02
I1111 07:37:13.537994 140264174335808 run_lib.py:153] step: 182850, training_loss: 1.41255e+02
I1111 07:37:23.939798 140264174335808 run_lib.py:153] step: 182900, training_loss: 1.20537e+02
I1111 07:37:34.154471 140264174335808 run_lib.py:153] step: 182950, training_loss: 1.15310e+02
I1111 07:37:44.285406 140264174335808 run_lib.py:153] step: 183000, training_loss: 1.19312e+02
I1111 07:37:54.648614 140264174335808 run_lib.py:153] step: 183050, training_loss: 1.32963e+02
I1111 07:38:04.908408 140264174335808 run_lib.py:153] step: 183100, training_loss: 9.90632e+01
I1111 07:38:15.077746 140264174335808 run_lib.py:153] step: 183150, training_loss: 1.23038e+02
I1111 07:38:25.280697 140264174335808 run_lib.py:153] step: 183200, training_loss: 1.34406e+02
I1111 07:38:35.828389 140264174335808 run_lib.py:153] step: 183250, training_loss: 1.27987e+02
I1111 07:38:46.337694 140264174335808 run_lib.py:153] step: 183300, training_loss: 1.10593e+02
I1111 07:38:56.543196 140264174335808 run_lib.py:153] step: 183350, training_loss: 1.14194e+02
I1111 07:39:06.829845 140264174335808 run_lib.py:153] step: 183400, training_loss: 1.12415e+02
I1111 07:39:17.034094 140264174335808 run_lib.py:153] step: 183450, training_loss: 1.12997e+02
I1111 07:39:26.802464 140264174335808 run_lib.py:153] step: 183500, training_loss: 1.51743e+02
I1111 07:39:36.773248 140264174335808 run_lib.py:153] step: 183550, training_loss: 1.29417e+02
I1111 07:39:46.795962 140264174335808 run_lib.py:153] step: 183600, training_loss: 1.38105e+02
I1111 07:39:56.169620 140264174335808 run_lib.py:153] step: 183650, training_loss: 1.06534e+02
I1111 07:40:06.014903 140264174335808 run_lib.py:153] step: 183700, training_loss: 1.16883e+02
I1111 07:40:17.070753 140264174335808 run_lib.py:153] step: 183750, training_loss: 1.38547e+02
I1111 07:40:26.893287 140264174335808 run_lib.py:153] step: 183800, training_loss: 1.35425e+02
I1111 07:40:36.668317 140264174335808 run_lib.py:153] step: 183850, training_loss: 1.12564e+02
I1111 07:40:47.360213 140264174335808 run_lib.py:153] step: 183900, training_loss: 8.03037e+01
I1111 07:40:57.114763 140264174335808 run_lib.py:153] step: 183950, training_loss: 1.53231e+02
I1111 07:41:06.853706 140264174335808 run_lib.py:153] step: 184000, training_loss: 1.61929e+02
I1111 07:41:16.349465 140264174335808 run_lib.py:153] step: 184050, training_loss: 1.33834e+02
I1111 07:41:27.070397 140264174335808 run_lib.py:153] step: 184100, training_loss: 1.22362e+02
I1111 07:41:37.554042 140264174335808 run_lib.py:153] step: 184150, training_loss: 1.30708e+02
I1111 07:41:48.116878 140264174335808 run_lib.py:153] step: 184200, training_loss: 1.23980e+02
I1111 07:41:58.162452 140264174335808 run_lib.py:153] step: 184250, training_loss: 1.25945e+02
I1111 07:42:07.968995 140264174335808 run_lib.py:153] step: 184300, training_loss: 1.31828e+02
I1111 07:42:18.206310 140264174335808 run_lib.py:153] step: 184350, training_loss: 1.20619e+02
I1111 07:42:28.385133 140264174335808 run_lib.py:153] step: 184400, training_loss: 1.18119e+02
I1111 07:42:38.386235 140264174335808 run_lib.py:153] step: 184450, training_loss: 9.65208e+01
I1111 07:42:49.122271 140264174335808 run_lib.py:153] step: 184500, training_loss: 1.30614e+02
I1111 07:42:58.943139 140264174335808 run_lib.py:153] step: 184550, training_loss: 1.07214e+02
I1111 07:43:08.731960 140264174335808 run_lib.py:153] step: 184600, training_loss: 1.42825e+02
I1111 07:43:18.810223 140264174335808 run_lib.py:153] step: 184650, training_loss: 1.43347e+02
I1111 07:43:28.812098 140264174335808 run_lib.py:153] step: 184700, training_loss: 1.16956e+02
I1111 07:43:38.695783 140264174335808 run_lib.py:153] step: 184750, training_loss: 1.05487e+02
I1111 07:43:48.553361 140264174335808 run_lib.py:153] step: 184800, training_loss: 1.69182e+02
I1111 07:43:58.308189 140264174335808 run_lib.py:153] step: 184850, training_loss: 1.06751e+02
I1111 07:44:09.271150 140264174335808 run_lib.py:153] step: 184900, training_loss: 1.25207e+02
I1111 07:44:19.208339 140264174335808 run_lib.py:153] step: 184950, training_loss: 1.62986e+02
I1111 07:44:29.435561 140264174335808 run_lib.py:153] step: 185000, training_loss: 1.10597e+02
I1111 07:44:29.573988 140264174335808 run_lib.py:166] step: 185000, eval_loss: 1.31506e+02
I1111 07:44:39.541891 140264174335808 run_lib.py:153] step: 185050, training_loss: 1.25883e+02
I1111 07:44:49.639237 140264174335808 run_lib.py:153] step: 185100, training_loss: 9.91201e+01
I1111 07:44:59.420111 140264174335808 run_lib.py:153] step: 185150, training_loss: 1.01633e+02
I1111 07:45:09.456093 140264174335808 run_lib.py:153] step: 185200, training_loss: 1.57289e+02
I1111 07:45:19.280472 140264174335808 run_lib.py:153] step: 185250, training_loss: 1.37670e+02
I1111 07:45:29.695215 140264174335808 run_lib.py:153] step: 185300, training_loss: 1.15789e+02
I1111 07:45:39.141578 140264174335808 run_lib.py:153] step: 185350, training_loss: 1.07161e+02
I1111 07:45:49.410704 140264174335808 run_lib.py:153] step: 185400, training_loss: 1.17456e+02
I1111 07:45:59.735884 140264174335808 run_lib.py:153] step: 185450, training_loss: 1.08154e+02
I1111 07:46:09.379310 140264174335808 run_lib.py:153] step: 185500, training_loss: 1.41540e+02
I1111 07:46:20.049882 140264174335808 run_lib.py:153] step: 185550, training_loss: 1.37048e+02
I1111 07:46:29.731274 140264174335808 run_lib.py:153] step: 185600, training_loss: 1.70429e+02
I1111 07:46:39.229851 140264174335808 run_lib.py:153] step: 185650, training_loss: 8.86287e+01
I1111 07:46:49.137322 140264174335808 run_lib.py:153] step: 185700, training_loss: 1.03158e+02
I1111 07:46:59.530505 140264174335808 run_lib.py:153] step: 185750, training_loss: 1.58700e+02
I1111 07:47:09.814283 140264174335808 run_lib.py:153] step: 185800, training_loss: 1.19477e+02
I1111 07:47:19.894345 140264174335808 run_lib.py:153] step: 185850, training_loss: 1.28955e+02
I1111 07:47:29.837929 140264174335808 run_lib.py:153] step: 185900, training_loss: 1.27461e+02
I1111 07:47:40.129426 140264174335808 run_lib.py:153] step: 185950, training_loss: 1.21171e+02
I1111 07:47:50.207359 140264174335808 run_lib.py:153] step: 186000, training_loss: 1.23551e+02
I1111 07:48:00.053374 140264174335808 run_lib.py:153] step: 186050, training_loss: 1.21442e+02
I1111 07:48:10.112458 140264174335808 run_lib.py:153] step: 186100, training_loss: 1.22770e+02
I1111 07:48:20.103483 140264174335808 run_lib.py:153] step: 186150, training_loss: 1.46883e+02
I1111 07:48:29.991819 140264174335808 run_lib.py:153] step: 186200, training_loss: 1.24805e+02
I1111 07:48:40.355723 140264174335808 run_lib.py:153] step: 186250, training_loss: 1.23803e+02
I1111 07:48:50.434377 140264174335808 run_lib.py:153] step: 186300, training_loss: 1.23991e+02
I1111 07:48:59.987776 140264174335808 run_lib.py:153] step: 186350, training_loss: 1.10457e+02
I1111 07:49:10.056069 140264174335808 run_lib.py:153] step: 186400, training_loss: 1.23770e+02
I1111 07:49:20.783704 140264174335808 run_lib.py:153] step: 186450, training_loss: 1.35232e+02
I1111 07:49:30.860129 140264174335808 run_lib.py:153] step: 186500, training_loss: 1.22543e+02
I1111 07:49:40.862621 140264174335808 run_lib.py:153] step: 186550, training_loss: 1.23097e+02
I1111 07:49:51.465654 140264174335808 run_lib.py:153] step: 186600, training_loss: 1.11392e+02
I1111 07:50:01.154175 140264174335808 run_lib.py:153] step: 186650, training_loss: 1.62280e+02
I1111 07:50:10.501460 140264174335808 run_lib.py:153] step: 186700, training_loss: 1.25102e+02
I1111 07:50:20.116809 140264174335808 run_lib.py:153] step: 186750, training_loss: 1.44319e+02
I1111 07:50:30.837970 140264174335808 run_lib.py:153] step: 186800, training_loss: 1.39360e+02
I1111 07:50:41.117711 140264174335808 run_lib.py:153] step: 186850, training_loss: 1.14624e+02
I1111 07:50:52.109968 140264174335808 run_lib.py:153] step: 186900, training_loss: 1.51786e+02
I1111 07:51:02.630801 140264174335808 run_lib.py:153] step: 186950, training_loss: 1.28901e+02
I1111 07:51:11.976644 140264174335808 run_lib.py:153] step: 187000, training_loss: 1.39393e+02
I1111 07:51:21.732563 140264174335808 run_lib.py:153] step: 187050, training_loss: 1.26548e+02
I1111 07:51:32.244831 140264174335808 run_lib.py:153] step: 187100, training_loss: 1.16943e+02
I1111 07:51:42.728972 140264174335808 run_lib.py:153] step: 187150, training_loss: 1.19355e+02
I1111 07:51:52.598436 140264174335808 run_lib.py:153] step: 187200, training_loss: 1.53542e+02
I1111 07:52:02.407093 140264174335808 run_lib.py:153] step: 187250, training_loss: 1.33877e+02
I1111 07:52:12.187614 140264174335808 run_lib.py:153] step: 187300, training_loss: 1.29935e+02
I1111 07:52:22.881819 140264174335808 run_lib.py:153] step: 187350, training_loss: 1.38108e+02
I1111 07:52:33.678111 140264174335808 run_lib.py:153] step: 187400, training_loss: 1.19616e+02
I1111 07:52:43.976015 140264174335808 run_lib.py:153] step: 187450, training_loss: 1.36668e+02
I1111 07:52:54.231974 140264174335808 run_lib.py:153] step: 187500, training_loss: 1.33395e+02
I1111 07:53:03.985205 140264174335808 run_lib.py:153] step: 187550, training_loss: 1.21790e+02
I1111 07:53:14.316376 140264174335808 run_lib.py:153] step: 187600, training_loss: 1.43417e+02
I1111 07:53:24.470881 140264174335808 run_lib.py:153] step: 187650, training_loss: 1.26367e+02
I1111 07:53:34.790014 140264174335808 run_lib.py:153] step: 187700, training_loss: 1.15330e+02
I1111 07:53:45.801510 140264174335808 run_lib.py:153] step: 187750, training_loss: 1.21307e+02
I1111 07:53:56.211395 140264174335808 run_lib.py:153] step: 187800, training_loss: 1.06187e+02
I1111 07:54:07.137878 140264174335808 run_lib.py:153] step: 187850, training_loss: 1.34356e+02
I1111 07:54:18.065865 140264174335808 run_lib.py:153] step: 187900, training_loss: 1.09056e+02
I1111 07:54:28.653856 140264174335808 run_lib.py:153] step: 187950, training_loss: 1.24740e+02
I1111 07:54:39.227904 140264174335808 run_lib.py:153] step: 188000, training_loss: 9.71929e+01
I1111 07:54:49.388552 140264174335808 run_lib.py:153] step: 188050, training_loss: 1.17143e+02
I1111 07:55:00.027228 140264174335808 run_lib.py:153] step: 188100, training_loss: 1.20315e+02
I1111 07:55:09.817721 140264174335808 run_lib.py:153] step: 188150, training_loss: 1.44178e+02
I1111 07:55:20.253994 140264174335808 run_lib.py:153] step: 188200, training_loss: 1.08617e+02
I1111 07:55:29.812604 140264174335808 run_lib.py:153] step: 188250, training_loss: 1.32486e+02
I1111 07:55:39.341013 140264174335808 run_lib.py:153] step: 188300, training_loss: 1.25686e+02
I1111 07:55:49.231817 140264174335808 run_lib.py:153] step: 188350, training_loss: 1.10245e+02
I1111 07:55:59.440587 140264174335808 run_lib.py:153] step: 188400, training_loss: 1.36265e+02
I1111 07:56:09.759065 140264174335808 run_lib.py:153] step: 188450, training_loss: 1.54822e+02
I1111 07:56:19.780653 140264174335808 run_lib.py:153] step: 188500, training_loss: 1.39878e+02
I1111 07:56:30.179985 140264174335808 run_lib.py:153] step: 188550, training_loss: 1.35205e+02
I1111 07:56:40.342355 140264174335808 run_lib.py:153] step: 188600, training_loss: 1.17145e+02
I1111 07:56:50.463986 140264174335808 run_lib.py:153] step: 188650, training_loss: 1.37593e+02
I1111 07:57:00.936851 140264174335808 run_lib.py:153] step: 188700, training_loss: 1.24519e+02
I1111 07:57:11.806025 140264174335808 run_lib.py:153] step: 188750, training_loss: 1.24175e+02
I1111 07:57:22.272104 140264174335808 run_lib.py:153] step: 188800, training_loss: 1.25959e+02
I1111 07:57:32.376735 140264174335808 run_lib.py:153] step: 188850, training_loss: 1.61565e+02
I1111 07:57:42.779954 140264174335808 run_lib.py:153] step: 188900, training_loss: 1.49406e+02
I1111 07:57:52.790723 140264174335808 run_lib.py:153] step: 188950, training_loss: 1.08970e+02
I1111 07:58:03.568039 140264174335808 run_lib.py:153] step: 189000, training_loss: 1.18583e+02
I1111 07:58:13.447334 140264174335808 run_lib.py:153] step: 189050, training_loss: 1.40706e+02
I1111 07:58:23.072022 140264174335808 run_lib.py:153] step: 189100, training_loss: 9.11901e+01
I1111 07:58:32.637032 140264174335808 run_lib.py:153] step: 189150, training_loss: 1.20243e+02
I1111 07:58:42.089535 140264174335808 run_lib.py:153] step: 189200, training_loss: 1.29972e+02
I1111 07:58:51.879319 140264174335808 run_lib.py:153] step: 189250, training_loss: 1.28816e+02
I1111 07:59:01.997223 140264174335808 run_lib.py:153] step: 189300, training_loss: 1.38389e+02
I1111 07:59:12.216502 140264174335808 run_lib.py:153] step: 189350, training_loss: 1.70826e+02
I1111 07:59:22.948319 140264174335808 run_lib.py:153] step: 189400, training_loss: 1.11608e+02
I1111 07:59:32.650850 140264174335808 run_lib.py:153] step: 189450, training_loss: 1.40068e+02
I1111 07:59:43.555862 140264174335808 run_lib.py:153] step: 189500, training_loss: 1.09896e+02
I1111 07:59:53.462871 140264174335808 run_lib.py:153] step: 189550, training_loss: 1.20541e+02
I1111 08:00:03.993154 140264174335808 run_lib.py:153] step: 189600, training_loss: 1.04438e+02
I1111 08:00:13.678258 140264174335808 run_lib.py:153] step: 189650, training_loss: 1.03519e+02
I1111 08:00:23.257708 140264174335808 run_lib.py:153] step: 189700, training_loss: 1.49391e+02
I1111 08:00:33.502655 140264174335808 run_lib.py:153] step: 189750, training_loss: 1.08028e+02
I1111 08:00:43.044252 140264174335808 run_lib.py:153] step: 189800, training_loss: 1.41072e+02
I1111 08:00:53.093470 140264174335808 run_lib.py:153] step: 189850, training_loss: 8.05251e+01
I1111 08:01:03.400507 140264174335808 run_lib.py:153] step: 189900, training_loss: 1.26835e+02
I1111 08:01:13.590665 140264174335808 run_lib.py:153] step: 189950, training_loss: 1.25080e+02
I1111 08:01:23.642670 140264174335808 run_lib.py:153] step: 190000, training_loss: 1.25126e+02
I1111 08:01:24.211295 140264174335808 run_lib.py:166] step: 190000, eval_loss: 1.25271e+02
I1111 08:01:34.481669 140264174335808 run_lib.py:153] step: 190050, training_loss: 1.08664e+02
I1111 08:01:44.883417 140264174335808 run_lib.py:153] step: 190100, training_loss: 1.53037e+02
I1111 08:01:54.697979 140264174335808 run_lib.py:153] step: 190150, training_loss: 1.10458e+02
I1111 08:02:04.576688 140264174335808 run_lib.py:153] step: 190200, training_loss: 1.62376e+02
I1111 08:02:13.993390 140264174335808 run_lib.py:153] step: 190250, training_loss: 1.59833e+02
I1111 08:02:23.538417 140264174335808 run_lib.py:153] step: 190300, training_loss: 1.14506e+02
I1111 08:02:33.694902 140264174335808 run_lib.py:153] step: 190350, training_loss: 1.00356e+02
I1111 08:02:44.228557 140264174335808 run_lib.py:153] step: 190400, training_loss: 1.12361e+02
I1111 08:02:54.716694 140264174335808 run_lib.py:153] step: 190450, training_loss: 1.12130e+02
I1111 08:03:04.687961 140264174335808 run_lib.py:153] step: 190500, training_loss: 1.03756e+02
I1111 08:03:15.198203 140264174335808 run_lib.py:153] step: 190550, training_loss: 1.23123e+02
I1111 08:03:26.267374 140264174335808 run_lib.py:153] step: 190600, training_loss: 9.65396e+01
I1111 08:03:36.245773 140264174335808 run_lib.py:153] step: 190650, training_loss: 1.13518e+02
I1111 08:03:47.292137 140264174335808 run_lib.py:153] step: 190700, training_loss: 1.25115e+02
I1111 08:03:58.009675 140264174335808 run_lib.py:153] step: 190750, training_loss: 1.22043e+02
I1111 08:04:08.614046 140264174335808 run_lib.py:153] step: 190800, training_loss: 1.50540e+02
I1111 08:04:18.740000 140264174335808 run_lib.py:153] step: 190850, training_loss: 1.29377e+02
I1111 08:04:29.149344 140264174335808 run_lib.py:153] step: 190900, training_loss: 1.19972e+02
I1111 08:04:38.944201 140264174335808 run_lib.py:153] step: 190950, training_loss: 1.19024e+02
I1111 08:04:48.939011 140264174335808 run_lib.py:153] step: 191000, training_loss: 1.54216e+02
I1111 08:04:58.497473 140264174335808 run_lib.py:153] step: 191050, training_loss: 1.36563e+02
I1111 08:05:08.539626 140264174335808 run_lib.py:153] step: 191100, training_loss: 1.02495e+02
I1111 08:05:17.965508 140264174335808 run_lib.py:153] step: 191150, training_loss: 1.27818e+02
I1111 08:05:28.519048 140264174335808 run_lib.py:153] step: 191200, training_loss: 1.68504e+02
I1111 08:05:38.455608 140264174335808 run_lib.py:153] step: 191250, training_loss: 1.48689e+02
I1111 08:05:48.172046 140264174335808 run_lib.py:153] step: 191300, training_loss: 1.48182e+02
I1111 08:05:58.120288 140264174335808 run_lib.py:153] step: 191350, training_loss: 1.01046e+02
I1111 08:06:08.428616 140264174335808 run_lib.py:153] step: 191400, training_loss: 1.24668e+02
I1111 08:06:18.405994 140264174335808 run_lib.py:153] step: 191450, training_loss: 1.58798e+02
I1111 08:06:27.666112 140264174335808 run_lib.py:153] step: 191500, training_loss: 1.07185e+02
I1111 08:06:37.889193 140264174335808 run_lib.py:153] step: 191550, training_loss: 1.53993e+02
I1111 08:06:47.864890 140264174335808 run_lib.py:153] step: 191600, training_loss: 1.43419e+02
I1111 08:06:57.691249 140264174335808 run_lib.py:153] step: 191650, training_loss: 1.28845e+02
I1111 08:07:07.749880 140264174335808 run_lib.py:153] step: 191700, training_loss: 1.23803e+02
I1111 08:07:18.356839 140264174335808 run_lib.py:153] step: 191750, training_loss: 1.45983e+02
I1111 08:07:28.452742 140264174335808 run_lib.py:153] step: 191800, training_loss: 1.33680e+02
I1111 08:07:38.895186 140264174335808 run_lib.py:153] step: 191850, training_loss: 1.19333e+02
I1111 08:07:48.930654 140264174335808 run_lib.py:153] step: 191900, training_loss: 1.39602e+02
I1111 08:07:59.177698 140264174335808 run_lib.py:153] step: 191950, training_loss: 1.25474e+02
I1111 08:08:09.925889 140264174335808 run_lib.py:153] step: 192000, training_loss: 1.44587e+02
I1111 08:08:20.858531 140264174335808 run_lib.py:153] step: 192050, training_loss: 1.17297e+02
I1111 08:08:30.444723 140264174335808 run_lib.py:153] step: 192100, training_loss: 1.26947e+02
I1111 08:08:40.519432 140264174335808 run_lib.py:153] step: 192150, training_loss: 1.28010e+02
I1111 08:08:49.875777 140264174335808 run_lib.py:153] step: 192200, training_loss: 1.40701e+02
I1111 08:08:59.563263 140264174335808 run_lib.py:153] step: 192250, training_loss: 1.01799e+02
I1111 08:09:09.894335 140264174335808 run_lib.py:153] step: 192300, training_loss: 1.66978e+02
I1111 08:09:20.180454 140264174335808 run_lib.py:153] step: 192350, training_loss: 1.41333e+02
I1111 08:09:29.803443 140264174335808 run_lib.py:153] step: 192400, training_loss: 1.06998e+02
I1111 08:09:39.435555 140264174335808 run_lib.py:153] step: 192450, training_loss: 1.15937e+02
I1111 08:09:49.373961 140264174335808 run_lib.py:153] step: 192500, training_loss: 1.33096e+02
I1111 08:09:58.840625 140264174335808 run_lib.py:153] step: 192550, training_loss: 1.29728e+02
I1111 08:10:08.684742 140264174335808 run_lib.py:153] step: 192600, training_loss: 1.65408e+02
I1111 08:10:18.066793 140264174335808 run_lib.py:153] step: 192650, training_loss: 1.16355e+02
I1111 08:10:27.312902 140264174335808 run_lib.py:153] step: 192700, training_loss: 1.25179e+02
I1111 08:10:37.098536 140264174335808 run_lib.py:153] step: 192750, training_loss: 1.23039e+02
I1111 08:10:47.055663 140264174335808 run_lib.py:153] step: 192800, training_loss: 1.08563e+02
I1111 08:10:56.707720 140264174335808 run_lib.py:153] step: 192850, training_loss: 1.25760e+02
I1111 08:11:06.458938 140264174335808 run_lib.py:153] step: 192900, training_loss: 1.19958e+02
I1111 08:11:16.547310 140264174335808 run_lib.py:153] step: 192950, training_loss: 1.15852e+02
I1111 08:11:26.426829 140264174335808 run_lib.py:153] step: 193000, training_loss: 1.16079e+02
I1111 08:11:36.099392 140264174335808 run_lib.py:153] step: 193050, training_loss: 1.03649e+02
I1111 08:11:45.696698 140264174335808 run_lib.py:153] step: 193100, training_loss: 1.04840e+02
I1111 08:11:55.399187 140264174335808 run_lib.py:153] step: 193150, training_loss: 1.14280e+02
I1111 08:12:05.000293 140264174335808 run_lib.py:153] step: 193200, training_loss: 1.22668e+02
I1111 08:12:14.672677 140264174335808 run_lib.py:153] step: 193250, training_loss: 1.08121e+02
I1111 08:12:24.283846 140264174335808 run_lib.py:153] step: 193300, training_loss: 1.62102e+02
I1111 08:12:34.009214 140264174335808 run_lib.py:153] step: 193350, training_loss: 1.22438e+02
I1111 08:12:43.929493 140264174335808 run_lib.py:153] step: 193400, training_loss: 1.34958e+02
I1111 08:12:53.210629 140264174335808 run_lib.py:153] step: 193450, training_loss: 1.35122e+02
I1111 08:13:03.020299 140264174335808 run_lib.py:153] step: 193500, training_loss: 1.23931e+02
I1111 08:13:12.870311 140264174335808 run_lib.py:153] step: 193550, training_loss: 1.33021e+02
I1111 08:13:22.776735 140264174335808 run_lib.py:153] step: 193600, training_loss: 8.09362e+01
I1111 08:13:32.230517 140264174335808 run_lib.py:153] step: 193650, training_loss: 1.14912e+02
I1111 08:13:41.925532 140264174335808 run_lib.py:153] step: 193700, training_loss: 1.47991e+02
I1111 08:13:52.069159 140264174335808 run_lib.py:153] step: 193750, training_loss: 1.33013e+02
I1111 08:14:01.516975 140264174335808 run_lib.py:153] step: 193800, training_loss: 1.32776e+02
I1111 08:14:11.143955 140264174335808 run_lib.py:153] step: 193850, training_loss: 1.20535e+02
I1111 08:14:20.729663 140264174335808 run_lib.py:153] step: 193900, training_loss: 1.22798e+02
I1111 08:14:30.239445 140264174335808 run_lib.py:153] step: 193950, training_loss: 1.31943e+02
I1111 08:14:39.901980 140264174335808 run_lib.py:153] step: 194000, training_loss: 1.34083e+02
I1111 08:14:50.373952 140264174335808 run_lib.py:153] step: 194050, training_loss: 1.35471e+02
I1111 08:15:00.578043 140264174335808 run_lib.py:153] step: 194100, training_loss: 1.11492e+02
I1111 08:15:10.558204 140264174335808 run_lib.py:153] step: 194150, training_loss: 1.27757e+02
I1111 08:15:20.207232 140264174335808 run_lib.py:153] step: 194200, training_loss: 1.32011e+02
I1111 08:15:29.622438 140264174335808 run_lib.py:153] step: 194250, training_loss: 1.17404e+02
I1111 08:15:39.082602 140264174335808 run_lib.py:153] step: 194300, training_loss: 1.23299e+02
I1111 08:15:48.918102 140264174335808 run_lib.py:153] step: 194350, training_loss: 1.71637e+02
I1111 08:15:58.265997 140264174335808 run_lib.py:153] step: 194400, training_loss: 1.38614e+02
I1111 08:16:08.746237 140264174335808 run_lib.py:153] step: 194450, training_loss: 1.47785e+02
I1111 08:16:19.051385 140264174335808 run_lib.py:153] step: 194500, training_loss: 1.22751e+02
I1111 08:16:28.703891 140264174335808 run_lib.py:153] step: 194550, training_loss: 1.19385e+02
I1111 08:16:38.492409 140264174335808 run_lib.py:153] step: 194600, training_loss: 1.44841e+02
I1111 08:16:48.262938 140264174335808 run_lib.py:153] step: 194650, training_loss: 1.41408e+02
I1111 08:16:58.114214 140264174335808 run_lib.py:153] step: 194700, training_loss: 1.20600e+02
I1111 08:17:07.716168 140264174335808 run_lib.py:153] step: 194750, training_loss: 1.48741e+02
I1111 08:17:17.778435 140264174335808 run_lib.py:153] step: 194800, training_loss: 1.31638e+02
I1111 08:17:27.893399 140264174335808 run_lib.py:153] step: 194850, training_loss: 1.14489e+02
I1111 08:17:37.674322 140264174335808 run_lib.py:153] step: 194900, training_loss: 1.39360e+02
I1111 08:17:47.925298 140264174335808 run_lib.py:153] step: 194950, training_loss: 1.15937e+02
I1111 08:17:57.963785 140264174335808 run_lib.py:153] step: 195000, training_loss: 1.36646e+02
I1111 08:17:58.064481 140264174335808 run_lib.py:166] step: 195000, eval_loss: 1.16337e+02
I1111 08:18:07.721704 140264174335808 run_lib.py:153] step: 195050, training_loss: 1.49758e+02
I1111 08:18:17.411221 140264174335808 run_lib.py:153] step: 195100, training_loss: 1.34293e+02
I1111 08:18:26.951701 140264174335808 run_lib.py:153] step: 195150, training_loss: 1.34440e+02
I1111 08:18:36.983527 140264174335808 run_lib.py:153] step: 195200, training_loss: 1.57618e+02
I1111 08:18:46.894493 140264174335808 run_lib.py:153] step: 195250, training_loss: 1.17855e+02
I1111 08:18:57.071511 140264174335808 run_lib.py:153] step: 195300, training_loss: 1.44320e+02
I1111 08:19:07.530826 140264174335808 run_lib.py:153] step: 195350, training_loss: 1.35483e+02
I1111 08:19:17.510087 140264174335808 run_lib.py:153] step: 195400, training_loss: 1.05726e+02
I1111 08:19:27.795448 140264174335808 run_lib.py:153] step: 195450, training_loss: 1.57308e+02
I1111 08:19:38.405323 140264174335808 run_lib.py:153] step: 195500, training_loss: 1.20566e+02
I1111 08:19:48.126349 140264174335808 run_lib.py:153] step: 195550, training_loss: 1.07062e+02
I1111 08:19:57.495583 140264174335808 run_lib.py:153] step: 195600, training_loss: 1.33815e+02
I1111 08:20:07.088289 140264174335808 run_lib.py:153] step: 195650, training_loss: 9.92785e+01
I1111 08:20:16.454359 140264174335808 run_lib.py:153] step: 195700, training_loss: 1.27950e+02
I1111 08:20:26.385677 140264174335808 run_lib.py:153] step: 195750, training_loss: 1.13066e+02
I1111 08:20:35.961547 140264174335808 run_lib.py:153] step: 195800, training_loss: 1.38905e+02
I1111 08:20:45.660755 140264174335808 run_lib.py:153] step: 195850, training_loss: 1.12591e+02
I1111 08:20:55.534603 140264174335808 run_lib.py:153] step: 195900, training_loss: 1.27461e+02
I1111 08:21:05.390787 140264174335808 run_lib.py:153] step: 195950, training_loss: 1.05973e+02
I1111 08:21:15.219902 140264174335808 run_lib.py:153] step: 196000, training_loss: 1.25307e+02
I1111 08:21:24.628148 140264174335808 run_lib.py:153] step: 196050, training_loss: 1.39634e+02
I1111 08:21:35.190749 140264174335808 run_lib.py:153] step: 196100, training_loss: 1.21822e+02
I1111 08:21:44.598683 140264174335808 run_lib.py:153] step: 196150, training_loss: 1.36891e+02
I1111 08:21:54.179907 140264174335808 run_lib.py:153] step: 196200, training_loss: 1.22526e+02
I1111 08:22:03.804821 140264174335808 run_lib.py:153] step: 196250, training_loss: 1.22830e+02
I1111 08:22:13.081794 140264174335808 run_lib.py:153] step: 196300, training_loss: 1.33373e+02
I1111 08:22:22.588894 140264174335808 run_lib.py:153] step: 196350, training_loss: 1.69120e+02
I1111 08:22:32.884961 140264174335808 run_lib.py:153] step: 196400, training_loss: 1.37001e+02
I1111 08:22:42.993072 140264174335808 run_lib.py:153] step: 196450, training_loss: 1.23568e+02
I1111 08:22:52.496888 140264174335808 run_lib.py:153] step: 196500, training_loss: 1.49392e+02
I1111 08:23:02.326304 140264174335808 run_lib.py:153] step: 196550, training_loss: 1.46707e+02
I1111 08:23:12.915537 140264174335808 run_lib.py:153] step: 196600, training_loss: 1.34626e+02
I1111 08:23:23.009226 140264174335808 run_lib.py:153] step: 196650, training_loss: 1.05050e+02
I1111 08:23:32.616976 140264174335808 run_lib.py:153] step: 196700, training_loss: 1.27576e+02
I1111 08:23:42.095011 140264174335808 run_lib.py:153] step: 196750, training_loss: 1.59295e+02
I1111 08:23:51.545549 140264174335808 run_lib.py:153] step: 196800, training_loss: 1.36725e+02
I1111 08:24:02.021708 140264174335808 run_lib.py:153] step: 196850, training_loss: 1.26341e+02
I1111 08:24:12.588132 140264174335808 run_lib.py:153] step: 196900, training_loss: 1.26153e+02
I1111 08:24:22.342507 140264174335808 run_lib.py:153] step: 196950, training_loss: 1.30628e+02
I1111 08:24:31.879506 140264174335808 run_lib.py:153] step: 197000, training_loss: 1.29099e+02
I1111 08:24:41.945547 140264174335808 run_lib.py:153] step: 197050, training_loss: 1.38738e+02
I1111 08:24:51.594866 140264174335808 run_lib.py:153] step: 197100, training_loss: 1.57798e+02
I1111 08:25:01.466581 140264174335808 run_lib.py:153] step: 197150, training_loss: 1.14431e+02
I1111 08:25:11.573995 140264174335808 run_lib.py:153] step: 197200, training_loss: 1.36844e+02
I1111 08:25:21.262749 140264174335808 run_lib.py:153] step: 197250, training_loss: 1.30269e+02
I1111 08:25:31.349276 140264174335808 run_lib.py:153] step: 197300, training_loss: 1.50568e+02
I1111 08:25:41.431731 140264174335808 run_lib.py:153] step: 197350, training_loss: 1.20640e+02
I1111 08:25:51.499444 140264174335808 run_lib.py:153] step: 197400, training_loss: 1.43647e+02
I1111 08:26:02.065784 140264174335808 run_lib.py:153] step: 197450, training_loss: 1.47230e+02
I1111 08:26:12.107256 140264174335808 run_lib.py:153] step: 197500, training_loss: 1.34503e+02
I1111 08:26:22.326630 140264174335808 run_lib.py:153] step: 197550, training_loss: 1.09246e+02
I1111 08:26:32.240325 140264174335808 run_lib.py:153] step: 197600, training_loss: 1.17858e+02
I1111 08:26:42.140033 140264174335808 run_lib.py:153] step: 197650, training_loss: 1.25129e+02
I1111 08:26:51.699848 140264174335808 run_lib.py:153] step: 197700, training_loss: 1.46257e+02
I1111 08:27:02.493018 140264174335808 run_lib.py:153] step: 197750, training_loss: 1.04504e+02
I1111 08:27:12.249070 140264174335808 run_lib.py:153] step: 197800, training_loss: 1.26786e+02
I1111 08:27:22.103727 140264174335808 run_lib.py:153] step: 197850, training_loss: 1.35969e+02
I1111 08:27:32.115309 140264174335808 run_lib.py:153] step: 197900, training_loss: 1.29997e+02
I1111 08:27:41.727101 140264174335808 run_lib.py:153] step: 197950, training_loss: 1.45974e+02
I1111 08:27:52.326705 140264174335808 run_lib.py:153] step: 198000, training_loss: 1.60899e+02
I1111 08:28:03.141117 140264174335808 run_lib.py:153] step: 198050, training_loss: 1.42336e+02
I1111 08:28:13.037001 140264174335808 run_lib.py:153] step: 198100, training_loss: 1.15914e+02
I1111 08:28:22.688468 140264174335808 run_lib.py:153] step: 198150, training_loss: 1.56529e+02
I1111 08:28:32.353225 140264174335808 run_lib.py:153] step: 198200, training_loss: 1.37133e+02
I1111 08:28:42.371253 140264174335808 run_lib.py:153] step: 198250, training_loss: 1.26781e+02
I1111 08:28:52.165357 140264174335808 run_lib.py:153] step: 198300, training_loss: 1.25770e+02
I1111 08:29:02.650510 140264174335808 run_lib.py:153] step: 198350, training_loss: 1.61418e+02
I1111 08:29:12.907602 140264174335808 run_lib.py:153] step: 198400, training_loss: 1.01453e+02
I1111 08:29:22.234019 140264174335808 run_lib.py:153] step: 198450, training_loss: 1.21535e+02
I1111 08:29:31.860455 140264174335808 run_lib.py:153] step: 198500, training_loss: 1.54097e+02
I1111 08:29:42.441903 140264174335808 run_lib.py:153] step: 198550, training_loss: 1.23541e+02
I1111 08:29:52.173493 140264174335808 run_lib.py:153] step: 198600, training_loss: 1.47233e+02
I1111 08:30:02.403246 140264174335808 run_lib.py:153] step: 198650, training_loss: 1.36831e+02
I1111 08:30:12.671263 140264174335808 run_lib.py:153] step: 198700, training_loss: 1.57455e+02
I1111 08:30:22.916374 140264174335808 run_lib.py:153] step: 198750, training_loss: 1.38053e+02
I1111 08:30:33.018121 140264174335808 run_lib.py:153] step: 198800, training_loss: 1.21439e+02
I1111 08:30:43.270128 140264174335808 run_lib.py:153] step: 198850, training_loss: 1.24557e+02
I1111 08:30:54.029229 140264174335808 run_lib.py:153] step: 198900, training_loss: 1.38378e+02
I1111 08:31:04.077784 140264174335808 run_lib.py:153] step: 198950, training_loss: 1.39604e+02
I1111 08:31:13.619572 140264174335808 run_lib.py:153] step: 199000, training_loss: 1.42441e+02
I1111 08:31:24.224342 140264174335808 run_lib.py:153] step: 199050, training_loss: 1.44544e+02
I1111 08:31:34.391057 140264174335808 run_lib.py:153] step: 199100, training_loss: 1.27068e+02
I1111 08:31:44.189967 140264174335808 run_lib.py:153] step: 199150, training_loss: 1.29216e+02
I1111 08:31:54.672472 140264174335808 run_lib.py:153] step: 199200, training_loss: 1.25465e+02
I1111 08:32:04.817890 140264174335808 run_lib.py:153] step: 199250, training_loss: 1.41142e+02
I1111 08:32:14.674248 140264174335808 run_lib.py:153] step: 199300, training_loss: 1.08698e+02
I1111 08:32:24.912160 140264174335808 run_lib.py:153] step: 199350, training_loss: 1.02721e+02
I1111 08:32:35.142781 140264174335808 run_lib.py:153] step: 199400, training_loss: 1.27844e+02
I1111 08:32:45.233018 140264174335808 run_lib.py:153] step: 199450, training_loss: 1.37564e+02
I1111 08:32:54.987312 140264174335808 run_lib.py:153] step: 199500, training_loss: 1.21807e+02
I1111 08:33:04.728334 140264174335808 run_lib.py:153] step: 199550, training_loss: 1.16421e+02
I1111 08:33:15.549859 140264174335808 run_lib.py:153] step: 199600, training_loss: 1.41607e+02
I1111 08:33:25.584356 140264174335808 run_lib.py:153] step: 199650, training_loss: 1.36778e+02
I1111 08:33:35.727328 140264174335808 run_lib.py:153] step: 199700, training_loss: 1.15409e+02
I1111 08:33:45.787012 140264174335808 run_lib.py:153] step: 199750, training_loss: 1.07245e+02
I1111 08:33:56.066373 140264174335808 run_lib.py:153] step: 199800, training_loss: 1.18964e+02
I1111 08:34:06.087264 140264174335808 run_lib.py:153] step: 199850, training_loss: 1.02161e+02
I1111 08:34:16.233494 140264174335808 run_lib.py:153] step: 199900, training_loss: 1.31863e+02
I1111 08:34:26.123856 140264174335808 run_lib.py:153] step: 199950, training_loss: 1.17568e+02
I1111 08:34:35.783911 140264174335808 run_lib.py:153] step: 200000, training_loss: 1.46145e+02
I1111 08:34:36.361802 140264174335808 run_lib.py:166] step: 200000, eval_loss: 1.20627e+02
I1111 08:34:46.791812 140264174335808 run_lib.py:153] step: 200050, training_loss: 1.46484e+02
I1111 08:34:57.462236 140264174335808 run_lib.py:153] step: 200100, training_loss: 1.03092e+02
I1111 08:35:07.643061 140264174335808 run_lib.py:153] step: 200150, training_loss: 1.26271e+02
I1111 08:35:18.339493 140264174335808 run_lib.py:153] step: 200200, training_loss: 1.50809e+02
I1111 08:35:27.723115 140264174335808 run_lib.py:153] step: 200250, training_loss: 1.59808e+02
I1111 08:35:38.387650 140264174335808 run_lib.py:153] step: 200300, training_loss: 1.31013e+02
I1111 08:35:48.005488 140264174335808 run_lib.py:153] step: 200350, training_loss: 1.40176e+02
I1111 08:35:58.127721 140264174335808 run_lib.py:153] step: 200400, training_loss: 1.62567e+02
I1111 08:36:07.601572 140264174335808 run_lib.py:153] step: 200450, training_loss: 1.38781e+02
I1111 08:36:17.752826 140264174335808 run_lib.py:153] step: 200500, training_loss: 1.45719e+02
I1111 08:36:27.311687 140264174335808 run_lib.py:153] step: 200550, training_loss: 1.09692e+02
I1111 08:36:37.598532 140264174335808 run_lib.py:153] step: 200600, training_loss: 1.07284e+02
I1111 08:36:48.008069 140264174335808 run_lib.py:153] step: 200650, training_loss: 1.28123e+02
I1111 08:36:57.332875 140264174335808 run_lib.py:153] step: 200700, training_loss: 1.31439e+02
I1111 08:37:06.923400 140264174335808 run_lib.py:153] step: 200750, training_loss: 1.50181e+02
I1111 08:37:16.967397 140264174335808 run_lib.py:153] step: 200800, training_loss: 9.48173e+01
I1111 08:37:27.720817 140264174335808 run_lib.py:153] step: 200850, training_loss: 1.40143e+02
I1111 08:37:37.555848 140264174335808 run_lib.py:153] step: 200900, training_loss: 1.44493e+02
I1111 08:37:48.154057 140264174335808 run_lib.py:153] step: 200950, training_loss: 1.09073e+02
I1111 08:37:58.905568 140264174335808 run_lib.py:153] step: 201000, training_loss: 1.34585e+02
I1111 08:38:08.697634 140264174335808 run_lib.py:153] step: 201050, training_loss: 1.18946e+02
I1111 08:38:18.819622 140264174335808 run_lib.py:153] step: 201100, training_loss: 1.46626e+02
I1111 08:38:29.460471 140264174335808 run_lib.py:153] step: 201150, training_loss: 1.07234e+02
I1111 08:38:39.252579 140264174335808 run_lib.py:153] step: 201200, training_loss: 1.31543e+02
I1111 08:38:49.311428 140264174335808 run_lib.py:153] step: 201250, training_loss: 1.29332e+02
I1111 08:38:59.337546 140264174335808 run_lib.py:153] step: 201300, training_loss: 1.27438e+02
I1111 08:39:09.459079 140264174335808 run_lib.py:153] step: 201350, training_loss: 1.69222e+02
I1111 08:39:19.743201 140264174335808 run_lib.py:153] step: 201400, training_loss: 1.65789e+02
I1111 08:39:29.593688 140264174335808 run_lib.py:153] step: 201450, training_loss: 1.15348e+02
I1111 08:39:39.376271 140264174335808 run_lib.py:153] step: 201500, training_loss: 1.23409e+02
I1111 08:39:49.164579 140264174335808 run_lib.py:153] step: 201550, training_loss: 1.65528e+02
I1111 08:39:59.068966 140264174335808 run_lib.py:153] step: 201600, training_loss: 1.44814e+02
I1111 08:40:09.095931 140264174335808 run_lib.py:153] step: 201650, training_loss: 1.32958e+02
I1111 08:40:18.981210 140264174335808 run_lib.py:153] step: 201700, training_loss: 1.42692e+02
I1111 08:40:29.188978 140264174335808 run_lib.py:153] step: 201750, training_loss: 1.30787e+02
I1111 08:40:39.207452 140264174335808 run_lib.py:153] step: 201800, training_loss: 1.55996e+02
I1111 08:40:49.152470 140264174335808 run_lib.py:153] step: 201850, training_loss: 1.24430e+02
I1111 08:40:59.128065 140264174335808 run_lib.py:153] step: 201900, training_loss: 1.46901e+02
I1111 08:41:08.576945 140264174335808 run_lib.py:153] step: 201950, training_loss: 1.67180e+02
I1111 08:41:19.093166 140264174335808 run_lib.py:153] step: 202000, training_loss: 1.31446e+02
I1111 08:41:29.333514 140264174335808 run_lib.py:153] step: 202050, training_loss: 1.25308e+02
I1111 08:41:39.150779 140264174335808 run_lib.py:153] step: 202100, training_loss: 1.10384e+02
I1111 08:41:49.153480 140264174335808 run_lib.py:153] step: 202150, training_loss: 1.15172e+02
I1111 08:41:59.179929 140264174335808 run_lib.py:153] step: 202200, training_loss: 1.29474e+02
I1111 08:42:08.613882 140264174335808 run_lib.py:153] step: 202250, training_loss: 1.26589e+02
I1111 08:42:18.763582 140264174335808 run_lib.py:153] step: 202300, training_loss: 1.36323e+02
I1111 08:42:29.213646 140264174335808 run_lib.py:153] step: 202350, training_loss: 1.09361e+02
I1111 08:42:39.797926 140264174335808 run_lib.py:153] step: 202400, training_loss: 1.12873e+02
I1111 08:42:50.325723 140264174335808 run_lib.py:153] step: 202450, training_loss: 1.28048e+02
I1111 08:43:00.333953 140264174335808 run_lib.py:153] step: 202500, training_loss: 1.36328e+02
I1111 08:43:10.307881 140264174335808 run_lib.py:153] step: 202550, training_loss: 1.32364e+02
I1111 08:43:20.035357 140264174335808 run_lib.py:153] step: 202600, training_loss: 1.38963e+02
I1111 08:43:30.646925 140264174335808 run_lib.py:153] step: 202650, training_loss: 9.65099e+01
I1111 08:43:40.925028 140264174335808 run_lib.py:153] step: 202700, training_loss: 1.37426e+02
I1111 08:43:50.803748 140264174335808 run_lib.py:153] step: 202750, training_loss: 1.34617e+02
I1111 08:44:00.281730 140264174335808 run_lib.py:153] step: 202800, training_loss: 1.19147e+02
I1111 08:44:09.866697 140264174335808 run_lib.py:153] step: 202850, training_loss: 1.04020e+02
I1111 08:44:19.786862 140264174335808 run_lib.py:153] step: 202900, training_loss: 1.07220e+02
I1111 08:44:30.512969 140264174335808 run_lib.py:153] step: 202950, training_loss: 1.29323e+02
I1111 08:44:40.266978 140264174335808 run_lib.py:153] step: 203000, training_loss: 1.26199e+02
I1111 08:44:50.174378 140264174335808 run_lib.py:153] step: 203050, training_loss: 1.11895e+02
I1111 08:45:00.456228 140264174335808 run_lib.py:153] step: 203100, training_loss: 1.50112e+02
I1111 08:45:10.411957 140264174335808 run_lib.py:153] step: 203150, training_loss: 1.19097e+02
I1111 08:45:20.006851 140264174335808 run_lib.py:153] step: 203200, training_loss: 1.27455e+02
I1111 08:45:29.899595 140264174335808 run_lib.py:153] step: 203250, training_loss: 1.55099e+02
I1111 08:45:39.878826 140264174335808 run_lib.py:153] step: 203300, training_loss: 1.64212e+02
I1111 08:45:49.918950 140264174335808 run_lib.py:153] step: 203350, training_loss: 1.04112e+02
I1111 08:45:59.627310 140264174335808 run_lib.py:153] step: 203400, training_loss: 1.31647e+02
I1111 08:46:09.173456 140264174335808 run_lib.py:153] step: 203450, training_loss: 1.19093e+02
I1111 08:46:18.822183 140264174335808 run_lib.py:153] step: 203500, training_loss: 1.09047e+02
I1111 08:46:29.228934 140264174335808 run_lib.py:153] step: 203550, training_loss: 1.27502e+02
I1111 08:46:39.603168 140264174335808 run_lib.py:153] step: 203600, training_loss: 1.28136e+02
I1111 08:46:49.157309 140264174335808 run_lib.py:153] step: 203650, training_loss: 1.45220e+02
I1111 08:46:58.535578 140264174335808 run_lib.py:153] step: 203700, training_loss: 1.04785e+02
I1111 08:47:08.363003 140264174335808 run_lib.py:153] step: 203750, training_loss: 1.41561e+02
I1111 08:47:18.267794 140264174335808 run_lib.py:153] step: 203800, training_loss: 1.31257e+02
I1111 08:47:28.303165 140264174335808 run_lib.py:153] step: 203850, training_loss: 1.50409e+02
I1111 08:47:39.326303 140264174335808 run_lib.py:153] step: 203900, training_loss: 1.39365e+02
I1111 08:47:49.232651 140264174335808 run_lib.py:153] step: 203950, training_loss: 1.17728e+02
I1111 08:47:59.162881 140264174335808 run_lib.py:153] step: 204000, training_loss: 1.44853e+02
I1111 08:48:08.914409 140264174335808 run_lib.py:153] step: 204050, training_loss: 1.30260e+02
I1111 08:48:18.566774 140264174335808 run_lib.py:153] step: 204100, training_loss: 1.20438e+02
I1111 08:48:28.967118 140264174335808 run_lib.py:153] step: 204150, training_loss: 1.61234e+02
I1111 08:48:38.901706 140264174335808 run_lib.py:153] step: 204200, training_loss: 1.51043e+02
I1111 08:48:48.989567 140264174335808 run_lib.py:153] step: 204250, training_loss: 1.20759e+02
I1111 08:48:58.921640 140264174335808 run_lib.py:153] step: 204300, training_loss: 1.18236e+02
I1111 08:49:08.480828 140264174335808 run_lib.py:153] step: 204350, training_loss: 1.55446e+02
I1111 08:49:18.347311 140264174335808 run_lib.py:153] step: 204400, training_loss: 1.27465e+02
I1111 08:49:28.741949 140264174335808 run_lib.py:153] step: 204450, training_loss: 1.46064e+02
I1111 08:49:38.374002 140264174335808 run_lib.py:153] step: 204500, training_loss: 1.30891e+02
I1111 08:49:49.019620 140264174335808 run_lib.py:153] step: 204550, training_loss: 1.42879e+02
I1111 08:49:59.328096 140264174335808 run_lib.py:153] step: 204600, training_loss: 1.50727e+02
I1111 08:50:09.276213 140264174335808 run_lib.py:153] step: 204650, training_loss: 1.64824e+02
I1111 08:50:19.190792 140264174335808 run_lib.py:153] step: 204700, training_loss: 9.91574e+01
I1111 08:50:28.961933 140264174335808 run_lib.py:153] step: 204750, training_loss: 1.44254e+02
I1111 08:50:39.268828 140264174335808 run_lib.py:153] step: 204800, training_loss: 1.22010e+02
I1111 08:50:49.313210 140264174335808 run_lib.py:153] step: 204850, training_loss: 1.29943e+02
I1111 08:50:58.734520 140264174335808 run_lib.py:153] step: 204900, training_loss: 1.23112e+02
I1111 08:51:09.070174 140264174335808 run_lib.py:153] step: 204950, training_loss: 1.23519e+02
I1111 08:51:19.453340 140264174335808 run_lib.py:153] step: 205000, training_loss: 1.20536e+02
I1111 08:51:19.559743 140264174335808 run_lib.py:166] step: 205000, eval_loss: 1.20107e+02
I1111 08:51:29.947017 140264174335808 run_lib.py:153] step: 205050, training_loss: 1.30299e+02
I1111 08:51:40.061014 140264174335808 run_lib.py:153] step: 205100, training_loss: 1.16506e+02
I1111 08:51:50.439825 140264174335808 run_lib.py:153] step: 205150, training_loss: 1.56184e+02
I1111 08:52:00.050554 140264174335808 run_lib.py:153] step: 205200, training_loss: 1.44842e+02
I1111 08:52:09.637497 140264174335808 run_lib.py:153] step: 205250, training_loss: 1.16346e+02
I1111 08:52:19.605831 140264174335808 run_lib.py:153] step: 205300, training_loss: 1.05453e+02
I1111 08:52:29.754035 140264174335808 run_lib.py:153] step: 205350, training_loss: 1.44759e+02
I1111 08:52:39.918870 140264174335808 run_lib.py:153] step: 205400, training_loss: 1.55947e+02
I1111 08:52:49.345647 140264174335808 run_lib.py:153] step: 205450, training_loss: 1.23416e+02
I1111 08:52:59.343553 140264174335808 run_lib.py:153] step: 205500, training_loss: 1.27039e+02
I1111 08:53:09.314487 140264174335808 run_lib.py:153] step: 205550, training_loss: 1.30208e+02
I1111 08:53:19.083456 140264174335808 run_lib.py:153] step: 205600, training_loss: 1.24380e+02
I1111 08:53:29.081555 140264174335808 run_lib.py:153] step: 205650, training_loss: 1.26082e+02
I1111 08:53:39.425163 140264174335808 run_lib.py:153] step: 205700, training_loss: 1.38917e+02
I1111 08:53:49.801532 140264174335808 run_lib.py:153] step: 205750, training_loss: 9.98380e+01
I1111 08:54:00.150343 140264174335808 run_lib.py:153] step: 205800, training_loss: 9.42329e+01
I1111 08:54:10.363443 140264174335808 run_lib.py:153] step: 205850, training_loss: 1.31998e+02
I1111 08:54:20.848458 140264174335808 run_lib.py:153] step: 205900, training_loss: 1.14475e+02
I1111 08:54:31.382855 140264174335808 run_lib.py:153] step: 205950, training_loss: 1.30547e+02
I1111 08:54:41.871482 140264174335808 run_lib.py:153] step: 206000, training_loss: 1.52301e+02
I1111 08:54:51.891613 140264174335808 run_lib.py:153] step: 206050, training_loss: 1.08624e+02
I1111 08:55:02.144619 140264174335808 run_lib.py:153] step: 206100, training_loss: 1.39478e+02
I1111 08:55:12.968711 140264174335808 run_lib.py:153] step: 206150, training_loss: 1.67098e+02
I1111 08:55:23.734412 140264174335808 run_lib.py:153] step: 206200, training_loss: 1.07980e+02
I1111 08:55:34.015553 140264174335808 run_lib.py:153] step: 206250, training_loss: 1.20893e+02
I1111 08:55:44.085561 140264174335808 run_lib.py:153] step: 206300, training_loss: 8.90369e+01
I1111 08:55:53.556518 140264174335808 run_lib.py:153] step: 206350, training_loss: 1.36080e+02
I1111 08:56:03.844917 140264174335808 run_lib.py:153] step: 206400, training_loss: 1.52914e+02
I1111 08:56:13.960762 140264174335808 run_lib.py:153] step: 206450, training_loss: 1.16246e+02
I1111 08:56:24.819034 140264174335808 run_lib.py:153] step: 206500, training_loss: 1.45889e+02
I1111 08:56:34.973769 140264174335808 run_lib.py:153] step: 206550, training_loss: 1.10589e+02
I1111 08:56:45.966885 140264174335808 run_lib.py:153] step: 206600, training_loss: 9.65463e+01
I1111 08:56:56.244290 140264174335808 run_lib.py:153] step: 206650, training_loss: 1.23212e+02
I1111 08:57:06.250059 140264174335808 run_lib.py:153] step: 206700, training_loss: 1.05202e+02
I1111 08:57:16.313874 140264174335808 run_lib.py:153] step: 206750, training_loss: 1.22778e+02
I1111 08:57:26.548449 140264174335808 run_lib.py:153] step: 206800, training_loss: 1.08766e+02
I1111 08:57:36.085958 140264174335808 run_lib.py:153] step: 206850, training_loss: 1.37089e+02
I1111 08:57:46.214095 140264174335808 run_lib.py:153] step: 206900, training_loss: 1.24076e+02
I1111 08:57:55.904058 140264174335808 run_lib.py:153] step: 206950, training_loss: 1.17854e+02
I1111 08:58:05.924968 140264174335808 run_lib.py:153] step: 207000, training_loss: 1.27466e+02
I1111 08:58:16.296052 140264174335808 run_lib.py:153] step: 207050, training_loss: 1.65607e+02
I1111 08:58:26.556107 140264174335808 run_lib.py:153] step: 207100, training_loss: 1.17431e+02
I1111 08:58:36.813954 140264174335808 run_lib.py:153] step: 207150, training_loss: 1.29540e+02
I1111 08:58:46.287225 140264174335808 run_lib.py:153] step: 207200, training_loss: 1.21012e+02
I1111 08:58:56.664667 140264174335808 run_lib.py:153] step: 207250, training_loss: 1.29009e+02
I1111 08:59:07.132513 140264174335808 run_lib.py:153] step: 207300, training_loss: 1.44293e+02
I1111 08:59:17.559188 140264174335808 run_lib.py:153] step: 207350, training_loss: 1.63018e+02
I1111 08:59:27.476990 140264174335808 run_lib.py:153] step: 207400, training_loss: 1.25962e+02
I1111 08:59:37.822950 140264174335808 run_lib.py:153] step: 207450, training_loss: 1.35608e+02
I1111 08:59:47.579395 140264174335808 run_lib.py:153] step: 207500, training_loss: 1.23703e+02
I1111 08:59:56.943524 140264174335808 run_lib.py:153] step: 207550, training_loss: 1.39814e+02
I1111 09:00:07.065833 140264174335808 run_lib.py:153] step: 207600, training_loss: 1.36801e+02
I1111 09:00:17.125799 140264174335808 run_lib.py:153] step: 207650, training_loss: 1.39629e+02
I1111 09:00:27.341577 140264174335808 run_lib.py:153] step: 207700, training_loss: 1.27728e+02
I1111 09:00:37.282665 140264174335808 run_lib.py:153] step: 207750, training_loss: 1.39349e+02
I1111 09:00:46.913823 140264174335808 run_lib.py:153] step: 207800, training_loss: 1.37551e+02
I1111 09:00:57.166761 140264174335808 run_lib.py:153] step: 207850, training_loss: 1.31774e+02
I1111 09:01:06.878007 140264174335808 run_lib.py:153] step: 207900, training_loss: 1.48398e+02
I1111 09:01:17.602201 140264174335808 run_lib.py:153] step: 207950, training_loss: 1.36668e+02
I1111 09:01:27.623528 140264174335808 run_lib.py:153] step: 208000, training_loss: 1.41344e+02
I1111 09:01:37.776015 140264174335808 run_lib.py:153] step: 208050, training_loss: 1.23569e+02
I1111 09:01:48.272553 140264174335808 run_lib.py:153] step: 208100, training_loss: 1.27500e+02
I1111 09:01:58.745660 140264174335808 run_lib.py:153] step: 208150, training_loss: 1.46869e+02
I1111 09:02:09.573281 140264174335808 run_lib.py:153] step: 208200, training_loss: 1.21556e+02
I1111 09:02:19.722641 140264174335808 run_lib.py:153] step: 208250, training_loss: 9.26914e+01
I1111 09:02:29.548463 140264174335808 run_lib.py:153] step: 208300, training_loss: 1.78381e+02
I1111 09:02:39.990325 140264174335808 run_lib.py:153] step: 208350, training_loss: 1.37450e+02
I1111 09:02:50.325401 140264174335808 run_lib.py:153] step: 208400, training_loss: 1.34475e+02
I1111 09:03:00.866532 140264174335808 run_lib.py:153] step: 208450, training_loss: 1.37282e+02
I1111 09:03:11.420924 140264174335808 run_lib.py:153] step: 208500, training_loss: 1.37776e+02
I1111 09:03:22.146173 140264174335808 run_lib.py:153] step: 208550, training_loss: 1.31535e+02
I1111 09:03:32.278670 140264174335808 run_lib.py:153] step: 208600, training_loss: 1.22919e+02
I1111 09:03:42.758683 140264174335808 run_lib.py:153] step: 208650, training_loss: 1.18295e+02
I1111 09:03:53.147572 140264174335808 run_lib.py:153] step: 208700, training_loss: 1.10177e+02
I1111 09:04:03.287167 140264174335808 run_lib.py:153] step: 208750, training_loss: 1.52496e+02
I1111 09:04:13.268736 140264174335808 run_lib.py:153] step: 208800, training_loss: 1.16797e+02
I1111 09:04:23.109478 140264174335808 run_lib.py:153] step: 208850, training_loss: 1.39979e+02
I1111 09:04:32.968073 140264174335808 run_lib.py:153] step: 208900, training_loss: 1.32359e+02
I1111 09:04:42.878861 140264174335808 run_lib.py:153] step: 208950, training_loss: 1.30110e+02
I1111 09:04:52.766561 140264174335808 run_lib.py:153] step: 209000, training_loss: 1.14613e+02
I1111 09:05:03.131628 140264174335808 run_lib.py:153] step: 209050, training_loss: 1.00174e+02
I1111 09:05:13.363584 140264174335808 run_lib.py:153] step: 209100, training_loss: 1.37241e+02
I1111 09:05:23.915660 140264174335808 run_lib.py:153] step: 209150, training_loss: 1.23746e+02
I1111 09:05:33.675693 140264174335808 run_lib.py:153] step: 209200, training_loss: 1.41079e+02
I1111 09:05:43.798031 140264174335808 run_lib.py:153] step: 209250, training_loss: 1.23037e+02
I1111 09:05:53.460103 140264174335808 run_lib.py:153] step: 209300, training_loss: 1.27794e+02
I1111 09:06:03.668260 140264174335808 run_lib.py:153] step: 209350, training_loss: 1.21097e+02
I1111 09:06:13.380246 140264174335808 run_lib.py:153] step: 209400, training_loss: 1.33738e+02
I1111 09:06:23.388726 140264174335808 run_lib.py:153] step: 209450, training_loss: 1.16854e+02
I1111 09:06:33.989955 140264174335808 run_lib.py:153] step: 209500, training_loss: 1.09779e+02
I1111 09:06:44.886810 140264174335808 run_lib.py:153] step: 209550, training_loss: 1.16712e+02
I1111 09:06:54.827787 140264174335808 run_lib.py:153] step: 209600, training_loss: 1.21781e+02
I1111 09:07:04.367615 140264174335808 run_lib.py:153] step: 209650, training_loss: 1.49024e+02
I1111 09:07:15.008789 140264174335808 run_lib.py:153] step: 209700, training_loss: 1.39916e+02
I1111 09:07:25.057621 140264174335808 run_lib.py:153] step: 209750, training_loss: 1.08266e+02
I1111 09:07:35.365403 140264174335808 run_lib.py:153] step: 209800, training_loss: 1.06230e+02
I1111 09:07:45.476807 140264174335808 run_lib.py:153] step: 209850, training_loss: 1.33082e+02
I1111 09:07:55.588500 140264174335808 run_lib.py:153] step: 209900, training_loss: 1.33905e+02
I1111 09:08:06.202218 140264174335808 run_lib.py:153] step: 209950, training_loss: 1.36415e+02
I1111 09:08:16.243036 140264174335808 run_lib.py:153] step: 210000, training_loss: 1.56107e+02
I1111 09:08:16.842078 140264174335808 run_lib.py:166] step: 210000, eval_loss: 1.13103e+02
I1111 09:08:27.070850 140264174335808 run_lib.py:153] step: 210050, training_loss: 1.19041e+02
I1111 09:08:37.074193 140264174335808 run_lib.py:153] step: 210100, training_loss: 1.21301e+02
I1111 09:08:47.191420 140264174335808 run_lib.py:153] step: 210150, training_loss: 1.31167e+02
I1111 09:08:57.630235 140264174335808 run_lib.py:153] step: 210200, training_loss: 1.18562e+02
I1111 09:09:07.053614 140264174335808 run_lib.py:153] step: 210250, training_loss: 1.04145e+02
I1111 09:09:17.910246 140264174335808 run_lib.py:153] step: 210300, training_loss: 1.31039e+02
I1111 09:09:28.639873 140264174335808 run_lib.py:153] step: 210350, training_loss: 1.36725e+02
I1111 09:09:39.276990 140264174335808 run_lib.py:153] step: 210400, training_loss: 1.08233e+02
I1111 09:09:49.640166 140264174335808 run_lib.py:153] step: 210450, training_loss: 1.58079e+02
I1111 09:09:59.866150 140264174335808 run_lib.py:153] step: 210500, training_loss: 1.33417e+02
I1111 09:10:09.904973 140264174335808 run_lib.py:153] step: 210550, training_loss: 1.31155e+02
I1111 09:10:20.071393 140264174335808 run_lib.py:153] step: 210600, training_loss: 1.42728e+02
I1111 09:10:30.122876 140264174335808 run_lib.py:153] step: 210650, training_loss: 1.28208e+02
I1111 09:10:39.891686 140264174335808 run_lib.py:153] step: 210700, training_loss: 1.41628e+02
I1111 09:10:50.175281 140264174335808 run_lib.py:153] step: 210750, training_loss: 1.22690e+02
I1111 09:11:00.263582 140264174335808 run_lib.py:153] step: 210800, training_loss: 1.16802e+02
I1111 09:11:10.242297 140264174335808 run_lib.py:153] step: 210850, training_loss: 1.20910e+02
I1111 09:11:20.595111 140264174335808 run_lib.py:153] step: 210900, training_loss: 1.25534e+02
I1111 09:11:30.746528 140264174335808 run_lib.py:153] step: 210950, training_loss: 1.24706e+02
I1111 09:11:40.889478 140264174335808 run_lib.py:153] step: 211000, training_loss: 1.33881e+02
I1111 09:11:50.596776 140264174335808 run_lib.py:153] step: 211050, training_loss: 1.18984e+02
I1111 09:12:01.499493 140264174335808 run_lib.py:153] step: 211100, training_loss: 1.46473e+02
I1111 09:12:11.765058 140264174335808 run_lib.py:153] step: 211150, training_loss: 1.35255e+02
I1111 09:12:22.305607 140264174335808 run_lib.py:153] step: 211200, training_loss: 1.34659e+02
I1111 09:12:32.723237 140264174335808 run_lib.py:153] step: 211250, training_loss: 1.25781e+02
I1111 09:12:43.196075 140264174335808 run_lib.py:153] step: 211300, training_loss: 1.26969e+02
I1111 09:12:52.729567 140264174335808 run_lib.py:153] step: 211350, training_loss: 1.13716e+02
I1111 09:13:02.655658 140264174335808 run_lib.py:153] step: 211400, training_loss: 1.19543e+02
I1111 09:13:13.233592 140264174335808 run_lib.py:153] step: 211450, training_loss: 1.20752e+02
I1111 09:13:22.680094 140264174335808 run_lib.py:153] step: 211500, training_loss: 1.36276e+02
I1111 09:13:32.508623 140264174335808 run_lib.py:153] step: 211550, training_loss: 1.33002e+02
I1111 09:13:42.043930 140264174335808 run_lib.py:153] step: 211600, training_loss: 1.24847e+02
I1111 09:13:51.834948 140264174335808 run_lib.py:153] step: 211650, training_loss: 1.21090e+02
I1111 09:14:01.218952 140264174335808 run_lib.py:153] step: 211700, training_loss: 1.19877e+02
I1111 09:14:11.811643 140264174335808 run_lib.py:153] step: 211750, training_loss: 1.40902e+02
I1111 09:14:22.192105 140264174335808 run_lib.py:153] step: 211800, training_loss: 1.19507e+02
I1111 09:14:32.251398 140264174335808 run_lib.py:153] step: 211850, training_loss: 1.17602e+02
I1111 09:14:42.983313 140264174335808 run_lib.py:153] step: 211900, training_loss: 1.16278e+02
I1111 09:14:52.986722 140264174335808 run_lib.py:153] step: 211950, training_loss: 1.15203e+02
I1111 09:15:02.527372 140264174335808 run_lib.py:153] step: 212000, training_loss: 1.38026e+02
I1111 09:15:12.617858 140264174335808 run_lib.py:153] step: 212050, training_loss: 1.39518e+02
I1111 09:15:22.511696 140264174335808 run_lib.py:153] step: 212100, training_loss: 1.39140e+02
I1111 09:15:32.555074 140264174335808 run_lib.py:153] step: 212150, training_loss: 1.34842e+02
I1111 09:15:42.279188 140264174335808 run_lib.py:153] step: 212200, training_loss: 1.34473e+02
I1111 09:15:52.355426 140264174335808 run_lib.py:153] step: 212250, training_loss: 1.11050e+02
I1111 09:16:01.875097 140264174335808 run_lib.py:153] step: 212300, training_loss: 1.49420e+02
I1111 09:16:11.897588 140264174335808 run_lib.py:153] step: 212350, training_loss: 1.44363e+02
I1111 09:16:21.431312 140264174335808 run_lib.py:153] step: 212400, training_loss: 1.13154e+02
I1111 09:16:31.212814 140264174335808 run_lib.py:153] step: 212450, training_loss: 1.43959e+02
I1111 09:16:41.294954 140264174335808 run_lib.py:153] step: 212500, training_loss: 1.42012e+02
I1111 09:16:51.473726 140264174335808 run_lib.py:153] step: 212550, training_loss: 1.07699e+02
I1111 09:17:01.334416 140264174335808 run_lib.py:153] step: 212600, training_loss: 1.47644e+02
I1111 09:17:11.331857 140264174335808 run_lib.py:153] step: 212650, training_loss: 1.24683e+02
I1111 09:17:20.875864 140264174335808 run_lib.py:153] step: 212700, training_loss: 1.33984e+02
I1111 09:17:31.208017 140264174335808 run_lib.py:153] step: 212750, training_loss: 1.34906e+02
I1111 09:17:41.186187 140264174335808 run_lib.py:153] step: 212800, training_loss: 1.32901e+02
I1111 09:17:50.812995 140264174335808 run_lib.py:153] step: 212850, training_loss: 1.16205e+02
I1111 09:18:00.863180 140264174335808 run_lib.py:153] step: 212900, training_loss: 1.14673e+02
I1111 09:18:11.280598 140264174335808 run_lib.py:153] step: 212950, training_loss: 1.25365e+02
I1111 09:18:21.813681 140264174335808 run_lib.py:153] step: 213000, training_loss: 1.17950e+02
I1111 09:18:31.520642 140264174335808 run_lib.py:153] step: 213050, training_loss: 1.18752e+02
I1111 09:18:41.373158 140264174335808 run_lib.py:153] step: 213100, training_loss: 1.50704e+02
I1111 09:18:51.337077 140264174335808 run_lib.py:153] step: 213150, training_loss: 1.21401e+02
I1111 09:19:01.347815 140264174335808 run_lib.py:153] step: 213200, training_loss: 1.19518e+02
I1111 09:19:10.683084 140264174335808 run_lib.py:153] step: 213250, training_loss: 1.19704e+02
I1111 09:19:20.611822 140264174335808 run_lib.py:153] step: 213300, training_loss: 9.68239e+01
I1111 09:19:31.132305 140264174335808 run_lib.py:153] step: 213350, training_loss: 1.46791e+02
I1111 09:19:41.435692 140264174335808 run_lib.py:153] step: 213400, training_loss: 1.32434e+02
I1111 09:19:51.458087 140264174335808 run_lib.py:153] step: 213450, training_loss: 1.31105e+02
I1111 09:20:01.060512 140264174335808 run_lib.py:153] step: 213500, training_loss: 1.14143e+02
I1111 09:20:10.855686 140264174335808 run_lib.py:153] step: 213550, training_loss: 1.00395e+02
I1111 09:20:20.613394 140264174335808 run_lib.py:153] step: 213600, training_loss: 1.16086e+02
I1111 09:20:30.568226 140264174335808 run_lib.py:153] step: 213650, training_loss: 1.14345e+02
I1111 09:20:40.065986 140264174335808 run_lib.py:153] step: 213700, training_loss: 1.39436e+02
I1111 09:20:50.730422 140264174335808 run_lib.py:153] step: 213750, training_loss: 1.34528e+02
I1111 09:21:00.860227 140264174335808 run_lib.py:153] step: 213800, training_loss: 1.23091e+02
I1111 09:21:11.217997 140264174335808 run_lib.py:153] step: 213850, training_loss: 1.29277e+02
I1111 09:21:21.043790 140264174335808 run_lib.py:153] step: 213900, training_loss: 1.01123e+02
I1111 09:21:30.622084 140264174335808 run_lib.py:153] step: 213950, training_loss: 1.44511e+02
I1111 09:21:40.596822 140264174335808 run_lib.py:153] step: 214000, training_loss: 1.09975e+02
I1111 09:21:50.451906 140264174335808 run_lib.py:153] step: 214050, training_loss: 1.19709e+02
I1111 09:22:00.050841 140264174335808 run_lib.py:153] step: 214100, training_loss: 1.39925e+02
I1111 09:22:10.664221 140264174335808 run_lib.py:153] step: 214150, training_loss: 1.03735e+02
I1111 09:22:21.318363 140264174335808 run_lib.py:153] step: 214200, training_loss: 1.39685e+02
I1111 09:22:31.872482 140264174335808 run_lib.py:153] step: 214250, training_loss: 1.51013e+02
I1111 09:22:42.359206 140264174335808 run_lib.py:153] step: 214300, training_loss: 1.34629e+02
I1111 09:22:51.605215 140264174335808 run_lib.py:153] step: 214350, training_loss: 1.37734e+02
I1111 09:23:01.813589 140264174335808 run_lib.py:153] step: 214400, training_loss: 1.17610e+02
I1111 09:23:11.990829 140264174335808 run_lib.py:153] step: 214450, training_loss: 1.58745e+02
I1111 09:23:22.158046 140264174335808 run_lib.py:153] step: 214500, training_loss: 1.07315e+02
I1111 09:23:32.167685 140264174335808 run_lib.py:153] step: 214550, training_loss: 1.06508e+02
I1111 09:23:42.519261 140264174335808 run_lib.py:153] step: 214600, training_loss: 1.32604e+02
I1111 09:23:52.296804 140264174335808 run_lib.py:153] step: 214650, training_loss: 1.60239e+02
I1111 09:24:02.990416 140264174335808 run_lib.py:153] step: 214700, training_loss: 1.38543e+02
I1111 09:24:13.482734 140264174335808 run_lib.py:153] step: 214750, training_loss: 1.23613e+02
I1111 09:24:23.643526 140264174335808 run_lib.py:153] step: 214800, training_loss: 1.40969e+02
I1111 09:24:34.209811 140264174335808 run_lib.py:153] step: 214850, training_loss: 1.39384e+02
I1111 09:24:44.446907 140264174335808 run_lib.py:153] step: 214900, training_loss: 1.06553e+02
I1111 09:24:54.863824 140264174335808 run_lib.py:153] step: 214950, training_loss: 1.39168e+02
I1111 09:25:04.866072 140264174335808 run_lib.py:153] step: 215000, training_loss: 1.48629e+02
I1111 09:25:04.970136 140264174335808 run_lib.py:166] step: 215000, eval_loss: 1.30813e+02
I1111 09:25:15.450313 140264174335808 run_lib.py:153] step: 215050, training_loss: 1.25313e+02
I1111 09:25:25.642287 140264174335808 run_lib.py:153] step: 215100, training_loss: 1.34131e+02
I1111 09:25:35.960367 140264174335808 run_lib.py:153] step: 215150, training_loss: 1.28844e+02
I1111 09:25:46.703316 140264174335808 run_lib.py:153] step: 215200, training_loss: 1.15740e+02
I1111 09:25:56.889728 140264174335808 run_lib.py:153] step: 215250, training_loss: 1.10916e+02
I1111 09:26:06.308945 140264174335808 run_lib.py:153] step: 215300, training_loss: 1.28281e+02
I1111 09:26:16.640594 140264174335808 run_lib.py:153] step: 215350, training_loss: 1.13870e+02
I1111 09:26:27.018241 140264174335808 run_lib.py:153] step: 215400, training_loss: 1.43609e+02
I1111 09:26:37.177916 140264174335808 run_lib.py:153] step: 215450, training_loss: 1.17195e+02
I1111 09:26:47.192469 140264174335808 run_lib.py:153] step: 215500, training_loss: 1.38661e+02
I1111 09:26:57.278885 140264174335808 run_lib.py:153] step: 215550, training_loss: 1.31777e+02
I1111 09:27:07.400586 140264174335808 run_lib.py:153] step: 215600, training_loss: 1.12715e+02
I1111 09:27:17.543709 140264174335808 run_lib.py:153] step: 215650, training_loss: 1.23268e+02
I1111 09:27:28.097399 140264174335808 run_lib.py:153] step: 215700, training_loss: 1.32206e+02
I1111 09:27:37.645234 140264174335808 run_lib.py:153] step: 215750, training_loss: 1.61317e+02
I1111 09:27:47.602689 140264174335808 run_lib.py:153] step: 215800, training_loss: 1.41104e+02
I1111 09:27:56.865987 140264174335808 run_lib.py:153] step: 215850, training_loss: 1.42753e+02
I1111 09:28:06.382318 140264174335808 run_lib.py:153] step: 215900, training_loss: 1.45091e+02
I1111 09:28:16.033131 140264174335808 run_lib.py:153] step: 215950, training_loss: 1.34065e+02
I1111 09:28:26.504657 140264174335808 run_lib.py:153] step: 216000, training_loss: 1.02766e+02
I1111 09:28:36.819950 140264174335808 run_lib.py:153] step: 216050, training_loss: 1.17626e+02
I1111 09:28:46.481857 140264174335808 run_lib.py:153] step: 216100, training_loss: 9.91210e+01
I1111 09:28:56.319087 140264174335808 run_lib.py:153] step: 216150, training_loss: 1.26578e+02
I1111 09:29:05.546669 140264174335808 run_lib.py:153] step: 216200, training_loss: 1.29525e+02
I1111 09:29:15.639823 140264174335808 run_lib.py:153] step: 216250, training_loss: 1.12242e+02
I1111 09:29:26.777195 140264174335808 run_lib.py:153] step: 216300, training_loss: 1.21443e+02
I1111 09:29:36.957988 140264174335808 run_lib.py:153] step: 216350, training_loss: 1.44697e+02
I1111 09:29:46.596506 140264174335808 run_lib.py:153] step: 216400, training_loss: 1.20682e+02
I1111 09:29:56.325382 140264174335808 run_lib.py:153] step: 216450, training_loss: 1.43309e+02
I1111 09:30:06.376780 140264174335808 run_lib.py:153] step: 216500, training_loss: 1.42032e+02
I1111 09:30:16.255276 140264174335808 run_lib.py:153] step: 216550, training_loss: 1.15072e+02
I1111 09:30:26.291929 140264174335808 run_lib.py:153] step: 216600, training_loss: 1.41783e+02
I1111 09:30:36.632346 140264174335808 run_lib.py:153] step: 216650, training_loss: 1.22548e+02
I1111 09:30:46.371620 140264174335808 run_lib.py:153] step: 216700, training_loss: 1.50489e+02
I1111 09:30:56.478517 140264174335808 run_lib.py:153] step: 216750, training_loss: 1.30413e+02
I1111 09:31:06.540456 140264174335808 run_lib.py:153] step: 216800, training_loss: 1.17736e+02
I1111 09:31:16.583629 140264174335808 run_lib.py:153] step: 216850, training_loss: 9.65305e+01
I1111 09:31:26.427362 140264174335808 run_lib.py:153] step: 216900, training_loss: 1.36840e+02
I1111 09:31:35.938005 140264174335808 run_lib.py:153] step: 216950, training_loss: 1.29136e+02
I1111 09:31:45.302280 140264174335808 run_lib.py:153] step: 217000, training_loss: 1.14978e+02
I1111 09:31:54.795979 140264174335808 run_lib.py:153] step: 217050, training_loss: 1.14588e+02
I1111 09:32:04.419767 140264174335808 run_lib.py:153] step: 217100, training_loss: 1.08900e+02
I1111 09:32:14.385348 140264174335808 run_lib.py:153] step: 217150, training_loss: 1.23726e+02
I1111 09:32:24.677842 140264174335808 run_lib.py:153] step: 217200, training_loss: 1.63256e+02
I1111 09:32:34.952525 140264174335808 run_lib.py:153] step: 217250, training_loss: 9.92236e+01
I1111 09:32:44.897598 140264174335808 run_lib.py:153] step: 217300, training_loss: 1.46563e+02
I1111 09:32:55.254777 140264174335808 run_lib.py:153] step: 217350, training_loss: 1.28031e+02
I1111 09:33:05.406607 140264174335808 run_lib.py:153] step: 217400, training_loss: 1.05013e+02
I1111 09:33:15.106931 140264174335808 run_lib.py:153] step: 217450, training_loss: 1.58582e+02
I1111 09:33:25.128920 140264174335808 run_lib.py:153] step: 217500, training_loss: 1.27639e+02
I1111 09:33:35.156557 140264174335808 run_lib.py:153] step: 217550, training_loss: 1.26828e+02
I1111 09:33:45.829607 140264174335808 run_lib.py:153] step: 217600, training_loss: 1.45116e+02
I1111 09:33:56.007043 140264174335808 run_lib.py:153] step: 217650, training_loss: 1.77977e+02
I1111 09:34:05.479235 140264174335808 run_lib.py:153] step: 217700, training_loss: 1.61007e+02
I1111 09:34:14.997314 140264174335808 run_lib.py:153] step: 217750, training_loss: 1.22789e+02
I1111 09:34:24.989168 140264174335808 run_lib.py:153] step: 217800, training_loss: 1.10110e+02
I1111 09:34:35.573608 140264174335808 run_lib.py:153] step: 217850, training_loss: 1.34758e+02
I1111 09:34:45.265163 140264174335808 run_lib.py:153] step: 217900, training_loss: 1.15547e+02
I1111 09:34:55.380182 140264174335808 run_lib.py:153] step: 217950, training_loss: 1.27546e+02
I1111 09:35:05.140926 140264174335808 run_lib.py:153] step: 218000, training_loss: 1.24026e+02
I1111 09:35:15.038715 140264174335808 run_lib.py:153] step: 218050, training_loss: 1.19903e+02
I1111 09:35:25.276106 140264174335808 run_lib.py:153] step: 218100, training_loss: 1.23516e+02
I1111 09:35:34.551892 140264174335808 run_lib.py:153] step: 218150, training_loss: 1.12047e+02
I1111 09:35:44.110605 140264174335808 run_lib.py:153] step: 218200, training_loss: 1.42734e+02
I1111 09:35:53.385094 140264174335808 run_lib.py:153] step: 218250, training_loss: 9.92880e+01
I1111 09:36:03.410122 140264174335808 run_lib.py:153] step: 218300, training_loss: 1.10050e+02
I1111 09:36:13.730218 140264174335808 run_lib.py:153] step: 218350, training_loss: 1.30080e+02
I1111 09:36:24.524661 140264174335808 run_lib.py:153] step: 218400, training_loss: 1.16189e+02
I1111 09:36:35.155778 140264174335808 run_lib.py:153] step: 218450, training_loss: 1.20743e+02
I1111 09:36:45.295250 140264174335808 run_lib.py:153] step: 218500, training_loss: 1.13797e+02
I1111 09:36:55.462062 140264174335808 run_lib.py:153] step: 218550, training_loss: 1.22635e+02
I1111 09:37:05.877353 140264174335808 run_lib.py:153] step: 218600, training_loss: 1.10459e+02
I1111 09:37:16.165954 140264174335808 run_lib.py:153] step: 218650, training_loss: 1.54477e+02
I1111 09:37:26.102573 140264174335808 run_lib.py:153] step: 218700, training_loss: 1.57372e+02
I1111 09:37:35.756740 140264174335808 run_lib.py:153] step: 218750, training_loss: 1.04253e+02
I1111 09:37:45.141902 140264174335808 run_lib.py:153] step: 218800, training_loss: 1.15454e+02
I1111 09:37:55.270286 140264174335808 run_lib.py:153] step: 218850, training_loss: 1.14641e+02
I1111 09:38:05.030167 140264174335808 run_lib.py:153] step: 218900, training_loss: 1.16228e+02
I1111 09:38:14.312430 140264174335808 run_lib.py:153] step: 218950, training_loss: 1.26415e+02
I1111 09:38:24.346113 140264174335808 run_lib.py:153] step: 219000, training_loss: 1.30974e+02
I1111 09:38:34.795897 140264174335808 run_lib.py:153] step: 219050, training_loss: 1.38149e+02
I1111 09:38:44.865676 140264174335808 run_lib.py:153] step: 219100, training_loss: 1.18426e+02
I1111 09:38:55.521958 140264174335808 run_lib.py:153] step: 219150, training_loss: 1.48283e+02
I1111 09:39:05.565218 140264174335808 run_lib.py:153] step: 219200, training_loss: 1.20901e+02
I1111 09:39:14.922236 140264174335808 run_lib.py:153] step: 219250, training_loss: 1.28401e+02
I1111 09:39:24.358894 140264174335808 run_lib.py:153] step: 219300, training_loss: 1.57269e+02
I1111 09:39:33.674309 140264174335808 run_lib.py:153] step: 219350, training_loss: 1.16236e+02
I1111 09:39:44.630046 140264174335808 run_lib.py:153] step: 219400, training_loss: 1.30970e+02
I1111 09:39:54.706350 140264174335808 run_lib.py:153] step: 219450, training_loss: 1.52718e+02
I1111 09:40:05.316460 140264174335808 run_lib.py:153] step: 219500, training_loss: 1.29686e+02
I1111 09:40:14.734458 140264174335808 run_lib.py:153] step: 219550, training_loss: 1.05341e+02
I1111 09:40:24.626023 140264174335808 run_lib.py:153] step: 219600, training_loss: 1.28488e+02
I1111 09:40:35.136208 140264174335808 run_lib.py:153] step: 219650, training_loss: 1.09232e+02
I1111 09:40:45.558650 140264174335808 run_lib.py:153] step: 219700, training_loss: 1.42882e+02
I1111 09:40:56.217676 140264174335808 run_lib.py:153] step: 219750, training_loss: 1.29014e+02
I1111 09:41:06.287808 140264174335808 run_lib.py:153] step: 219800, training_loss: 1.12547e+02
I1111 09:41:16.649874 140264174335808 run_lib.py:153] step: 219850, training_loss: 1.14010e+02
I1111 09:41:27.494619 140264174335808 run_lib.py:153] step: 219900, training_loss: 1.43405e+02
I1111 09:41:37.795525 140264174335808 run_lib.py:153] step: 219950, training_loss: 1.04740e+02
I1111 09:41:47.285130 140264174335808 run_lib.py:153] step: 220000, training_loss: 1.25962e+02
I1111 09:41:47.865916 140264174335808 run_lib.py:166] step: 220000, eval_loss: 1.24972e+02
I1111 09:41:58.188381 140264174335808 run_lib.py:153] step: 220050, training_loss: 1.21570e+02
I1111 09:42:08.326610 140264174335808 run_lib.py:153] step: 220100, training_loss: 9.34798e+01
I1111 09:42:18.564311 140264174335808 run_lib.py:153] step: 220150, training_loss: 1.33703e+02
I1111 09:42:28.413947 140264174335808 run_lib.py:153] step: 220200, training_loss: 1.27341e+02
I1111 09:42:38.079738 140264174335808 run_lib.py:153] step: 220250, training_loss: 9.69473e+01
I1111 09:42:48.374752 140264174335808 run_lib.py:153] step: 220300, training_loss: 1.42384e+02
I1111 09:42:58.190707 140264174335808 run_lib.py:153] step: 220350, training_loss: 1.39830e+02
I1111 09:43:07.738260 140264174335808 run_lib.py:153] step: 220400, training_loss: 1.30431e+02
I1111 09:43:17.250278 140264174335808 run_lib.py:153] step: 220450, training_loss: 1.33320e+02
I1111 09:43:27.107791 140264174335808 run_lib.py:153] step: 220500, training_loss: 1.19517e+02
I1111 09:43:37.581701 140264174335808 run_lib.py:153] step: 220550, training_loss: 1.27487e+02
I1111 09:43:47.785509 140264174335808 run_lib.py:153] step: 220600, training_loss: 1.16690e+02
I1111 09:43:58.116362 140264174335808 run_lib.py:153] step: 220650, training_loss: 1.28161e+02
I1111 09:44:07.808466 140264174335808 run_lib.py:153] step: 220700, training_loss: 1.08662e+02
I1111 09:44:18.324406 140264174335808 run_lib.py:153] step: 220750, training_loss: 1.52574e+02
I1111 09:44:28.338249 140264174335808 run_lib.py:153] step: 220800, training_loss: 1.20314e+02
I1111 09:44:38.610506 140264174335808 run_lib.py:153] step: 220850, training_loss: 1.45010e+02
I1111 09:44:47.922107 140264174335808 run_lib.py:153] step: 220900, training_loss: 1.37065e+02
I1111 09:44:58.499015 140264174335808 run_lib.py:153] step: 220950, training_loss: 1.17939e+02
I1111 09:45:08.711061 140264174335808 run_lib.py:153] step: 221000, training_loss: 1.49837e+02
I1111 09:45:19.029533 140264174335808 run_lib.py:153] step: 221050, training_loss: 1.13280e+02
I1111 09:45:28.838454 140264174335808 run_lib.py:153] step: 221100, training_loss: 1.14088e+02
I1111 09:45:39.127549 140264174335808 run_lib.py:153] step: 221150, training_loss: 1.48376e+02
I1111 09:45:48.380269 140264174335808 run_lib.py:153] step: 221200, training_loss: 1.38110e+02
I1111 09:45:58.686522 140264174335808 run_lib.py:153] step: 221250, training_loss: 1.21039e+02
I1111 09:46:08.632538 140264174335808 run_lib.py:153] step: 221300, training_loss: 1.11992e+02
I1111 09:46:18.828044 140264174335808 run_lib.py:153] step: 221350, training_loss: 1.54728e+02
I1111 09:46:28.731958 140264174335808 run_lib.py:153] step: 221400, training_loss: 1.39468e+02
I1111 09:46:38.184731 140264174335808 run_lib.py:153] step: 221450, training_loss: 1.31623e+02
I1111 09:46:48.226046 140264174335808 run_lib.py:153] step: 221500, training_loss: 1.30015e+02
I1111 09:46:58.350475 140264174335808 run_lib.py:153] step: 221550, training_loss: 1.02640e+02
I1111 09:47:08.169062 140264174335808 run_lib.py:153] step: 221600, training_loss: 1.21193e+02
I1111 09:47:18.051373 140264174335808 run_lib.py:153] step: 221650, training_loss: 1.20757e+02
I1111 09:47:28.104327 140264174335808 run_lib.py:153] step: 221700, training_loss: 1.04576e+02
I1111 09:47:37.755402 140264174335808 run_lib.py:153] step: 221750, training_loss: 1.48602e+02
I1111 09:47:47.460674 140264174335808 run_lib.py:153] step: 221800, training_loss: 1.16090e+02
I1111 09:47:57.409747 140264174335808 run_lib.py:153] step: 221850, training_loss: 1.68959e+02
I1111 09:48:06.814468 140264174335808 run_lib.py:153] step: 221900, training_loss: 8.87195e+01
I1111 09:48:17.128272 140264174335808 run_lib.py:153] step: 221950, training_loss: 1.03928e+02
I1111 09:48:27.063826 140264174335808 run_lib.py:153] step: 222000, training_loss: 1.29900e+02
I1111 09:48:36.944454 140264174335808 run_lib.py:153] step: 222050, training_loss: 1.38530e+02
I1111 09:48:46.709074 140264174335808 run_lib.py:153] step: 222100, training_loss: 9.45136e+01
I1111 09:48:57.077768 140264174335808 run_lib.py:153] step: 222150, training_loss: 1.12026e+02
I1111 09:49:06.738541 140264174335808 run_lib.py:153] step: 222200, training_loss: 1.43393e+02
I1111 09:49:17.076096 140264174335808 run_lib.py:153] step: 222250, training_loss: 1.37524e+02
I1111 09:49:27.351615 140264174335808 run_lib.py:153] step: 222300, training_loss: 1.41792e+02
I1111 09:49:36.895624 140264174335808 run_lib.py:153] step: 222350, training_loss: 1.05106e+02
I1111 09:49:46.836350 140264174335808 run_lib.py:153] step: 222400, training_loss: 1.23355e+02
I1111 09:49:56.930990 140264174335808 run_lib.py:153] step: 222450, training_loss: 1.26515e+02
I1111 09:50:07.041286 140264174335808 run_lib.py:153] step: 222500, training_loss: 1.11758e+02
I1111 09:50:17.371476 140264174335808 run_lib.py:153] step: 222550, training_loss: 1.68721e+02
I1111 09:50:27.918243 140264174335808 run_lib.py:153] step: 222600, training_loss: 1.32227e+02
I1111 09:50:37.808572 140264174335808 run_lib.py:153] step: 222650, training_loss: 1.17012e+02
I1111 09:50:48.192612 140264174335808 run_lib.py:153] step: 222700, training_loss: 1.13826e+02
I1111 09:50:58.259661 140264174335808 run_lib.py:153] step: 222750, training_loss: 1.29243e+02
I1111 09:51:07.988147 140264174335808 run_lib.py:153] step: 222800, training_loss: 1.42287e+02
I1111 09:51:17.704627 140264174335808 run_lib.py:153] step: 222850, training_loss: 1.41047e+02
I1111 09:51:27.252668 140264174335808 run_lib.py:153] step: 222900, training_loss: 1.14883e+02
I1111 09:51:36.870642 140264174335808 run_lib.py:153] step: 222950, training_loss: 1.16093e+02
I1111 09:51:46.706043 140264174335808 run_lib.py:153] step: 223000, training_loss: 1.35384e+02
I1111 09:51:56.005790 140264174335808 run_lib.py:153] step: 223050, training_loss: 1.25367e+02
I1111 09:52:05.673173 140264174335808 run_lib.py:153] step: 223100, training_loss: 1.07086e+02
I1111 09:52:15.367253 140264174335808 run_lib.py:153] step: 223150, training_loss: 1.45121e+02
I1111 09:52:25.073822 140264174335808 run_lib.py:153] step: 223200, training_loss: 1.38172e+02
I1111 09:52:35.082133 140264174335808 run_lib.py:153] step: 223250, training_loss: 1.07167e+02
I1111 09:52:45.385348 140264174335808 run_lib.py:153] step: 223300, training_loss: 1.16950e+02
I1111 09:52:55.180231 140264174335808 run_lib.py:153] step: 223350, training_loss: 1.06025e+02
I1111 09:53:04.800845 140264174335808 run_lib.py:153] step: 223400, training_loss: 1.41462e+02
I1111 09:53:14.945513 140264174335808 run_lib.py:153] step: 223450, training_loss: 1.31742e+02
I1111 09:53:24.775097 140264174335808 run_lib.py:153] step: 223500, training_loss: 1.46995e+02
I1111 09:53:35.089453 140264174335808 run_lib.py:153] step: 223550, training_loss: 1.13957e+02
I1111 09:53:44.901137 140264174335808 run_lib.py:153] step: 223600, training_loss: 1.37478e+02
I1111 09:53:55.358691 140264174335808 run_lib.py:153] step: 223650, training_loss: 1.05149e+02
I1111 09:54:04.929888 140264174335808 run_lib.py:153] step: 223700, training_loss: 1.37728e+02
I1111 09:54:14.537344 140264174335808 run_lib.py:153] step: 223750, training_loss: 1.11560e+02
I1111 09:54:24.739817 140264174335808 run_lib.py:153] step: 223800, training_loss: 1.34994e+02
I1111 09:54:35.106999 140264174335808 run_lib.py:153] step: 223850, training_loss: 1.44370e+02
I1111 09:54:45.126807 140264174335808 run_lib.py:153] step: 223900, training_loss: 1.26769e+02
I1111 09:54:55.830981 140264174335808 run_lib.py:153] step: 223950, training_loss: 1.30177e+02
I1111 09:55:05.963862 140264174335808 run_lib.py:153] step: 224000, training_loss: 1.52835e+02
I1111 09:55:15.895281 140264174335808 run_lib.py:153] step: 224050, training_loss: 1.45157e+02
I1111 09:55:25.358269 140264174335808 run_lib.py:153] step: 224100, training_loss: 1.21638e+02
I1111 09:55:36.322974 140264174335808 run_lib.py:153] step: 224150, training_loss: 1.34484e+02
I1111 09:55:47.269273 140264174335808 run_lib.py:153] step: 224200, training_loss: 1.03942e+02
I1111 09:55:56.725364 140264174335808 run_lib.py:153] step: 224250, training_loss: 1.30835e+02
I1111 09:56:06.887022 140264174335808 run_lib.py:153] step: 224300, training_loss: 1.24397e+02
I1111 09:56:17.214431 140264174335808 run_lib.py:153] step: 224350, training_loss: 1.02105e+02
I1111 09:56:27.641978 140264174335808 run_lib.py:153] step: 224400, training_loss: 1.13920e+02
I1111 09:56:37.966415 140264174335808 run_lib.py:153] step: 224450, training_loss: 1.26628e+02
I1111 09:56:47.888561 140264174335808 run_lib.py:153] step: 224500, training_loss: 1.25823e+02
I1111 09:56:58.688404 140264174335808 run_lib.py:153] step: 224550, training_loss: 1.46627e+02
I1111 09:57:08.691929 140264174335808 run_lib.py:153] step: 224600, training_loss: 1.26522e+02
I1111 09:57:18.053647 140264174335808 run_lib.py:153] step: 224650, training_loss: 1.03658e+02
I1111 09:57:27.704384 140264174335808 run_lib.py:153] step: 224700, training_loss: 1.39348e+02
I1111 09:57:37.824156 140264174335808 run_lib.py:153] step: 224750, training_loss: 1.39018e+02
I1111 09:57:47.621976 140264174335808 run_lib.py:153] step: 224800, training_loss: 1.16195e+02
I1111 09:57:57.075998 140264174335808 run_lib.py:153] step: 224850, training_loss: 1.53212e+02
I1111 09:58:07.350921 140264174335808 run_lib.py:153] step: 224900, training_loss: 1.38489e+02
I1111 09:58:17.471629 140264174335808 run_lib.py:153] step: 224950, training_loss: 1.23085e+02
I1111 09:58:27.096268 140264174335808 run_lib.py:153] step: 225000, training_loss: 1.03994e+02
I1111 09:58:27.199657 140264174335808 run_lib.py:166] step: 225000, eval_loss: 1.18447e+02
I1111 09:58:36.431940 140264174335808 run_lib.py:153] step: 225050, training_loss: 1.28673e+02
I1111 09:58:46.016345 140264174335808 run_lib.py:153] step: 225100, training_loss: 1.05420e+02
I1111 09:58:56.264742 140264174335808 run_lib.py:153] step: 225150, training_loss: 1.37601e+02
I1111 09:59:05.903081 140264174335808 run_lib.py:153] step: 225200, training_loss: 1.45041e+02
I1111 09:59:15.877945 140264174335808 run_lib.py:153] step: 225250, training_loss: 1.31290e+02
I1111 09:59:25.882140 140264174335808 run_lib.py:153] step: 225300, training_loss: 1.17546e+02
I1111 09:59:36.470543 140264174335808 run_lib.py:153] step: 225350, training_loss: 1.09748e+02
I1111 09:59:46.952148 140264174335808 run_lib.py:153] step: 225400, training_loss: 1.25917e+02
I1111 09:59:56.939866 140264174335808 run_lib.py:153] step: 225450, training_loss: 1.15270e+02
I1111 10:00:06.616714 140264174335808 run_lib.py:153] step: 225500, training_loss: 1.45224e+02
I1111 10:00:16.705193 140264174335808 run_lib.py:153] step: 225550, training_loss: 1.35524e+02
I1111 10:00:26.689216 140264174335808 run_lib.py:153] step: 225600, training_loss: 1.10596e+02
I1111 10:00:36.837736 140264174335808 run_lib.py:153] step: 225650, training_loss: 1.16053e+02
I1111 10:00:47.235990 140264174335808 run_lib.py:153] step: 225700, training_loss: 1.59483e+02
I1111 10:00:57.098776 140264174335808 run_lib.py:153] step: 225750, training_loss: 1.49632e+02
I1111 10:01:07.096178 140264174335808 run_lib.py:153] step: 225800, training_loss: 1.09602e+02
I1111 10:01:17.208777 140264174335808 run_lib.py:153] step: 225850, training_loss: 1.15865e+02
I1111 10:01:27.337435 140264174335808 run_lib.py:153] step: 225900, training_loss: 9.39173e+01
I1111 10:01:36.885542 140264174335808 run_lib.py:153] step: 225950, training_loss: 1.20728e+02
I1111 10:01:47.748162 140264174335808 run_lib.py:153] step: 226000, training_loss: 1.34201e+02
I1111 10:01:57.864281 140264174335808 run_lib.py:153] step: 226050, training_loss: 1.45298e+02
I1111 10:02:08.316386 140264174335808 run_lib.py:153] step: 226100, training_loss: 1.24787e+02
I1111 10:02:17.538559 140264174335808 run_lib.py:153] step: 226150, training_loss: 1.34086e+02
I1111 10:02:26.757663 140264174335808 run_lib.py:153] step: 226200, training_loss: 1.21757e+02
I1111 10:02:36.793128 140264174335808 run_lib.py:153] step: 226250, training_loss: 1.26548e+02
I1111 10:02:46.334355 140264174335808 run_lib.py:153] step: 226300, training_loss: 1.35914e+02
I1111 10:02:56.715393 140264174335808 run_lib.py:153] step: 226350, training_loss: 1.07710e+02
I1111 10:03:06.926007 140264174335808 run_lib.py:153] step: 226400, training_loss: 1.15333e+02
I1111 10:03:17.772615 140264174335808 run_lib.py:153] step: 226450, training_loss: 1.26271e+02
I1111 10:03:27.763855 140264174335808 run_lib.py:153] step: 226500, training_loss: 1.42218e+02
I1111 10:03:37.390295 140264174335808 run_lib.py:153] step: 226550, training_loss: 1.29277e+02
I1111 10:03:47.382350 140264174335808 run_lib.py:153] step: 226600, training_loss: 1.16249e+02
I1111 10:03:57.220035 140264174335808 run_lib.py:153] step: 226650, training_loss: 1.36143e+02
I1111 10:04:07.747226 140264174335808 run_lib.py:153] step: 226700, training_loss: 1.19982e+02
I1111 10:04:17.309465 140264174335808 run_lib.py:153] step: 226750, training_loss: 1.50231e+02
I1111 10:04:28.143958 140264174335808 run_lib.py:153] step: 226800, training_loss: 1.41351e+02
I1111 10:04:38.536725 140264174335808 run_lib.py:153] step: 226850, training_loss: 1.14497e+02
I1111 10:04:48.483319 140264174335808 run_lib.py:153] step: 226900, training_loss: 1.10477e+02
I1111 10:04:58.550812 140264174335808 run_lib.py:153] step: 226950, training_loss: 1.10947e+02
I1111 10:05:08.627423 140264174335808 run_lib.py:153] step: 227000, training_loss: 1.10734e+02
I1111 10:05:18.768918 140264174335808 run_lib.py:153] step: 227050, training_loss: 1.36672e+02
I1111 10:05:28.421020 140264174335808 run_lib.py:153] step: 227100, training_loss: 1.37757e+02
I1111 10:05:38.243102 140264174335808 run_lib.py:153] step: 227150, training_loss: 1.47289e+02
I1111 10:05:48.788565 140264174335808 run_lib.py:153] step: 227200, training_loss: 1.45131e+02
I1111 10:05:58.768639 140264174335808 run_lib.py:153] step: 227250, training_loss: 1.33101e+02
I1111 10:06:08.545842 140264174335808 run_lib.py:153] step: 227300, training_loss: 1.73713e+02
I1111 10:06:18.632873 140264174335808 run_lib.py:153] step: 227350, training_loss: 1.24888e+02
I1111 10:06:29.205651 140264174335808 run_lib.py:153] step: 227400, training_loss: 1.18622e+02
I1111 10:06:39.190230 140264174335808 run_lib.py:153] step: 227450, training_loss: 1.35110e+02
I1111 10:06:48.973560 140264174335808 run_lib.py:153] step: 227500, training_loss: 1.47608e+02
I1111 10:06:58.753099 140264174335808 run_lib.py:153] step: 227550, training_loss: 1.30207e+02
I1111 10:07:08.668097 140264174335808 run_lib.py:153] step: 227600, training_loss: 1.06777e+02
I1111 10:07:18.888200 140264174335808 run_lib.py:153] step: 227650, training_loss: 1.27486e+02
I1111 10:07:29.114728 140264174335808 run_lib.py:153] step: 227700, training_loss: 1.26702e+02
I1111 10:07:39.784113 140264174335808 run_lib.py:153] step: 227750, training_loss: 1.31510e+02
I1111 10:07:49.684749 140264174335808 run_lib.py:153] step: 227800, training_loss: 1.27765e+02
I1111 10:07:59.485025 140264174335808 run_lib.py:153] step: 227850, training_loss: 1.15464e+02
I1111 10:08:10.059980 140264174335808 run_lib.py:153] step: 227900, training_loss: 1.11062e+02
I1111 10:08:20.776355 140264174335808 run_lib.py:153] step: 227950, training_loss: 1.10912e+02
I1111 10:08:30.535084 140264174335808 run_lib.py:153] step: 228000, training_loss: 1.50431e+02
I1111 10:08:40.433883 140264174335808 run_lib.py:153] step: 228050, training_loss: 1.57471e+02
I1111 10:08:50.399938 140264174335808 run_lib.py:153] step: 228100, training_loss: 1.25044e+02
I1111 10:09:00.758075 140264174335808 run_lib.py:153] step: 228150, training_loss: 1.35136e+02
I1111 10:09:11.104112 140264174335808 run_lib.py:153] step: 228200, training_loss: 1.31012e+02
I1111 10:09:21.234063 140264174335808 run_lib.py:153] step: 228250, training_loss: 1.24703e+02
I1111 10:09:30.733258 140264174335808 run_lib.py:153] step: 228300, training_loss: 1.40782e+02
I1111 10:09:40.717935 140264174335808 run_lib.py:153] step: 228350, training_loss: 1.28017e+02
I1111 10:09:50.307497 140264174335808 run_lib.py:153] step: 228400, training_loss: 1.25746e+02
I1111 10:10:00.077827 140264174335808 run_lib.py:153] step: 228450, training_loss: 1.31657e+02
I1111 10:10:09.867947 140264174335808 run_lib.py:153] step: 228500, training_loss: 1.27670e+02
I1111 10:10:20.318218 140264174335808 run_lib.py:153] step: 228550, training_loss: 1.37007e+02
I1111 10:10:30.173373 140264174335808 run_lib.py:153] step: 228600, training_loss: 1.52913e+02
I1111 10:10:40.417514 140264174335808 run_lib.py:153] step: 228650, training_loss: 1.13202e+02
I1111 10:10:50.831994 140264174335808 run_lib.py:153] step: 228700, training_loss: 1.39527e+02
I1111 10:11:01.106268 140264174335808 run_lib.py:153] step: 228750, training_loss: 1.22692e+02
I1111 10:11:10.809453 140264174335808 run_lib.py:153] step: 228800, training_loss: 1.26944e+02
I1111 10:11:20.922243 140264174335808 run_lib.py:153] step: 228850, training_loss: 1.13849e+02
I1111 10:11:30.827514 140264174335808 run_lib.py:153] step: 228900, training_loss: 1.37644e+02
I1111 10:11:41.511017 140264174335808 run_lib.py:153] step: 228950, training_loss: 1.50354e+02
I1111 10:11:51.774184 140264174335808 run_lib.py:153] step: 229000, training_loss: 1.27323e+02
I1111 10:12:01.376911 140264174335808 run_lib.py:153] step: 229050, training_loss: 1.03708e+02
I1111 10:12:11.389955 140264174335808 run_lib.py:153] step: 229100, training_loss: 1.16201e+02
I1111 10:12:21.477040 140264174335808 run_lib.py:153] step: 229150, training_loss: 1.42699e+02
I1111 10:12:31.449340 140264174335808 run_lib.py:153] step: 229200, training_loss: 1.36669e+02
I1111 10:12:42.247591 140264174335808 run_lib.py:153] step: 229250, training_loss: 1.22073e+02
I1111 10:12:52.680768 140264174335808 run_lib.py:153] step: 229300, training_loss: 1.33567e+02
I1111 10:13:03.092609 140264174335808 run_lib.py:153] step: 229350, training_loss: 1.12313e+02
I1111 10:13:12.801877 140264174335808 run_lib.py:153] step: 229400, training_loss: 1.25824e+02
I1111 10:13:22.718545 140264174335808 run_lib.py:153] step: 229450, training_loss: 1.40794e+02
I1111 10:13:32.656450 140264174335808 run_lib.py:153] step: 229500, training_loss: 1.41522e+02
I1111 10:13:42.840957 140264174335808 run_lib.py:153] step: 229550, training_loss: 1.11744e+02
I1111 10:13:53.081638 140264174335808 run_lib.py:153] step: 229600, training_loss: 1.40741e+02
I1111 10:14:03.790690 140264174335808 run_lib.py:153] step: 229650, training_loss: 1.31003e+02
I1111 10:14:13.692805 140264174335808 run_lib.py:153] step: 229700, training_loss: 9.75801e+01
I1111 10:14:23.931538 140264174335808 run_lib.py:153] step: 229750, training_loss: 1.13885e+02
I1111 10:14:33.805299 140264174335808 run_lib.py:153] step: 229800, training_loss: 1.38646e+02
I1111 10:14:43.154762 140264174335808 run_lib.py:153] step: 229850, training_loss: 1.42548e+02
I1111 10:14:52.534307 140264174335808 run_lib.py:153] step: 229900, training_loss: 1.44907e+02
I1111 10:15:02.060668 140264174335808 run_lib.py:153] step: 229950, training_loss: 1.26650e+02
I1111 10:15:11.744807 140264174335808 run_lib.py:153] step: 230000, training_loss: 1.12964e+02
I1111 10:15:12.285761 140264174335808 run_lib.py:166] step: 230000, eval_loss: 1.17732e+02
I1111 10:15:21.948599 140264174335808 run_lib.py:153] step: 230050, training_loss: 1.16881e+02
I1111 10:15:32.183332 140264174335808 run_lib.py:153] step: 230100, training_loss: 1.30235e+02
I1111 10:15:42.451907 140264174335808 run_lib.py:153] step: 230150, training_loss: 1.39906e+02
I1111 10:15:52.046898 140264174335808 run_lib.py:153] step: 230200, training_loss: 1.06461e+02
I1111 10:16:02.855234 140264174335808 run_lib.py:153] step: 230250, training_loss: 1.12356e+02
I1111 10:16:13.354252 140264174335808 run_lib.py:153] step: 230300, training_loss: 1.21880e+02
I1111 10:16:23.782258 140264174335808 run_lib.py:153] step: 230350, training_loss: 1.31248e+02
I1111 10:16:33.817875 140264174335808 run_lib.py:153] step: 230400, training_loss: 1.55018e+02
I1111 10:16:43.896839 140264174335808 run_lib.py:153] step: 230450, training_loss: 1.26411e+02
I1111 10:16:53.452085 140264174335808 run_lib.py:153] step: 230500, training_loss: 1.29279e+02
I1111 10:17:02.997105 140264174335808 run_lib.py:153] step: 230550, training_loss: 1.16931e+02
I1111 10:17:13.400899 140264174335808 run_lib.py:153] step: 230600, training_loss: 1.38441e+02
I1111 10:17:23.997343 140264174335808 run_lib.py:153] step: 230650, training_loss: 1.48993e+02
I1111 10:17:34.262126 140264174335808 run_lib.py:153] step: 230700, training_loss: 1.47195e+02
I1111 10:17:44.407490 140264174335808 run_lib.py:153] step: 230750, training_loss: 1.04047e+02
I1111 10:17:54.229228 140264174335808 run_lib.py:153] step: 230800, training_loss: 1.28827e+02
I1111 10:18:04.376921 140264174335808 run_lib.py:153] step: 230850, training_loss: 1.60741e+02
I1111 10:18:13.966493 140264174335808 run_lib.py:153] step: 230900, training_loss: 1.04947e+02
I1111 10:18:24.741405 140264174335808 run_lib.py:153] step: 230950, training_loss: 1.08389e+02
I1111 10:18:34.632532 140264174335808 run_lib.py:153] step: 231000, training_loss: 1.39279e+02
I1111 10:18:44.767428 140264174335808 run_lib.py:153] step: 231050, training_loss: 1.36520e+02
I1111 10:18:56.058681 140264174335808 run_lib.py:153] step: 231100, training_loss: 1.30595e+02
I1111 10:19:05.965842 140264174335808 run_lib.py:153] step: 231150, training_loss: 1.30546e+02
I1111 10:19:15.800732 140264174335808 run_lib.py:153] step: 231200, training_loss: 1.32546e+02
I1111 10:19:25.524274 140264174335808 run_lib.py:153] step: 231250, training_loss: 1.41923e+02
I1111 10:19:35.812353 140264174335808 run_lib.py:153] step: 231300, training_loss: 1.21307e+02
I1111 10:19:45.417800 140264174335808 run_lib.py:153] step: 231350, training_loss: 1.29718e+02
I1111 10:19:55.937330 140264174335808 run_lib.py:153] step: 231400, training_loss: 1.30922e+02
I1111 10:20:05.824857 140264174335808 run_lib.py:153] step: 231450, training_loss: 1.13616e+02
I1111 10:20:15.785560 140264174335808 run_lib.py:153] step: 231500, training_loss: 1.23381e+02
I1111 10:20:25.360507 140264174335808 run_lib.py:153] step: 231550, training_loss: 1.43678e+02
I1111 10:20:35.349271 140264174335808 run_lib.py:153] step: 231600, training_loss: 1.13069e+02
I1111 10:20:44.986384 140264174335808 run_lib.py:153] step: 231650, training_loss: 1.63705e+02
I1111 10:20:54.862213 140264174335808 run_lib.py:153] step: 231700, training_loss: 1.71355e+02
I1111 10:21:04.107784 140264174335808 run_lib.py:153] step: 231750, training_loss: 1.22670e+02
I1111 10:21:14.187715 140264174335808 run_lib.py:153] step: 231800, training_loss: 1.41001e+02
I1111 10:21:24.318763 140264174335808 run_lib.py:153] step: 231850, training_loss: 1.21760e+02
I1111 10:21:33.811007 140264174335808 run_lib.py:153] step: 231900, training_loss: 1.05791e+02
I1111 10:21:43.656012 140264174335808 run_lib.py:153] step: 231950, training_loss: 1.27520e+02
I1111 10:21:53.761701 140264174335808 run_lib.py:153] step: 232000, training_loss: 1.29854e+02
I1111 10:22:03.426129 140264174335808 run_lib.py:153] step: 232050, training_loss: 1.33615e+02
I1111 10:22:13.413219 140264174335808 run_lib.py:153] step: 232100, training_loss: 1.24783e+02
I1111 10:22:23.863445 140264174335808 run_lib.py:153] step: 232150, training_loss: 1.28914e+02
I1111 10:22:34.134252 140264174335808 run_lib.py:153] step: 232200, training_loss: 1.35061e+02
I1111 10:22:44.484637 140264174335808 run_lib.py:153] step: 232250, training_loss: 1.09618e+02
I1111 10:22:54.488250 140264174335808 run_lib.py:153] step: 232300, training_loss: 1.36401e+02
I1111 10:23:04.626089 140264174335808 run_lib.py:153] step: 232350, training_loss: 1.16552e+02
I1111 10:23:14.544000 140264174335808 run_lib.py:153] step: 232400, training_loss: 1.18827e+02
I1111 10:23:24.810420 140264174335808 run_lib.py:153] step: 232450, training_loss: 1.01564e+02
I1111 10:23:34.504962 140264174335808 run_lib.py:153] step: 232500, training_loss: 1.54838e+02
I1111 10:23:43.831722 140264174335808 run_lib.py:153] step: 232550, training_loss: 1.18787e+02
I1111 10:23:54.085539 140264174335808 run_lib.py:153] step: 232600, training_loss: 1.20829e+02
I1111 10:24:03.795615 140264174335808 run_lib.py:153] step: 232650, training_loss: 1.60530e+02
I1111 10:24:14.270071 140264174335808 run_lib.py:153] step: 232700, training_loss: 1.36018e+02
I1111 10:24:24.497974 140264174335808 run_lib.py:153] step: 232750, training_loss: 1.52466e+02
I1111 10:24:34.299800 140264174335808 run_lib.py:153] step: 232800, training_loss: 1.39885e+02
I1111 10:24:44.520845 140264174335808 run_lib.py:153] step: 232850, training_loss: 1.38271e+02
I1111 10:24:54.169273 140264174335808 run_lib.py:153] step: 232900, training_loss: 1.18264e+02
I1111 10:25:04.809754 140264174335808 run_lib.py:153] step: 232950, training_loss: 1.67027e+02
I1111 10:25:14.424139 140264174335808 run_lib.py:153] step: 233000, training_loss: 1.41542e+02
I1111 10:25:24.838581 140264174335808 run_lib.py:153] step: 233050, training_loss: 1.20792e+02
I1111 10:25:34.690325 140264174335808 run_lib.py:153] step: 233100, training_loss: 1.02545e+02
I1111 10:25:44.887264 140264174335808 run_lib.py:153] step: 233150, training_loss: 1.47819e+02
I1111 10:25:54.558652 140264174335808 run_lib.py:153] step: 233200, training_loss: 1.26809e+02
I1111 10:26:04.192994 140264174335808 run_lib.py:153] step: 233250, training_loss: 1.13532e+02
I1111 10:26:13.509894 140264174335808 run_lib.py:153] step: 233300, training_loss: 1.60132e+02
I1111 10:26:23.079033 140264174335808 run_lib.py:153] step: 233350, training_loss: 1.32316e+02
I1111 10:26:32.513445 140264174335808 run_lib.py:153] step: 233400, training_loss: 1.18615e+02
I1111 10:26:42.554319 140264174335808 run_lib.py:153] step: 233450, training_loss: 1.22130e+02
I1111 10:26:52.317664 140264174335808 run_lib.py:153] step: 233500, training_loss: 1.26901e+02
I1111 10:27:02.960512 140264174335808 run_lib.py:153] step: 233550, training_loss: 1.10911e+02
I1111 10:27:12.885453 140264174335808 run_lib.py:153] step: 233600, training_loss: 1.18602e+02
I1111 10:27:22.573788 140264174335808 run_lib.py:153] step: 233650, training_loss: 1.34717e+02
I1111 10:27:32.084280 140264174335808 run_lib.py:153] step: 233700, training_loss: 1.10578e+02
I1111 10:27:41.335042 140264174335808 run_lib.py:153] step: 233750, training_loss: 1.43772e+02
I1111 10:27:50.608513 140264174335808 run_lib.py:153] step: 233800, training_loss: 1.34317e+02
I1111 10:28:01.065670 140264174335808 run_lib.py:153] step: 233850, training_loss: 1.28058e+02
I1111 10:28:11.119470 140264174335808 run_lib.py:153] step: 233900, training_loss: 1.37407e+02
I1111 10:28:20.852525 140264174335808 run_lib.py:153] step: 233950, training_loss: 1.21612e+02
I1111 10:28:30.290267 140264174335808 run_lib.py:153] step: 234000, training_loss: 1.49782e+02
I1111 10:28:40.090306 140264174335808 run_lib.py:153] step: 234050, training_loss: 1.10376e+02
I1111 10:28:50.023360 140264174335808 run_lib.py:153] step: 234100, training_loss: 1.28947e+02
I1111 10:29:00.104819 140264174335808 run_lib.py:153] step: 234150, training_loss: 1.17228e+02
I1111 10:29:09.914090 140264174335808 run_lib.py:153] step: 234200, training_loss: 1.38353e+02
I1111 10:29:20.383198 140264174335808 run_lib.py:153] step: 234250, training_loss: 1.00712e+02
I1111 10:29:30.983452 140264174335808 run_lib.py:153] step: 234300, training_loss: 1.50864e+02
I1111 10:29:40.213874 140264174335808 run_lib.py:153] step: 234350, training_loss: 9.69985e+01
I1111 10:29:49.803498 140264174335808 run_lib.py:153] step: 234400, training_loss: 1.52690e+02
I1111 10:29:59.888108 140264174335808 run_lib.py:153] step: 234450, training_loss: 9.48184e+01
I1111 10:30:09.689450 140264174335808 run_lib.py:153] step: 234500, training_loss: 1.47752e+02
I1111 10:30:20.407737 140264174335808 run_lib.py:153] step: 234550, training_loss: 1.10103e+02
I1111 10:30:30.183727 140264174335808 run_lib.py:153] step: 234600, training_loss: 1.22404e+02
I1111 10:30:40.400870 140264174335808 run_lib.py:153] step: 234650, training_loss: 1.40984e+02
I1111 10:30:50.417058 140264174335808 run_lib.py:153] step: 234700, training_loss: 1.20828e+02
I1111 10:31:00.326279 140264174335808 run_lib.py:153] step: 234750, training_loss: 9.94695e+01
I1111 10:31:10.458569 140264174335808 run_lib.py:153] step: 234800, training_loss: 1.22304e+02
I1111 10:31:20.251000 140264174335808 run_lib.py:153] step: 234850, training_loss: 1.22429e+02
I1111 10:31:29.604993 140264174335808 run_lib.py:153] step: 234900, training_loss: 1.29127e+02
I1111 10:31:39.526845 140264174335808 run_lib.py:153] step: 234950, training_loss: 1.14351e+02
I1111 10:31:49.163298 140264174335808 run_lib.py:153] step: 235000, training_loss: 1.37241e+02
I1111 10:31:49.300489 140264174335808 run_lib.py:166] step: 235000, eval_loss: 1.56427e+02
I1111 10:31:59.400028 140264174335808 run_lib.py:153] step: 235050, training_loss: 1.42041e+02
I1111 10:32:09.590934 140264174335808 run_lib.py:153] step: 235100, training_loss: 1.21905e+02
I1111 10:32:19.665475 140264174335808 run_lib.py:153] step: 235150, training_loss: 1.40944e+02
I1111 10:32:29.174959 140264174335808 run_lib.py:153] step: 235200, training_loss: 1.08604e+02
I1111 10:32:38.391793 140264174335808 run_lib.py:153] step: 235250, training_loss: 1.35060e+02
I1111 10:32:48.201172 140264174335808 run_lib.py:153] step: 235300, training_loss: 1.09522e+02
I1111 10:32:57.953835 140264174335808 run_lib.py:153] step: 235350, training_loss: 1.02587e+02
I1111 10:33:08.363897 140264174335808 run_lib.py:153] step: 235400, training_loss: 1.43099e+02
I1111 10:33:17.906610 140264174335808 run_lib.py:153] step: 235450, training_loss: 1.15322e+02
I1111 10:33:28.302901 140264174335808 run_lib.py:153] step: 235500, training_loss: 1.40110e+02
I1111 10:33:37.938337 140264174335808 run_lib.py:153] step: 235550, training_loss: 1.18162e+02
I1111 10:33:48.081034 140264174335808 run_lib.py:153] step: 235600, training_loss: 1.36646e+02
I1111 10:33:57.870726 140264174335808 run_lib.py:153] step: 235650, training_loss: 1.28388e+02
I1111 10:34:07.536256 140264174335808 run_lib.py:153] step: 235700, training_loss: 1.23986e+02
I1111 10:34:17.482558 140264174335808 run_lib.py:153] step: 235750, training_loss: 1.43218e+02
I1111 10:34:27.810312 140264174335808 run_lib.py:153] step: 235800, training_loss: 1.37147e+02
I1111 10:34:37.631750 140264174335808 run_lib.py:153] step: 235850, training_loss: 1.35055e+02
I1111 10:34:47.360156 140264174335808 run_lib.py:153] step: 235900, training_loss: 1.34380e+02
I1111 10:34:57.944219 140264174335808 run_lib.py:153] step: 235950, training_loss: 1.23674e+02
I1111 10:35:08.549212 140264174335808 run_lib.py:153] step: 236000, training_loss: 1.09330e+02
I1111 10:35:18.426346 140264174335808 run_lib.py:153] step: 236050, training_loss: 1.44654e+02
I1111 10:35:28.097720 140264174335808 run_lib.py:153] step: 236100, training_loss: 1.30780e+02
I1111 10:35:37.614523 140264174335808 run_lib.py:153] step: 236150, training_loss: 1.28699e+02
I1111 10:35:47.318068 140264174335808 run_lib.py:153] step: 236200, training_loss: 1.11553e+02
I1111 10:35:57.628340 140264174335808 run_lib.py:153] step: 236250, training_loss: 1.38935e+02
I1111 10:36:06.884127 140264174335808 run_lib.py:153] step: 236300, training_loss: 1.32754e+02
I1111 10:36:17.124861 140264174335808 run_lib.py:153] step: 236350, training_loss: 1.16857e+02
I1111 10:36:27.717359 140264174335808 run_lib.py:153] step: 236400, training_loss: 1.30803e+02
I1111 10:36:38.357035 140264174335808 run_lib.py:153] step: 236450, training_loss: 1.32680e+02
I1111 10:36:48.633890 140264174335808 run_lib.py:153] step: 236500, training_loss: 1.48008e+02
I1111 10:36:58.612018 140264174335808 run_lib.py:153] step: 236550, training_loss: 1.24043e+02
I1111 10:37:08.108554 140264174335808 run_lib.py:153] step: 236600, training_loss: 1.45228e+02
I1111 10:37:17.463319 140264174335808 run_lib.py:153] step: 236650, training_loss: 9.26410e+01
I1111 10:37:27.003125 140264174335808 run_lib.py:153] step: 236700, training_loss: 1.10690e+02
I1111 10:37:36.616434 140264174335808 run_lib.py:153] step: 236750, training_loss: 1.10946e+02
I1111 10:37:46.547997 140264174335808 run_lib.py:153] step: 236800, training_loss: 1.14532e+02
I1111 10:37:56.000245 140264174335808 run_lib.py:153] step: 236850, training_loss: 1.43819e+02
I1111 10:38:06.143932 140264174335808 run_lib.py:153] step: 236900, training_loss: 1.60422e+02
I1111 10:38:16.677948 140264174335808 run_lib.py:153] step: 236950, training_loss: 1.31741e+02
I1111 10:38:26.597074 140264174335808 run_lib.py:153] step: 237000, training_loss: 1.50022e+02
I1111 10:38:36.265738 140264174335808 run_lib.py:153] step: 237050, training_loss: 8.41346e+01
I1111 10:38:46.346469 140264174335808 run_lib.py:153] step: 237100, training_loss: 1.19892e+02
I1111 10:38:56.658470 140264174335808 run_lib.py:153] step: 237150, training_loss: 1.34043e+02
I1111 10:39:06.133908 140264174335808 run_lib.py:153] step: 237200, training_loss: 1.33210e+02
I1111 10:39:15.753726 140264174335808 run_lib.py:153] step: 237250, training_loss: 1.15217e+02
I1111 10:39:25.110499 140264174335808 run_lib.py:153] step: 237300, training_loss: 1.13703e+02
I1111 10:39:34.413581 140264174335808 run_lib.py:153] step: 237350, training_loss: 1.30616e+02
I1111 10:39:44.312868 140264174335808 run_lib.py:153] step: 237400, training_loss: 1.34630e+02
I1111 10:39:53.650469 140264174335808 run_lib.py:153] step: 237450, training_loss: 1.30456e+02
I1111 10:40:03.192078 140264174335808 run_lib.py:153] step: 237500, training_loss: 1.10257e+02
I1111 10:40:13.032993 140264174335808 run_lib.py:153] step: 237550, training_loss: 1.15150e+02
I1111 10:40:23.575367 140264174335808 run_lib.py:153] step: 237600, training_loss: 1.42226e+02
I1111 10:40:34.279808 140264174335808 run_lib.py:153] step: 237650, training_loss: 1.09358e+02
I1111 10:40:44.576815 140264174335808 run_lib.py:153] step: 237700, training_loss: 1.06819e+02
I1111 10:40:55.065039 140264174335808 run_lib.py:153] step: 237750, training_loss: 1.13699e+02
I1111 10:41:05.849481 140264174335808 run_lib.py:153] step: 237800, training_loss: 1.34800e+02
I1111 10:41:16.517077 140264174335808 run_lib.py:153] step: 237850, training_loss: 9.78570e+01
I1111 10:41:26.604219 140264174335808 run_lib.py:153] step: 237900, training_loss: 1.30215e+02
I1111 10:41:36.538519 140264174335808 run_lib.py:153] step: 237950, training_loss: 1.23493e+02
I1111 10:41:46.625334 140264174335808 run_lib.py:153] step: 238000, training_loss: 1.36759e+02
I1111 10:41:56.249508 140264174335808 run_lib.py:153] step: 238050, training_loss: 1.48959e+02
I1111 10:42:06.454303 140264174335808 run_lib.py:153] step: 238100, training_loss: 1.33386e+02
I1111 10:42:15.981596 140264174335808 run_lib.py:153] step: 238150, training_loss: 1.40277e+02
I1111 10:42:25.743060 140264174335808 run_lib.py:153] step: 238200, training_loss: 1.08481e+02
I1111 10:42:35.647614 140264174335808 run_lib.py:153] step: 238250, training_loss: 1.26298e+02
I1111 10:42:45.821453 140264174335808 run_lib.py:153] step: 238300, training_loss: 1.17442e+02
I1111 10:42:56.565362 140264174335808 run_lib.py:153] step: 238350, training_loss: 1.52929e+02
I1111 10:43:06.230476 140264174335808 run_lib.py:153] step: 238400, training_loss: 1.23036e+02
I1111 10:43:16.121385 140264174335808 run_lib.py:153] step: 238450, training_loss: 1.34329e+02
I1111 10:43:25.641880 140264174335808 run_lib.py:153] step: 238500, training_loss: 1.26395e+02
I1111 10:43:35.406851 140264174335808 run_lib.py:153] step: 238550, training_loss: 1.37097e+02
I1111 10:43:45.596016 140264174335808 run_lib.py:153] step: 238600, training_loss: 1.08953e+02
I1111 10:43:55.713828 140264174335808 run_lib.py:153] step: 238650, training_loss: 1.13203e+02
I1111 10:44:06.427516 140264174335808 run_lib.py:153] step: 238700, training_loss: 1.29967e+02
I1111 10:44:16.464670 140264174335808 run_lib.py:153] step: 238750, training_loss: 1.44427e+02
I1111 10:44:26.526471 140264174335808 run_lib.py:153] step: 238800, training_loss: 1.11527e+02
I1111 10:44:36.606281 140264174335808 run_lib.py:153] step: 238850, training_loss: 1.09840e+02
I1111 10:44:47.179393 140264174335808 run_lib.py:153] step: 238900, training_loss: 1.16993e+02
I1111 10:44:57.248057 140264174335808 run_lib.py:153] step: 238950, training_loss: 1.34024e+02
I1111 10:45:07.287698 140264174335808 run_lib.py:153] step: 239000, training_loss: 1.07465e+02
I1111 10:45:17.035495 140264174335808 run_lib.py:153] step: 239050, training_loss: 1.38721e+02
I1111 10:45:27.168134 140264174335808 run_lib.py:153] step: 239100, training_loss: 1.19186e+02
I1111 10:45:37.130571 140264174335808 run_lib.py:153] step: 239150, training_loss: 1.54475e+02
I1111 10:45:47.902615 140264174335808 run_lib.py:153] step: 239200, training_loss: 1.34208e+02
I1111 10:45:57.827261 140264174335808 run_lib.py:153] step: 239250, training_loss: 1.35305e+02
I1111 10:46:07.779034 140264174335808 run_lib.py:153] step: 239300, training_loss: 1.12705e+02
I1111 10:46:17.915328 140264174335808 run_lib.py:153] step: 239350, training_loss: 1.68918e+02
I1111 10:46:28.123850 140264174335808 run_lib.py:153] step: 239400, training_loss: 1.40510e+02
I1111 10:46:37.344109 140264174335808 run_lib.py:153] step: 239450, training_loss: 1.32011e+02
I1111 10:46:47.163784 140264174335808 run_lib.py:153] step: 239500, training_loss: 1.49695e+02
I1111 10:46:57.177074 140264174335808 run_lib.py:153] step: 239550, training_loss: 1.23316e+02
I1111 10:47:07.516060 140264174335808 run_lib.py:153] step: 239600, training_loss: 1.11179e+02
I1111 10:47:17.270037 140264174335808 run_lib.py:153] step: 239650, training_loss: 1.39318e+02
I1111 10:47:27.208362 140264174335808 run_lib.py:153] step: 239700, training_loss: 1.18340e+02
I1111 10:47:36.916558 140264174335808 run_lib.py:153] step: 239750, training_loss: 1.06406e+02
I1111 10:47:46.491390 140264174335808 run_lib.py:153] step: 239800, training_loss: 1.52941e+02
I1111 10:47:55.989738 140264174335808 run_lib.py:153] step: 239850, training_loss: 1.15343e+02
I1111 10:48:05.769047 140264174335808 run_lib.py:153] step: 239900, training_loss: 1.22394e+02
I1111 10:48:15.247400 140264174335808 run_lib.py:153] step: 239950, training_loss: 1.41234e+02
I1111 10:48:24.991287 140264174335808 run_lib.py:153] step: 240000, training_loss: 1.08864e+02
I1111 10:48:25.570422 140264174335808 run_lib.py:166] step: 240000, eval_loss: 1.29696e+02
I1111 10:48:35.674909 140264174335808 run_lib.py:153] step: 240050, training_loss: 1.34632e+02
I1111 10:48:45.770149 140264174335808 run_lib.py:153] step: 240100, training_loss: 1.36452e+02
I1111 10:48:55.638689 140264174335808 run_lib.py:153] step: 240150, training_loss: 1.11171e+02
I1111 10:49:05.765336 140264174335808 run_lib.py:153] step: 240200, training_loss: 1.02965e+02
I1111 10:49:15.086677 140264174335808 run_lib.py:153] step: 240250, training_loss: 1.31218e+02
I1111 10:49:25.731067 140264174335808 run_lib.py:153] step: 240300, training_loss: 1.40697e+02
I1111 10:49:36.228184 140264174335808 run_lib.py:153] step: 240350, training_loss: 1.23249e+02
I1111 10:49:46.249379 140264174335808 run_lib.py:153] step: 240400, training_loss: 1.20855e+02
I1111 10:49:56.491519 140264174335808 run_lib.py:153] step: 240450, training_loss: 1.18394e+02
I1111 10:50:06.568101 140264174335808 run_lib.py:153] step: 240500, training_loss: 1.26207e+02
I1111 10:50:16.717637 140264174335808 run_lib.py:153] step: 240550, training_loss: 1.24683e+02
I1111 10:50:27.626259 140264174335808 run_lib.py:153] step: 240600, training_loss: 1.07622e+02
I1111 10:50:37.708214 140264174335808 run_lib.py:153] step: 240650, training_loss: 1.35844e+02
I1111 10:50:47.999158 140264174335808 run_lib.py:153] step: 240700, training_loss: 1.35177e+02
I1111 10:50:58.700071 140264174335808 run_lib.py:153] step: 240750, training_loss: 1.37786e+02
I1111 10:51:08.354398 140264174335808 run_lib.py:153] step: 240800, training_loss: 1.49072e+02
I1111 10:51:18.308699 140264174335808 run_lib.py:153] step: 240850, training_loss: 1.38071e+02
I1111 10:51:29.146322 140264174335808 run_lib.py:153] step: 240900, training_loss: 1.31880e+02
I1111 10:51:39.210776 140264174335808 run_lib.py:153] step: 240950, training_loss: 1.19090e+02
I1111 10:51:49.151599 140264174335808 run_lib.py:153] step: 241000, training_loss: 1.05431e+02
I1111 10:51:59.056587 140264174335808 run_lib.py:153] step: 241050, training_loss: 1.41791e+02
I1111 10:52:08.830256 140264174335808 run_lib.py:153] step: 241100, training_loss: 1.43033e+02
I1111 10:52:18.792917 140264174335808 run_lib.py:153] step: 241150, training_loss: 1.42182e+02
I1111 10:52:28.592509 140264174335808 run_lib.py:153] step: 241200, training_loss: 1.26615e+02
I1111 10:52:38.073618 140264174335808 run_lib.py:153] step: 241250, training_loss: 1.36144e+02
I1111 10:52:47.704226 140264174335808 run_lib.py:153] step: 241300, training_loss: 1.16910e+02
I1111 10:52:57.213154 140264174335808 run_lib.py:153] step: 241350, training_loss: 1.12782e+02
I1111 10:53:06.660643 140264174335808 run_lib.py:153] step: 241400, training_loss: 1.45786e+02
I1111 10:53:16.659413 140264174335808 run_lib.py:153] step: 241450, training_loss: 1.21306e+02
I1111 10:53:26.303651 140264174335808 run_lib.py:153] step: 241500, training_loss: 1.29677e+02
I1111 10:53:36.500826 140264174335808 run_lib.py:153] step: 241550, training_loss: 1.21683e+02
I1111 10:53:46.318095 140264174335808 run_lib.py:153] step: 241600, training_loss: 1.17126e+02
I1111 10:53:56.419552 140264174335808 run_lib.py:153] step: 241650, training_loss: 1.17928e+02
I1111 10:54:06.317538 140264174335808 run_lib.py:153] step: 241700, training_loss: 1.35839e+02
I1111 10:54:15.675421 140264174335808 run_lib.py:153] step: 241750, training_loss: 1.05808e+02
I1111 10:54:25.056586 140264174335808 run_lib.py:153] step: 241800, training_loss: 1.19156e+02
I1111 10:54:35.162836 140264174335808 run_lib.py:153] step: 241850, training_loss: 9.09867e+01
I1111 10:54:46.014539 140264174335808 run_lib.py:153] step: 241900, training_loss: 1.08034e+02
I1111 10:54:56.422983 140264174335808 run_lib.py:153] step: 241950, training_loss: 1.20105e+02
I1111 10:55:07.493033 140264174335808 run_lib.py:153] step: 242000, training_loss: 1.21530e+02
I1111 10:55:17.998653 140264174335808 run_lib.py:153] step: 242050, training_loss: 1.30114e+02
I1111 10:55:28.157308 140264174335808 run_lib.py:153] step: 242100, training_loss: 1.24092e+02
I1111 10:55:38.030149 140264174335808 run_lib.py:153] step: 242150, training_loss: 1.70222e+02
I1111 10:55:47.955212 140264174335808 run_lib.py:153] step: 242200, training_loss: 1.44645e+02
I1111 10:55:57.366554 140264174335808 run_lib.py:153] step: 242250, training_loss: 1.34626e+02
I1111 10:56:07.529978 140264174335808 run_lib.py:153] step: 242300, training_loss: 1.25436e+02
I1111 10:56:17.742502 140264174335808 run_lib.py:153] step: 242350, training_loss: 1.22031e+02
I1111 10:56:27.953164 140264174335808 run_lib.py:153] step: 242400, training_loss: 1.16617e+02
I1111 10:56:38.393435 140264174335808 run_lib.py:153] step: 242450, training_loss: 1.33235e+02
I1111 10:56:48.126544 140264174335808 run_lib.py:153] step: 242500, training_loss: 1.38953e+02
I1111 10:56:58.672086 140264174335808 run_lib.py:153] step: 242550, training_loss: 1.16403e+02
I1111 10:57:09.088374 140264174335808 run_lib.py:153] step: 242600, training_loss: 1.35713e+02
I1111 10:57:19.161180 140264174335808 run_lib.py:153] step: 242650, training_loss: 1.16663e+02
I1111 10:57:29.213443 140264174335808 run_lib.py:153] step: 242700, training_loss: 1.27781e+02
I1111 10:57:39.721849 140264174335808 run_lib.py:153] step: 242750, training_loss: 1.18909e+02
I1111 10:57:49.741611 140264174335808 run_lib.py:153] step: 242800, training_loss: 1.33897e+02
I1111 10:58:00.349153 140264174335808 run_lib.py:153] step: 242850, training_loss: 1.53180e+02
I1111 10:58:11.299165 140264174335808 run_lib.py:153] step: 242900, training_loss: 1.37562e+02
I1111 10:58:21.852815 140264174335808 run_lib.py:153] step: 242950, training_loss: 1.71773e+02
I1111 10:58:32.176543 140264174335808 run_lib.py:153] step: 243000, training_loss: 1.32601e+02
I1111 10:58:42.090404 140264174335808 run_lib.py:153] step: 243050, training_loss: 1.39185e+02
I1111 10:58:52.789094 140264174335808 run_lib.py:153] step: 243100, training_loss: 1.42195e+02
I1111 10:59:02.692565 140264174335808 run_lib.py:153] step: 243150, training_loss: 1.00234e+02
I1111 10:59:12.855355 140264174335808 run_lib.py:153] step: 243200, training_loss: 1.40094e+02
I1111 10:59:23.086664 140264174335808 run_lib.py:153] step: 243250, training_loss: 1.03068e+02
I1111 10:59:32.674428 140264174335808 run_lib.py:153] step: 243300, training_loss: 1.29682e+02
I1111 10:59:43.043660 140264174335808 run_lib.py:153] step: 243350, training_loss: 1.37986e+02
I1111 10:59:52.988489 140264174335808 run_lib.py:153] step: 243400, training_loss: 1.19109e+02
I1111 11:00:03.247903 140264174335808 run_lib.py:153] step: 243450, training_loss: 1.49681e+02
I1111 11:00:13.157717 140264174335808 run_lib.py:153] step: 243500, training_loss: 1.35818e+02
I1111 11:00:22.558662 140264174335808 run_lib.py:153] step: 243550, training_loss: 1.25963e+02
I1111 11:00:32.488941 140264174335808 run_lib.py:153] step: 243600, training_loss: 1.15397e+02
I1111 11:00:41.831844 140264174335808 run_lib.py:153] step: 243650, training_loss: 1.28218e+02
I1111 11:00:51.626984 140264174335808 run_lib.py:153] step: 243700, training_loss: 1.38639e+02
I1111 11:01:01.207375 140264174335808 run_lib.py:153] step: 243750, training_loss: 1.34374e+02
I1111 11:01:11.230675 140264174335808 run_lib.py:153] step: 243800, training_loss: 1.38603e+02
I1111 11:01:21.402727 140264174335808 run_lib.py:153] step: 243850, training_loss: 1.50101e+02
I1111 11:01:31.804221 140264174335808 run_lib.py:153] step: 243900, training_loss: 1.30041e+02
I1111 11:01:41.949511 140264174335808 run_lib.py:153] step: 243950, training_loss: 1.23901e+02
I1111 11:01:52.113502 140264174335808 run_lib.py:153] step: 244000, training_loss: 1.37521e+02
I1111 11:02:02.190370 140264174335808 run_lib.py:153] step: 244050, training_loss: 1.35759e+02
I1111 11:02:12.494903 140264174335808 run_lib.py:153] step: 244100, training_loss: 1.45995e+02
I1111 11:02:22.924337 140264174335808 run_lib.py:153] step: 244150, training_loss: 1.34639e+02
I1111 11:02:33.123804 140264174335808 run_lib.py:153] step: 244200, training_loss: 1.17774e+02
I1111 11:02:42.569039 140264174335808 run_lib.py:153] step: 244250, training_loss: 1.15257e+02
I1111 11:02:52.291074 140264174335808 run_lib.py:153] step: 244300, training_loss: 1.28044e+02
I1111 11:03:02.802018 140264174335808 run_lib.py:153] step: 244350, training_loss: 9.97081e+01
I1111 11:03:12.495948 140264174335808 run_lib.py:153] step: 244400, training_loss: 1.64287e+02
I1111 11:03:22.080101 140264174335808 run_lib.py:153] step: 244450, training_loss: 1.35356e+02
I1111 11:03:32.163758 140264174335808 run_lib.py:153] step: 244500, training_loss: 1.21393e+02
I1111 11:03:41.668751 140264174335808 run_lib.py:153] step: 244550, training_loss: 1.06301e+02
I1111 11:03:51.912790 140264174335808 run_lib.py:153] step: 244600, training_loss: 1.09571e+02
I1111 11:04:01.660317 140264174335808 run_lib.py:153] step: 244650, training_loss: 1.21838e+02
I1111 11:04:12.341797 140264174335808 run_lib.py:153] step: 244700, training_loss: 1.39832e+02
I1111 11:04:22.976187 140264174335808 run_lib.py:153] step: 244750, training_loss: 1.18590e+02
I1111 11:04:33.322295 140264174335808 run_lib.py:153] step: 244800, training_loss: 1.17839e+02
I1111 11:04:43.797403 140264174335808 run_lib.py:153] step: 244850, training_loss: 1.40506e+02
I1111 11:04:53.725892 140264174335808 run_lib.py:153] step: 244900, training_loss: 1.33733e+02
I1111 11:05:04.024158 140264174335808 run_lib.py:153] step: 244950, training_loss: 1.27986e+02
I1111 11:05:13.942269 140264174335808 run_lib.py:153] step: 245000, training_loss: 9.05902e+01
I1111 11:05:14.049938 140264174335808 run_lib.py:166] step: 245000, eval_loss: 1.23264e+02
I1111 11:05:23.864195 140264174335808 run_lib.py:153] step: 245050, training_loss: 1.00414e+02
I1111 11:05:34.364412 140264174335808 run_lib.py:153] step: 245100, training_loss: 1.48400e+02
I1111 11:05:44.362871 140264174335808 run_lib.py:153] step: 245150, training_loss: 1.15022e+02
I1111 11:05:54.741026 140264174335808 run_lib.py:153] step: 245200, training_loss: 1.02790e+02
I1111 11:06:04.717537 140264174335808 run_lib.py:153] step: 245250, training_loss: 1.18464e+02
I1111 11:06:15.145744 140264174335808 run_lib.py:153] step: 245300, training_loss: 1.24251e+02
I1111 11:06:25.200780 140264174335808 run_lib.py:153] step: 245350, training_loss: 1.39207e+02
I1111 11:06:36.240497 140264174335808 run_lib.py:153] step: 245400, training_loss: 1.34495e+02
I1111 11:06:46.434641 140264174335808 run_lib.py:153] step: 245450, training_loss: 1.48224e+02
I1111 11:06:57.429816 140264174335808 run_lib.py:153] step: 245500, training_loss: 1.29927e+02
I1111 11:07:08.375689 140264174335808 run_lib.py:153] step: 245550, training_loss: 1.64541e+02
I1111 11:07:19.801322 140264174335808 run_lib.py:153] step: 245600, training_loss: 1.41119e+02
I1111 11:07:31.128102 140264174335808 run_lib.py:153] step: 245650, training_loss: 1.32905e+02
I1111 11:07:42.002121 140264174335808 run_lib.py:153] step: 245700, training_loss: 1.36407e+02
I1111 11:07:53.036177 140264174335808 run_lib.py:153] step: 245750, training_loss: 1.46764e+02
I1111 11:08:04.366095 140264174335808 run_lib.py:153] step: 245800, training_loss: 1.15330e+02
I1111 11:08:15.544353 140264174335808 run_lib.py:153] step: 245850, training_loss: 1.20781e+02
I1111 11:08:25.972112 140264174335808 run_lib.py:153] step: 245900, training_loss: 1.13875e+02
I1111 11:08:35.811947 140264174335808 run_lib.py:153] step: 245950, training_loss: 1.36783e+02
I1111 11:08:45.864635 140264174335808 run_lib.py:153] step: 246000, training_loss: 1.17996e+02
I1111 11:08:55.590218 140264174335808 run_lib.py:153] step: 246050, training_loss: 1.04824e+02
I1111 11:09:04.888044 140264174335808 run_lib.py:153] step: 246100, training_loss: 1.20600e+02
I1111 11:09:14.941551 140264174335808 run_lib.py:153] step: 246150, training_loss: 1.44176e+02
I1111 11:09:24.651649 140264174335808 run_lib.py:153] step: 246200, training_loss: 1.42515e+02
I1111 11:09:34.184536 140264174335808 run_lib.py:153] step: 246250, training_loss: 1.49393e+02
I1111 11:09:43.695136 140264174335808 run_lib.py:153] step: 246300, training_loss: 1.23401e+02
I1111 11:09:53.358013 140264174335808 run_lib.py:153] step: 246350, training_loss: 1.16539e+02
I1111 11:10:02.899997 140264174335808 run_lib.py:153] step: 246400, training_loss: 1.67912e+02
I1111 11:10:12.872122 140264174335808 run_lib.py:153] step: 246450, training_loss: 1.06617e+02
I1111 11:10:22.809257 140264174335808 run_lib.py:153] step: 246500, training_loss: 1.22797e+02
I1111 11:10:32.776089 140264174335808 run_lib.py:153] step: 246550, training_loss: 1.10535e+02
I1111 11:10:42.837603 140264174335808 run_lib.py:153] step: 246600, training_loss: 1.57230e+02
I1111 11:10:52.838405 140264174335808 run_lib.py:153] step: 246650, training_loss: 1.16035e+02
I1111 11:11:02.359845 140264174335808 run_lib.py:153] step: 246700, training_loss: 1.14506e+02
I1111 11:11:12.672658 140264174335808 run_lib.py:153] step: 246750, training_loss: 1.03214e+02
I1111 11:11:22.230490 140264174335808 run_lib.py:153] step: 246800, training_loss: 1.23976e+02
I1111 11:11:32.139641 140264174335808 run_lib.py:153] step: 246850, training_loss: 1.25784e+02
I1111 11:11:41.796213 140264174335808 run_lib.py:153] step: 246900, training_loss: 1.40144e+02
I1111 11:11:51.790243 140264174335808 run_lib.py:153] step: 246950, training_loss: 1.49018e+02
I1111 11:12:01.896872 140264174335808 run_lib.py:153] step: 247000, training_loss: 1.50480e+02
I1111 11:12:12.613348 140264174335808 run_lib.py:153] step: 247050, training_loss: 1.54594e+02
I1111 11:12:22.404032 140264174335808 run_lib.py:153] step: 247100, training_loss: 8.83891e+01
I1111 11:12:32.607371 140264174335808 run_lib.py:153] step: 247150, training_loss: 1.13959e+02
I1111 11:12:42.363278 140264174335808 run_lib.py:153] step: 247200, training_loss: 1.17927e+02
I1111 11:12:52.570659 140264174335808 run_lib.py:153] step: 247250, training_loss: 9.74601e+01
I1111 11:13:02.103324 140264174335808 run_lib.py:153] step: 247300, training_loss: 1.29079e+02
I1111 11:13:11.850308 140264174335808 run_lib.py:153] step: 247350, training_loss: 1.11201e+02
I1111 11:13:21.398532 140264174335808 run_lib.py:153] step: 247400, training_loss: 1.16076e+02
I1111 11:13:30.809003 140264174335808 run_lib.py:153] step: 247450, training_loss: 9.23109e+01
I1111 11:13:40.404157 140264174335808 run_lib.py:153] step: 247500, training_loss: 1.31728e+02
I1111 11:13:50.179014 140264174335808 run_lib.py:153] step: 247550, training_loss: 1.45372e+02
I1111 11:14:00.062552 140264174335808 run_lib.py:153] step: 247600, training_loss: 1.38793e+02
I1111 11:14:10.063750 140264174335808 run_lib.py:153] step: 247650, training_loss: 1.33712e+02
I1111 11:14:19.760257 140264174335808 run_lib.py:153] step: 247700, training_loss: 1.16696e+02
I1111 11:14:30.400127 140264174335808 run_lib.py:153] step: 247750, training_loss: 1.12552e+02
I1111 11:14:41.121869 140264174335808 run_lib.py:153] step: 247800, training_loss: 1.29708e+02
I1111 11:14:51.137136 140264174335808 run_lib.py:153] step: 247850, training_loss: 1.21469e+02
I1111 11:15:01.406681 140264174335808 run_lib.py:153] step: 247900, training_loss: 1.33394e+02
I1111 11:15:11.738580 140264174335808 run_lib.py:153] step: 247950, training_loss: 1.50314e+02
I1111 11:15:22.159334 140264174335808 run_lib.py:153] step: 248000, training_loss: 1.15949e+02
I1111 11:15:32.172074 140264174335808 run_lib.py:153] step: 248050, training_loss: 1.15550e+02
I1111 11:15:42.212139 140264174335808 run_lib.py:153] step: 248100, training_loss: 1.09851e+02
I1111 11:15:52.589182 140264174335808 run_lib.py:153] step: 248150, training_loss: 1.41608e+02
I1111 11:16:03.446818 140264174335808 run_lib.py:153] step: 248200, training_loss: 1.48793e+02
I1111 11:16:13.656919 140264174335808 run_lib.py:153] step: 248250, training_loss: 1.28088e+02
I1111 11:16:24.295686 140264174335808 run_lib.py:153] step: 248300, training_loss: 1.40894e+02
I1111 11:16:33.949717 140264174335808 run_lib.py:153] step: 248350, training_loss: 1.35407e+02
I1111 11:16:44.061531 140264174335808 run_lib.py:153] step: 248400, training_loss: 1.18959e+02
I1111 11:16:54.244215 140264174335808 run_lib.py:153] step: 248450, training_loss: 1.56522e+02
I1111 11:17:03.770160 140264174335808 run_lib.py:153] step: 248500, training_loss: 1.37124e+02
I1111 11:17:14.836526 140264174335808 run_lib.py:153] step: 248550, training_loss: 1.25922e+02
I1111 11:17:24.901525 140264174335808 run_lib.py:153] step: 248600, training_loss: 1.08206e+02
I1111 11:17:35.165837 140264174335808 run_lib.py:153] step: 248650, training_loss: 1.48507e+02
I1111 11:17:45.771613 140264174335808 run_lib.py:153] step: 248700, training_loss: 1.23823e+02
I1111 11:17:56.299872 140264174335808 run_lib.py:153] step: 248750, training_loss: 1.12358e+02
I1111 11:18:06.480328 140264174335808 run_lib.py:153] step: 248800, training_loss: 1.11078e+02
I1111 11:18:16.874967 140264174335808 run_lib.py:153] step: 248850, training_loss: 1.26026e+02
I1111 11:18:27.393573 140264174335808 run_lib.py:153] step: 248900, training_loss: 1.26070e+02
I1111 11:18:37.024412 140264174335808 run_lib.py:153] step: 248950, training_loss: 9.47767e+01
I1111 11:18:47.495070 140264174335808 run_lib.py:153] step: 249000, training_loss: 1.22772e+02
I1111 11:18:57.862152 140264174335808 run_lib.py:153] step: 249050, training_loss: 1.09798e+02
I1111 11:19:08.129227 140264174335808 run_lib.py:153] step: 249100, training_loss: 1.19857e+02
I1111 11:19:18.826258 140264174335808 run_lib.py:153] step: 249150, training_loss: 1.28352e+02
I1111 11:19:29.426849 140264174335808 run_lib.py:153] step: 249200, training_loss: 1.03943e+02
I1111 11:19:39.459008 140264174335808 run_lib.py:153] step: 249250, training_loss: 1.43365e+02
I1111 11:19:49.600119 140264174335808 run_lib.py:153] step: 249300, training_loss: 1.36362e+02
I1111 11:19:59.485832 140264174335808 run_lib.py:153] step: 249350, training_loss: 1.34453e+02
I1111 11:20:09.760414 140264174335808 run_lib.py:153] step: 249400, training_loss: 1.31491e+02
I1111 11:20:19.368878 140264174335808 run_lib.py:153] step: 249450, training_loss: 1.22646e+02
I1111 11:20:29.566783 140264174335808 run_lib.py:153] step: 249500, training_loss: 1.15349e+02
I1111 11:20:40.003581 140264174335808 run_lib.py:153] step: 249550, training_loss: 1.13753e+02
I1111 11:20:50.566575 140264174335808 run_lib.py:153] step: 249600, training_loss: 1.30920e+02
I1111 11:21:01.070867 140264174335808 run_lib.py:153] step: 249650, training_loss: 1.32985e+02
I1111 11:21:11.768874 140264174335808 run_lib.py:153] step: 249700, training_loss: 1.12564e+02
I1111 11:21:21.986055 140264174335808 run_lib.py:153] step: 249750, training_loss: 1.59579e+02
I1111 11:21:31.551614 140264174335808 run_lib.py:153] step: 249800, training_loss: 1.16081e+02
I1111 11:21:41.619412 140264174335808 run_lib.py:153] step: 249850, training_loss: 1.26854e+02
I1111 11:21:51.065356 140264174335808 run_lib.py:153] step: 249900, training_loss: 1.35218e+02
I1111 11:22:01.654888 140264174335808 run_lib.py:153] step: 249950, training_loss: 1.10180e+02
I1111 11:22:11.602509 140264174335808 run_lib.py:153] step: 250000, training_loss: 1.37228e+02
I1111 11:22:12.173507 140264174335808 run_lib.py:166] step: 250000, eval_loss: 1.42770e+02
I1111 11:22:22.713140 140264174335808 run_lib.py:153] step: 250050, training_loss: 1.40902e+02
I1111 11:22:32.897310 140264174335808 run_lib.py:153] step: 250100, training_loss: 1.19790e+02
I1111 11:22:42.319625 140264174335808 run_lib.py:153] step: 250150, training_loss: 1.21914e+02
I1111 11:22:52.546082 140264174335808 run_lib.py:153] step: 250200, training_loss: 1.42425e+02
I1111 11:23:02.196526 140264174335808 run_lib.py:153] step: 250250, training_loss: 9.87350e+01
I1111 11:23:12.454705 140264174335808 run_lib.py:153] step: 250300, training_loss: 1.29848e+02
I1111 11:23:22.084450 140264174335808 run_lib.py:153] step: 250350, training_loss: 1.31481e+02
I1111 11:23:32.082317 140264174335808 run_lib.py:153] step: 250400, training_loss: 1.15297e+02
I1111 11:23:41.684724 140264174335808 run_lib.py:153] step: 250450, training_loss: 1.54115e+02
I1111 11:23:52.470721 140264174335808 run_lib.py:153] step: 250500, training_loss: 1.28121e+02
I1111 11:24:02.677879 140264174335808 run_lib.py:153] step: 250550, training_loss: 1.28682e+02
I1111 11:24:13.390789 140264174335808 run_lib.py:153] step: 250600, training_loss: 1.02663e+02
I1111 11:24:23.416679 140264174335808 run_lib.py:153] step: 250650, training_loss: 1.18754e+02
I1111 11:24:33.542305 140264174335808 run_lib.py:153] step: 250700, training_loss: 1.21917e+02
I1111 11:24:43.493753 140264174335808 run_lib.py:153] step: 250750, training_loss: 1.17261e+02
I1111 11:24:54.031886 140264174335808 run_lib.py:153] step: 250800, training_loss: 1.59628e+02
I1111 11:25:03.491653 140264174335808 run_lib.py:153] step: 250850, training_loss: 1.10127e+02
I1111 11:25:12.799851 140264174335808 run_lib.py:153] step: 250900, training_loss: 1.05981e+02
I1111 11:25:22.923246 140264174335808 run_lib.py:153] step: 250950, training_loss: 1.19846e+02
I1111 11:25:32.550709 140264174335808 run_lib.py:153] step: 251000, training_loss: 1.28501e+02
I1111 11:25:42.611619 140264174335808 run_lib.py:153] step: 251050, training_loss: 1.31223e+02
I1111 11:25:52.631579 140264174335808 run_lib.py:153] step: 251100, training_loss: 1.22414e+02
I1111 11:26:02.493786 140264174335808 run_lib.py:153] step: 251150, training_loss: 1.43209e+02
I1111 11:26:12.323714 140264174335808 run_lib.py:153] step: 251200, training_loss: 1.15669e+02
I1111 11:26:22.484602 140264174335808 run_lib.py:153] step: 251250, training_loss: 1.55906e+02
I1111 11:26:33.007459 140264174335808 run_lib.py:153] step: 251300, training_loss: 1.04771e+02
I1111 11:26:43.153582 140264174335808 run_lib.py:153] step: 251350, training_loss: 1.42411e+02
I1111 11:26:52.798082 140264174335808 run_lib.py:153] step: 251400, training_loss: 1.29388e+02
I1111 11:27:03.075106 140264174335808 run_lib.py:153] step: 251450, training_loss: 1.69342e+02
I1111 11:27:13.288484 140264174335808 run_lib.py:153] step: 251500, training_loss: 1.25690e+02
I1111 11:27:23.363423 140264174335808 run_lib.py:153] step: 251550, training_loss: 1.09540e+02
I1111 11:27:32.954140 140264174335808 run_lib.py:153] step: 251600, training_loss: 1.59909e+02
I1111 11:27:42.861257 140264174335808 run_lib.py:153] step: 251650, training_loss: 1.18172e+02
I1111 11:27:52.964260 140264174335808 run_lib.py:153] step: 251700, training_loss: 1.22439e+02
I1111 11:28:03.273232 140264174335808 run_lib.py:153] step: 251750, training_loss: 1.24442e+02
I1111 11:28:13.728659 140264174335808 run_lib.py:153] step: 251800, training_loss: 1.29447e+02
I1111 11:28:24.151332 140264174335808 run_lib.py:153] step: 251850, training_loss: 1.59238e+02
I1111 11:28:34.491447 140264174335808 run_lib.py:153] step: 251900, training_loss: 1.20581e+02
I1111 11:28:44.875970 140264174335808 run_lib.py:153] step: 251950, training_loss: 1.17584e+02
I1111 11:28:55.131516 140264174335808 run_lib.py:153] step: 252000, training_loss: 1.27653e+02
I1111 11:29:04.965042 140264174335808 run_lib.py:153] step: 252050, training_loss: 1.33390e+02
I1111 11:29:15.115283 140264174335808 run_lib.py:153] step: 252100, training_loss: 9.62115e+01
I1111 11:29:25.443371 140264174335808 run_lib.py:153] step: 252150, training_loss: 1.53752e+02
I1111 11:29:34.968338 140264174335808 run_lib.py:153] step: 252200, training_loss: 1.21061e+02
I1111 11:29:45.337322 140264174335808 run_lib.py:153] step: 252250, training_loss: 1.01713e+02
I1111 11:29:55.762736 140264174335808 run_lib.py:153] step: 252300, training_loss: 1.23650e+02
I1111 11:30:05.930359 140264174335808 run_lib.py:153] step: 252350, training_loss: 1.37458e+02
I1111 11:30:16.405535 140264174335808 run_lib.py:153] step: 252400, training_loss: 1.33526e+02
I1111 11:30:26.997611 140264174335808 run_lib.py:153] step: 252450, training_loss: 1.35361e+02
I1111 11:30:37.022893 140264174335808 run_lib.py:153] step: 252500, training_loss: 1.65072e+02
I1111 11:30:47.466377 140264174335808 run_lib.py:153] step: 252550, training_loss: 1.05051e+02
I1111 11:30:57.844752 140264174335808 run_lib.py:153] step: 252600, training_loss: 1.01828e+02
I1111 11:31:08.737190 140264174335808 run_lib.py:153] step: 252650, training_loss: 1.35285e+02
I1111 11:31:18.624496 140264174335808 run_lib.py:153] step: 252700, training_loss: 1.30665e+02
I1111 11:31:28.486752 140264174335808 run_lib.py:153] step: 252750, training_loss: 1.42176e+02
I1111 11:31:38.266581 140264174335808 run_lib.py:153] step: 252800, training_loss: 1.31366e+02
I1111 11:31:48.110781 140264174335808 run_lib.py:153] step: 252850, training_loss: 1.38605e+02
I1111 11:31:58.883066 140264174335808 run_lib.py:153] step: 252900, training_loss: 1.34377e+02
I1111 11:32:08.952218 140264174335808 run_lib.py:153] step: 252950, training_loss: 1.16620e+02
I1111 11:32:18.996314 140264174335808 run_lib.py:153] step: 253000, training_loss: 1.18465e+02
I1111 11:32:28.748080 140264174335808 run_lib.py:153] step: 253050, training_loss: 1.11993e+02
I1111 11:32:38.841434 140264174335808 run_lib.py:153] step: 253100, training_loss: 1.67327e+02
I1111 11:32:48.881934 140264174335808 run_lib.py:153] step: 253150, training_loss: 1.33136e+02
I1111 11:32:58.819193 140264174335808 run_lib.py:153] step: 253200, training_loss: 1.17591e+02
I1111 11:33:08.977033 140264174335808 run_lib.py:153] step: 253250, training_loss: 1.41579e+02
I1111 11:33:19.171008 140264174335808 run_lib.py:153] step: 253300, training_loss: 9.53830e+01
I1111 11:33:29.868771 140264174335808 run_lib.py:153] step: 253350, training_loss: 1.30595e+02
I1111 11:33:40.360687 140264174335808 run_lib.py:153] step: 253400, training_loss: 1.35690e+02
I1111 11:33:51.303187 140264174335808 run_lib.py:153] step: 253450, training_loss: 1.57547e+02
I1111 11:34:01.555934 140264174335808 run_lib.py:153] step: 253500, training_loss: 1.35800e+02
I1111 11:34:11.289613 140264174335808 run_lib.py:153] step: 253550, training_loss: 1.19025e+02
I1111 11:34:21.828708 140264174335808 run_lib.py:153] step: 253600, training_loss: 1.34554e+02
I1111 11:34:31.727940 140264174335808 run_lib.py:153] step: 253650, training_loss: 9.94279e+01
I1111 11:34:41.765215 140264174335808 run_lib.py:153] step: 253700, training_loss: 1.00137e+02
I1111 11:34:51.710534 140264174335808 run_lib.py:153] step: 253750, training_loss: 1.47443e+02
I1111 11:35:01.400671 140264174335808 run_lib.py:153] step: 253800, training_loss: 1.25176e+02
I1111 11:35:11.499822 140264174335808 run_lib.py:153] step: 253850, training_loss: 1.27089e+02
I1111 11:35:21.672295 140264174335808 run_lib.py:153] step: 253900, training_loss: 1.21881e+02
I1111 11:35:31.438952 140264174335808 run_lib.py:153] step: 253950, training_loss: 1.00536e+02
I1111 11:35:41.405363 140264174335808 run_lib.py:153] step: 254000, training_loss: 1.42000e+02
I1111 11:35:51.306468 140264174335808 run_lib.py:153] step: 254050, training_loss: 1.65927e+02
I1111 11:36:01.519826 140264174335808 run_lib.py:153] step: 254100, training_loss: 1.35450e+02
I1111 11:36:12.480580 140264174335808 run_lib.py:153] step: 254150, training_loss: 1.06544e+02
I1111 11:36:22.348238 140264174335808 run_lib.py:153] step: 254200, training_loss: 1.43577e+02
I1111 11:36:32.078672 140264174335808 run_lib.py:153] step: 254250, training_loss: 1.06127e+02
I1111 11:36:42.114551 140264174335808 run_lib.py:153] step: 254300, training_loss: 1.58862e+02
I1111 11:36:52.161444 140264174335808 run_lib.py:153] step: 254350, training_loss: 1.27301e+02
I1111 11:37:01.805137 140264174335808 run_lib.py:153] step: 254400, training_loss: 1.47138e+02
I1111 11:37:11.632253 140264174335808 run_lib.py:153] step: 254450, training_loss: 1.47810e+02
I1111 11:37:21.267254 140264174335808 run_lib.py:153] step: 254500, training_loss: 1.17199e+02
I1111 11:37:30.969758 140264174335808 run_lib.py:153] step: 254550, training_loss: 1.01827e+02
I1111 11:37:40.972739 140264174335808 run_lib.py:153] step: 254600, training_loss: 1.60140e+02
I1111 11:37:50.753569 140264174335808 run_lib.py:153] step: 254650, training_loss: 1.45451e+02
I1111 11:38:01.141533 140264174335808 run_lib.py:153] step: 254700, training_loss: 1.17366e+02
I1111 11:38:11.111478 140264174335808 run_lib.py:153] step: 254750, training_loss: 9.13435e+01
I1111 11:38:21.323221 140264174335808 run_lib.py:153] step: 254800, training_loss: 1.42232e+02
I1111 11:38:30.814525 140264174335808 run_lib.py:153] step: 254850, training_loss: 1.48333e+02
I1111 11:38:40.961214 140264174335808 run_lib.py:153] step: 254900, training_loss: 1.45298e+02
I1111 11:38:50.991536 140264174335808 run_lib.py:153] step: 254950, training_loss: 1.30047e+02
I1111 11:39:00.392935 140264174335808 run_lib.py:153] step: 255000, training_loss: 1.49960e+02
I1111 11:39:00.495143 140264174335808 run_lib.py:166] step: 255000, eval_loss: 1.22249e+02
I1111 11:39:11.044421 140264174335808 run_lib.py:153] step: 255050, training_loss: 1.32080e+02
I1111 11:39:21.653815 140264174335808 run_lib.py:153] step: 255100, training_loss: 1.32729e+02
I1111 11:39:31.004665 140264174335808 run_lib.py:153] step: 255150, training_loss: 1.26639e+02
I1111 11:39:41.165399 140264174335808 run_lib.py:153] step: 255200, training_loss: 1.17027e+02
I1111 11:39:51.282097 140264174335808 run_lib.py:153] step: 255250, training_loss: 1.38619e+02
I1111 11:40:01.118622 140264174335808 run_lib.py:153] step: 255300, training_loss: 1.46898e+02
I1111 11:40:11.766294 140264174335808 run_lib.py:153] step: 255350, training_loss: 1.10387e+02
I1111 11:40:22.503583 140264174335808 run_lib.py:153] step: 255400, training_loss: 1.35930e+02
I1111 11:40:33.019861 140264174335808 run_lib.py:153] step: 255450, training_loss: 1.67002e+02
I1111 11:40:43.587205 140264174335808 run_lib.py:153] step: 255500, training_loss: 1.29305e+02
I1111 11:40:53.641925 140264174335808 run_lib.py:153] step: 255550, training_loss: 1.00440e+02
I1111 11:41:03.250084 140264174335808 run_lib.py:153] step: 255600, training_loss: 1.21247e+02
I1111 11:41:12.980018 140264174335808 run_lib.py:153] step: 255650, training_loss: 9.90778e+01
I1111 11:41:22.749195 140264174335808 run_lib.py:153] step: 255700, training_loss: 1.21719e+02
I1111 11:41:32.919441 140264174335808 run_lib.py:153] step: 255750, training_loss: 1.01972e+02
I1111 11:41:42.277923 140264174335808 run_lib.py:153] step: 255800, training_loss: 1.06269e+02
I1111 11:41:52.183224 140264174335808 run_lib.py:153] step: 255850, training_loss: 1.32309e+02
I1111 11:42:02.188492 140264174335808 run_lib.py:153] step: 255900, training_loss: 9.68900e+01
I1111 11:42:12.033880 140264174335808 run_lib.py:153] step: 255950, training_loss: 1.47361e+02
I1111 11:42:22.270309 140264174335808 run_lib.py:153] step: 256000, training_loss: 1.24270e+02
I1111 11:42:32.097914 140264174335808 run_lib.py:153] step: 256050, training_loss: 1.26337e+02
I1111 11:42:41.922995 140264174335808 run_lib.py:153] step: 256100, training_loss: 1.25389e+02
I1111 11:42:51.665423 140264174335808 run_lib.py:153] step: 256150, training_loss: 1.22464e+02
I1111 11:43:01.097915 140264174335808 run_lib.py:153] step: 256200, training_loss: 1.31438e+02
I1111 11:43:10.560168 140264174335808 run_lib.py:153] step: 256250, training_loss: 1.54353e+02
I1111 11:43:20.421070 140264174335808 run_lib.py:153] step: 256300, training_loss: 1.30143e+02
I1111 11:43:30.708225 140264174335808 run_lib.py:153] step: 256350, training_loss: 1.55309e+02
I1111 11:43:41.140186 140264174335808 run_lib.py:153] step: 256400, training_loss: 1.47496e+02
I1111 11:43:51.571573 140264174335808 run_lib.py:153] step: 256450, training_loss: 1.10164e+02
I1111 11:44:02.362965 140264174335808 run_lib.py:153] step: 256500, training_loss: 1.41708e+02
I1111 11:44:11.841814 140264174335808 run_lib.py:153] step: 256550, training_loss: 1.24470e+02
I1111 11:44:23.014072 140264174335808 run_lib.py:153] step: 256600, training_loss: 1.27870e+02
I1111 11:44:33.184297 140264174335808 run_lib.py:153] step: 256650, training_loss: 1.28939e+02
I1111 11:44:43.156673 140264174335808 run_lib.py:153] step: 256700, training_loss: 1.18988e+02
I1111 11:44:53.471382 140264174335808 run_lib.py:153] step: 256750, training_loss: 1.28875e+02
I1111 11:45:03.311434 140264174335808 run_lib.py:153] step: 256800, training_loss: 1.13968e+02
I1111 11:45:13.715924 140264174335808 run_lib.py:153] step: 256850, training_loss: 1.25511e+02
I1111 11:45:23.948987 140264174335808 run_lib.py:153] step: 256900, training_loss: 1.16080e+02
I1111 11:45:34.533682 140264174335808 run_lib.py:153] step: 256950, training_loss: 1.27822e+02
I1111 11:45:44.671510 140264174335808 run_lib.py:153] step: 257000, training_loss: 1.44740e+02
I1111 11:45:54.788368 140264174335808 run_lib.py:153] step: 257050, training_loss: 1.33461e+02
I1111 11:46:05.610176 140264174335808 run_lib.py:153] step: 257100, training_loss: 1.31583e+02
I1111 11:46:15.691402 140264174335808 run_lib.py:153] step: 257150, training_loss: 1.32403e+02
I1111 11:46:26.379830 140264174335808 run_lib.py:153] step: 257200, training_loss: 1.25100e+02
I1111 11:46:37.361099 140264174335808 run_lib.py:153] step: 257250, training_loss: 1.35793e+02
I1111 11:46:48.164952 140264174335808 run_lib.py:153] step: 257300, training_loss: 1.65975e+02
I1111 11:46:58.606423 140264174335808 run_lib.py:153] step: 257350, training_loss: 1.18339e+02
I1111 11:47:09.052776 140264174335808 run_lib.py:153] step: 257400, training_loss: 1.27578e+02
I1111 11:47:19.278396 140264174335808 run_lib.py:153] step: 257450, training_loss: 1.03381e+02
I1111 11:47:28.951526 140264174335808 run_lib.py:153] step: 257500, training_loss: 1.09523e+02
I1111 11:47:39.170571 140264174335808 run_lib.py:153] step: 257550, training_loss: 1.38895e+02
I1111 11:47:49.740695 140264174335808 run_lib.py:153] step: 257600, training_loss: 1.32813e+02
I1111 11:48:00.175945 140264174335808 run_lib.py:153] step: 257650, training_loss: 9.57739e+01
I1111 11:48:10.527836 140264174335808 run_lib.py:153] step: 257700, training_loss: 1.49543e+02
I1111 11:48:20.695172 140264174335808 run_lib.py:153] step: 257750, training_loss: 1.14331e+02
I1111 11:48:31.019017 140264174335808 run_lib.py:153] step: 257800, training_loss: 1.21253e+02
I1111 11:48:41.281801 140264174335808 run_lib.py:153] step: 257850, training_loss: 1.57064e+02
I1111 11:48:51.712953 140264174335808 run_lib.py:153] step: 257900, training_loss: 1.45079e+02
I1111 11:49:01.467938 140264174335808 run_lib.py:153] step: 257950, training_loss: 1.42807e+02
I1111 11:49:11.442586 140264174335808 run_lib.py:153] step: 258000, training_loss: 1.23977e+02
I1111 11:49:22.396314 140264174335808 run_lib.py:153] step: 258050, training_loss: 1.09640e+02
I1111 11:49:32.897154 140264174335808 run_lib.py:153] step: 258100, training_loss: 1.30492e+02
I1111 11:49:42.675786 140264174335808 run_lib.py:153] step: 258150, training_loss: 1.18768e+02
I1111 11:49:52.455925 140264174335808 run_lib.py:153] step: 258200, training_loss: 1.11377e+02
I1111 11:50:02.703839 140264174335808 run_lib.py:153] step: 258250, training_loss: 1.23199e+02
I1111 11:50:12.681346 140264174335808 run_lib.py:153] step: 258300, training_loss: 1.13786e+02
I1111 11:50:22.855269 140264174335808 run_lib.py:153] step: 258350, training_loss: 1.34610e+02
I1111 11:50:32.558640 140264174335808 run_lib.py:153] step: 258400, training_loss: 1.24097e+02
I1111 11:50:42.687794 140264174335808 run_lib.py:153] step: 258450, training_loss: 1.15228e+02
I1111 11:50:52.773773 140264174335808 run_lib.py:153] step: 258500, training_loss: 1.14204e+02
I1111 11:51:02.982441 140264174335808 run_lib.py:153] step: 258550, training_loss: 1.08844e+02
I1111 11:51:12.671615 140264174335808 run_lib.py:153] step: 258600, training_loss: 1.29524e+02
I1111 11:51:22.380994 140264174335808 run_lib.py:153] step: 258650, training_loss: 1.10366e+02
I1111 11:51:32.309497 140264174335808 run_lib.py:153] step: 258700, training_loss: 1.22127e+02
I1111 11:51:42.277234 140264174335808 run_lib.py:153] step: 258750, training_loss: 1.37275e+02
I1111 11:51:51.986021 140264174335808 run_lib.py:153] step: 258800, training_loss: 1.37279e+02
I1111 11:52:02.200825 140264174335808 run_lib.py:153] step: 258850, training_loss: 1.50066e+02
I1111 11:52:12.780956 140264174335808 run_lib.py:153] step: 258900, training_loss: 1.34988e+02
I1111 11:52:22.222886 140264174335808 run_lib.py:153] step: 258950, training_loss: 1.25008e+02
I1111 11:52:32.080923 140264174335808 run_lib.py:153] step: 259000, training_loss: 1.47966e+02
I1111 11:52:41.861074 140264174335808 run_lib.py:153] step: 259050, training_loss: 1.30666e+02
I1111 11:52:52.445819 140264174335808 run_lib.py:153] step: 259100, training_loss: 1.73861e+02
I1111 11:53:02.695040 140264174335808 run_lib.py:153] step: 259150, training_loss: 1.08361e+02
I1111 11:53:11.950938 140264174335808 run_lib.py:153] step: 259200, training_loss: 1.20346e+02
I1111 11:53:21.362318 140264174335808 run_lib.py:153] step: 259250, training_loss: 1.04502e+02
I1111 11:53:31.924106 140264174335808 run_lib.py:153] step: 259300, training_loss: 1.21122e+02
I1111 11:53:41.508906 140264174335808 run_lib.py:153] step: 259350, training_loss: 1.20013e+02
I1111 11:53:51.591823 140264174335808 run_lib.py:153] step: 259400, training_loss: 1.33081e+02
I1111 11:54:02.001977 140264174335808 run_lib.py:153] step: 259450, training_loss: 1.18753e+02
I1111 11:54:13.055805 140264174335808 run_lib.py:153] step: 259500, training_loss: 1.14008e+02
I1111 11:54:23.408483 140264174335808 run_lib.py:153] step: 259550, training_loss: 1.22015e+02
I1111 11:54:33.134452 140264174335808 run_lib.py:153] step: 259600, training_loss: 1.35887e+02
I1111 11:54:42.779480 140264174335808 run_lib.py:153] step: 259650, training_loss: 1.19118e+02
I1111 11:54:52.597680 140264174335808 run_lib.py:153] step: 259700, training_loss: 1.32679e+02
I1111 11:55:03.077586 140264174335808 run_lib.py:153] step: 259750, training_loss: 1.34547e+02
I1111 11:55:13.856931 140264174335808 run_lib.py:153] step: 259800, training_loss: 1.31213e+02
I1111 11:55:24.233090 140264174335808 run_lib.py:153] step: 259850, training_loss: 9.98601e+01
I1111 11:55:33.824716 140264174335808 run_lib.py:153] step: 259900, training_loss: 1.17601e+02
I1111 11:55:43.919022 140264174335808 run_lib.py:153] step: 259950, training_loss: 1.17961e+02
I1111 11:55:53.339332 140264174335808 run_lib.py:153] step: 260000, training_loss: 1.42683e+02
I1111 11:55:53.904546 140264174335808 run_lib.py:166] step: 260000, eval_loss: 1.15072e+02
I1111 11:56:03.773847 140264174335808 run_lib.py:153] step: 260050, training_loss: 1.22875e+02
I1111 11:56:13.952601 140264174335808 run_lib.py:153] step: 260100, training_loss: 1.28348e+02
I1111 11:56:24.184907 140264174335808 run_lib.py:153] step: 260150, training_loss: 1.15702e+02
I1111 11:56:33.520971 140264174335808 run_lib.py:153] step: 260200, training_loss: 1.50670e+02
I1111 11:56:44.173314 140264174335808 run_lib.py:153] step: 260250, training_loss: 9.65287e+01
I1111 11:56:53.857639 140264174335808 run_lib.py:153] step: 260300, training_loss: 1.45545e+02
I1111 11:57:03.526185 140264174335808 run_lib.py:153] step: 260350, training_loss: 1.46673e+02
I1111 11:57:13.446586 140264174335808 run_lib.py:153] step: 260400, training_loss: 1.17028e+02
I1111 11:57:23.541982 140264174335808 run_lib.py:153] step: 260450, training_loss: 1.05784e+02
I1111 11:57:32.829703 140264174335808 run_lib.py:153] step: 260500, training_loss: 1.45408e+02
I1111 11:57:42.472123 140264174335808 run_lib.py:153] step: 260550, training_loss: 1.67318e+02
I1111 11:57:53.312454 140264174335808 run_lib.py:153] step: 260600, training_loss: 1.26211e+02
I1111 11:58:03.243006 140264174335808 run_lib.py:153] step: 260650, training_loss: 1.51167e+02
I1111 11:58:13.443324 140264174335808 run_lib.py:153] step: 260700, training_loss: 1.27974e+02
I1111 11:58:23.113950 140264174335808 run_lib.py:153] step: 260750, training_loss: 1.29937e+02
I1111 11:58:32.994915 140264174335808 run_lib.py:153] step: 260800, training_loss: 1.27042e+02
I1111 11:58:43.030299 140264174335808 run_lib.py:153] step: 260850, training_loss: 1.30841e+02
I1111 11:58:52.545790 140264174335808 run_lib.py:153] step: 260900, training_loss: 1.50807e+02
I1111 11:59:02.339051 140264174335808 run_lib.py:153] step: 260950, training_loss: 1.24111e+02
I1111 11:59:12.662144 140264174335808 run_lib.py:153] step: 261000, training_loss: 1.00430e+02
I1111 11:59:23.390425 140264174335808 run_lib.py:153] step: 261050, training_loss: 1.30484e+02
I1111 11:59:33.740724 140264174335808 run_lib.py:153] step: 261100, training_loss: 1.14121e+02
I1111 11:59:43.998072 140264174335808 run_lib.py:153] step: 261150, training_loss: 9.59637e+01
I1111 11:59:54.264496 140264174335808 run_lib.py:153] step: 261200, training_loss: 1.34627e+02
I1111 12:00:03.744128 140264174335808 run_lib.py:153] step: 261250, training_loss: 1.60615e+02
I1111 12:00:13.727877 140264174335808 run_lib.py:153] step: 261300, training_loss: 9.88958e+01
I1111 12:00:24.497040 140264174335808 run_lib.py:153] step: 261350, training_loss: 1.37450e+02
I1111 12:00:34.629238 140264174335808 run_lib.py:153] step: 261400, training_loss: 1.45172e+02
I1111 12:00:44.438311 140264174335808 run_lib.py:153] step: 261450, training_loss: 1.32022e+02
I1111 12:00:54.286029 140264174335808 run_lib.py:153] step: 261500, training_loss: 1.13146e+02
I1111 12:01:04.081742 140264174335808 run_lib.py:153] step: 261550, training_loss: 1.11815e+02
I1111 12:01:14.247372 140264174335808 run_lib.py:153] step: 261600, training_loss: 1.24960e+02
I1111 12:01:24.279601 140264174335808 run_lib.py:153] step: 261650, training_loss: 1.23675e+02
I1111 12:01:35.278033 140264174335808 run_lib.py:153] step: 261700, training_loss: 1.21114e+02
I1111 12:01:45.383754 140264174335808 run_lib.py:153] step: 261750, training_loss: 1.11381e+02
I1111 12:01:55.091387 140264174335808 run_lib.py:153] step: 261800, training_loss: 1.17994e+02
I1111 12:02:05.253641 140264174335808 run_lib.py:153] step: 261850, training_loss: 1.00345e+02
I1111 12:02:15.549958 140264174335808 run_lib.py:153] step: 261900, training_loss: 1.46455e+02
I1111 12:02:25.171005 140264174335808 run_lib.py:153] step: 261950, training_loss: 1.43257e+02
I1111 12:02:34.718783 140264174335808 run_lib.py:153] step: 262000, training_loss: 1.28646e+02
I1111 12:02:44.624124 140264174335808 run_lib.py:153] step: 262050, training_loss: 1.43749e+02
I1111 12:02:54.383635 140264174335808 run_lib.py:153] step: 262100, training_loss: 1.27460e+02
I1111 12:03:04.903563 140264174335808 run_lib.py:153] step: 262150, training_loss: 1.35260e+02
I1111 12:03:14.949646 140264174335808 run_lib.py:153] step: 262200, training_loss: 1.27869e+02
I1111 12:03:24.508111 140264174335808 run_lib.py:153] step: 262250, training_loss: 1.36466e+02
I1111 12:03:34.126875 140264174335808 run_lib.py:153] step: 262300, training_loss: 1.39187e+02
I1111 12:03:44.518583 140264174335808 run_lib.py:153] step: 262350, training_loss: 1.38470e+02
I1111 12:03:55.019651 140264174335808 run_lib.py:153] step: 262400, training_loss: 1.37085e+02
I1111 12:04:05.894937 140264174335808 run_lib.py:153] step: 262450, training_loss: 1.28406e+02
I1111 12:04:16.315003 140264174335808 run_lib.py:153] step: 262500, training_loss: 1.31463e+02
I1111 12:04:26.092382 140264174335808 run_lib.py:153] step: 262550, training_loss: 1.21226e+02
I1111 12:04:36.124675 140264174335808 run_lib.py:153] step: 262600, training_loss: 1.05723e+02
I1111 12:04:46.473582 140264174335808 run_lib.py:153] step: 262650, training_loss: 1.50341e+02
I1111 12:04:56.280665 140264174335808 run_lib.py:153] step: 262700, training_loss: 1.15100e+02
I1111 12:05:06.497895 140264174335808 run_lib.py:153] step: 262750, training_loss: 1.11891e+02
I1111 12:05:17.585218 140264174335808 run_lib.py:153] step: 262800, training_loss: 1.18227e+02
I1111 12:05:26.945019 140264174335808 run_lib.py:153] step: 262850, training_loss: 1.47416e+02
I1111 12:05:37.655292 140264174335808 run_lib.py:153] step: 262900, training_loss: 1.10395e+02
I1111 12:05:48.460241 140264174335808 run_lib.py:153] step: 262950, training_loss: 1.23858e+02
I1111 12:05:58.792229 140264174335808 run_lib.py:153] step: 263000, training_loss: 1.04055e+02
I1111 12:06:08.600586 140264174335808 run_lib.py:153] step: 263050, training_loss: 1.19649e+02
I1111 12:06:18.704658 140264174335808 run_lib.py:153] step: 263100, training_loss: 1.06037e+02
I1111 12:06:28.877262 140264174335808 run_lib.py:153] step: 263150, training_loss: 1.49197e+02
I1111 12:06:39.359398 140264174335808 run_lib.py:153] step: 263200, training_loss: 1.25360e+02
I1111 12:06:49.578017 140264174335808 run_lib.py:153] step: 263250, training_loss: 1.32744e+02
I1111 12:06:59.146693 140264174335808 run_lib.py:153] step: 263300, training_loss: 1.56720e+02
I1111 12:07:08.749116 140264174335808 run_lib.py:153] step: 263350, training_loss: 1.32550e+02
I1111 12:07:18.870273 140264174335808 run_lib.py:153] step: 263400, training_loss: 1.27577e+02
I1111 12:07:28.789744 140264174335808 run_lib.py:153] step: 263450, training_loss: 1.30512e+02
I1111 12:07:38.128576 140264174335808 run_lib.py:153] step: 263500, training_loss: 1.67243e+02
I1111 12:07:48.095913 140264174335808 run_lib.py:153] step: 263550, training_loss: 1.18963e+02
I1111 12:07:58.661926 140264174335808 run_lib.py:153] step: 263600, training_loss: 1.20280e+02
I1111 12:08:08.378324 140264174335808 run_lib.py:153] step: 263650, training_loss: 1.09794e+02
I1111 12:08:18.884985 140264174335808 run_lib.py:153] step: 263700, training_loss: 1.26248e+02
I1111 12:08:28.694584 140264174335808 run_lib.py:153] step: 263750, training_loss: 1.38862e+02
I1111 12:08:39.024392 140264174335808 run_lib.py:153] step: 263800, training_loss: 1.13577e+02
I1111 12:08:49.086964 140264174335808 run_lib.py:153] step: 263850, training_loss: 1.23123e+02
I1111 12:08:59.736144 140264174335808 run_lib.py:153] step: 263900, training_loss: 1.59739e+02
I1111 12:09:09.268218 140264174335808 run_lib.py:153] step: 263950, training_loss: 1.54824e+02
I1111 12:09:19.277182 140264174335808 run_lib.py:153] step: 264000, training_loss: 1.24674e+02
I1111 12:09:29.372836 140264174335808 run_lib.py:153] step: 264050, training_loss: 1.48476e+02
I1111 12:09:39.070086 140264174335808 run_lib.py:153] step: 264100, training_loss: 1.30772e+02
I1111 12:09:49.500906 140264174335808 run_lib.py:153] step: 264150, training_loss: 1.58924e+02
I1111 12:09:59.208588 140264174335808 run_lib.py:153] step: 264200, training_loss: 1.26026e+02
I1111 12:10:09.420984 140264174335808 run_lib.py:153] step: 264250, training_loss: 1.35153e+02
I1111 12:10:19.154192 140264174335808 run_lib.py:153] step: 264300, training_loss: 1.25544e+02
I1111 12:10:29.528230 140264174335808 run_lib.py:153] step: 264350, training_loss: 1.23885e+02
I1111 12:10:39.353631 140264174335808 run_lib.py:153] step: 264400, training_loss: 1.31060e+02
I1111 12:10:50.060811 140264174335808 run_lib.py:153] step: 264450, training_loss: 1.32445e+02
I1111 12:11:00.136535 140264174335808 run_lib.py:153] step: 264500, training_loss: 1.25114e+02
I1111 12:11:10.585657 140264174335808 run_lib.py:153] step: 264550, training_loss: 1.44664e+02
I1111 12:11:20.760733 140264174335808 run_lib.py:153] step: 264600, training_loss: 1.09955e+02
I1111 12:11:30.639609 140264174335808 run_lib.py:153] step: 264650, training_loss: 1.58099e+02
I1111 12:11:41.559829 140264174335808 run_lib.py:153] step: 264700, training_loss: 1.16383e+02
I1111 12:11:51.621542 140264174335808 run_lib.py:153] step: 264750, training_loss: 1.37719e+02
I1111 12:12:02.082256 140264174335808 run_lib.py:153] step: 264800, training_loss: 9.66460e+01
I1111 12:12:12.166007 140264174335808 run_lib.py:153] step: 264850, training_loss: 1.52782e+02
I1111 12:12:21.986192 140264174335808 run_lib.py:153] step: 264900, training_loss: 1.42546e+02
I1111 12:12:32.354339 140264174335808 run_lib.py:153] step: 264950, training_loss: 1.21937e+02
I1111 12:12:42.581148 140264174335808 run_lib.py:153] step: 265000, training_loss: 1.10731e+02
I1111 12:12:42.684169 140264174335808 run_lib.py:166] step: 265000, eval_loss: 1.29010e+02
I1111 12:12:52.652581 140264174335808 run_lib.py:153] step: 265050, training_loss: 1.46727e+02
I1111 12:13:02.335524 140264174335808 run_lib.py:153] step: 265100, training_loss: 1.25557e+02
I1111 12:13:12.325842 140264174335808 run_lib.py:153] step: 265150, training_loss: 1.24889e+02
I1111 12:13:22.280201 140264174335808 run_lib.py:153] step: 265200, training_loss: 1.45003e+02
I1111 12:13:32.382014 140264174335808 run_lib.py:153] step: 265250, training_loss: 1.27059e+02
I1111 12:13:42.742012 140264174335808 run_lib.py:153] step: 265300, training_loss: 9.43732e+01
I1111 12:13:52.248417 140264174335808 run_lib.py:153] step: 265350, training_loss: 1.01060e+02
I1111 12:14:01.846230 140264174335808 run_lib.py:153] step: 265400, training_loss: 1.27389e+02
I1111 12:14:11.540067 140264174335808 run_lib.py:153] step: 265450, training_loss: 1.23362e+02
I1111 12:14:21.373032 140264174335808 run_lib.py:153] step: 265500, training_loss: 1.40144e+02
I1111 12:14:30.631211 140264174335808 run_lib.py:153] step: 265550, training_loss: 1.17402e+02
I1111 12:14:41.047254 140264174335808 run_lib.py:153] step: 265600, training_loss: 8.56744e+01
I1111 12:14:51.319251 140264174335808 run_lib.py:153] step: 265650, training_loss: 1.06176e+02
I1111 12:15:01.446708 140264174335808 run_lib.py:153] step: 265700, training_loss: 1.33718e+02
I1111 12:15:12.175346 140264174335808 run_lib.py:153] step: 265750, training_loss: 1.14670e+02
I1111 12:15:22.385682 140264174335808 run_lib.py:153] step: 265800, training_loss: 1.54953e+02
I1111 12:15:32.343032 140264174335808 run_lib.py:153] step: 265850, training_loss: 1.27718e+02
I1111 12:15:42.116874 140264174335808 run_lib.py:153] step: 265900, training_loss: 1.47453e+02
I1111 12:15:51.961954 140264174335808 run_lib.py:153] step: 265950, training_loss: 1.28355e+02
I1111 12:16:01.813327 140264174335808 run_lib.py:153] step: 266000, training_loss: 1.37704e+02
I1111 12:16:11.408491 140264174335808 run_lib.py:153] step: 266050, training_loss: 1.17618e+02
I1111 12:16:21.555772 140264174335808 run_lib.py:153] step: 266100, training_loss: 1.39482e+02
I1111 12:16:31.563090 140264174335808 run_lib.py:153] step: 266150, training_loss: 1.48277e+02
I1111 12:16:41.448884 140264174335808 run_lib.py:153] step: 266200, training_loss: 1.41533e+02
I1111 12:16:51.299751 140264174335808 run_lib.py:153] step: 266250, training_loss: 1.41029e+02
I1111 12:17:01.071705 140264174335808 run_lib.py:153] step: 266300, training_loss: 1.01898e+02
I1111 12:17:10.916910 140264174335808 run_lib.py:153] step: 266350, training_loss: 1.07576e+02
I1111 12:17:20.743643 140264174335808 run_lib.py:153] step: 266400, training_loss: 1.54614e+02
I1111 12:17:31.338345 140264174335808 run_lib.py:153] step: 266450, training_loss: 1.41560e+02
I1111 12:17:41.967462 140264174335808 run_lib.py:153] step: 266500, training_loss: 1.13698e+02
I1111 12:17:51.873204 140264174335808 run_lib.py:153] step: 266550, training_loss: 1.53947e+02
I1111 12:18:02.527106 140264174335808 run_lib.py:153] step: 266600, training_loss: 1.44670e+02
I1111 12:18:12.470208 140264174335808 run_lib.py:153] step: 266650, training_loss: 1.18136e+02
I1111 12:18:22.855864 140264174335808 run_lib.py:153] step: 266700, training_loss: 1.54595e+02
I1111 12:18:33.427627 140264174335808 run_lib.py:153] step: 266750, training_loss: 1.00020e+02
I1111 12:18:43.648417 140264174335808 run_lib.py:153] step: 266800, training_loss: 1.57509e+02
I1111 12:18:53.366839 140264174335808 run_lib.py:153] step: 266850, training_loss: 1.58148e+02
I1111 12:19:03.671494 140264174335808 run_lib.py:153] step: 266900, training_loss: 1.41956e+02
I1111 12:19:13.870512 140264174335808 run_lib.py:153] step: 266950, training_loss: 1.36495e+02
I1111 12:19:24.112442 140264174335808 run_lib.py:153] step: 267000, training_loss: 1.32019e+02
I1111 12:19:33.812739 140264174335808 run_lib.py:153] step: 267050, training_loss: 1.15819e+02
I1111 12:19:43.762767 140264174335808 run_lib.py:153] step: 267100, training_loss: 1.38825e+02
I1111 12:19:54.171768 140264174335808 run_lib.py:153] step: 267150, training_loss: 1.14777e+02
I1111 12:20:04.612178 140264174335808 run_lib.py:153] step: 267200, training_loss: 1.30537e+02
I1111 12:20:14.887396 140264174335808 run_lib.py:153] step: 267250, training_loss: 1.53719e+02
I1111 12:20:24.813790 140264174335808 run_lib.py:153] step: 267300, training_loss: 1.50431e+02
I1111 12:20:34.789915 140264174335808 run_lib.py:153] step: 267350, training_loss: 1.15988e+02
I1111 12:20:45.071043 140264174335808 run_lib.py:153] step: 267400, training_loss: 1.33643e+02
I1111 12:20:55.393396 140264174335808 run_lib.py:153] step: 267450, training_loss: 1.06775e+02
I1111 12:21:05.183818 140264174335808 run_lib.py:153] step: 267500, training_loss: 1.35505e+02
I1111 12:21:15.507241 140264174335808 run_lib.py:153] step: 267550, training_loss: 9.81597e+01
I1111 12:21:26.135538 140264174335808 run_lib.py:153] step: 267600, training_loss: 1.23885e+02
I1111 12:21:36.357706 140264174335808 run_lib.py:153] step: 267650, training_loss: 1.56856e+02
I1111 12:21:46.238133 140264174335808 run_lib.py:153] step: 267700, training_loss: 1.30937e+02
I1111 12:21:56.299065 140264174335808 run_lib.py:153] step: 267750, training_loss: 1.67446e+02
I1111 12:22:06.371896 140264174335808 run_lib.py:153] step: 267800, training_loss: 1.34968e+02
I1111 12:22:16.719223 140264174335808 run_lib.py:153] step: 267850, training_loss: 1.35598e+02
I1111 12:22:27.531800 140264174335808 run_lib.py:153] step: 267900, training_loss: 1.45248e+02
I1111 12:22:37.213121 140264174335808 run_lib.py:153] step: 267950, training_loss: 1.22467e+02
I1111 12:22:47.554785 140264174335808 run_lib.py:153] step: 268000, training_loss: 1.34920e+02
I1111 12:22:57.296294 140264174335808 run_lib.py:153] step: 268050, training_loss: 1.29641e+02
I1111 12:23:07.919396 140264174335808 run_lib.py:153] step: 268100, training_loss: 1.32661e+02
I1111 12:23:18.209541 140264174335808 run_lib.py:153] step: 268150, training_loss: 1.35270e+02
I1111 12:23:28.204433 140264174335808 run_lib.py:153] step: 268200, training_loss: 1.38476e+02
I1111 12:23:38.076843 140264174335808 run_lib.py:153] step: 268250, training_loss: 1.08011e+02
I1111 12:23:48.480797 140264174335808 run_lib.py:153] step: 268300, training_loss: 1.03050e+02
I1111 12:23:58.414470 140264174335808 run_lib.py:153] step: 268350, training_loss: 1.21424e+02
I1111 12:24:08.884876 140264174335808 run_lib.py:153] step: 268400, training_loss: 1.42076e+02
I1111 12:24:18.789746 140264174335808 run_lib.py:153] step: 268450, training_loss: 1.52971e+02
I1111 12:24:29.247525 140264174335808 run_lib.py:153] step: 268500, training_loss: 1.47715e+02
I1111 12:24:39.240840 140264174335808 run_lib.py:153] step: 268550, training_loss: 1.22787e+02
I1111 12:24:49.355908 140264174335808 run_lib.py:153] step: 268600, training_loss: 1.23765e+02
I1111 12:25:00.103662 140264174335808 run_lib.py:153] step: 268650, training_loss: 1.40286e+02
I1111 12:25:10.294691 140264174335808 run_lib.py:153] step: 268700, training_loss: 1.02984e+02
I1111 12:25:19.812139 140264174335808 run_lib.py:153] step: 268750, training_loss: 1.17235e+02
I1111 12:25:29.262859 140264174335808 run_lib.py:153] step: 268800, training_loss: 1.25167e+02
I1111 12:25:39.835907 140264174335808 run_lib.py:153] step: 268850, training_loss: 1.16046e+02
I1111 12:25:50.038945 140264174335808 run_lib.py:153] step: 268900, training_loss: 1.43108e+02
I1111 12:25:59.989102 140264174335808 run_lib.py:153] step: 268950, training_loss: 1.09697e+02
I1111 12:26:09.742936 140264174335808 run_lib.py:153] step: 269000, training_loss: 1.16388e+02
I1111 12:26:19.851296 140264174335808 run_lib.py:153] step: 269050, training_loss: 1.21708e+02
I1111 12:26:30.377918 140264174335808 run_lib.py:153] step: 269100, training_loss: 1.19653e+02
I1111 12:26:40.009705 140264174335808 run_lib.py:153] step: 269150, training_loss: 1.15184e+02
I1111 12:26:50.068627 140264174335808 run_lib.py:153] step: 269200, training_loss: 9.20525e+01
I1111 12:27:00.865718 140264174335808 run_lib.py:153] step: 269250, training_loss: 1.37171e+02
I1111 12:27:10.891232 140264174335808 run_lib.py:153] step: 269300, training_loss: 1.06073e+02
I1111 12:27:20.556271 140264174335808 run_lib.py:153] step: 269350, training_loss: 9.39009e+01
I1111 12:27:30.021232 140264174335808 run_lib.py:153] step: 269400, training_loss: 1.22286e+02
I1111 12:27:40.458480 140264174335808 run_lib.py:153] step: 269450, training_loss: 1.01721e+02
I1111 12:27:50.709090 140264174335808 run_lib.py:153] step: 269500, training_loss: 1.35735e+02
I1111 12:28:01.017270 140264174335808 run_lib.py:153] step: 269550, training_loss: 9.36553e+01
I1111 12:28:10.918146 140264174335808 run_lib.py:153] step: 269600, training_loss: 1.25283e+02
I1111 12:28:21.309083 140264174335808 run_lib.py:153] step: 269650, training_loss: 1.43160e+02
I1111 12:28:31.438205 140264174335808 run_lib.py:153] step: 269700, training_loss: 1.37838e+02
I1111 12:28:42.030753 140264174335808 run_lib.py:153] step: 269750, training_loss: 1.06355e+02
I1111 12:28:52.909079 140264174335808 run_lib.py:153] step: 269800, training_loss: 1.27757e+02
I1111 12:29:03.712103 140264174335808 run_lib.py:153] step: 269850, training_loss: 1.15007e+02
I1111 12:29:13.879884 140264174335808 run_lib.py:153] step: 269900, training_loss: 1.29245e+02
I1111 12:29:24.030742 140264174335808 run_lib.py:153] step: 269950, training_loss: 1.49560e+02
I1111 12:29:34.067281 140264174335808 run_lib.py:153] step: 270000, training_loss: 1.42058e+02
I1111 12:29:34.641116 140264174335808 run_lib.py:166] step: 270000, eval_loss: 1.22439e+02
I1111 12:29:44.595750 140264174335808 run_lib.py:153] step: 270050, training_loss: 1.13509e+02
I1111 12:29:54.473884 140264174335808 run_lib.py:153] step: 270100, training_loss: 1.37329e+02
I1111 12:30:04.259090 140264174335808 run_lib.py:153] step: 270150, training_loss: 1.16798e+02
I1111 12:30:14.522331 140264174335808 run_lib.py:153] step: 270200, training_loss: 1.39472e+02
I1111 12:30:24.723333 140264174335808 run_lib.py:153] step: 270250, training_loss: 1.12420e+02
I1111 12:30:34.617845 140264174335808 run_lib.py:153] step: 270300, training_loss: 1.19999e+02
I1111 12:30:45.384801 140264174335808 run_lib.py:153] step: 270350, training_loss: 1.34296e+02
I1111 12:30:55.235334 140264174335808 run_lib.py:153] step: 270400, training_loss: 1.01103e+02
I1111 12:31:04.762961 140264174335808 run_lib.py:153] step: 270450, training_loss: 1.18233e+02
I1111 12:31:14.594378 140264174335808 run_lib.py:153] step: 270500, training_loss: 1.10112e+02
I1111 12:31:24.806424 140264174335808 run_lib.py:153] step: 270550, training_loss: 1.34373e+02
I1111 12:31:34.984021 140264174335808 run_lib.py:153] step: 270600, training_loss: 1.27028e+02
I1111 12:31:44.494485 140264174335808 run_lib.py:153] step: 270650, training_loss: 1.33189e+02
I1111 12:31:54.412291 140264174335808 run_lib.py:153] step: 270700, training_loss: 1.38229e+02
I1111 12:32:04.500059 140264174335808 run_lib.py:153] step: 270750, training_loss: 1.53892e+02
I1111 12:32:14.551291 140264174335808 run_lib.py:153] step: 270800, training_loss: 1.36726e+02
I1111 12:32:24.501058 140264174335808 run_lib.py:153] step: 270850, training_loss: 1.21346e+02
I1111 12:32:33.936987 140264174335808 run_lib.py:153] step: 270900, training_loss: 1.25306e+02
I1111 12:32:43.325029 140264174335808 run_lib.py:153] step: 270950, training_loss: 1.26005e+02
I1111 12:32:53.340456 140264174335808 run_lib.py:153] step: 271000, training_loss: 1.29834e+02
I1111 12:33:03.127767 140264174335808 run_lib.py:153] step: 271050, training_loss: 1.29512e+02
I1111 12:33:12.353020 140264174335808 run_lib.py:153] step: 271100, training_loss: 1.12987e+02
I1111 12:33:22.313907 140264174335808 run_lib.py:153] step: 271150, training_loss: 1.63772e+02
I1111 12:33:32.270204 140264174335808 run_lib.py:153] step: 271200, training_loss: 1.18015e+02
I1111 12:33:42.323627 140264174335808 run_lib.py:153] step: 271250, training_loss: 1.32496e+02
I1111 12:33:52.122341 140264174335808 run_lib.py:153] step: 271300, training_loss: 1.13183e+02
I1111 12:34:01.991522 140264174335808 run_lib.py:153] step: 271350, training_loss: 1.24699e+02
I1111 12:34:11.872524 140264174335808 run_lib.py:153] step: 271400, training_loss: 1.56344e+02
I1111 12:34:21.952692 140264174335808 run_lib.py:153] step: 271450, training_loss: 1.27979e+02
I1111 12:34:31.677341 140264174335808 run_lib.py:153] step: 271500, training_loss: 1.04738e+02
I1111 12:34:41.746146 140264174335808 run_lib.py:153] step: 271550, training_loss: 1.41344e+02
I1111 12:34:51.448274 140264174335808 run_lib.py:153] step: 271600, training_loss: 1.20324e+02
I1111 12:35:02.002906 140264174335808 run_lib.py:153] step: 271650, training_loss: 1.19264e+02
I1111 12:35:12.083519 140264174335808 run_lib.py:153] step: 271700, training_loss: 1.66411e+02
I1111 12:35:22.203248 140264174335808 run_lib.py:153] step: 271750, training_loss: 1.38492e+02
I1111 12:35:32.516270 140264174335808 run_lib.py:153] step: 271800, training_loss: 1.27625e+02
I1111 12:35:43.099249 140264174335808 run_lib.py:153] step: 271850, training_loss: 1.15612e+02
I1111 12:35:53.894512 140264174335808 run_lib.py:153] step: 271900, training_loss: 1.47396e+02
I1111 12:36:04.840395 140264174335808 run_lib.py:153] step: 271950, training_loss: 9.99670e+01
I1111 12:36:16.018373 140264174335808 run_lib.py:153] step: 272000, training_loss: 1.22503e+02
I1111 12:36:26.660634 140264174335808 run_lib.py:153] step: 272050, training_loss: 1.08284e+02
I1111 12:36:36.587331 140264174335808 run_lib.py:153] step: 272100, training_loss: 1.29131e+02
I1111 12:36:47.445047 140264174335808 run_lib.py:153] step: 272150, training_loss: 1.32305e+02
I1111 12:36:57.388077 140264174335808 run_lib.py:153] step: 272200, training_loss: 1.17144e+02
I1111 12:37:07.867509 140264174335808 run_lib.py:153] step: 272250, training_loss: 1.33134e+02
I1111 12:37:17.579813 140264174335808 run_lib.py:153] step: 272300, training_loss: 1.54267e+02
I1111 12:37:28.110001 140264174335808 run_lib.py:153] step: 272350, training_loss: 1.06885e+02
I1111 12:37:38.985229 140264174335808 run_lib.py:153] step: 272400, training_loss: 1.33222e+02
I1111 12:37:49.404487 140264174335808 run_lib.py:153] step: 272450, training_loss: 1.32946e+02
I1111 12:37:59.576269 140264174335808 run_lib.py:153] step: 272500, training_loss: 1.04045e+02
I1111 12:38:09.502323 140264174335808 run_lib.py:153] step: 272550, training_loss: 1.29341e+02
I1111 12:38:19.091019 140264174335808 run_lib.py:153] step: 272600, training_loss: 1.04530e+02
I1111 12:38:28.624105 140264174335808 run_lib.py:153] step: 272650, training_loss: 1.15426e+02
I1111 12:38:38.640792 140264174335808 run_lib.py:153] step: 272700, training_loss: 1.16415e+02
I1111 12:38:49.102695 140264174335808 run_lib.py:153] step: 272750, training_loss: 1.47439e+02
I1111 12:38:59.647511 140264174335808 run_lib.py:153] step: 272800, training_loss: 1.45521e+02
I1111 12:39:09.485626 140264174335808 run_lib.py:153] step: 272850, training_loss: 1.26121e+02
I1111 12:39:20.153470 140264174335808 run_lib.py:153] step: 272900, training_loss: 1.35086e+02
I1111 12:39:30.392650 140264174335808 run_lib.py:153] step: 272950, training_loss: 1.16498e+02
I1111 12:39:41.113966 140264174335808 run_lib.py:153] step: 273000, training_loss: 1.56261e+02
I1111 12:39:50.846381 140264174335808 run_lib.py:153] step: 273050, training_loss: 1.29928e+02
I1111 12:40:00.495206 140264174335808 run_lib.py:153] step: 273100, training_loss: 1.69825e+02
I1111 12:40:10.160978 140264174335808 run_lib.py:153] step: 273150, training_loss: 1.22557e+02
I1111 12:40:20.157217 140264174335808 run_lib.py:153] step: 273200, training_loss: 1.10743e+02
I1111 12:40:30.356692 140264174335808 run_lib.py:153] step: 273250, training_loss: 1.43412e+02
I1111 12:40:39.809936 140264174335808 run_lib.py:153] step: 273300, training_loss: 1.05322e+02
I1111 12:40:49.529673 140264174335808 run_lib.py:153] step: 273350, training_loss: 1.24242e+02
I1111 12:40:59.066149 140264174335808 run_lib.py:153] step: 273400, training_loss: 1.49689e+02
I1111 12:41:09.333492 140264174335808 run_lib.py:153] step: 273450, training_loss: 1.24375e+02
I1111 12:41:19.324551 140264174335808 run_lib.py:153] step: 273500, training_loss: 1.18849e+02
I1111 12:41:28.631873 140264174335808 run_lib.py:153] step: 273550, training_loss: 1.51635e+02
I1111 12:41:38.780328 140264174335808 run_lib.py:153] step: 273600, training_loss: 1.38412e+02
I1111 12:41:48.273755 140264174335808 run_lib.py:153] step: 273650, training_loss: 1.13550e+02
I1111 12:41:58.069415 140264174335808 run_lib.py:153] step: 273700, training_loss: 1.39815e+02
I1111 12:42:07.601516 140264174335808 run_lib.py:153] step: 273750, training_loss: 1.60528e+02
I1111 12:42:17.200166 140264174335808 run_lib.py:153] step: 273800, training_loss: 8.42417e+01
I1111 12:42:26.541222 140264174335808 run_lib.py:153] step: 273850, training_loss: 1.67767e+02
I1111 12:42:36.103159 140264174335808 run_lib.py:153] step: 273900, training_loss: 1.13917e+02
I1111 12:42:45.524567 140264174335808 run_lib.py:153] step: 273950, training_loss: 1.20252e+02
I1111 12:42:55.077742 140264174335808 run_lib.py:153] step: 274000, training_loss: 1.53588e+02
I1111 12:43:04.981390 140264174335808 run_lib.py:153] step: 274050, training_loss: 1.17451e+02
I1111 12:43:14.700586 140264174335808 run_lib.py:153] step: 274100, training_loss: 1.32559e+02
I1111 12:43:25.416426 140264174335808 run_lib.py:153] step: 274150, training_loss: 1.22988e+02
I1111 12:43:35.523805 140264174335808 run_lib.py:153] step: 274200, training_loss: 1.41635e+02
I1111 12:43:46.083948 140264174335808 run_lib.py:153] step: 274250, training_loss: 1.16627e+02
I1111 12:43:55.384330 140264174335808 run_lib.py:153] step: 274300, training_loss: 1.30982e+02
I1111 12:44:05.475850 140264174335808 run_lib.py:153] step: 274350, training_loss: 1.06836e+02
I1111 12:44:15.180397 140264174335808 run_lib.py:153] step: 274400, training_loss: 1.35219e+02
I1111 12:44:25.402016 140264174335808 run_lib.py:153] step: 274450, training_loss: 1.29862e+02
I1111 12:44:35.818650 140264174335808 run_lib.py:153] step: 274500, training_loss: 1.35559e+02
I1111 12:44:46.005264 140264174335808 run_lib.py:153] step: 274550, training_loss: 1.49059e+02
I1111 12:44:55.675815 140264174335808 run_lib.py:153] step: 274600, training_loss: 1.07141e+02
I1111 12:45:05.385603 140264174335808 run_lib.py:153] step: 274650, training_loss: 1.46426e+02
I1111 12:45:15.559188 140264174335808 run_lib.py:153] step: 274700, training_loss: 1.14595e+02
I1111 12:45:25.245187 140264174335808 run_lib.py:153] step: 274750, training_loss: 1.30501e+02
I1111 12:45:35.487746 140264174335808 run_lib.py:153] step: 274800, training_loss: 1.17490e+02
I1111 12:45:45.516992 140264174335808 run_lib.py:153] step: 274850, training_loss: 1.14330e+02
I1111 12:45:55.913809 140264174335808 run_lib.py:153] step: 274900, training_loss: 1.42350e+02
I1111 12:46:06.730902 140264174335808 run_lib.py:153] step: 274950, training_loss: 1.15624e+02
I1111 12:46:17.025931 140264174335808 run_lib.py:153] step: 275000, training_loss: 1.12492e+02
I1111 12:46:17.162958 140264174335808 run_lib.py:166] step: 275000, eval_loss: 1.51892e+02
I1111 12:46:27.658590 140264174335808 run_lib.py:153] step: 275050, training_loss: 1.26487e+02
I1111 12:46:37.368053 140264174335808 run_lib.py:153] step: 275100, training_loss: 1.36278e+02
I1111 12:46:47.258566 140264174335808 run_lib.py:153] step: 275150, training_loss: 1.49644e+02
I1111 12:46:57.195964 140264174335808 run_lib.py:153] step: 275200, training_loss: 1.42170e+02
I1111 12:47:07.470324 140264174335808 run_lib.py:153] step: 275250, training_loss: 1.02952e+02
I1111 12:47:17.283897 140264174335808 run_lib.py:153] step: 275300, training_loss: 1.47951e+02
I1111 12:47:26.604004 140264174335808 run_lib.py:153] step: 275350, training_loss: 1.18845e+02
I1111 12:47:37.173071 140264174335808 run_lib.py:153] step: 275400, training_loss: 1.23540e+02
I1111 12:47:47.633956 140264174335808 run_lib.py:153] step: 275450, training_loss: 9.55184e+01
I1111 12:47:57.398015 140264174335808 run_lib.py:153] step: 275500, training_loss: 1.28126e+02
I1111 12:48:06.896344 140264174335808 run_lib.py:153] step: 275550, training_loss: 1.10048e+02
I1111 12:48:16.848860 140264174335808 run_lib.py:153] step: 275600, training_loss: 9.66019e+01
I1111 12:48:26.380688 140264174335808 run_lib.py:153] step: 275650, training_loss: 1.35079e+02
I1111 12:48:36.760239 140264174335808 run_lib.py:153] step: 275700, training_loss: 1.39399e+02
I1111 12:48:47.405866 140264174335808 run_lib.py:153] step: 275750, training_loss: 1.06389e+02
I1111 12:48:57.107323 140264174335808 run_lib.py:153] step: 275800, training_loss: 1.49758e+02
I1111 12:49:07.039541 140264174335808 run_lib.py:153] step: 275850, training_loss: 1.04858e+02
I1111 12:49:16.495241 140264174335808 run_lib.py:153] step: 275900, training_loss: 1.32681e+02
I1111 12:49:26.642257 140264174335808 run_lib.py:153] step: 275950, training_loss: 1.58063e+02
I1111 12:49:36.561503 140264174335808 run_lib.py:153] step: 276000, training_loss: 1.25228e+02
I1111 12:49:46.834813 140264174335808 run_lib.py:153] step: 276050, training_loss: 1.47598e+02
I1111 12:49:56.547713 140264174335808 run_lib.py:153] step: 276100, training_loss: 1.50724e+02
I1111 12:50:06.119673 140264174335808 run_lib.py:153] step: 276150, training_loss: 1.42365e+02
I1111 12:50:16.343820 140264174335808 run_lib.py:153] step: 276200, training_loss: 1.34410e+02
I1111 12:50:26.386550 140264174335808 run_lib.py:153] step: 276250, training_loss: 1.27466e+02
I1111 12:50:35.768673 140264174335808 run_lib.py:153] step: 276300, training_loss: 1.32483e+02
I1111 12:50:45.002503 140264174335808 run_lib.py:153] step: 276350, training_loss: 9.88463e+01
I1111 12:50:54.266870 140264174335808 run_lib.py:153] step: 276400, training_loss: 1.24617e+02
I1111 12:51:03.993052 140264174335808 run_lib.py:153] step: 276450, training_loss: 1.28128e+02
I1111 12:51:13.486372 140264174335808 run_lib.py:153] step: 276500, training_loss: 1.21813e+02
I1111 12:51:22.970210 140264174335808 run_lib.py:153] step: 276550, training_loss: 1.47148e+02
I1111 12:51:32.689448 140264174335808 run_lib.py:153] step: 276600, training_loss: 1.32103e+02
I1111 12:51:42.254677 140264174335808 run_lib.py:153] step: 276650, training_loss: 1.16204e+02
I1111 12:51:52.107956 140264174335808 run_lib.py:153] step: 276700, training_loss: 1.15873e+02
I1111 12:52:02.747397 140264174335808 run_lib.py:153] step: 276750, training_loss: 1.40533e+02
I1111 12:52:12.806497 140264174335808 run_lib.py:153] step: 276800, training_loss: 1.45102e+02
I1111 12:52:23.351363 140264174335808 run_lib.py:153] step: 276850, training_loss: 1.21124e+02
I1111 12:52:34.130153 140264174335808 run_lib.py:153] step: 276900, training_loss: 1.34489e+02
I1111 12:52:44.632178 140264174335808 run_lib.py:153] step: 276950, training_loss: 1.05050e+02
I1111 12:52:55.539113 140264174335808 run_lib.py:153] step: 277000, training_loss: 1.20564e+02
I1111 12:53:05.482250 140264174335808 run_lib.py:153] step: 277050, training_loss: 1.18266e+02
I1111 12:53:15.404756 140264174335808 run_lib.py:153] step: 277100, training_loss: 1.19342e+02
I1111 12:53:25.251984 140264174335808 run_lib.py:153] step: 277150, training_loss: 1.35452e+02
I1111 12:53:35.568626 140264174335808 run_lib.py:153] step: 277200, training_loss: 1.26288e+02
I1111 12:53:45.824412 140264174335808 run_lib.py:153] step: 277250, training_loss: 1.11602e+02
I1111 12:53:55.830628 140264174335808 run_lib.py:153] step: 277300, training_loss: 1.33085e+02
I1111 12:54:05.846893 140264174335808 run_lib.py:153] step: 277350, training_loss: 1.20733e+02
I1111 12:54:15.824615 140264174335808 run_lib.py:153] step: 277400, training_loss: 1.63748e+02
I1111 12:54:25.810717 140264174335808 run_lib.py:153] step: 277450, training_loss: 1.28503e+02
I1111 12:54:36.155754 140264174335808 run_lib.py:153] step: 277500, training_loss: 1.21923e+02
I1111 12:54:46.726499 140264174335808 run_lib.py:153] step: 277550, training_loss: 1.14548e+02
I1111 12:54:57.321003 140264174335808 run_lib.py:153] step: 277600, training_loss: 1.19399e+02
I1111 12:55:07.148076 140264174335808 run_lib.py:153] step: 277650, training_loss: 1.34665e+02
I1111 12:55:17.191072 140264174335808 run_lib.py:153] step: 277700, training_loss: 1.51255e+02
I1111 12:55:27.371377 140264174335808 run_lib.py:153] step: 277750, training_loss: 1.20154e+02
I1111 12:55:36.996895 140264174335808 run_lib.py:153] step: 277800, training_loss: 1.11039e+02
I1111 12:55:47.241180 140264174335808 run_lib.py:153] step: 277850, training_loss: 1.09542e+02
I1111 12:55:56.876393 140264174335808 run_lib.py:153] step: 277900, training_loss: 9.25506e+01
I1111 12:56:07.282150 140264174335808 run_lib.py:153] step: 277950, training_loss: 1.24501e+02
I1111 12:56:17.886497 140264174335808 run_lib.py:153] step: 278000, training_loss: 1.40478e+02
I1111 12:56:27.884335 140264174335808 run_lib.py:153] step: 278050, training_loss: 1.31522e+02
I1111 12:56:37.739004 140264174335808 run_lib.py:153] step: 278100, training_loss: 1.21691e+02
I1111 12:56:47.763774 140264174335808 run_lib.py:153] step: 278150, training_loss: 1.37719e+02
I1111 12:56:57.505981 140264174335808 run_lib.py:153] step: 278200, training_loss: 1.24873e+02
I1111 12:57:07.956041 140264174335808 run_lib.py:153] step: 278250, training_loss: 1.52549e+02
I1111 12:57:18.371679 140264174335808 run_lib.py:153] step: 278300, training_loss: 9.76264e+01
I1111 12:57:29.149410 140264174335808 run_lib.py:153] step: 278350, training_loss: 1.46601e+02
I1111 12:57:39.059266 140264174335808 run_lib.py:153] step: 278400, training_loss: 1.22349e+02
I1111 12:57:48.828716 140264174335808 run_lib.py:153] step: 278450, training_loss: 1.11439e+02
I1111 12:57:58.187056 140264174335808 run_lib.py:153] step: 278500, training_loss: 1.38497e+02
I1111 12:58:08.254213 140264174335808 run_lib.py:153] step: 278550, training_loss: 9.68084e+01
I1111 12:58:17.827152 140264174335808 run_lib.py:153] step: 278600, training_loss: 1.13304e+02
I1111 12:58:27.926152 140264174335808 run_lib.py:153] step: 278650, training_loss: 1.20247e+02
I1111 12:58:37.841123 140264174335808 run_lib.py:153] step: 278700, training_loss: 1.39278e+02
I1111 12:58:48.259515 140264174335808 run_lib.py:153] step: 278750, training_loss: 9.13353e+01
I1111 12:58:58.170224 140264174335808 run_lib.py:153] step: 278800, training_loss: 1.03690e+02
I1111 12:59:08.315456 140264174335808 run_lib.py:153] step: 278850, training_loss: 1.02606e+02
I1111 12:59:18.383812 140264174335808 run_lib.py:153] step: 278900, training_loss: 1.38031e+02
I1111 12:59:28.420991 140264174335808 run_lib.py:153] step: 278950, training_loss: 1.27017e+02
I1111 12:59:38.713333 140264174335808 run_lib.py:153] step: 279000, training_loss: 1.28463e+02
I1111 12:59:49.014272 140264174335808 run_lib.py:153] step: 279050, training_loss: 1.39539e+02
I1111 12:59:58.457630 140264174335808 run_lib.py:153] step: 279100, training_loss: 1.13379e+02
I1111 13:00:08.784634 140264174335808 run_lib.py:153] step: 279150, training_loss: 1.46482e+02
I1111 13:00:18.747386 140264174335808 run_lib.py:153] step: 279200, training_loss: 1.11120e+02
I1111 13:00:29.231630 140264174335808 run_lib.py:153] step: 279250, training_loss: 1.28682e+02
I1111 13:00:39.277498 140264174335808 run_lib.py:153] step: 279300, training_loss: 9.26084e+01
I1111 13:00:48.932455 140264174335808 run_lib.py:153] step: 279350, training_loss: 1.35174e+02
I1111 13:00:58.703781 140264174335808 run_lib.py:153] step: 279400, training_loss: 1.11310e+02
I1111 13:01:08.618232 140264174335808 run_lib.py:153] step: 279450, training_loss: 1.40904e+02
I1111 13:01:18.451112 140264174335808 run_lib.py:153] step: 279500, training_loss: 1.34907e+02
I1111 13:01:28.502870 140264174335808 run_lib.py:153] step: 279550, training_loss: 1.28977e+02
I1111 13:01:39.086056 140264174335808 run_lib.py:153] step: 279600, training_loss: 1.37691e+02
I1111 13:01:48.899653 140264174335808 run_lib.py:153] step: 279650, training_loss: 1.15637e+02
I1111 13:01:59.222217 140264174335808 run_lib.py:153] step: 279700, training_loss: 1.42084e+02
I1111 13:02:09.241346 140264174335808 run_lib.py:153] step: 279750, training_loss: 1.09154e+02
I1111 13:02:19.272729 140264174335808 run_lib.py:153] step: 279800, training_loss: 1.58705e+02
I1111 13:02:29.822536 140264174335808 run_lib.py:153] step: 279850, training_loss: 1.24548e+02
I1111 13:02:40.790892 140264174335808 run_lib.py:153] step: 279900, training_loss: 1.40863e+02
I1111 13:02:50.677936 140264174335808 run_lib.py:153] step: 279950, training_loss: 1.22858e+02
I1111 13:03:01.424812 140264174335808 run_lib.py:153] step: 280000, training_loss: 1.39656e+02
I1111 13:03:02.048615 140264174335808 run_lib.py:166] step: 280000, eval_loss: 1.18739e+02
I1111 13:03:12.433001 140264174335808 run_lib.py:153] step: 280050, training_loss: 1.35878e+02
I1111 13:03:22.881101 140264174335808 run_lib.py:153] step: 280100, training_loss: 1.02355e+02
I1111 13:03:33.178394 140264174335808 run_lib.py:153] step: 280150, training_loss: 1.47860e+02
I1111 13:03:42.985300 140264174335808 run_lib.py:153] step: 280200, training_loss: 1.29584e+02
I1111 13:03:52.883726 140264174335808 run_lib.py:153] step: 280250, training_loss: 1.29372e+02
I1111 13:04:02.150610 140264174335808 run_lib.py:153] step: 280300, training_loss: 1.05467e+02
I1111 13:04:12.317796 140264174335808 run_lib.py:153] step: 280350, training_loss: 1.49894e+02
I1111 13:04:22.457752 140264174335808 run_lib.py:153] step: 280400, training_loss: 1.03604e+02
I1111 13:04:32.791400 140264174335808 run_lib.py:153] step: 280450, training_loss: 1.23246e+02
I1111 13:04:42.811671 140264174335808 run_lib.py:153] step: 280500, training_loss: 1.42755e+02
I1111 13:04:52.630110 140264174335808 run_lib.py:153] step: 280550, training_loss: 1.24082e+02
I1111 13:05:03.172977 140264174335808 run_lib.py:153] step: 280600, training_loss: 1.28105e+02
I1111 13:05:13.477840 140264174335808 run_lib.py:153] step: 280650, training_loss: 1.37272e+02
I1111 13:05:23.862594 140264174335808 run_lib.py:153] step: 280700, training_loss: 1.22177e+02
I1111 13:05:34.163577 140264174335808 run_lib.py:153] step: 280750, training_loss: 1.26217e+02
I1111 13:05:44.643857 140264174335808 run_lib.py:153] step: 280800, training_loss: 1.40991e+02
I1111 13:05:54.410826 140264174335808 run_lib.py:153] step: 280850, training_loss: 1.41108e+02
I1111 13:06:04.323613 140264174335808 run_lib.py:153] step: 280900, training_loss: 1.12970e+02
I1111 13:06:14.050155 140264174335808 run_lib.py:153] step: 280950, training_loss: 1.34003e+02
I1111 13:06:23.771421 140264174335808 run_lib.py:153] step: 281000, training_loss: 1.19438e+02
I1111 13:06:33.415380 140264174335808 run_lib.py:153] step: 281050, training_loss: 1.29114e+02
I1111 13:06:43.264165 140264174335808 run_lib.py:153] step: 281100, training_loss: 1.22927e+02
I1111 13:06:52.845120 140264174335808 run_lib.py:153] step: 281150, training_loss: 1.37939e+02
I1111 13:07:03.107992 140264174335808 run_lib.py:153] step: 281200, training_loss: 1.36861e+02
I1111 13:07:13.111578 140264174335808 run_lib.py:153] step: 281250, training_loss: 1.33579e+02
I1111 13:07:22.807836 140264174335808 run_lib.py:153] step: 281300, training_loss: 1.16942e+02
I1111 13:07:32.584103 140264174335808 run_lib.py:153] step: 281350, training_loss: 1.65816e+02
I1111 13:07:42.449369 140264174335808 run_lib.py:153] step: 281400, training_loss: 1.13869e+02
I1111 13:07:53.380120 140264174335808 run_lib.py:153] step: 281450, training_loss: 1.18257e+02
I1111 13:08:03.647199 140264174335808 run_lib.py:153] step: 281500, training_loss: 1.42944e+02
I1111 13:08:13.628081 140264174335808 run_lib.py:153] step: 281550, training_loss: 1.15724e+02
I1111 13:08:23.264177 140264174335808 run_lib.py:153] step: 281600, training_loss: 1.18718e+02
I1111 13:08:32.961354 140264174335808 run_lib.py:153] step: 281650, training_loss: 1.45296e+02
I1111 13:08:42.814580 140264174335808 run_lib.py:153] step: 281700, training_loss: 1.19696e+02
I1111 13:08:52.945925 140264174335808 run_lib.py:153] step: 281750, training_loss: 1.20504e+02
I1111 13:09:03.187192 140264174335808 run_lib.py:153] step: 281800, training_loss: 1.27072e+02
I1111 13:09:12.991283 140264174335808 run_lib.py:153] step: 281850, training_loss: 1.53986e+02
I1111 13:09:22.824983 140264174335808 run_lib.py:153] step: 281900, training_loss: 1.13317e+02
I1111 13:09:32.317460 140264174335808 run_lib.py:153] step: 281950, training_loss: 1.28892e+02
I1111 13:09:42.128712 140264174335808 run_lib.py:153] step: 282000, training_loss: 1.43842e+02
I1111 13:09:51.557827 140264174335808 run_lib.py:153] step: 282050, training_loss: 1.57122e+02
I1111 13:10:01.009319 140264174335808 run_lib.py:153] step: 282100, training_loss: 1.65381e+02
I1111 13:10:11.525456 140264174335808 run_lib.py:153] step: 282150, training_loss: 1.37041e+02
I1111 13:10:21.811787 140264174335808 run_lib.py:153] step: 282200, training_loss: 1.11713e+02
I1111 13:10:32.205297 140264174335808 run_lib.py:153] step: 282250, training_loss: 1.12427e+02
I1111 13:10:41.997604 140264174335808 run_lib.py:153] step: 282300, training_loss: 1.13347e+02
I1111 13:10:52.204365 140264174335808 run_lib.py:153] step: 282350, training_loss: 9.48749e+01
I1111 13:11:02.908289 140264174335808 run_lib.py:153] step: 282400, training_loss: 1.44506e+02
I1111 13:11:12.396695 140264174335808 run_lib.py:153] step: 282450, training_loss: 9.58226e+01
I1111 13:11:22.273849 140264174335808 run_lib.py:153] step: 282500, training_loss: 9.85899e+01
I1111 13:11:32.376342 140264174335808 run_lib.py:153] step: 282550, training_loss: 1.21073e+02
I1111 13:11:41.965336 140264174335808 run_lib.py:153] step: 282600, training_loss: 1.22179e+02
I1111 13:11:51.724644 140264174335808 run_lib.py:153] step: 282650, training_loss: 1.40530e+02
I1111 13:12:01.076030 140264174335808 run_lib.py:153] step: 282700, training_loss: 1.27647e+02
I1111 13:12:10.853314 140264174335808 run_lib.py:153] step: 282750, training_loss: 1.47647e+02
I1111 13:12:20.799480 140264174335808 run_lib.py:153] step: 282800, training_loss: 9.03844e+01
I1111 13:12:31.143440 140264174335808 run_lib.py:153] step: 282850, training_loss: 9.95645e+01
I1111 13:12:41.226955 140264174335808 run_lib.py:153] step: 282900, training_loss: 1.27541e+02
I1111 13:12:51.008658 140264174335808 run_lib.py:153] step: 282950, training_loss: 1.05475e+02
I1111 13:13:00.983622 140264174335808 run_lib.py:153] step: 283000, training_loss: 1.24903e+02
I1111 13:13:10.517949 140264174335808 run_lib.py:153] step: 283050, training_loss: 1.35012e+02
I1111 13:13:20.199274 140264174335808 run_lib.py:153] step: 283100, training_loss: 1.56212e+02
I1111 13:13:30.377007 140264174335808 run_lib.py:153] step: 283150, training_loss: 1.20336e+02
I1111 13:13:40.221061 140264174335808 run_lib.py:153] step: 283200, training_loss: 1.41542e+02
I1111 13:13:49.446221 140264174335808 run_lib.py:153] step: 283250, training_loss: 1.47071e+02
I1111 13:13:59.332775 140264174335808 run_lib.py:153] step: 283300, training_loss: 1.41006e+02
I1111 13:14:09.134922 140264174335808 run_lib.py:153] step: 283350, training_loss: 1.29916e+02
I1111 13:14:19.297042 140264174335808 run_lib.py:153] step: 283400, training_loss: 1.11142e+02
I1111 13:14:29.242674 140264174335808 run_lib.py:153] step: 283450, training_loss: 1.06022e+02
I1111 13:14:38.643015 140264174335808 run_lib.py:153] step: 283500, training_loss: 1.12203e+02
I1111 13:14:48.525942 140264174335808 run_lib.py:153] step: 283550, training_loss: 1.35075e+02
I1111 13:14:59.780445 140264174335808 run_lib.py:153] step: 283600, training_loss: 1.30103e+02
I1111 13:15:09.342875 140264174335808 run_lib.py:153] step: 283650, training_loss: 1.16158e+02
I1111 13:15:19.930605 140264174335808 run_lib.py:153] step: 283700, training_loss: 1.21708e+02
I1111 13:15:30.104642 140264174335808 run_lib.py:153] step: 283750, training_loss: 1.27385e+02
I1111 13:15:40.127364 140264174335808 run_lib.py:153] step: 283800, training_loss: 1.25261e+02
I1111 13:15:50.317108 140264174335808 run_lib.py:153] step: 283850, training_loss: 1.07375e+02
I1111 13:16:00.914523 140264174335808 run_lib.py:153] step: 283900, training_loss: 1.01717e+02
I1111 13:16:10.555955 140264174335808 run_lib.py:153] step: 283950, training_loss: 1.24639e+02
I1111 13:16:20.672369 140264174335808 run_lib.py:153] step: 284000, training_loss: 1.24672e+02
I1111 13:16:30.248137 140264174335808 run_lib.py:153] step: 284050, training_loss: 1.10415e+02
I1111 13:16:39.808849 140264174335808 run_lib.py:153] step: 284100, training_loss: 1.18584e+02
I1111 13:16:49.897604 140264174335808 run_lib.py:153] step: 284150, training_loss: 1.37179e+02
I1111 13:17:00.342402 140264174335808 run_lib.py:153] step: 284200, training_loss: 1.23153e+02
I1111 13:17:10.035544 140264174335808 run_lib.py:153] step: 284250, training_loss: 1.46771e+02
I1111 13:17:20.006872 140264174335808 run_lib.py:153] step: 284300, training_loss: 1.58994e+02
I1111 13:17:30.105386 140264174335808 run_lib.py:153] step: 284350, training_loss: 1.30625e+02
I1111 13:17:40.710923 140264174335808 run_lib.py:153] step: 284400, training_loss: 1.02572e+02
I1111 13:17:50.171648 140264174335808 run_lib.py:153] step: 284450, training_loss: 1.13125e+02
I1111 13:18:00.061704 140264174335808 run_lib.py:153] step: 284500, training_loss: 1.44322e+02
I1111 13:18:09.425262 140264174335808 run_lib.py:153] step: 284550, training_loss: 1.29191e+02
I1111 13:18:20.226794 140264174335808 run_lib.py:153] step: 284600, training_loss: 1.05765e+02
I1111 13:18:29.847697 140264174335808 run_lib.py:153] step: 284650, training_loss: 1.16186e+02
I1111 13:18:40.195771 140264174335808 run_lib.py:153] step: 284700, training_loss: 1.44202e+02
I1111 13:18:50.270117 140264174335808 run_lib.py:153] step: 284750, training_loss: 1.39543e+02
I1111 13:18:59.954746 140264174335808 run_lib.py:153] step: 284800, training_loss: 1.35697e+02
I1111 13:19:10.068682 140264174335808 run_lib.py:153] step: 284850, training_loss: 1.21992e+02
I1111 13:19:19.974995 140264174335808 run_lib.py:153] step: 284900, training_loss: 1.24070e+02
I1111 13:19:30.202634 140264174335808 run_lib.py:153] step: 284950, training_loss: 1.22679e+02
I1111 13:19:40.724572 140264174335808 run_lib.py:153] step: 285000, training_loss: 1.26212e+02
I1111 13:19:40.825897 140264174335808 run_lib.py:166] step: 285000, eval_loss: 1.08812e+02
I1111 13:19:50.716313 140264174335808 run_lib.py:153] step: 285050, training_loss: 1.84446e+02
I1111 13:20:00.931845 140264174335808 run_lib.py:153] step: 285100, training_loss: 1.18062e+02
I1111 13:20:10.994128 140264174335808 run_lib.py:153] step: 285150, training_loss: 1.20550e+02
I1111 13:20:21.759464 140264174335808 run_lib.py:153] step: 285200, training_loss: 1.31885e+02
I1111 13:20:32.379435 140264174335808 run_lib.py:153] step: 285250, training_loss: 1.10310e+02
I1111 13:20:42.392638 140264174335808 run_lib.py:153] step: 285300, training_loss: 1.04665e+02
I1111 13:20:51.750798 140264174335808 run_lib.py:153] step: 285350, training_loss: 1.22105e+02
I1111 13:21:01.655394 140264174335808 run_lib.py:153] step: 285400, training_loss: 1.19831e+02
I1111 13:21:11.558258 140264174335808 run_lib.py:153] step: 285450, training_loss: 1.48782e+02
I1111 13:21:21.292859 140264174335808 run_lib.py:153] step: 285500, training_loss: 1.42429e+02
I1111 13:21:30.871922 140264174335808 run_lib.py:153] step: 285550, training_loss: 1.34093e+02
I1111 13:21:41.018688 140264174335808 run_lib.py:153] step: 285600, training_loss: 1.11391e+02
I1111 13:21:50.457979 140264174335808 run_lib.py:153] step: 285650, training_loss: 1.24675e+02
I1111 13:22:00.054834 140264174335808 run_lib.py:153] step: 285700, training_loss: 1.15711e+02
I1111 13:22:10.096497 140264174335808 run_lib.py:153] step: 285750, training_loss: 1.26399e+02
I1111 13:22:19.662348 140264174335808 run_lib.py:153] step: 285800, training_loss: 1.12006e+02
I1111 13:22:29.588609 140264174335808 run_lib.py:153] step: 285850, training_loss: 1.38908e+02
I1111 13:22:38.880673 140264174335808 run_lib.py:153] step: 285900, training_loss: 1.04098e+02
I1111 13:22:48.734529 140264174335808 run_lib.py:153] step: 285950, training_loss: 1.18294e+02
I1111 13:22:58.690449 140264174335808 run_lib.py:153] step: 286000, training_loss: 1.16436e+02
I1111 13:23:08.509926 140264174335808 run_lib.py:153] step: 286050, training_loss: 1.17424e+02
I1111 13:23:18.789292 140264174335808 run_lib.py:153] step: 286100, training_loss: 1.31634e+02
I1111 13:23:29.295997 140264174335808 run_lib.py:153] step: 286150, training_loss: 1.06322e+02
I1111 13:23:39.423958 140264174335808 run_lib.py:153] step: 286200, training_loss: 1.43089e+02
I1111 13:23:50.314138 140264174335808 run_lib.py:153] step: 286250, training_loss: 1.66772e+02
I1111 13:24:00.456104 140264174335808 run_lib.py:153] step: 286300, training_loss: 1.32320e+02
I1111 13:24:10.764807 140264174335808 run_lib.py:153] step: 286350, training_loss: 1.23409e+02
I1111 13:24:20.639782 140264174335808 run_lib.py:153] step: 286400, training_loss: 1.18918e+02
I1111 13:24:30.644093 140264174335808 run_lib.py:153] step: 286450, training_loss: 1.07040e+02
I1111 13:24:40.723382 140264174335808 run_lib.py:153] step: 286500, training_loss: 1.26642e+02
I1111 13:24:50.772757 140264174335808 run_lib.py:153] step: 286550, training_loss: 1.57698e+02
I1111 13:25:00.806985 140264174335808 run_lib.py:153] step: 286600, training_loss: 1.27300e+02
I1111 13:25:10.866069 140264174335808 run_lib.py:153] step: 286650, training_loss: 1.19563e+02
I1111 13:25:20.366953 140264174335808 run_lib.py:153] step: 286700, training_loss: 1.30458e+02
I1111 13:25:30.075285 140264174335808 run_lib.py:153] step: 286750, training_loss: 1.46930e+02
I1111 13:25:40.109231 140264174335808 run_lib.py:153] step: 286800, training_loss: 9.20640e+01
I1111 13:25:50.231384 140264174335808 run_lib.py:153] step: 286850, training_loss: 1.17645e+02
I1111 13:26:00.726382 140264174335808 run_lib.py:153] step: 286900, training_loss: 1.24085e+02
I1111 13:26:10.831236 140264174335808 run_lib.py:153] step: 286950, training_loss: 1.36928e+02
I1111 13:26:20.561979 140264174335808 run_lib.py:153] step: 287000, training_loss: 1.33291e+02
I1111 13:26:30.770465 140264174335808 run_lib.py:153] step: 287050, training_loss: 1.57816e+02
I1111 13:26:40.471090 140264174335808 run_lib.py:153] step: 287100, training_loss: 1.10370e+02
I1111 13:26:50.500027 140264174335808 run_lib.py:153] step: 287150, training_loss: 1.17969e+02
I1111 13:27:00.286175 140264174335808 run_lib.py:153] step: 287200, training_loss: 1.13297e+02
I1111 13:27:09.683611 140264174335808 run_lib.py:153] step: 287250, training_loss: 1.38374e+02
I1111 13:27:19.402673 140264174335808 run_lib.py:153] step: 287300, training_loss: 8.16665e+01
I1111 13:27:29.304788 140264174335808 run_lib.py:153] step: 287350, training_loss: 1.57687e+02
I1111 13:27:38.601105 140264174335808 run_lib.py:153] step: 287400, training_loss: 9.85239e+01
I1111 13:27:47.858504 140264174335808 run_lib.py:153] step: 287450, training_loss: 1.44819e+02
I1111 13:27:57.939803 140264174335808 run_lib.py:153] step: 287500, training_loss: 1.31647e+02
I1111 13:28:08.024915 140264174335808 run_lib.py:153] step: 287550, training_loss: 1.44859e+02
I1111 13:28:18.320511 140264174335808 run_lib.py:153] step: 287600, training_loss: 9.30501e+01
I1111 13:28:28.550554 140264174335808 run_lib.py:153] step: 287650, training_loss: 1.11579e+02
I1111 13:28:39.289966 140264174335808 run_lib.py:153] step: 287700, training_loss: 1.32370e+02
I1111 13:28:50.157063 140264174335808 run_lib.py:153] step: 287750, training_loss: 1.30048e+02
I1111 13:29:00.615852 140264174335808 run_lib.py:153] step: 287800, training_loss: 1.14236e+02
I1111 13:29:10.609185 140264174335808 run_lib.py:153] step: 287850, training_loss: 1.28578e+02
I1111 13:29:20.471836 140264174335808 run_lib.py:153] step: 287900, training_loss: 1.29923e+02
I1111 13:29:30.259373 140264174335808 run_lib.py:153] step: 287950, training_loss: 9.94870e+01
I1111 13:29:40.694658 140264174335808 run_lib.py:153] step: 288000, training_loss: 1.67686e+02
I1111 13:29:51.177896 140264174335808 run_lib.py:153] step: 288050, training_loss: 1.29428e+02
I1111 13:30:00.782272 140264174335808 run_lib.py:153] step: 288100, training_loss: 1.24331e+02
I1111 13:30:10.584551 140264174335808 run_lib.py:153] step: 288150, training_loss: 1.42673e+02
I1111 13:30:20.721335 140264174335808 run_lib.py:153] step: 288200, training_loss: 1.35318e+02
I1111 13:30:31.062489 140264174335808 run_lib.py:153] step: 288250, training_loss: 1.02574e+02
I1111 13:30:41.105426 140264174335808 run_lib.py:153] step: 288300, training_loss: 1.07392e+02
I1111 13:30:51.646712 140264174335808 run_lib.py:153] step: 288350, training_loss: 1.10974e+02
I1111 13:31:01.642167 140264174335808 run_lib.py:153] step: 288400, training_loss: 1.35794e+02
I1111 13:31:11.720973 140264174335808 run_lib.py:153] step: 288450, training_loss: 1.24756e+02
I1111 13:31:21.225169 140264174335808 run_lib.py:153] step: 288500, training_loss: 1.10929e+02
I1111 13:31:31.381285 140264174335808 run_lib.py:153] step: 288550, training_loss: 1.42708e+02
I1111 13:31:40.943874 140264174335808 run_lib.py:153] step: 288600, training_loss: 1.39312e+02
I1111 13:31:50.839456 140264174335808 run_lib.py:153] step: 288650, training_loss: 1.28963e+02
I1111 13:32:00.912709 140264174335808 run_lib.py:153] step: 288700, training_loss: 1.35920e+02
I1111 13:32:10.454190 140264174335808 run_lib.py:153] step: 288750, training_loss: 1.48068e+02
I1111 13:32:20.497014 140264174335808 run_lib.py:153] step: 288800, training_loss: 1.30767e+02
I1111 13:32:30.541683 140264174335808 run_lib.py:153] step: 288850, training_loss: 1.30032e+02
I1111 13:32:40.733635 140264174335808 run_lib.py:153] step: 288900, training_loss: 1.03403e+02
I1111 13:32:50.117870 140264174335808 run_lib.py:153] step: 288950, training_loss: 1.48888e+02
I1111 13:33:00.813549 140264174335808 run_lib.py:153] step: 289000, training_loss: 1.13691e+02
I1111 13:33:11.021199 140264174335808 run_lib.py:153] step: 289050, training_loss: 1.11569e+02
I1111 13:33:21.429311 140264174335808 run_lib.py:153] step: 289100, training_loss: 1.15264e+02
I1111 13:33:31.420559 140264174335808 run_lib.py:153] step: 289150, training_loss: 1.03970e+02
I1111 13:33:41.289556 140264174335808 run_lib.py:153] step: 289200, training_loss: 1.03140e+02
I1111 13:33:51.088481 140264174335808 run_lib.py:153] step: 289250, training_loss: 1.36652e+02
I1111 13:34:01.056201 140264174335808 run_lib.py:153] step: 289300, training_loss: 1.27848e+02
I1111 13:34:11.198317 140264174335808 run_lib.py:153] step: 289350, training_loss: 1.19793e+02
I1111 13:34:21.072895 140264174335808 run_lib.py:153] step: 289400, training_loss: 1.21166e+02
I1111 13:34:30.468545 140264174335808 run_lib.py:153] step: 289450, training_loss: 1.22953e+02
I1111 13:34:40.188747 140264174335808 run_lib.py:153] step: 289500, training_loss: 1.17498e+02
I1111 13:34:49.572414 140264174335808 run_lib.py:153] step: 289550, training_loss: 1.24677e+02
I1111 13:34:59.917310 140264174335808 run_lib.py:153] step: 289600, training_loss: 1.42747e+02
I1111 13:35:09.547997 140264174335808 run_lib.py:153] step: 289650, training_loss: 1.06837e+02
I1111 13:35:19.544643 140264174335808 run_lib.py:153] step: 289700, training_loss: 1.29191e+02
I1111 13:35:28.993618 140264174335808 run_lib.py:153] step: 289750, training_loss: 1.30839e+02
I1111 13:35:38.563726 140264174335808 run_lib.py:153] step: 289800, training_loss: 1.19033e+02
I1111 13:35:48.274142 140264174335808 run_lib.py:153] step: 289850, training_loss: 1.26643e+02
I1111 13:35:58.866353 140264174335808 run_lib.py:153] step: 289900, training_loss: 1.41744e+02
I1111 13:36:08.897605 140264174335808 run_lib.py:153] step: 289950, training_loss: 1.47965e+02
I1111 13:36:18.159609 140264174335808 run_lib.py:153] step: 290000, training_loss: 1.00262e+02
I1111 13:36:18.744706 140264174335808 run_lib.py:166] step: 290000, eval_loss: 1.22172e+02
I1111 13:36:28.239516 140264174335808 run_lib.py:153] step: 290050, training_loss: 1.27997e+02
I1111 13:36:38.134263 140264174335808 run_lib.py:153] step: 290100, training_loss: 9.20830e+01
I1111 13:36:47.975217 140264174335808 run_lib.py:153] step: 290150, training_loss: 1.31433e+02
I1111 13:36:58.147672 140264174335808 run_lib.py:153] step: 290200, training_loss: 1.03334e+02
I1111 13:37:08.310046 140264174335808 run_lib.py:153] step: 290250, training_loss: 1.45808e+02
I1111 13:37:17.854042 140264174335808 run_lib.py:153] step: 290300, training_loss: 1.31904e+02
I1111 13:37:27.717517 140264174335808 run_lib.py:153] step: 290350, training_loss: 1.14289e+02
I1111 13:37:37.846094 140264174335808 run_lib.py:153] step: 290400, training_loss: 1.56583e+02
I1111 13:37:47.350664 140264174335808 run_lib.py:153] step: 290450, training_loss: 1.41611e+02
I1111 13:37:56.978627 140264174335808 run_lib.py:153] step: 290500, training_loss: 1.20084e+02
I1111 13:38:06.975792 140264174335808 run_lib.py:153] step: 290550, training_loss: 1.62027e+02
I1111 13:38:17.256366 140264174335808 run_lib.py:153] step: 290600, training_loss: 1.50094e+02
I1111 13:38:27.443687 140264174335808 run_lib.py:153] step: 290650, training_loss: 1.23513e+02
I1111 13:38:37.101372 140264174335808 run_lib.py:153] step: 290700, training_loss: 1.17047e+02
I1111 13:38:46.964416 140264174335808 run_lib.py:153] step: 290750, training_loss: 1.10134e+02
I1111 13:38:56.896950 140264174335808 run_lib.py:153] step: 290800, training_loss: 1.20124e+02
I1111 13:39:06.740527 140264174335808 run_lib.py:153] step: 290850, training_loss: 1.27423e+02
I1111 13:39:16.304066 140264174335808 run_lib.py:153] step: 290900, training_loss: 1.31919e+02
I1111 13:39:26.397572 140264174335808 run_lib.py:153] step: 290950, training_loss: 1.04976e+02
I1111 13:39:36.612410 140264174335808 run_lib.py:153] step: 291000, training_loss: 1.38025e+02
I1111 13:39:46.292312 140264174335808 run_lib.py:153] step: 291050, training_loss: 1.23562e+02
I1111 13:39:55.990885 140264174335808 run_lib.py:153] step: 291100, training_loss: 1.19566e+02
I1111 13:40:06.586393 140264174335808 run_lib.py:153] step: 291150, training_loss: 1.59019e+02
I1111 13:40:16.829882 140264174335808 run_lib.py:153] step: 291200, training_loss: 1.29982e+02
I1111 13:40:26.240974 140264174335808 run_lib.py:153] step: 291250, training_loss: 9.32150e+01
I1111 13:40:36.133032 140264174335808 run_lib.py:153] step: 291300, training_loss: 1.14488e+02
I1111 13:40:45.488930 140264174335808 run_lib.py:153] step: 291350, training_loss: 1.28230e+02
I1111 13:40:55.012686 140264174335808 run_lib.py:153] step: 291400, training_loss: 1.21149e+02
I1111 13:41:04.279631 140264174335808 run_lib.py:153] step: 291450, training_loss: 1.46240e+02
I1111 13:41:13.540997 140264174335808 run_lib.py:153] step: 291500, training_loss: 1.41610e+02
I1111 13:41:22.873297 140264174335808 run_lib.py:153] step: 291550, training_loss: 1.56871e+02
I1111 13:41:32.383330 140264174335808 run_lib.py:153] step: 291600, training_loss: 1.05931e+02
I1111 13:41:42.619184 140264174335808 run_lib.py:153] step: 291650, training_loss: 1.51600e+02
I1111 13:41:52.296427 140264174335808 run_lib.py:153] step: 291700, training_loss: 1.00474e+02
I1111 13:42:01.861171 140264174335808 run_lib.py:153] step: 291750, training_loss: 1.33110e+02
I1111 13:42:11.901829 140264174335808 run_lib.py:153] step: 291800, training_loss: 1.10764e+02
I1111 13:42:21.694893 140264174335808 run_lib.py:153] step: 291850, training_loss: 1.39500e+02
I1111 13:42:31.719959 140264174335808 run_lib.py:153] step: 291900, training_loss: 1.18567e+02
I1111 13:42:42.042317 140264174335808 run_lib.py:153] step: 291950, training_loss: 1.32399e+02
I1111 13:42:52.064241 140264174335808 run_lib.py:153] step: 292000, training_loss: 1.07133e+02
I1111 13:43:02.146075 140264174335808 run_lib.py:153] step: 292050, training_loss: 1.16254e+02
I1111 13:43:12.425318 140264174335808 run_lib.py:153] step: 292100, training_loss: 1.46724e+02
I1111 13:43:22.900076 140264174335808 run_lib.py:153] step: 292150, training_loss: 1.41928e+02
I1111 13:43:32.733004 140264174335808 run_lib.py:153] step: 292200, training_loss: 1.13517e+02
I1111 13:43:42.453437 140264174335808 run_lib.py:153] step: 292250, training_loss: 1.32543e+02
I1111 13:43:52.753171 140264174335808 run_lib.py:153] step: 292300, training_loss: 1.29099e+02
I1111 13:44:02.714744 140264174335808 run_lib.py:153] step: 292350, training_loss: 1.38181e+02
I1111 13:44:13.110611 140264174335808 run_lib.py:153] step: 292400, training_loss: 1.39216e+02
I1111 13:44:23.328416 140264174335808 run_lib.py:153] step: 292450, training_loss: 1.26722e+02
I1111 13:44:33.680423 140264174335808 run_lib.py:153] step: 292500, training_loss: 1.46162e+02
I1111 13:44:44.008786 140264174335808 run_lib.py:153] step: 292550, training_loss: 1.19069e+02
I1111 13:44:54.161499 140264174335808 run_lib.py:153] step: 292600, training_loss: 1.19209e+02
I1111 13:45:03.366043 140264174335808 run_lib.py:153] step: 292650, training_loss: 1.20710e+02
I1111 13:45:13.216362 140264174335808 run_lib.py:153] step: 292700, training_loss: 9.59873e+01
I1111 13:45:23.048053 140264174335808 run_lib.py:153] step: 292750, training_loss: 1.25072e+02
I1111 13:45:32.896865 140264174335808 run_lib.py:153] step: 292800, training_loss: 1.24318e+02
I1111 13:45:42.409053 140264174335808 run_lib.py:153] step: 292850, training_loss: 1.34606e+02
I1111 13:45:51.990160 140264174335808 run_lib.py:153] step: 292900, training_loss: 1.26733e+02
I1111 13:46:01.940286 140264174335808 run_lib.py:153] step: 292950, training_loss: 1.14140e+02
I1111 13:46:11.953110 140264174335808 run_lib.py:153] step: 293000, training_loss: 1.19476e+02
I1111 13:46:23.045076 140264174335808 run_lib.py:153] step: 293050, training_loss: 1.06296e+02
I1111 13:46:32.761230 140264174335808 run_lib.py:153] step: 293100, training_loss: 1.05453e+02
I1111 13:46:42.026042 140264174335808 run_lib.py:153] step: 293150, training_loss: 1.38675e+02
I1111 13:46:51.931437 140264174335808 run_lib.py:153] step: 293200, training_loss: 1.13924e+02
I1111 13:47:02.320322 140264174335808 run_lib.py:153] step: 293250, training_loss: 1.10970e+02
I1111 13:47:12.607744 140264174335808 run_lib.py:153] step: 293300, training_loss: 1.23164e+02
I1111 13:47:22.813868 140264174335808 run_lib.py:153] step: 293350, training_loss: 1.27914e+02
I1111 13:47:32.424744 140264174335808 run_lib.py:153] step: 293400, training_loss: 1.20435e+02
I1111 13:47:42.546497 140264174335808 run_lib.py:153] step: 293450, training_loss: 1.27752e+02
I1111 13:47:52.594906 140264174335808 run_lib.py:153] step: 293500, training_loss: 1.15261e+02
I1111 13:48:02.348866 140264174335808 run_lib.py:153] step: 293550, training_loss: 1.12696e+02
I1111 13:48:12.625035 140264174335808 run_lib.py:153] step: 293600, training_loss: 1.07733e+02
I1111 13:48:23.483315 140264174335808 run_lib.py:153] step: 293650, training_loss: 1.64960e+02
I1111 13:48:33.618336 140264174335808 run_lib.py:153] step: 293700, training_loss: 1.61251e+02
I1111 13:48:43.345508 140264174335808 run_lib.py:153] step: 293750, training_loss: 1.46943e+02
I1111 13:48:53.401987 140264174335808 run_lib.py:153] step: 293800, training_loss: 1.47414e+02
I1111 13:49:03.519040 140264174335808 run_lib.py:153] step: 293850, training_loss: 1.19441e+02
I1111 13:49:13.821484 140264174335808 run_lib.py:153] step: 293900, training_loss: 1.24330e+02
I1111 13:49:24.247340 140264174335808 run_lib.py:153] step: 293950, training_loss: 1.04727e+02
I1111 13:49:33.926457 140264174335808 run_lib.py:153] step: 294000, training_loss: 1.08840e+02
I1111 13:49:43.450928 140264174335808 run_lib.py:153] step: 294050, training_loss: 1.09329e+02
I1111 13:49:53.557901 140264174335808 run_lib.py:153] step: 294100, training_loss: 1.06878e+02
I1111 13:50:03.859432 140264174335808 run_lib.py:153] step: 294150, training_loss: 1.30492e+02
I1111 13:50:13.396622 140264174335808 run_lib.py:153] step: 294200, training_loss: 1.19921e+02
I1111 13:50:22.819424 140264174335808 run_lib.py:153] step: 294250, training_loss: 1.42008e+02
I1111 13:50:32.924770 140264174335808 run_lib.py:153] step: 294300, training_loss: 1.20429e+02
I1111 13:50:43.087324 140264174335808 run_lib.py:153] step: 294350, training_loss: 1.21762e+02
I1111 13:50:53.293745 140264174335808 run_lib.py:153] step: 294400, training_loss: 1.46605e+02
I1111 13:51:03.388239 140264174335808 run_lib.py:153] step: 294450, training_loss: 1.26956e+02
I1111 13:51:13.468515 140264174335808 run_lib.py:153] step: 294500, training_loss: 1.16578e+02
I1111 13:51:23.437835 140264174335808 run_lib.py:153] step: 294550, training_loss: 1.31815e+02
I1111 13:51:33.662737 140264174335808 run_lib.py:153] step: 294600, training_loss: 1.66683e+02
I1111 13:51:43.283277 140264174335808 run_lib.py:153] step: 294650, training_loss: 9.77171e+01
I1111 13:51:53.103018 140264174335808 run_lib.py:153] step: 294700, training_loss: 9.63597e+01
I1111 13:52:03.378891 140264174335808 run_lib.py:153] step: 294750, training_loss: 1.42414e+02
I1111 13:52:13.471601 140264174335808 run_lib.py:153] step: 294800, training_loss: 1.30158e+02
I1111 13:52:23.412207 140264174335808 run_lib.py:153] step: 294850, training_loss: 1.25851e+02
I1111 13:52:33.114257 140264174335808 run_lib.py:153] step: 294900, training_loss: 1.36594e+02
I1111 13:52:43.651686 140264174335808 run_lib.py:153] step: 294950, training_loss: 1.07598e+02
I1111 13:52:53.627202 140264174335808 run_lib.py:153] step: 295000, training_loss: 1.31135e+02
I1111 13:52:53.774063 140264174335808 run_lib.py:166] step: 295000, eval_loss: 1.36527e+02
I1111 13:53:04.286838 140264174335808 run_lib.py:153] step: 295050, training_loss: 8.41691e+01
I1111 13:53:15.369891 140264174335808 run_lib.py:153] step: 295100, training_loss: 1.52636e+02
I1111 13:53:25.673805 140264174335808 run_lib.py:153] step: 295150, training_loss: 9.20876e+01
I1111 13:53:36.238063 140264174335808 run_lib.py:153] step: 295200, training_loss: 1.36390e+02
I1111 13:53:47.048504 140264174335808 run_lib.py:153] step: 295250, training_loss: 1.66680e+02
I1111 13:53:56.617406 140264174335808 run_lib.py:153] step: 295300, training_loss: 1.45267e+02
I1111 13:54:06.527469 140264174335808 run_lib.py:153] step: 295350, training_loss: 1.58480e+02
I1111 13:54:16.301611 140264174335808 run_lib.py:153] step: 295400, training_loss: 1.18807e+02
I1111 13:54:25.985911 140264174335808 run_lib.py:153] step: 295450, training_loss: 1.41796e+02
I1111 13:54:35.965270 140264174335808 run_lib.py:153] step: 295500, training_loss: 1.28162e+02
I1111 13:54:45.871900 140264174335808 run_lib.py:153] step: 295550, training_loss: 1.32897e+02
I1111 13:54:56.081723 140264174335808 run_lib.py:153] step: 295600, training_loss: 1.31710e+02
I1111 13:55:06.340583 140264174335808 run_lib.py:153] step: 295650, training_loss: 1.13032e+02
I1111 13:55:16.271776 140264174335808 run_lib.py:153] step: 295700, training_loss: 1.59266e+02
I1111 13:55:26.150861 140264174335808 run_lib.py:153] step: 295750, training_loss: 1.19368e+02
I1111 13:55:36.330703 140264174335808 run_lib.py:153] step: 295800, training_loss: 1.43318e+02
I1111 13:55:46.753292 140264174335808 run_lib.py:153] step: 295850, training_loss: 9.04149e+01
I1111 13:55:57.219874 140264174335808 run_lib.py:153] step: 295900, training_loss: 1.26871e+02
I1111 13:56:06.961166 140264174335808 run_lib.py:153] step: 295950, training_loss: 1.39088e+02
I1111 13:56:16.877690 140264174335808 run_lib.py:153] step: 296000, training_loss: 1.33489e+02
I1111 13:56:26.731369 140264174335808 run_lib.py:153] step: 296050, training_loss: 1.12601e+02
I1111 13:56:36.852655 140264174335808 run_lib.py:153] step: 296100, training_loss: 1.51727e+02
I1111 13:56:47.001259 140264174335808 run_lib.py:153] step: 296150, training_loss: 1.53193e+02
I1111 13:56:56.940614 140264174335808 run_lib.py:153] step: 296200, training_loss: 1.15294e+02
I1111 13:57:06.438299 140264174335808 run_lib.py:153] step: 296250, training_loss: 1.34851e+02
I1111 13:57:16.954876 140264174335808 run_lib.py:153] step: 296300, training_loss: 1.28315e+02
I1111 13:57:27.275909 140264174335808 run_lib.py:153] step: 296350, training_loss: 1.36573e+02
I1111 13:57:37.728842 140264174335808 run_lib.py:153] step: 296400, training_loss: 1.28694e+02
I1111 13:57:47.721957 140264174335808 run_lib.py:153] step: 296450, training_loss: 1.15560e+02
I1111 13:57:57.694569 140264174335808 run_lib.py:153] step: 296500, training_loss: 1.14048e+02
I1111 13:58:07.865166 140264174335808 run_lib.py:153] step: 296550, training_loss: 1.68632e+02
I1111 13:58:17.475947 140264174335808 run_lib.py:153] step: 296600, training_loss: 1.18746e+02
I1111 13:58:27.625125 140264174335808 run_lib.py:153] step: 296650, training_loss: 1.25924e+02
I1111 13:58:38.051329 140264174335808 run_lib.py:153] step: 296700, training_loss: 1.37061e+02
I1111 13:58:47.602957 140264174335808 run_lib.py:153] step: 296750, training_loss: 1.30578e+02
I1111 13:58:57.512086 140264174335808 run_lib.py:153] step: 296800, training_loss: 1.33934e+02
I1111 13:59:07.251486 140264174335808 run_lib.py:153] step: 296850, training_loss: 1.27527e+02
I1111 13:59:17.413238 140264174335808 run_lib.py:153] step: 296900, training_loss: 1.33213e+02
I1111 13:59:28.188317 140264174335808 run_lib.py:153] step: 296950, training_loss: 1.22005e+02
I1111 13:59:38.611550 140264174335808 run_lib.py:153] step: 297000, training_loss: 1.35559e+02
I1111 13:59:49.422119 140264174335808 run_lib.py:153] step: 297050, training_loss: 1.22582e+02
I1111 13:59:59.194528 140264174335808 run_lib.py:153] step: 297100, training_loss: 1.45145e+02
I1111 14:00:08.945486 140264174335808 run_lib.py:153] step: 297150, training_loss: 1.01782e+02
I1111 14:00:18.706946 140264174335808 run_lib.py:153] step: 297200, training_loss: 1.06972e+02
I1111 14:00:28.002914 140264174335808 run_lib.py:153] step: 297250, training_loss: 1.06754e+02
I1111 14:00:37.503228 140264174335808 run_lib.py:153] step: 297300, training_loss: 1.37115e+02
I1111 14:00:47.039948 140264174335808 run_lib.py:153] step: 297350, training_loss: 1.04748e+02
I1111 14:00:56.902560 140264174335808 run_lib.py:153] step: 297400, training_loss: 1.41270e+02
I1111 14:01:06.300756 140264174335808 run_lib.py:153] step: 297450, training_loss: 1.37276e+02
I1111 14:01:15.978685 140264174335808 run_lib.py:153] step: 297500, training_loss: 1.21826e+02
I1111 14:01:25.812213 140264174335808 run_lib.py:153] step: 297550, training_loss: 1.32547e+02
I1111 14:01:35.940097 140264174335808 run_lib.py:153] step: 297600, training_loss: 1.26460e+02
I1111 14:01:46.394689 140264174335808 run_lib.py:153] step: 297650, training_loss: 1.07071e+02
I1111 14:01:56.469213 140264174335808 run_lib.py:153] step: 297700, training_loss: 1.27718e+02
I1111 14:02:06.804303 140264174335808 run_lib.py:153] step: 297750, training_loss: 1.28345e+02
I1111 14:02:16.687887 140264174335808 run_lib.py:153] step: 297800, training_loss: 1.29308e+02
I1111 14:02:27.290013 140264174335808 run_lib.py:153] step: 297850, training_loss: 1.39633e+02
I1111 14:02:37.595530 140264174335808 run_lib.py:153] step: 297900, training_loss: 1.24854e+02
I1111 14:02:47.155154 140264174335808 run_lib.py:153] step: 297950, training_loss: 1.52860e+02
I1111 14:02:56.586799 140264174335808 run_lib.py:153] step: 298000, training_loss: 1.19164e+02
I1111 14:03:06.674742 140264174335808 run_lib.py:153] step: 298050, training_loss: 1.46475e+02
I1111 14:03:16.866086 140264174335808 run_lib.py:153] step: 298100, training_loss: 1.37650e+02
I1111 14:03:27.609125 140264174335808 run_lib.py:153] step: 298150, training_loss: 1.29176e+02
I1111 14:03:37.464142 140264174335808 run_lib.py:153] step: 298200, training_loss: 1.14997e+02
I1111 14:03:47.093739 140264174335808 run_lib.py:153] step: 298250, training_loss: 1.08282e+02
I1111 14:03:56.673580 140264174335808 run_lib.py:153] step: 298300, training_loss: 1.28187e+02
I1111 14:04:07.003454 140264174335808 run_lib.py:153] step: 298350, training_loss: 9.59576e+01
I1111 14:04:17.085502 140264174335808 run_lib.py:153] step: 298400, training_loss: 1.15017e+02
I1111 14:04:27.065911 140264174335808 run_lib.py:153] step: 298450, training_loss: 1.14642e+02
I1111 14:04:37.337729 140264174335808 run_lib.py:153] step: 298500, training_loss: 1.16618e+02
I1111 14:04:47.611438 140264174335808 run_lib.py:153] step: 298550, training_loss: 1.07368e+02
I1111 14:04:57.772632 140264174335808 run_lib.py:153] step: 298600, training_loss: 1.32859e+02
I1111 14:05:08.096968 140264174335808 run_lib.py:153] step: 298650, training_loss: 1.19025e+02
I1111 14:05:18.632194 140264174335808 run_lib.py:153] step: 298700, training_loss: 1.16885e+02
I1111 14:05:28.933154 140264174335808 run_lib.py:153] step: 298750, training_loss: 1.28357e+02
I1111 14:05:39.170827 140264174335808 run_lib.py:153] step: 298800, training_loss: 1.34551e+02
I1111 14:05:49.293441 140264174335808 run_lib.py:153] step: 298850, training_loss: 1.22815e+02
I1111 14:05:59.962893 140264174335808 run_lib.py:153] step: 298900, training_loss: 1.32548e+02
I1111 14:06:10.476366 140264174335808 run_lib.py:153] step: 298950, training_loss: 1.34784e+02
I1111 14:06:19.698953 140264174335808 run_lib.py:153] step: 299000, training_loss: 1.59991e+02
I1111 14:06:29.732241 140264174335808 run_lib.py:153] step: 299050, training_loss: 1.39626e+02
I1111 14:06:39.379782 140264174335808 run_lib.py:153] step: 299100, training_loss: 1.13062e+02
I1111 14:06:48.969319 140264174335808 run_lib.py:153] step: 299150, training_loss: 1.51038e+02
I1111 14:06:59.432651 140264174335808 run_lib.py:153] step: 299200, training_loss: 1.54530e+02
I1111 14:07:09.291889 140264174335808 run_lib.py:153] step: 299250, training_loss: 1.42603e+02
I1111 14:07:19.555261 140264174335808 run_lib.py:153] step: 299300, training_loss: 1.34105e+02
I1111 14:07:29.541933 140264174335808 run_lib.py:153] step: 299350, training_loss: 1.46448e+02
I1111 14:07:39.346531 140264174335808 run_lib.py:153] step: 299400, training_loss: 1.15663e+02
I1111 14:07:49.031503 140264174335808 run_lib.py:153] step: 299450, training_loss: 1.06793e+02
I1111 14:07:58.757688 140264174335808 run_lib.py:153] step: 299500, training_loss: 1.17227e+02
I1111 14:08:08.807410 140264174335808 run_lib.py:153] step: 299550, training_loss: 1.34284e+02
I1111 14:08:18.866931 140264174335808 run_lib.py:153] step: 299600, training_loss: 1.27212e+02
I1111 14:08:28.535135 140264174335808 run_lib.py:153] step: 299650, training_loss: 1.28823e+02
I1111 14:08:38.206057 140264174335808 run_lib.py:153] step: 299700, training_loss: 9.49571e+01
I1111 14:08:48.709217 140264174335808 run_lib.py:153] step: 299750, training_loss: 1.24371e+02
I1111 14:08:58.645131 140264174335808 run_lib.py:153] step: 299800, training_loss: 1.16581e+02
I1111 14:09:08.934452 140264174335808 run_lib.py:153] step: 299850, training_loss: 1.47252e+02
I1111 14:09:18.544854 140264174335808 run_lib.py:153] step: 299900, training_loss: 1.58510e+02
I1111 14:09:28.509545 140264174335808 run_lib.py:153] step: 299950, training_loss: 1.47274e+02
I1111 14:09:39.056097 140264174335808 run_lib.py:153] step: 300000, training_loss: 1.23109e+02
I1111 14:09:39.619948 140264174335808 run_lib.py:166] step: 300000, eval_loss: 1.17402e+02
I1111 14:09:49.995615 140264174335808 run_lib.py:153] step: 300050, training_loss: 1.32631e+02
I1111 14:10:00.368347 140264174335808 run_lib.py:153] step: 300100, training_loss: 1.31092e+02
I1111 14:10:10.910562 140264174335808 run_lib.py:153] step: 300150, training_loss: 1.27321e+02
I1111 14:10:21.084034 140264174335808 run_lib.py:153] step: 300200, training_loss: 1.16341e+02
I1111 14:10:31.341687 140264174335808 run_lib.py:153] step: 300250, training_loss: 1.29377e+02
I1111 14:10:40.992527 140264174335808 run_lib.py:153] step: 300300, training_loss: 1.54521e+02
I1111 14:10:51.250620 140264174335808 run_lib.py:153] step: 300350, training_loss: 1.38738e+02
I1111 14:11:01.688713 140264174335808 run_lib.py:153] step: 300400, training_loss: 1.45664e+02
I1111 14:11:11.994698 140264174335808 run_lib.py:153] step: 300450, training_loss: 1.06459e+02
I1111 14:11:22.595028 140264174335808 run_lib.py:153] step: 300500, training_loss: 1.44550e+02
I1111 14:11:33.338746 140264174335808 run_lib.py:153] step: 300550, training_loss: 1.18449e+02
I1111 14:11:43.559849 140264174335808 run_lib.py:153] step: 300600, training_loss: 1.32257e+02
I1111 14:11:53.913149 140264174335808 run_lib.py:153] step: 300650, training_loss: 9.39403e+01
I1111 14:12:03.502635 140264174335808 run_lib.py:153] step: 300700, training_loss: 1.22586e+02
I1111 14:12:13.446833 140264174335808 run_lib.py:153] step: 300750, training_loss: 9.47864e+01
I1111 14:12:23.744650 140264174335808 run_lib.py:153] step: 300800, training_loss: 1.24943e+02
I1111 14:12:33.853657 140264174335808 run_lib.py:153] step: 300850, training_loss: 1.13178e+02
I1111 14:12:44.067348 140264174335808 run_lib.py:153] step: 300900, training_loss: 1.20763e+02
I1111 14:12:54.459196 140264174335808 run_lib.py:153] step: 300950, training_loss: 1.30203e+02
I1111 14:13:04.452464 140264174335808 run_lib.py:153] step: 301000, training_loss: 1.65428e+02
I1111 14:13:14.909168 140264174335808 run_lib.py:153] step: 301050, training_loss: 1.40894e+02
I1111 14:13:24.963510 140264174335808 run_lib.py:153] step: 301100, training_loss: 1.29988e+02
I1111 14:13:35.568746 140264174335808 run_lib.py:153] step: 301150, training_loss: 1.15567e+02
I1111 14:13:45.544028 140264174335808 run_lib.py:153] step: 301200, training_loss: 1.31122e+02
I1111 14:13:55.773720 140264174335808 run_lib.py:153] step: 301250, training_loss: 1.64567e+02
I1111 14:14:05.623281 140264174335808 run_lib.py:153] step: 301300, training_loss: 1.25748e+02
I1111 14:14:15.543380 140264174335808 run_lib.py:153] step: 301350, training_loss: 1.38871e+02
I1111 14:14:26.111718 140264174335808 run_lib.py:153] step: 301400, training_loss: 1.01114e+02
I1111 14:14:36.400564 140264174335808 run_lib.py:153] step: 301450, training_loss: 1.54053e+02
I1111 14:14:46.806756 140264174335808 run_lib.py:153] step: 301500, training_loss: 9.27303e+01
I1111 14:14:56.694918 140264174335808 run_lib.py:153] step: 301550, training_loss: 1.47049e+02
I1111 14:15:06.738830 140264174335808 run_lib.py:153] step: 301600, training_loss: 1.31600e+02
I1111 14:15:16.255580 140264174335808 run_lib.py:153] step: 301650, training_loss: 1.42063e+02
I1111 14:15:26.162027 140264174335808 run_lib.py:153] step: 301700, training_loss: 1.37498e+02
I1111 14:15:35.698768 140264174335808 run_lib.py:153] step: 301750, training_loss: 1.02330e+02
I1111 14:15:45.847917 140264174335808 run_lib.py:153] step: 301800, training_loss: 1.42911e+02
I1111 14:15:55.831807 140264174335808 run_lib.py:153] step: 301850, training_loss: 1.14817e+02
I1111 14:16:05.917001 140264174335808 run_lib.py:153] step: 301900, training_loss: 1.18630e+02
I1111 14:16:15.647381 140264174335808 run_lib.py:153] step: 301950, training_loss: 1.16092e+02
I1111 14:16:25.798511 140264174335808 run_lib.py:153] step: 302000, training_loss: 1.25278e+02
I1111 14:16:35.734737 140264174335808 run_lib.py:153] step: 302050, training_loss: 1.19912e+02
I1111 14:16:45.408770 140264174335808 run_lib.py:153] step: 302100, training_loss: 1.13465e+02
I1111 14:16:55.398490 140264174335808 run_lib.py:153] step: 302150, training_loss: 1.32405e+02
I1111 14:17:05.275956 140264174335808 run_lib.py:153] step: 302200, training_loss: 1.30445e+02
I1111 14:17:16.109940 140264174335808 run_lib.py:153] step: 302250, training_loss: 1.20716e+02
I1111 14:17:26.471574 140264174335808 run_lib.py:153] step: 302300, training_loss: 1.54968e+02
I1111 14:17:36.752248 140264174335808 run_lib.py:153] step: 302350, training_loss: 1.12459e+02
I1111 14:17:46.815074 140264174335808 run_lib.py:153] step: 302400, training_loss: 1.26069e+02
I1111 14:17:56.603865 140264174335808 run_lib.py:153] step: 302450, training_loss: 1.35124e+02
I1111 14:18:06.865996 140264174335808 run_lib.py:153] step: 302500, training_loss: 1.32394e+02
I1111 14:18:16.869180 140264174335808 run_lib.py:153] step: 302550, training_loss: 1.28833e+02
I1111 14:18:26.749286 140264174335808 run_lib.py:153] step: 302600, training_loss: 1.35933e+02
I1111 14:18:37.244042 140264174335808 run_lib.py:153] step: 302650, training_loss: 1.30093e+02
I1111 14:18:47.384709 140264174335808 run_lib.py:153] step: 302700, training_loss: 1.50037e+02
I1111 14:18:57.335632 140264174335808 run_lib.py:153] step: 302750, training_loss: 1.39650e+02
I1111 14:19:07.503066 140264174335808 run_lib.py:153] step: 302800, training_loss: 1.52783e+02
I1111 14:19:17.637356 140264174335808 run_lib.py:153] step: 302850, training_loss: 1.48521e+02
I1111 14:19:27.161103 140264174335808 run_lib.py:153] step: 302900, training_loss: 1.47464e+02
I1111 14:19:37.495157 140264174335808 run_lib.py:153] step: 302950, training_loss: 1.12470e+02
I1111 14:19:47.624179 140264174335808 run_lib.py:153] step: 303000, training_loss: 1.25936e+02
I1111 14:19:57.996081 140264174335808 run_lib.py:153] step: 303050, training_loss: 1.31612e+02
I1111 14:20:07.980517 140264174335808 run_lib.py:153] step: 303100, training_loss: 1.38014e+02
I1111 14:20:18.331695 140264174335808 run_lib.py:153] step: 303150, training_loss: 1.18278e+02
I1111 14:20:28.225491 140264174335808 run_lib.py:153] step: 303200, training_loss: 1.07178e+02
I1111 14:20:38.126304 140264174335808 run_lib.py:153] step: 303250, training_loss: 1.07733e+02
I1111 14:20:48.755263 140264174335808 run_lib.py:153] step: 303300, training_loss: 1.25774e+02
I1111 14:20:58.801470 140264174335808 run_lib.py:153] step: 303350, training_loss: 9.25348e+01
I1111 14:21:09.311734 140264174335808 run_lib.py:153] step: 303400, training_loss: 1.26226e+02
I1111 14:21:18.976201 140264174335808 run_lib.py:153] step: 303450, training_loss: 1.40138e+02
I1111 14:21:28.765398 140264174335808 run_lib.py:153] step: 303500, training_loss: 1.34784e+02
I1111 14:21:38.970344 140264174335808 run_lib.py:153] step: 303550, training_loss: 1.14061e+02
I1111 14:21:49.069488 140264174335808 run_lib.py:153] step: 303600, training_loss: 1.19373e+02
I1111 14:21:58.734598 140264174335808 run_lib.py:153] step: 303650, training_loss: 1.18962e+02
I1111 14:22:08.180894 140264174335808 run_lib.py:153] step: 303700, training_loss: 1.25475e+02
I1111 14:22:17.999445 140264174335808 run_lib.py:153] step: 303750, training_loss: 1.35799e+02
I1111 14:22:27.555402 140264174335808 run_lib.py:153] step: 303800, training_loss: 1.20261e+02
I1111 14:22:37.588906 140264174335808 run_lib.py:153] step: 303850, training_loss: 1.34943e+02
I1111 14:22:48.030979 140264174335808 run_lib.py:153] step: 303900, training_loss: 1.47270e+02
I1111 14:22:58.092338 140264174335808 run_lib.py:153] step: 303950, training_loss: 1.36585e+02
I1111 14:23:07.811227 140264174335808 run_lib.py:153] step: 304000, training_loss: 9.25533e+01
I1111 14:23:17.656082 140264174335808 run_lib.py:153] step: 304050, training_loss: 1.43126e+02
I1111 14:23:27.129083 140264174335808 run_lib.py:153] step: 304100, training_loss: 1.37818e+02
I1111 14:23:37.298648 140264174335808 run_lib.py:153] step: 304150, training_loss: 1.29886e+02
I1111 14:23:47.487092 140264174335808 run_lib.py:153] step: 304200, training_loss: 1.38575e+02
I1111 14:23:57.049015 140264174335808 run_lib.py:153] step: 304250, training_loss: 1.34234e+02
I1111 14:24:07.337908 140264174335808 run_lib.py:153] step: 304300, training_loss: 1.76216e+02
I1111 14:24:16.970853 140264174335808 run_lib.py:153] step: 304350, training_loss: 1.50311e+02
I1111 14:24:27.506470 140264174335808 run_lib.py:153] step: 304400, training_loss: 1.21252e+02
I1111 14:24:37.944189 140264174335808 run_lib.py:153] step: 304450, training_loss: 1.43732e+02
I1111 14:24:47.458536 140264174335808 run_lib.py:153] step: 304500, training_loss: 1.33877e+02
I1111 14:24:57.718116 140264174335808 run_lib.py:153] step: 304550, training_loss: 1.24411e+02
I1111 14:25:07.755740 140264174335808 run_lib.py:153] step: 304600, training_loss: 1.02816e+02
I1111 14:25:18.291164 140264174335808 run_lib.py:153] step: 304650, training_loss: 1.37267e+02
I1111 14:25:28.080300 140264174335808 run_lib.py:153] step: 304700, training_loss: 9.33824e+01
I1111 14:25:37.400382 140264174335808 run_lib.py:153] step: 304750, training_loss: 1.27770e+02
I1111 14:25:47.196631 140264174335808 run_lib.py:153] step: 304800, training_loss: 1.36043e+02
I1111 14:25:57.284124 140264174335808 run_lib.py:153] step: 304850, training_loss: 1.17563e+02
I1111 14:26:06.887417 140264174335808 run_lib.py:153] step: 304900, training_loss: 1.45224e+02
I1111 14:26:17.385564 140264174335808 run_lib.py:153] step: 304950, training_loss: 1.07818e+02
I1111 14:26:27.959782 140264174335808 run_lib.py:153] step: 305000, training_loss: 9.17935e+01
I1111 14:26:28.059987 140264174335808 run_lib.py:166] step: 305000, eval_loss: 1.07463e+02
I1111 14:26:37.911780 140264174335808 run_lib.py:153] step: 305050, training_loss: 1.29902e+02
I1111 14:26:47.505650 140264174335808 run_lib.py:153] step: 305100, training_loss: 1.37690e+02
I1111 14:26:57.311008 140264174335808 run_lib.py:153] step: 305150, training_loss: 1.11037e+02
I1111 14:27:07.973628 140264174335808 run_lib.py:153] step: 305200, training_loss: 1.17347e+02
I1111 14:27:18.022629 140264174335808 run_lib.py:153] step: 305250, training_loss: 1.40539e+02
I1111 14:27:28.438499 140264174335808 run_lib.py:153] step: 305300, training_loss: 1.22231e+02
I1111 14:27:38.178954 140264174335808 run_lib.py:153] step: 305350, training_loss: 1.36650e+02
I1111 14:27:48.930881 140264174335808 run_lib.py:153] step: 305400, training_loss: 1.37235e+02
I1111 14:27:59.168392 140264174335808 run_lib.py:153] step: 305450, training_loss: 1.21345e+02
I1111 14:28:09.043631 140264174335808 run_lib.py:153] step: 305500, training_loss: 1.16072e+02
I1111 14:28:18.922519 140264174335808 run_lib.py:153] step: 305550, training_loss: 1.23384e+02
I1111 14:28:28.849172 140264174335808 run_lib.py:153] step: 305600, training_loss: 1.37548e+02
I1111 14:28:38.631509 140264174335808 run_lib.py:153] step: 305650, training_loss: 1.15883e+02
I1111 14:28:48.382121 140264174335808 run_lib.py:153] step: 305700, training_loss: 1.42187e+02
I1111 14:28:58.144723 140264174335808 run_lib.py:153] step: 305750, training_loss: 1.08628e+02
I1111 14:29:07.843877 140264174335808 run_lib.py:153] step: 305800, training_loss: 1.00429e+02
I1111 14:29:17.218510 140264174335808 run_lib.py:153] step: 305850, training_loss: 1.09137e+02
I1111 14:29:27.018649 140264174335808 run_lib.py:153] step: 305900, training_loss: 1.39183e+02
I1111 14:29:36.528146 140264174335808 run_lib.py:153] step: 305950, training_loss: 1.15476e+02
I1111 14:29:46.492959 140264174335808 run_lib.py:153] step: 306000, training_loss: 1.32546e+02
I1111 14:29:56.823731 140264174335808 run_lib.py:153] step: 306050, training_loss: 1.52250e+02
I1111 14:30:06.681601 140264174335808 run_lib.py:153] step: 306100, training_loss: 1.21077e+02
I1111 14:30:16.042991 140264174335808 run_lib.py:153] step: 306150, training_loss: 1.07513e+02
I1111 14:30:26.654556 140264174335808 run_lib.py:153] step: 306200, training_loss: 1.34485e+02
I1111 14:30:37.030323 140264174335808 run_lib.py:153] step: 306250, training_loss: 1.12965e+02
I1111 14:30:46.511317 140264174335808 run_lib.py:153] step: 306300, training_loss: 1.27926e+02
I1111 14:30:56.290302 140264174335808 run_lib.py:153] step: 306350, training_loss: 1.12803e+02
I1111 14:31:06.056241 140264174335808 run_lib.py:153] step: 306400, training_loss: 1.07148e+02
I1111 14:31:15.859647 140264174335808 run_lib.py:153] step: 306450, training_loss: 1.33249e+02
I1111 14:31:26.395612 140264174335808 run_lib.py:153] step: 306500, training_loss: 1.33558e+02
I1111 14:31:36.414916 140264174335808 run_lib.py:153] step: 306550, training_loss: 1.53757e+02
I1111 14:31:46.186365 140264174335808 run_lib.py:153] step: 306600, training_loss: 1.24790e+02
I1111 14:31:56.250438 140264174335808 run_lib.py:153] step: 306650, training_loss: 1.12662e+02
I1111 14:32:06.515848 140264174335808 run_lib.py:153] step: 306700, training_loss: 1.38996e+02
I1111 14:32:16.577344 140264174335808 run_lib.py:153] step: 306750, training_loss: 1.36707e+02
I1111 14:32:26.135505 140264174335808 run_lib.py:153] step: 306800, training_loss: 1.46032e+02
I1111 14:32:35.759207 140264174335808 run_lib.py:153] step: 306850, training_loss: 1.10169e+02
I1111 14:32:46.126382 140264174335808 run_lib.py:153] step: 306900, training_loss: 1.36079e+02
I1111 14:32:56.022797 140264174335808 run_lib.py:153] step: 306950, training_loss: 1.12897e+02
I1111 14:33:05.906612 140264174335808 run_lib.py:153] step: 307000, training_loss: 9.95823e+01
I1111 14:33:15.872081 140264174335808 run_lib.py:153] step: 307050, training_loss: 1.18324e+02
I1111 14:33:25.679630 140264174335808 run_lib.py:153] step: 307100, training_loss: 1.19510e+02
I1111 14:33:35.358314 140264174335808 run_lib.py:153] step: 307150, training_loss: 9.70934e+01
I1111 14:33:45.494901 140264174335808 run_lib.py:153] step: 307200, training_loss: 9.94391e+01
I1111 14:33:55.133971 140264174335808 run_lib.py:153] step: 307250, training_loss: 1.32307e+02
I1111 14:34:04.721872 140264174335808 run_lib.py:153] step: 307300, training_loss: 1.21464e+02
I1111 14:34:14.488595 140264174335808 run_lib.py:153] step: 307350, training_loss: 1.37345e+02
I1111 14:34:24.045578 140264174335808 run_lib.py:153] step: 307400, training_loss: 9.85628e+01
I1111 14:34:33.897789 140264174335808 run_lib.py:153] step: 307450, training_loss: 1.02843e+02
I1111 14:34:43.693952 140264174335808 run_lib.py:153] step: 307500, training_loss: 1.59126e+02
I1111 14:34:53.255415 140264174335808 run_lib.py:153] step: 307550, training_loss: 1.11247e+02
I1111 14:35:02.743539 140264174335808 run_lib.py:153] step: 307600, training_loss: 1.37356e+02
I1111 14:35:13.356061 140264174335808 run_lib.py:153] step: 307650, training_loss: 1.16381e+02
I1111 14:35:24.057942 140264174335808 run_lib.py:153] step: 307700, training_loss: 1.37698e+02
I1111 14:35:34.227617 140264174335808 run_lib.py:153] step: 307750, training_loss: 1.17021e+02
I1111 14:35:43.783647 140264174335808 run_lib.py:153] step: 307800, training_loss: 1.27477e+02
I1111 14:35:53.794949 140264174335808 run_lib.py:153] step: 307850, training_loss: 1.42098e+02
I1111 14:36:03.820619 140264174335808 run_lib.py:153] step: 307900, training_loss: 1.17020e+02
I1111 14:36:13.835459 140264174335808 run_lib.py:153] step: 307950, training_loss: 1.35546e+02
I1111 14:36:23.723764 140264174335808 run_lib.py:153] step: 308000, training_loss: 1.10216e+02
I1111 14:36:33.166264 140264174335808 run_lib.py:153] step: 308050, training_loss: 1.11593e+02
I1111 14:36:42.681652 140264174335808 run_lib.py:153] step: 308100, training_loss: 1.33987e+02
I1111 14:36:52.028292 140264174335808 run_lib.py:153] step: 308150, training_loss: 1.35328e+02
I1111 14:37:02.153604 140264174335808 run_lib.py:153] step: 308200, training_loss: 1.31575e+02
I1111 14:37:12.282347 140264174335808 run_lib.py:153] step: 308250, training_loss: 1.64109e+02
I1111 14:37:22.476167 140264174335808 run_lib.py:153] step: 308300, training_loss: 1.65931e+02
I1111 14:37:32.723536 140264174335808 run_lib.py:153] step: 308350, training_loss: 1.15585e+02
I1111 14:37:42.794551 140264174335808 run_lib.py:153] step: 308400, training_loss: 1.26446e+02
I1111 14:37:52.556819 140264174335808 run_lib.py:153] step: 308450, training_loss: 1.36164e+02
I1111 14:38:02.766229 140264174335808 run_lib.py:153] step: 308500, training_loss: 1.45483e+02
I1111 14:38:12.983337 140264174335808 run_lib.py:153] step: 308550, training_loss: 1.04176e+02
I1111 14:38:22.568909 140264174335808 run_lib.py:153] step: 308600, training_loss: 1.26726e+02
I1111 14:38:32.417626 140264174335808 run_lib.py:153] step: 308650, training_loss: 1.39051e+02
I1111 14:38:42.457827 140264174335808 run_lib.py:153] step: 308700, training_loss: 1.62880e+02
I1111 14:38:52.043759 140264174335808 run_lib.py:153] step: 308750, training_loss: 1.53904e+02
I1111 14:39:02.363139 140264174335808 run_lib.py:153] step: 308800, training_loss: 1.42162e+02
I1111 14:39:12.510438 140264174335808 run_lib.py:153] step: 308850, training_loss: 1.32937e+02
I1111 14:39:22.591539 140264174335808 run_lib.py:153] step: 308900, training_loss: 1.33960e+02
I1111 14:39:32.448703 140264174335808 run_lib.py:153] step: 308950, training_loss: 1.21070e+02
I1111 14:39:42.426756 140264174335808 run_lib.py:153] step: 309000, training_loss: 1.66953e+02
I1111 14:39:51.861784 140264174335808 run_lib.py:153] step: 309050, training_loss: 1.10738e+02
I1111 14:40:01.214717 140264174335808 run_lib.py:153] step: 309100, training_loss: 9.28606e+01
I1111 14:40:10.772219 140264174335808 run_lib.py:153] step: 309150, training_loss: 1.20049e+02
I1111 14:40:20.330443 140264174335808 run_lib.py:153] step: 309200, training_loss: 1.32406e+02
I1111 14:40:29.970181 140264174335808 run_lib.py:153] step: 309250, training_loss: 1.14259e+02
I1111 14:40:39.335076 140264174335808 run_lib.py:153] step: 309300, training_loss: 1.22996e+02
I1111 14:40:49.225116 140264174335808 run_lib.py:153] step: 309350, training_loss: 1.23971e+02
I1111 14:40:58.810436 140264174335808 run_lib.py:153] step: 309400, training_loss: 1.48672e+02
I1111 14:41:08.594773 140264174335808 run_lib.py:153] step: 309450, training_loss: 1.29154e+02
I1111 14:41:18.987818 140264174335808 run_lib.py:153] step: 309500, training_loss: 1.29396e+02
I1111 14:41:28.522277 140264174335808 run_lib.py:153] step: 309550, training_loss: 1.35413e+02
I1111 14:41:37.891634 140264174335808 run_lib.py:153] step: 309600, training_loss: 1.70657e+02
I1111 14:41:48.765257 140264174335808 run_lib.py:153] step: 309650, training_loss: 1.69724e+02
I1111 14:41:58.807886 140264174335808 run_lib.py:153] step: 309700, training_loss: 1.47268e+02
I1111 14:42:08.451361 140264174335808 run_lib.py:153] step: 309750, training_loss: 1.24779e+02
I1111 14:42:18.379611 140264174335808 run_lib.py:153] step: 309800, training_loss: 1.31607e+02
I1111 14:42:28.535692 140264174335808 run_lib.py:153] step: 309850, training_loss: 1.38477e+02
I1111 14:42:38.144133 140264174335808 run_lib.py:153] step: 309900, training_loss: 1.35553e+02
I1111 14:42:48.078442 140264174335808 run_lib.py:153] step: 309950, training_loss: 1.34282e+02
I1111 14:42:57.697077 140264174335808 run_lib.py:153] step: 310000, training_loss: 1.01817e+02
I1111 14:42:58.244769 140264174335808 run_lib.py:166] step: 310000, eval_loss: 1.41108e+02
I1111 14:43:08.255537 140264174335808 run_lib.py:153] step: 310050, training_loss: 1.40889e+02
I1111 14:43:18.686222 140264174335808 run_lib.py:153] step: 310100, training_loss: 9.56398e+01
I1111 14:43:28.484663 140264174335808 run_lib.py:153] step: 310150, training_loss: 1.27797e+02
I1111 14:43:38.020223 140264174335808 run_lib.py:153] step: 310200, training_loss: 1.19843e+02
I1111 14:43:48.398041 140264174335808 run_lib.py:153] step: 310250, training_loss: 1.04495e+02
I1111 14:43:58.197944 140264174335808 run_lib.py:153] step: 310300, training_loss: 1.00190e+02
I1111 14:44:08.272017 140264174335808 run_lib.py:153] step: 310350, training_loss: 1.09339e+02
I1111 14:44:18.314991 140264174335808 run_lib.py:153] step: 310400, training_loss: 1.15481e+02
I1111 14:44:28.881402 140264174335808 run_lib.py:153] step: 310450, training_loss: 1.42462e+02
I1111 14:44:39.172449 140264174335808 run_lib.py:153] step: 310500, training_loss: 1.02370e+02
I1111 14:44:49.176795 140264174335808 run_lib.py:153] step: 310550, training_loss: 1.37601e+02
I1111 14:44:59.463910 140264174335808 run_lib.py:153] step: 310600, training_loss: 1.22659e+02
I1111 14:45:09.280939 140264174335808 run_lib.py:153] step: 310650, training_loss: 1.36206e+02
I1111 14:45:18.975237 140264174335808 run_lib.py:153] step: 310700, training_loss: 1.45948e+02
I1111 14:45:28.643559 140264174335808 run_lib.py:153] step: 310750, training_loss: 1.46137e+02
I1111 14:45:38.445723 140264174335808 run_lib.py:153] step: 310800, training_loss: 1.36140e+02
I1111 14:45:48.463068 140264174335808 run_lib.py:153] step: 310850, training_loss: 1.41797e+02
I1111 14:45:58.559655 140264174335808 run_lib.py:153] step: 310900, training_loss: 1.44441e+02
I1111 14:46:08.664420 140264174335808 run_lib.py:153] step: 310950, training_loss: 1.36685e+02
I1111 14:46:18.691945 140264174335808 run_lib.py:153] step: 311000, training_loss: 1.40395e+02
I1111 14:46:29.003516 140264174335808 run_lib.py:153] step: 311050, training_loss: 1.49255e+02
I1111 14:46:39.453731 140264174335808 run_lib.py:153] step: 311100, training_loss: 1.22277e+02
I1111 14:46:49.310713 140264174335808 run_lib.py:153] step: 311150, training_loss: 1.22495e+02
I1111 14:46:59.434251 140264174335808 run_lib.py:153] step: 311200, training_loss: 9.89447e+01
I1111 14:47:09.105112 140264174335808 run_lib.py:153] step: 311250, training_loss: 1.58871e+02
I1111 14:47:19.053210 140264174335808 run_lib.py:153] step: 311300, training_loss: 1.33103e+02
I1111 14:47:28.969169 140264174335808 run_lib.py:153] step: 311350, training_loss: 1.66795e+02
I1111 14:47:38.621588 140264174335808 run_lib.py:153] step: 311400, training_loss: 1.46779e+02
I1111 14:47:48.723083 140264174335808 run_lib.py:153] step: 311450, training_loss: 1.23071e+02
I1111 14:47:59.389796 140264174335808 run_lib.py:153] step: 311500, training_loss: 1.27289e+02
I1111 14:48:09.710220 140264174335808 run_lib.py:153] step: 311550, training_loss: 1.12047e+02
I1111 14:48:19.908066 140264174335808 run_lib.py:153] step: 311600, training_loss: 1.54971e+02
I1111 14:48:29.313016 140264174335808 run_lib.py:153] step: 311650, training_loss: 1.16378e+02
I1111 14:48:38.966040 140264174335808 run_lib.py:153] step: 311700, training_loss: 1.36231e+02
I1111 14:48:48.540957 140264174335808 run_lib.py:153] step: 311750, training_loss: 1.32828e+02
I1111 14:48:58.195661 140264174335808 run_lib.py:153] step: 311800, training_loss: 1.17963e+02
I1111 14:49:08.232207 140264174335808 run_lib.py:153] step: 311850, training_loss: 1.22065e+02
I1111 14:49:18.147338 140264174335808 run_lib.py:153] step: 311900, training_loss: 1.21917e+02
I1111 14:49:27.752929 140264174335808 run_lib.py:153] step: 311950, training_loss: 1.02416e+02
I1111 14:49:37.660150 140264174335808 run_lib.py:153] step: 312000, training_loss: 1.07283e+02
I1111 14:49:47.377422 140264174335808 run_lib.py:153] step: 312050, training_loss: 1.39917e+02
I1111 14:49:57.778144 140264174335808 run_lib.py:153] step: 312100, training_loss: 1.31588e+02
I1111 14:50:07.554630 140264174335808 run_lib.py:153] step: 312150, training_loss: 1.38474e+02
I1111 14:50:17.578005 140264174335808 run_lib.py:153] step: 312200, training_loss: 1.18552e+02
I1111 14:50:27.779812 140264174335808 run_lib.py:153] step: 312250, training_loss: 1.40860e+02
I1111 14:50:37.385833 140264174335808 run_lib.py:153] step: 312300, training_loss: 1.21394e+02
I1111 14:50:47.561061 140264174335808 run_lib.py:153] step: 312350, training_loss: 1.47083e+02
I1111 14:50:57.476031 140264174335808 run_lib.py:153] step: 312400, training_loss: 1.48730e+02
I1111 14:51:07.098918 140264174335808 run_lib.py:153] step: 312450, training_loss: 1.35530e+02
I1111 14:51:17.435384 140264174335808 run_lib.py:153] step: 312500, training_loss: 8.59367e+01
I1111 14:51:27.281887 140264174335808 run_lib.py:153] step: 312550, training_loss: 1.32023e+02
I1111 14:51:37.107573 140264174335808 run_lib.py:153] step: 312600, training_loss: 9.34151e+01
I1111 14:51:46.482440 140264174335808 run_lib.py:153] step: 312650, training_loss: 1.09553e+02
I1111 14:51:56.425058 140264174335808 run_lib.py:153] step: 312700, training_loss: 1.31828e+02
I1111 14:52:06.463037 140264174335808 run_lib.py:153] step: 312750, training_loss: 1.43858e+02
I1111 14:52:16.318523 140264174335808 run_lib.py:153] step: 312800, training_loss: 8.00432e+01
I1111 14:52:26.307965 140264174335808 run_lib.py:153] step: 312850, training_loss: 1.25075e+02
I1111 14:52:35.746489 140264174335808 run_lib.py:153] step: 312900, training_loss: 1.31992e+02
I1111 14:52:45.482312 140264174335808 run_lib.py:153] step: 312950, training_loss: 1.44791e+02
I1111 14:52:54.995896 140264174335808 run_lib.py:153] step: 313000, training_loss: 1.40690e+02
I1111 14:53:05.008417 140264174335808 run_lib.py:153] step: 313050, training_loss: 1.12974e+02
I1111 14:53:14.683279 140264174335808 run_lib.py:153] step: 313100, training_loss: 9.82662e+01
I1111 14:53:24.098957 140264174335808 run_lib.py:153] step: 313150, training_loss: 1.08217e+02
I1111 14:53:33.381158 140264174335808 run_lib.py:153] step: 313200, training_loss: 1.46080e+02
I1111 14:53:43.144531 140264174335808 run_lib.py:153] step: 313250, training_loss: 1.48667e+02
I1111 14:53:53.110523 140264174335808 run_lib.py:153] step: 313300, training_loss: 8.80492e+01
I1111 14:54:03.080059 140264174335808 run_lib.py:153] step: 313350, training_loss: 1.23262e+02
I1111 14:54:12.792231 140264174335808 run_lib.py:153] step: 313400, training_loss: 1.69299e+02
I1111 14:54:22.686151 140264174335808 run_lib.py:153] step: 313450, training_loss: 1.07257e+02
I1111 14:54:32.760541 140264174335808 run_lib.py:153] step: 313500, training_loss: 1.10984e+02
I1111 14:54:42.849302 140264174335808 run_lib.py:153] step: 313550, training_loss: 1.53827e+02
I1111 14:54:53.155755 140264174335808 run_lib.py:153] step: 313600, training_loss: 1.44379e+02
I1111 14:55:02.842791 140264174335808 run_lib.py:153] step: 313650, training_loss: 1.39217e+02
I1111 14:55:12.323708 140264174335808 run_lib.py:153] step: 313700, training_loss: 1.01301e+02
I1111 14:55:21.887541 140264174335808 run_lib.py:153] step: 313750, training_loss: 1.28790e+02
I1111 14:55:31.224324 140264174335808 run_lib.py:153] step: 313800, training_loss: 1.62802e+02
I1111 14:55:40.763083 140264174335808 run_lib.py:153] step: 313850, training_loss: 1.19396e+02
I1111 14:55:50.253659 140264174335808 run_lib.py:153] step: 313900, training_loss: 1.11854e+02
I1111 14:55:59.757593 140264174335808 run_lib.py:153] step: 313950, training_loss: 1.40839e+02
I1111 14:56:09.683190 140264174335808 run_lib.py:153] step: 314000, training_loss: 1.00666e+02
I1111 14:56:19.631836 140264174335808 run_lib.py:153] step: 314050, training_loss: 1.18525e+02
I1111 14:56:29.232759 140264174335808 run_lib.py:153] step: 314100, training_loss: 1.36057e+02
I1111 14:56:39.192703 140264174335808 run_lib.py:153] step: 314150, training_loss: 1.39005e+02
I1111 14:56:48.465548 140264174335808 run_lib.py:153] step: 314200, training_loss: 1.59826e+02
I1111 14:56:58.136145 140264174335808 run_lib.py:153] step: 314250, training_loss: 1.33193e+02
I1111 14:57:07.503907 140264174335808 run_lib.py:153] step: 314300, training_loss: 1.18111e+02
I1111 14:57:17.110417 140264174335808 run_lib.py:153] step: 314350, training_loss: 1.31808e+02
I1111 14:57:26.893256 140264174335808 run_lib.py:153] step: 314400, training_loss: 1.24462e+02
I1111 14:57:37.181272 140264174335808 run_lib.py:153] step: 314450, training_loss: 1.43129e+02
I1111 14:57:47.269672 140264174335808 run_lib.py:153] step: 314500, training_loss: 1.29277e+02
I1111 14:57:57.103657 140264174335808 run_lib.py:153] step: 314550, training_loss: 1.20222e+02
I1111 14:58:07.794855 140264174335808 run_lib.py:153] step: 314600, training_loss: 9.02915e+01
I1111 14:58:17.837587 140264174335808 run_lib.py:153] step: 314650, training_loss: 1.02389e+02
I1111 14:58:28.201479 140264174335808 run_lib.py:153] step: 314700, training_loss: 1.20676e+02
I1111 14:58:37.905605 140264174335808 run_lib.py:153] step: 314750, training_loss: 1.32956e+02
I1111 14:58:48.285593 140264174335808 run_lib.py:153] step: 314800, training_loss: 1.61709e+02
I1111 14:58:58.279000 140264174335808 run_lib.py:153] step: 314850, training_loss: 9.18838e+01
I1111 14:59:08.199830 140264174335808 run_lib.py:153] step: 314900, training_loss: 1.39578e+02
I1111 14:59:17.866133 140264174335808 run_lib.py:153] step: 314950, training_loss: 1.31879e+02
I1111 14:59:28.203426 140264174335808 run_lib.py:153] step: 315000, training_loss: 1.18806e+02
I1111 14:59:28.339982 140264174335808 run_lib.py:166] step: 315000, eval_loss: 1.10039e+02
I1111 14:59:38.073945 140264174335808 run_lib.py:153] step: 315050, training_loss: 8.66591e+01
I1111 14:59:47.898791 140264174335808 run_lib.py:153] step: 315100, training_loss: 1.19557e+02
I1111 14:59:57.570704 140264174335808 run_lib.py:153] step: 315150, training_loss: 1.55249e+02
I1111 15:00:08.449768 140264174335808 run_lib.py:153] step: 315200, training_loss: 1.19132e+02
I1111 15:00:18.487676 140264174335808 run_lib.py:153] step: 315250, training_loss: 1.45428e+02
I1111 15:00:28.210373 140264174335808 run_lib.py:153] step: 315300, training_loss: 1.18455e+02
I1111 15:00:38.560378 140264174335808 run_lib.py:153] step: 315350, training_loss: 1.25729e+02
I1111 15:00:49.264807 140264174335808 run_lib.py:153] step: 315400, training_loss: 1.46233e+02
I1111 15:00:58.703610 140264174335808 run_lib.py:153] step: 315450, training_loss: 1.45411e+02
I1111 15:01:08.230676 140264174335808 run_lib.py:153] step: 315500, training_loss: 1.02171e+02
I1111 15:01:18.901976 140264174335808 run_lib.py:153] step: 315550, training_loss: 1.41785e+02
I1111 15:01:29.974768 140264174335808 run_lib.py:153] step: 315600, training_loss: 1.59672e+02
I1111 15:01:40.477447 140264174335808 run_lib.py:153] step: 315650, training_loss: 1.30480e+02
I1111 15:01:50.515424 140264174335808 run_lib.py:153] step: 315700, training_loss: 1.48352e+02
I1111 15:02:00.183891 140264174335808 run_lib.py:153] step: 315750, training_loss: 1.27951e+02
I1111 15:02:10.051490 140264174335808 run_lib.py:153] step: 315800, training_loss: 1.35825e+02
I1111 15:02:19.437306 140264174335808 run_lib.py:153] step: 315850, training_loss: 1.03716e+02
I1111 15:02:30.415768 140264174335808 run_lib.py:153] step: 315900, training_loss: 1.13262e+02
I1111 15:02:40.486390 140264174335808 run_lib.py:153] step: 315950, training_loss: 1.17482e+02
I1111 15:02:50.658996 140264174335808 run_lib.py:153] step: 316000, training_loss: 1.00981e+02
I1111 15:03:01.339781 140264174335808 run_lib.py:153] step: 316050, training_loss: 1.07524e+02
I1111 15:03:10.775890 140264174335808 run_lib.py:153] step: 316100, training_loss: 1.15133e+02
I1111 15:03:20.151616 140264174335808 run_lib.py:153] step: 316150, training_loss: 1.34712e+02
I1111 15:03:30.384602 140264174335808 run_lib.py:153] step: 316200, training_loss: 1.59382e+02
I1111 15:03:40.298929 140264174335808 run_lib.py:153] step: 316250, training_loss: 1.26340e+02
I1111 15:03:50.858991 140264174335808 run_lib.py:153] step: 316300, training_loss: 1.25909e+02
I1111 15:04:01.001153 140264174335808 run_lib.py:153] step: 316350, training_loss: 1.44345e+02
I1111 15:04:11.968023 140264174335808 run_lib.py:153] step: 316400, training_loss: 1.06953e+02
I1111 15:04:22.674220 140264174335808 run_lib.py:153] step: 316450, training_loss: 1.36133e+02
I1111 15:04:32.857898 140264174335808 run_lib.py:153] step: 316500, training_loss: 1.43235e+02
I1111 15:04:42.347674 140264174335808 run_lib.py:153] step: 316550, training_loss: 1.27905e+02
I1111 15:04:52.684530 140264174335808 run_lib.py:153] step: 316600, training_loss: 1.15560e+02
I1111 15:05:03.266890 140264174335808 run_lib.py:153] step: 316650, training_loss: 1.40144e+02
I1111 15:05:13.036859 140264174335808 run_lib.py:153] step: 316700, training_loss: 9.92610e+01
I1111 15:05:23.780301 140264174335808 run_lib.py:153] step: 316750, training_loss: 1.09392e+02
I1111 15:05:33.831098 140264174335808 run_lib.py:153] step: 316800, training_loss: 1.41881e+02
I1111 15:05:44.110434 140264174335808 run_lib.py:153] step: 316850, training_loss: 1.07094e+02
I1111 15:05:53.651900 140264174335808 run_lib.py:153] step: 316900, training_loss: 1.40976e+02
I1111 15:06:04.130768 140264174335808 run_lib.py:153] step: 316950, training_loss: 1.15679e+02
I1111 15:06:13.949546 140264174335808 run_lib.py:153] step: 317000, training_loss: 9.25237e+01
I1111 15:06:23.531502 140264174335808 run_lib.py:153] step: 317050, training_loss: 1.11902e+02
I1111 15:06:33.649965 140264174335808 run_lib.py:153] step: 317100, training_loss: 1.17935e+02
I1111 15:06:43.673343 140264174335808 run_lib.py:153] step: 317150, training_loss: 1.34289e+02
I1111 15:06:53.100158 140264174335808 run_lib.py:153] step: 317200, training_loss: 1.23162e+02
I1111 15:07:02.861154 140264174335808 run_lib.py:153] step: 317250, training_loss: 1.27128e+02
I1111 15:07:12.675823 140264174335808 run_lib.py:153] step: 317300, training_loss: 1.27675e+02
I1111 15:07:22.514545 140264174335808 run_lib.py:153] step: 317350, training_loss: 1.07401e+02
I1111 15:07:33.518728 140264174335808 run_lib.py:153] step: 317400, training_loss: 1.33306e+02
I1111 15:07:44.453804 140264174335808 run_lib.py:153] step: 317450, training_loss: 1.36168e+02
I1111 15:07:54.468585 140264174335808 run_lib.py:153] step: 317500, training_loss: 1.08163e+02
I1111 15:08:04.115371 140264174335808 run_lib.py:153] step: 317550, training_loss: 1.17666e+02
I1111 15:08:14.803927 140264174335808 run_lib.py:153] step: 317600, training_loss: 1.36415e+02
I1111 15:08:24.811372 140264174335808 run_lib.py:153] step: 317650, training_loss: 1.17520e+02
I1111 15:08:34.951825 140264174335808 run_lib.py:153] step: 317700, training_loss: 1.23764e+02
I1111 15:08:44.988444 140264174335808 run_lib.py:153] step: 317750, training_loss: 1.52638e+02
I1111 15:08:54.600642 140264174335808 run_lib.py:153] step: 317800, training_loss: 1.29759e+02
I1111 15:09:04.203750 140264174335808 run_lib.py:153] step: 317850, training_loss: 1.34071e+02
I1111 15:09:14.889055 140264174335808 run_lib.py:153] step: 317900, training_loss: 1.30766e+02
I1111 15:09:25.337400 140264174335808 run_lib.py:153] step: 317950, training_loss: 1.25112e+02
I1111 15:09:35.869324 140264174335808 run_lib.py:153] step: 318000, training_loss: 1.20333e+02
I1111 15:09:46.048182 140264174335808 run_lib.py:153] step: 318050, training_loss: 1.42358e+02
I1111 15:09:56.592569 140264174335808 run_lib.py:153] step: 318100, training_loss: 1.12685e+02
I1111 15:10:06.993125 140264174335808 run_lib.py:153] step: 318150, training_loss: 1.48088e+02
I1111 15:10:17.280548 140264174335808 run_lib.py:153] step: 318200, training_loss: 1.63744e+02
I1111 15:10:27.599659 140264174335808 run_lib.py:153] step: 318250, training_loss: 1.29801e+02
I1111 15:10:38.655770 140264174335808 run_lib.py:153] step: 318300, training_loss: 1.34459e+02
I1111 15:10:49.047635 140264174335808 run_lib.py:153] step: 318350, training_loss: 1.23544e+02
I1111 15:10:58.927130 140264174335808 run_lib.py:153] step: 318400, training_loss: 1.25792e+02
I1111 15:11:08.670697 140264174335808 run_lib.py:153] step: 318450, training_loss: 1.46433e+02
I1111 15:11:18.802760 140264174335808 run_lib.py:153] step: 318500, training_loss: 1.24514e+02
I1111 15:11:29.163540 140264174335808 run_lib.py:153] step: 318550, training_loss: 1.28472e+02
I1111 15:11:39.175258 140264174335808 run_lib.py:153] step: 318600, training_loss: 1.41158e+02
I1111 15:11:49.521282 140264174335808 run_lib.py:153] step: 318650, training_loss: 1.39375e+02
I1111 15:11:58.905344 140264174335808 run_lib.py:153] step: 318700, training_loss: 1.35942e+02
I1111 15:12:08.595593 140264174335808 run_lib.py:153] step: 318750, training_loss: 1.69755e+02
I1111 15:12:18.215750 140264174335808 run_lib.py:153] step: 318800, training_loss: 1.33803e+02
I1111 15:12:27.786317 140264174335808 run_lib.py:153] step: 318850, training_loss: 1.68777e+02
I1111 15:12:37.911983 140264174335808 run_lib.py:153] step: 318900, training_loss: 1.21214e+02
I1111 15:12:48.169323 140264174335808 run_lib.py:153] step: 318950, training_loss: 1.27739e+02
I1111 15:12:58.286798 140264174335808 run_lib.py:153] step: 319000, training_loss: 1.47494e+02
I1111 15:13:08.036386 140264174335808 run_lib.py:153] step: 319050, training_loss: 1.21009e+02
I1111 15:13:18.198727 140264174335808 run_lib.py:153] step: 319100, training_loss: 1.22156e+02
I1111 15:13:27.913360 140264174335808 run_lib.py:153] step: 319150, training_loss: 1.16734e+02
I1111 15:13:38.289704 140264174335808 run_lib.py:153] step: 319200, training_loss: 1.34828e+02
I1111 15:13:47.768997 140264174335808 run_lib.py:153] step: 319250, training_loss: 1.35526e+02
I1111 15:13:58.025726 140264174335808 run_lib.py:153] step: 319300, training_loss: 1.35335e+02
I1111 15:14:08.828774 140264174335808 run_lib.py:153] step: 319350, training_loss: 1.17965e+02
I1111 15:14:19.055904 140264174335808 run_lib.py:153] step: 319400, training_loss: 1.27698e+02
I1111 15:14:28.750519 140264174335808 run_lib.py:153] step: 319450, training_loss: 1.42404e+02
I1111 15:14:39.464816 140264174335808 run_lib.py:153] step: 319500, training_loss: 1.33453e+02
I1111 15:14:49.551832 140264174335808 run_lib.py:153] step: 319550, training_loss: 1.34989e+02
I1111 15:14:59.218559 140264174335808 run_lib.py:153] step: 319600, training_loss: 1.40208e+02
I1111 15:15:09.689491 140264174335808 run_lib.py:153] step: 319650, training_loss: 1.03708e+02
I1111 15:15:19.544009 140264174335808 run_lib.py:153] step: 319700, training_loss: 1.15130e+02
I1111 15:15:29.297381 140264174335808 run_lib.py:153] step: 319750, training_loss: 1.30456e+02
I1111 15:15:39.400741 140264174335808 run_lib.py:153] step: 319800, training_loss: 1.26064e+02
I1111 15:15:49.323332 140264174335808 run_lib.py:153] step: 319850, training_loss: 1.47757e+02
I1111 15:15:59.245945 140264174335808 run_lib.py:153] step: 319900, training_loss: 1.37763e+02
I1111 15:16:09.560027 140264174335808 run_lib.py:153] step: 319950, training_loss: 1.34659e+02
I1111 15:16:19.737527 140264174335808 run_lib.py:153] step: 320000, training_loss: 1.42821e+02
I1111 15:16:20.314008 140264174335808 run_lib.py:166] step: 320000, eval_loss: 1.18919e+02
I1111 15:16:30.417394 140264174335808 run_lib.py:153] step: 320050, training_loss: 1.24237e+02
I1111 15:16:40.213478 140264174335808 run_lib.py:153] step: 320100, training_loss: 1.28046e+02
I1111 15:16:50.192200 140264174335808 run_lib.py:153] step: 320150, training_loss: 1.26404e+02
I1111 15:16:59.943339 140264174335808 run_lib.py:153] step: 320200, training_loss: 1.13380e+02
I1111 15:17:11.055770 140264174335808 run_lib.py:153] step: 320250, training_loss: 1.12397e+02
I1111 15:17:20.747987 140264174335808 run_lib.py:153] step: 320300, training_loss: 9.12302e+01
I1111 15:17:30.165231 140264174335808 run_lib.py:153] step: 320350, training_loss: 1.36924e+02
I1111 15:17:40.984633 140264174335808 run_lib.py:153] step: 320400, training_loss: 1.32950e+02
I1111 15:17:51.328951 140264174335808 run_lib.py:153] step: 320450, training_loss: 1.28048e+02
I1111 15:18:02.078054 140264174335808 run_lib.py:153] step: 320500, training_loss: 1.18985e+02
I1111 15:18:11.984182 140264174335808 run_lib.py:153] step: 320550, training_loss: 1.49546e+02
I1111 15:18:21.701957 140264174335808 run_lib.py:153] step: 320600, training_loss: 1.22144e+02
I1111 15:18:31.858036 140264174335808 run_lib.py:153] step: 320650, training_loss: 1.18817e+02
I1111 15:18:41.818858 140264174335808 run_lib.py:153] step: 320700, training_loss: 1.21125e+02
I1111 15:18:52.403028 140264174335808 run_lib.py:153] step: 320750, training_loss: 1.19235e+02
I1111 15:19:02.434486 140264174335808 run_lib.py:153] step: 320800, training_loss: 1.48695e+02
I1111 15:19:12.742506 140264174335808 run_lib.py:153] step: 320850, training_loss: 1.37805e+02
I1111 15:19:22.671961 140264174335808 run_lib.py:153] step: 320900, training_loss: 1.56039e+02
I1111 15:19:32.642192 140264174335808 run_lib.py:153] step: 320950, training_loss: 1.32760e+02
I1111 15:19:42.219962 140264174335808 run_lib.py:153] step: 321000, training_loss: 1.27366e+02
I1111 15:19:52.468724 140264174335808 run_lib.py:153] step: 321050, training_loss: 1.43144e+02
I1111 15:20:02.997732 140264174335808 run_lib.py:153] step: 321100, training_loss: 1.40760e+02
I1111 15:20:13.134850 140264174335808 run_lib.py:153] step: 321150, training_loss: 1.49665e+02
I1111 15:20:23.432063 140264174335808 run_lib.py:153] step: 321200, training_loss: 1.39292e+02
I1111 15:20:33.636477 140264174335808 run_lib.py:153] step: 321250, training_loss: 1.27320e+02
I1111 15:20:44.234668 140264174335808 run_lib.py:153] step: 321300, training_loss: 1.10130e+02
I1111 15:20:54.023714 140264174335808 run_lib.py:153] step: 321350, training_loss: 1.26228e+02
I1111 15:21:04.675657 140264174335808 run_lib.py:153] step: 321400, training_loss: 1.40863e+02
I1111 15:21:15.122005 140264174335808 run_lib.py:153] step: 321450, training_loss: 1.27678e+02
I1111 15:21:25.341449 140264174335808 run_lib.py:153] step: 321500, training_loss: 1.45473e+02
I1111 15:21:35.419076 140264174335808 run_lib.py:153] step: 321550, training_loss: 1.25991e+02
I1111 15:21:45.531664 140264174335808 run_lib.py:153] step: 321600, training_loss: 1.48404e+02
I1111 15:21:55.449013 140264174335808 run_lib.py:153] step: 321650, training_loss: 1.56204e+02
I1111 15:22:05.841296 140264174335808 run_lib.py:153] step: 321700, training_loss: 1.49506e+02
I1111 15:22:15.625287 140264174335808 run_lib.py:153] step: 321750, training_loss: 1.48222e+02
I1111 15:22:25.913912 140264174335808 run_lib.py:153] step: 321800, training_loss: 9.56587e+01
I1111 15:22:35.962943 140264174335808 run_lib.py:153] step: 321850, training_loss: 1.05299e+02
I1111 15:22:46.246874 140264174335808 run_lib.py:153] step: 321900, training_loss: 1.22794e+02
I1111 15:22:56.037795 140264174335808 run_lib.py:153] step: 321950, training_loss: 1.22823e+02
I1111 15:23:06.299806 140264174335808 run_lib.py:153] step: 322000, training_loss: 1.22243e+02
I1111 15:23:16.042595 140264174335808 run_lib.py:153] step: 322050, training_loss: 1.38019e+02
I1111 15:23:25.715134 140264174335808 run_lib.py:153] step: 322100, training_loss: 1.11593e+02
I1111 15:23:35.428841 140264174335808 run_lib.py:153] step: 322150, training_loss: 1.25359e+02
I1111 15:23:45.421085 140264174335808 run_lib.py:153] step: 322200, training_loss: 1.17771e+02
I1111 15:23:55.200901 140264174335808 run_lib.py:153] step: 322250, training_loss: 1.16539e+02
I1111 15:24:06.114358 140264174335808 run_lib.py:153] step: 322300, training_loss: 1.02277e+02
I1111 15:24:16.454375 140264174335808 run_lib.py:153] step: 322350, training_loss: 1.27428e+02
I1111 15:24:27.036684 140264174335808 run_lib.py:153] step: 322400, training_loss: 1.26278e+02
I1111 15:24:37.242230 140264174335808 run_lib.py:153] step: 322450, training_loss: 1.52990e+02
I1111 15:24:47.254758 140264174335808 run_lib.py:153] step: 322500, training_loss: 1.32696e+02
I1111 15:24:57.373109 140264174335808 run_lib.py:153] step: 322550, training_loss: 1.28583e+02
I1111 15:25:08.546777 140264174335808 run_lib.py:153] step: 322600, training_loss: 1.66806e+02
I1111 15:25:18.284609 140264174335808 run_lib.py:153] step: 322650, training_loss: 8.79450e+01
I1111 15:25:28.395194 140264174335808 run_lib.py:153] step: 322700, training_loss: 1.24611e+02
I1111 15:25:38.497417 140264174335808 run_lib.py:153] step: 322750, training_loss: 1.07976e+02
I1111 15:25:48.040383 140264174335808 run_lib.py:153] step: 322800, training_loss: 1.16134e+02
I1111 15:25:58.086463 140264174335808 run_lib.py:153] step: 322850, training_loss: 1.46541e+02
I1111 15:26:07.934864 140264174335808 run_lib.py:153] step: 322900, training_loss: 1.41135e+02
I1111 15:26:17.614148 140264174335808 run_lib.py:153] step: 322950, training_loss: 1.25774e+02
I1111 15:26:27.209526 140264174335808 run_lib.py:153] step: 323000, training_loss: 1.11745e+02
I1111 15:26:37.401911 140264174335808 run_lib.py:153] step: 323050, training_loss: 1.36238e+02
I1111 15:26:47.479994 140264174335808 run_lib.py:153] step: 323100, training_loss: 1.36120e+02
I1111 15:26:58.337448 140264174335808 run_lib.py:153] step: 323150, training_loss: 1.58278e+02
I1111 15:27:09.015191 140264174335808 run_lib.py:153] step: 323200, training_loss: 1.20933e+02
I1111 15:27:19.571653 140264174335808 run_lib.py:153] step: 323250, training_loss: 1.26132e+02
I1111 15:27:29.544452 140264174335808 run_lib.py:153] step: 323300, training_loss: 1.15330e+02
I1111 15:27:39.500700 140264174335808 run_lib.py:153] step: 323350, training_loss: 1.08567e+02
I1111 15:27:49.345141 140264174335808 run_lib.py:153] step: 323400, training_loss: 9.91874e+01
I1111 15:27:59.247865 140264174335808 run_lib.py:153] step: 323450, training_loss: 1.25926e+02
I1111 15:28:09.426903 140264174335808 run_lib.py:153] step: 323500, training_loss: 1.59140e+02
I1111 15:28:20.052353 140264174335808 run_lib.py:153] step: 323550, training_loss: 1.33611e+02
I1111 15:28:30.904705 140264174335808 run_lib.py:153] step: 323600, training_loss: 1.35819e+02
I1111 15:28:41.279420 140264174335808 run_lib.py:153] step: 323650, training_loss: 1.58146e+02
I1111 15:28:51.094686 140264174335808 run_lib.py:153] step: 323700, training_loss: 1.55644e+02
I1111 15:29:01.109832 140264174335808 run_lib.py:153] step: 323750, training_loss: 1.31533e+02
I1111 15:29:10.930085 140264174335808 run_lib.py:153] step: 323800, training_loss: 1.11020e+02
I1111 15:29:20.895779 140264174335808 run_lib.py:153] step: 323850, training_loss: 1.42058e+02
I1111 15:29:31.023586 140264174335808 run_lib.py:153] step: 323900, training_loss: 1.30027e+02
I1111 15:29:41.255455 140264174335808 run_lib.py:153] step: 323950, training_loss: 1.22028e+02
I1111 15:29:51.470386 140264174335808 run_lib.py:153] step: 324000, training_loss: 1.33862e+02
I1111 15:30:01.582972 140264174335808 run_lib.py:153] step: 324050, training_loss: 1.34795e+02
I1111 15:30:11.315566 140264174335808 run_lib.py:153] step: 324100, training_loss: 1.13975e+02
I1111 15:30:21.664485 140264174335808 run_lib.py:153] step: 324150, training_loss: 1.11758e+02
I1111 15:30:31.511119 140264174335808 run_lib.py:153] step: 324200, training_loss: 1.31928e+02
I1111 15:30:41.193233 140264174335808 run_lib.py:153] step: 324250, training_loss: 1.24412e+02
I1111 15:30:51.074349 140264174335808 run_lib.py:153] step: 324300, training_loss: 1.38448e+02
I1111 15:31:01.009994 140264174335808 run_lib.py:153] step: 324350, training_loss: 1.10576e+02
I1111 15:31:10.564429 140264174335808 run_lib.py:153] step: 324400, training_loss: 1.49916e+02
I1111 15:31:20.112061 140264174335808 run_lib.py:153] step: 324450, training_loss: 1.18867e+02
I1111 15:31:30.650955 140264174335808 run_lib.py:153] step: 324500, training_loss: 1.49048e+02
I1111 15:31:39.894054 140264174335808 run_lib.py:153] step: 324550, training_loss: 1.39542e+02
I1111 15:31:49.380498 140264174335808 run_lib.py:153] step: 324600, training_loss: 1.37346e+02
I1111 15:31:58.866018 140264174335808 run_lib.py:153] step: 324650, training_loss: 1.24272e+02
I1111 15:32:09.123146 140264174335808 run_lib.py:153] step: 324700, training_loss: 1.28719e+02
I1111 15:32:19.589896 140264174335808 run_lib.py:153] step: 324750, training_loss: 1.38562e+02
I1111 15:32:29.285212 140264174335808 run_lib.py:153] step: 324800, training_loss: 1.20978e+02
I1111 15:32:39.551091 140264174335808 run_lib.py:153] step: 324850, training_loss: 1.13358e+02
I1111 15:32:49.992165 140264174335808 run_lib.py:153] step: 324900, training_loss: 1.19996e+02
I1111 15:33:00.052841 140264174335808 run_lib.py:153] step: 324950, training_loss: 1.37380e+02
I1111 15:33:10.402751 140264174335808 run_lib.py:153] step: 325000, training_loss: 1.27025e+02
I1111 15:33:10.507410 140264174335808 run_lib.py:166] step: 325000, eval_loss: 1.28730e+02
I1111 15:33:20.281280 140264174335808 run_lib.py:153] step: 325050, training_loss: 1.04901e+02
I1111 15:33:30.193000 140264174335808 run_lib.py:153] step: 325100, training_loss: 1.00198e+02
I1111 15:33:39.795582 140264174335808 run_lib.py:153] step: 325150, training_loss: 1.26954e+02
I1111 15:33:50.216721 140264174335808 run_lib.py:153] step: 325200, training_loss: 1.08341e+02
I1111 15:34:00.239521 140264174335808 run_lib.py:153] step: 325250, training_loss: 1.48246e+02
I1111 15:34:10.740593 140264174335808 run_lib.py:153] step: 325300, training_loss: 1.37945e+02
I1111 15:34:20.798635 140264174335808 run_lib.py:153] step: 325350, training_loss: 1.11968e+02
I1111 15:34:31.432741 140264174335808 run_lib.py:153] step: 325400, training_loss: 9.77163e+01
I1111 15:34:41.405443 140264174335808 run_lib.py:153] step: 325450, training_loss: 1.13306e+02
I1111 15:34:51.029743 140264174335808 run_lib.py:153] step: 325500, training_loss: 1.16150e+02
I1111 15:35:00.648121 140264174335808 run_lib.py:153] step: 325550, training_loss: 1.23359e+02
I1111 15:35:11.232440 140264174335808 run_lib.py:153] step: 325600, training_loss: 1.09134e+02
I1111 15:35:21.775978 140264174335808 run_lib.py:153] step: 325650, training_loss: 1.17378e+02
I1111 15:35:31.668421 140264174335808 run_lib.py:153] step: 325700, training_loss: 1.18413e+02
I1111 15:35:41.624422 140264174335808 run_lib.py:153] step: 325750, training_loss: 1.16673e+02
I1111 15:35:51.067757 140264174335808 run_lib.py:153] step: 325800, training_loss: 1.23452e+02
I1111 15:36:00.574857 140264174335808 run_lib.py:153] step: 325850, training_loss: 1.01538e+02
I1111 15:36:10.346298 140264174335808 run_lib.py:153] step: 325900, training_loss: 1.47242e+02
I1111 15:36:20.308829 140264174335808 run_lib.py:153] step: 325950, training_loss: 1.16118e+02
I1111 15:36:30.719988 140264174335808 run_lib.py:153] step: 326000, training_loss: 1.60938e+02
I1111 15:36:41.656275 140264174335808 run_lib.py:153] step: 326050, training_loss: 1.23527e+02
I1111 15:36:51.374032 140264174335808 run_lib.py:153] step: 326100, training_loss: 9.61408e+01
I1111 15:37:01.382123 140264174335808 run_lib.py:153] step: 326150, training_loss: 1.40374e+02
I1111 15:37:11.084543 140264174335808 run_lib.py:153] step: 326200, training_loss: 1.22136e+02
I1111 15:37:21.685110 140264174335808 run_lib.py:153] step: 326250, training_loss: 1.25189e+02
I1111 15:37:31.643594 140264174335808 run_lib.py:153] step: 326300, training_loss: 1.13857e+02
I1111 15:37:42.085102 140264174335808 run_lib.py:153] step: 326350, training_loss: 1.34485e+02
I1111 15:37:52.102410 140264174335808 run_lib.py:153] step: 326400, training_loss: 1.49268e+02
I1111 15:38:01.819761 140264174335808 run_lib.py:153] step: 326450, training_loss: 1.44038e+02
I1111 15:38:11.957854 140264174335808 run_lib.py:153] step: 326500, training_loss: 1.31043e+02
I1111 15:38:22.439446 140264174335808 run_lib.py:153] step: 326550, training_loss: 1.35006e+02
I1111 15:38:33.082693 140264174335808 run_lib.py:153] step: 326600, training_loss: 1.15670e+02
I1111 15:38:43.289238 140264174335808 run_lib.py:153] step: 326650, training_loss: 1.22584e+02
I1111 15:38:53.458620 140264174335808 run_lib.py:153] step: 326700, training_loss: 1.08261e+02
I1111 15:39:03.530354 140264174335808 run_lib.py:153] step: 326750, training_loss: 1.18138e+02
I1111 15:39:13.655179 140264174335808 run_lib.py:153] step: 326800, training_loss: 1.15220e+02
I1111 15:39:24.087757 140264174335808 run_lib.py:153] step: 326850, training_loss: 1.17397e+02
I1111 15:39:34.600920 140264174335808 run_lib.py:153] step: 326900, training_loss: 1.29513e+02
I1111 15:39:45.346937 140264174335808 run_lib.py:153] step: 326950, training_loss: 1.29294e+02
I1111 15:39:55.027709 140264174335808 run_lib.py:153] step: 327000, training_loss: 1.32161e+02
I1111 15:40:05.415975 140264174335808 run_lib.py:153] step: 327050, training_loss: 1.34480e+02
I1111 15:40:14.990459 140264174335808 run_lib.py:153] step: 327100, training_loss: 1.43523e+02
I1111 15:40:24.979466 140264174335808 run_lib.py:153] step: 327150, training_loss: 1.21058e+02
I1111 15:40:35.305323 140264174335808 run_lib.py:153] step: 327200, training_loss: 1.06352e+02
I1111 15:40:45.131034 140264174335808 run_lib.py:153] step: 327250, training_loss: 1.31640e+02
I1111 15:40:54.699702 140264174335808 run_lib.py:153] step: 327300, training_loss: 1.49914e+02
I1111 15:41:05.375693 140264174335808 run_lib.py:153] step: 327350, training_loss: 1.34138e+02
I1111 15:41:15.283671 140264174335808 run_lib.py:153] step: 327400, training_loss: 1.04867e+02
I1111 15:41:24.976299 140264174335808 run_lib.py:153] step: 327450, training_loss: 1.01422e+02
I1111 15:41:34.262712 140264174335808 run_lib.py:153] step: 327500, training_loss: 1.19937e+02
I1111 15:41:43.587502 140264174335808 run_lib.py:153] step: 327550, training_loss: 1.17586e+02
I1111 15:41:53.449548 140264174335808 run_lib.py:153] step: 327600, training_loss: 1.41643e+02
I1111 15:42:03.182140 140264174335808 run_lib.py:153] step: 327650, training_loss: 9.26889e+01
I1111 15:42:12.667355 140264174335808 run_lib.py:153] step: 327700, training_loss: 1.29882e+02
I1111 15:42:23.166566 140264174335808 run_lib.py:153] step: 327750, training_loss: 1.51256e+02
I1111 15:42:32.998418 140264174335808 run_lib.py:153] step: 327800, training_loss: 1.31166e+02
I1111 15:42:42.548147 140264174335808 run_lib.py:153] step: 327850, training_loss: 1.18859e+02
I1111 15:42:52.308442 140264174335808 run_lib.py:153] step: 327900, training_loss: 1.24690e+02
I1111 15:43:02.224202 140264174335808 run_lib.py:153] step: 327950, training_loss: 1.21396e+02
I1111 15:43:12.188320 140264174335808 run_lib.py:153] step: 328000, training_loss: 1.03816e+02
I1111 15:43:22.906353 140264174335808 run_lib.py:153] step: 328050, training_loss: 1.29562e+02
I1111 15:43:33.053066 140264174335808 run_lib.py:153] step: 328100, training_loss: 1.24274e+02
I1111 15:43:42.592398 140264174335808 run_lib.py:153] step: 328150, training_loss: 1.21829e+02
I1111 15:43:52.590746 140264174335808 run_lib.py:153] step: 328200, training_loss: 1.17022e+02
I1111 15:44:03.229370 140264174335808 run_lib.py:153] step: 328250, training_loss: 1.08718e+02
I1111 15:44:14.058610 140264174335808 run_lib.py:153] step: 328300, training_loss: 1.32611e+02
I1111 15:44:24.761873 140264174335808 run_lib.py:153] step: 328350, training_loss: 1.20792e+02
I1111 15:44:35.261300 140264174335808 run_lib.py:153] step: 328400, training_loss: 1.69586e+02
I1111 15:44:45.327479 140264174335808 run_lib.py:153] step: 328450, training_loss: 1.38708e+02
I1111 15:44:55.358288 140264174335808 run_lib.py:153] step: 328500, training_loss: 1.02902e+02
I1111 15:45:04.864704 140264174335808 run_lib.py:153] step: 328550, training_loss: 1.16405e+02
I1111 15:45:14.841836 140264174335808 run_lib.py:153] step: 328600, training_loss: 1.24158e+02
I1111 15:45:24.497411 140264174335808 run_lib.py:153] step: 328650, training_loss: 1.32951e+02
I1111 15:45:34.601765 140264174335808 run_lib.py:153] step: 328700, training_loss: 1.25705e+02
I1111 15:45:45.106437 140264174335808 run_lib.py:153] step: 328750, training_loss: 1.25191e+02
I1111 15:45:54.801955 140264174335808 run_lib.py:153] step: 328800, training_loss: 1.08143e+02
I1111 15:46:05.249477 140264174335808 run_lib.py:153] step: 328850, training_loss: 1.20145e+02
I1111 15:46:15.889845 140264174335808 run_lib.py:153] step: 328900, training_loss: 1.11549e+02
I1111 15:46:25.908411 140264174335808 run_lib.py:153] step: 328950, training_loss: 1.09462e+02
I1111 15:46:36.286178 140264174335808 run_lib.py:153] step: 329000, training_loss: 1.37385e+02
I1111 15:46:46.554300 140264174335808 run_lib.py:153] step: 329050, training_loss: 1.80604e+02
I1111 15:46:56.257091 140264174335808 run_lib.py:153] step: 329100, training_loss: 1.21433e+02
I1111 15:47:06.499608 140264174335808 run_lib.py:153] step: 329150, training_loss: 1.34796e+02
I1111 15:47:16.536684 140264174335808 run_lib.py:153] step: 329200, training_loss: 1.19529e+02
I1111 15:47:25.943556 140264174335808 run_lib.py:153] step: 329250, training_loss: 1.01433e+02
I1111 15:47:35.655972 140264174335808 run_lib.py:153] step: 329300, training_loss: 1.05421e+02
I1111 15:47:45.758259 140264174335808 run_lib.py:153] step: 329350, training_loss: 1.22975e+02
I1111 15:47:55.576586 140264174335808 run_lib.py:153] step: 329400, training_loss: 9.47029e+01
I1111 15:48:05.233613 140264174335808 run_lib.py:153] step: 329450, training_loss: 1.35029e+02
I1111 15:48:14.872642 140264174335808 run_lib.py:153] step: 329500, training_loss: 9.72489e+01
I1111 15:48:25.081482 140264174335808 run_lib.py:153] step: 329550, training_loss: 1.53620e+02
I1111 15:48:35.094060 140264174335808 run_lib.py:153] step: 329600, training_loss: 1.21634e+02
I1111 15:48:44.587539 140264174335808 run_lib.py:153] step: 329650, training_loss: 1.43908e+02
I1111 15:48:54.320085 140264174335808 run_lib.py:153] step: 329700, training_loss: 1.44009e+02
I1111 15:49:04.639754 140264174335808 run_lib.py:153] step: 329750, training_loss: 1.03383e+02
I1111 15:49:14.596307 140264174335808 run_lib.py:153] step: 329800, training_loss: 1.45241e+02
I1111 15:49:24.300067 140264174335808 run_lib.py:153] step: 329850, training_loss: 1.31956e+02
I1111 15:49:34.914228 140264174335808 run_lib.py:153] step: 329900, training_loss: 1.15425e+02
I1111 15:49:44.849260 140264174335808 run_lib.py:153] step: 329950, training_loss: 1.47729e+02
I1111 15:49:55.614057 140264174335808 run_lib.py:153] step: 330000, training_loss: 1.48803e+02
I1111 15:49:56.194851 140264174335808 run_lib.py:166] step: 330000, eval_loss: 1.49979e+02
I1111 15:50:06.051882 140264174335808 run_lib.py:153] step: 330050, training_loss: 1.10438e+02
I1111 15:50:16.248177 140264174335808 run_lib.py:153] step: 330100, training_loss: 1.23058e+02
I1111 15:50:26.947942 140264174335808 run_lib.py:153] step: 330150, training_loss: 1.30921e+02
I1111 15:50:37.671553 140264174335808 run_lib.py:153] step: 330200, training_loss: 1.37658e+02
I1111 15:50:47.220162 140264174335808 run_lib.py:153] step: 330250, training_loss: 9.60292e+01
I1111 15:50:56.859849 140264174335808 run_lib.py:153] step: 330300, training_loss: 1.17319e+02
I1111 15:51:06.925749 140264174335808 run_lib.py:153] step: 330350, training_loss: 1.09239e+02
I1111 15:51:16.953949 140264174335808 run_lib.py:153] step: 330400, training_loss: 9.35255e+01
I1111 15:51:26.462430 140264174335808 run_lib.py:153] step: 330450, training_loss: 1.25119e+02
I1111 15:51:36.262859 140264174335808 run_lib.py:153] step: 330500, training_loss: 1.52880e+02
I1111 15:51:45.675526 140264174335808 run_lib.py:153] step: 330550, training_loss: 1.08854e+02
I1111 15:51:55.334424 140264174335808 run_lib.py:153] step: 330600, training_loss: 1.25362e+02
I1111 15:52:05.094512 140264174335808 run_lib.py:153] step: 330650, training_loss: 1.07763e+02
I1111 15:52:14.656511 140264174335808 run_lib.py:153] step: 330700, training_loss: 1.49800e+02
I1111 15:52:24.504552 140264174335808 run_lib.py:153] step: 330750, training_loss: 1.38457e+02
I1111 15:52:34.268527 140264174335808 run_lib.py:153] step: 330800, training_loss: 1.35287e+02
I1111 15:52:43.662623 140264174335808 run_lib.py:153] step: 330850, training_loss: 1.26831e+02
I1111 15:52:54.474572 140264174335808 run_lib.py:153] step: 330900, training_loss: 1.11898e+02
I1111 15:53:05.277946 140264174335808 run_lib.py:153] step: 330950, training_loss: 1.07729e+02
I1111 15:53:15.523517 140264174335808 run_lib.py:153] step: 331000, training_loss: 1.24841e+02
I1111 15:53:25.260869 140264174335808 run_lib.py:153] step: 331050, training_loss: 1.40005e+02
I1111 15:53:35.518768 140264174335808 run_lib.py:153] step: 331100, training_loss: 1.37712e+02
I1111 15:53:45.531038 140264174335808 run_lib.py:153] step: 331150, training_loss: 1.29844e+02
I1111 15:53:55.098892 140264174335808 run_lib.py:153] step: 331200, training_loss: 1.08858e+02
I1111 15:54:04.719048 140264174335808 run_lib.py:153] step: 331250, training_loss: 1.35847e+02
I1111 15:54:14.850224 140264174335808 run_lib.py:153] step: 331300, training_loss: 1.23900e+02
I1111 15:54:25.665470 140264174335808 run_lib.py:153] step: 331350, training_loss: 1.02107e+02
I1111 15:54:36.470541 140264174335808 run_lib.py:153] step: 331400, training_loss: 1.41964e+02
I1111 15:54:47.078111 140264174335808 run_lib.py:153] step: 331450, training_loss: 1.29570e+02
I1111 15:54:57.533811 140264174335808 run_lib.py:153] step: 331500, training_loss: 1.25891e+02
I1111 15:55:08.127725 140264174335808 run_lib.py:153] step: 331550, training_loss: 1.19136e+02
I1111 15:55:17.909698 140264174335808 run_lib.py:153] step: 331600, training_loss: 1.20328e+02
I1111 15:55:27.543554 140264174335808 run_lib.py:153] step: 331650, training_loss: 1.24002e+02
I1111 15:55:37.728111 140264174335808 run_lib.py:153] step: 331700, training_loss: 1.41195e+02
I1111 15:55:47.640859 140264174335808 run_lib.py:153] step: 331750, training_loss: 1.35624e+02
I1111 15:55:57.649832 140264174335808 run_lib.py:153] step: 331800, training_loss: 1.47652e+02
I1111 15:56:07.913761 140264174335808 run_lib.py:153] step: 331850, training_loss: 1.34700e+02
I1111 15:56:18.040880 140264174335808 run_lib.py:153] step: 331900, training_loss: 1.52378e+02
I1111 15:56:27.758472 140264174335808 run_lib.py:153] step: 331950, training_loss: 1.32601e+02
I1111 15:56:37.531044 140264174335808 run_lib.py:153] step: 332000, training_loss: 1.17504e+02
I1111 15:56:47.489869 140264174335808 run_lib.py:153] step: 332050, training_loss: 1.25699e+02
I1111 15:56:56.976248 140264174335808 run_lib.py:153] step: 332100, training_loss: 1.50002e+02
I1111 15:57:06.761086 140264174335808 run_lib.py:153] step: 332150, training_loss: 1.31169e+02
I1111 15:57:16.598685 140264174335808 run_lib.py:153] step: 332200, training_loss: 1.16637e+02
I1111 15:57:26.847091 140264174335808 run_lib.py:153] step: 332250, training_loss: 1.12173e+02
I1111 15:57:36.205488 140264174335808 run_lib.py:153] step: 332300, training_loss: 1.06367e+02
I1111 15:57:45.784325 140264174335808 run_lib.py:153] step: 332350, training_loss: 1.14025e+02
I1111 15:57:55.658701 140264174335808 run_lib.py:153] step: 332400, training_loss: 1.34900e+02
I1111 15:58:05.233073 140264174335808 run_lib.py:153] step: 332450, training_loss: 1.37531e+02
I1111 15:58:15.249280 140264174335808 run_lib.py:153] step: 332500, training_loss: 1.33155e+02
I1111 15:58:25.164753 140264174335808 run_lib.py:153] step: 332550, training_loss: 1.32156e+02
I1111 15:58:35.164544 140264174335808 run_lib.py:153] step: 332600, training_loss: 1.54278e+02
I1111 15:58:45.052690 140264174335808 run_lib.py:153] step: 332650, training_loss: 1.31676e+02
I1111 15:58:54.672378 140264174335808 run_lib.py:153] step: 332700, training_loss: 1.46052e+02
I1111 15:59:04.947602 140264174335808 run_lib.py:153] step: 332750, training_loss: 1.24211e+02
I1111 15:59:14.737860 140264174335808 run_lib.py:153] step: 332800, training_loss: 1.33797e+02
I1111 15:59:24.432023 140264174335808 run_lib.py:153] step: 332850, training_loss: 1.34946e+02
I1111 15:59:33.886659 140264174335808 run_lib.py:153] step: 332900, training_loss: 1.43848e+02
I1111 15:59:44.147999 140264174335808 run_lib.py:153] step: 332950, training_loss: 1.21741e+02
I1111 15:59:54.039700 140264174335808 run_lib.py:153] step: 333000, training_loss: 1.11564e+02
I1111 16:00:04.173039 140264174335808 run_lib.py:153] step: 333050, training_loss: 1.16706e+02
I1111 16:00:13.587390 140264174335808 run_lib.py:153] step: 333100, training_loss: 1.47058e+02
I1111 16:00:23.564597 140264174335808 run_lib.py:153] step: 333150, training_loss: 1.26737e+02
I1111 16:00:32.906297 140264174335808 run_lib.py:153] step: 333200, training_loss: 1.38226e+02
I1111 16:00:42.694676 140264174335808 run_lib.py:153] step: 333250, training_loss: 1.37082e+02
I1111 16:00:52.563158 140264174335808 run_lib.py:153] step: 333300, training_loss: 1.39403e+02
I1111 16:01:02.441579 140264174335808 run_lib.py:153] step: 333350, training_loss: 1.18806e+02
I1111 16:01:11.721299 140264174335808 run_lib.py:153] step: 333400, training_loss: 1.33195e+02
I1111 16:01:21.088818 140264174335808 run_lib.py:153] step: 333450, training_loss: 1.13847e+02
I1111 16:01:30.887082 140264174335808 run_lib.py:153] step: 333500, training_loss: 1.32096e+02
I1111 16:01:40.709912 140264174335808 run_lib.py:153] step: 333550, training_loss: 1.22680e+02
I1111 16:01:50.946995 140264174335808 run_lib.py:153] step: 333600, training_loss: 1.53822e+02
I1111 16:02:00.666554 140264174335808 run_lib.py:153] step: 333650, training_loss: 1.06673e+02
I1111 16:02:10.925992 140264174335808 run_lib.py:153] step: 333700, training_loss: 1.37082e+02
I1111 16:02:20.145565 140264174335808 run_lib.py:153] step: 333750, training_loss: 1.00291e+02
I1111 16:02:30.185444 140264174335808 run_lib.py:153] step: 333800, training_loss: 1.22872e+02
I1111 16:02:40.421915 140264174335808 run_lib.py:153] step: 333850, training_loss: 1.30027e+02
I1111 16:02:49.935315 140264174335808 run_lib.py:153] step: 333900, training_loss: 1.15002e+02
I1111 16:02:59.614423 140264174335808 run_lib.py:153] step: 333950, training_loss: 1.15175e+02
I1111 16:03:09.076293 140264174335808 run_lib.py:153] step: 334000, training_loss: 1.19537e+02
I1111 16:03:18.717291 140264174335808 run_lib.py:153] step: 334050, training_loss: 1.38634e+02
I1111 16:03:28.314920 140264174335808 run_lib.py:153] step: 334100, training_loss: 1.21647e+02
I1111 16:03:38.590839 140264174335808 run_lib.py:153] step: 334150, training_loss: 1.26260e+02
I1111 16:03:48.907326 140264174335808 run_lib.py:153] step: 334200, training_loss: 1.32040e+02
I1111 16:03:58.791593 140264174335808 run_lib.py:153] step: 334250, training_loss: 1.39017e+02
I1111 16:04:09.042856 140264174335808 run_lib.py:153] step: 334300, training_loss: 1.40796e+02
I1111 16:04:19.044287 140264174335808 run_lib.py:153] step: 334350, training_loss: 1.21717e+02
I1111 16:04:28.615211 140264174335808 run_lib.py:153] step: 334400, training_loss: 1.11533e+02
I1111 16:04:38.260791 140264174335808 run_lib.py:153] step: 334450, training_loss: 9.26982e+01
I1111 16:04:48.254029 140264174335808 run_lib.py:153] step: 334500, training_loss: 8.67524e+01
I1111 16:04:58.178031 140264174335808 run_lib.py:153] step: 334550, training_loss: 1.14982e+02
I1111 16:05:07.891525 140264174335808 run_lib.py:153] step: 334600, training_loss: 1.32446e+02
I1111 16:05:18.273341 140264174335808 run_lib.py:153] step: 334650, training_loss: 8.93820e+01
I1111 16:05:28.903017 140264174335808 run_lib.py:153] step: 334700, training_loss: 1.22773e+02
I1111 16:05:38.859949 140264174335808 run_lib.py:153] step: 334750, training_loss: 1.29169e+02
I1111 16:05:48.588691 140264174335808 run_lib.py:153] step: 334800, training_loss: 1.07912e+02
I1111 16:05:58.454431 140264174335808 run_lib.py:153] step: 334850, training_loss: 1.24593e+02
I1111 16:06:08.219729 140264174335808 run_lib.py:153] step: 334900, training_loss: 1.21011e+02
I1111 16:06:17.866250 140264174335808 run_lib.py:153] step: 334950, training_loss: 1.32260e+02
I1111 16:06:27.858760 140264174335808 run_lib.py:153] step: 335000, training_loss: 1.07902e+02
I1111 16:06:27.961757 140264174335808 run_lib.py:166] step: 335000, eval_loss: 1.13134e+02
I1111 16:06:37.312663 140264174335808 run_lib.py:153] step: 335050, training_loss: 1.51770e+02
I1111 16:06:46.734234 140264174335808 run_lib.py:153] step: 335100, training_loss: 9.66082e+01
I1111 16:06:57.112507 140264174335808 run_lib.py:153] step: 335150, training_loss: 1.48303e+02
I1111 16:07:07.855143 140264174335808 run_lib.py:153] step: 335200, training_loss: 1.28755e+02
I1111 16:07:17.373440 140264174335808 run_lib.py:153] step: 335250, training_loss: 1.04901e+02
I1111 16:07:27.460580 140264174335808 run_lib.py:153] step: 335300, training_loss: 9.82096e+01
I1111 16:07:37.668109 140264174335808 run_lib.py:153] step: 335350, training_loss: 1.04770e+02
I1111 16:07:47.734646 140264174335808 run_lib.py:153] step: 335400, training_loss: 1.25294e+02
I1111 16:07:58.348605 140264174335808 run_lib.py:153] step: 335450, training_loss: 1.24297e+02
I1111 16:08:08.014730 140264174335808 run_lib.py:153] step: 335500, training_loss: 1.26906e+02
I1111 16:08:17.728045 140264174335808 run_lib.py:153] step: 335550, training_loss: 1.39875e+02
I1111 16:08:27.269144 140264174335808 run_lib.py:153] step: 335600, training_loss: 1.38843e+02
I1111 16:08:36.875789 140264174335808 run_lib.py:153] step: 335650, training_loss: 1.19975e+02
I1111 16:08:46.348186 140264174335808 run_lib.py:153] step: 335700, training_loss: 1.13362e+02
I1111 16:08:56.218270 140264174335808 run_lib.py:153] step: 335750, training_loss: 1.49322e+02
I1111 16:09:05.976886 140264174335808 run_lib.py:153] step: 335800, training_loss: 8.03883e+01
I1111 16:09:15.366506 140264174335808 run_lib.py:153] step: 335850, training_loss: 1.06856e+02
I1111 16:09:25.513889 140264174335808 run_lib.py:153] step: 335900, training_loss: 1.12224e+02
I1111 16:09:35.239721 140264174335808 run_lib.py:153] step: 335950, training_loss: 1.17282e+02
I1111 16:09:45.695428 140264174335808 run_lib.py:153] step: 336000, training_loss: 1.28712e+02
I1111 16:09:56.469686 140264174335808 run_lib.py:153] step: 336050, training_loss: 1.03294e+02
I1111 16:10:06.408485 140264174335808 run_lib.py:153] step: 336100, training_loss: 1.28780e+02
I1111 16:10:16.108773 140264174335808 run_lib.py:153] step: 336150, training_loss: 1.05563e+02
I1111 16:10:25.647795 140264174335808 run_lib.py:153] step: 336200, training_loss: 1.25004e+02
I1111 16:10:35.154095 140264174335808 run_lib.py:153] step: 336250, training_loss: 1.11989e+02
I1111 16:10:45.183516 140264174335808 run_lib.py:153] step: 336300, training_loss: 1.18614e+02
I1111 16:10:54.966414 140264174335808 run_lib.py:153] step: 336350, training_loss: 1.37092e+02
I1111 16:11:05.013841 140264174335808 run_lib.py:153] step: 336400, training_loss: 1.34512e+02
I1111 16:11:15.084869 140264174335808 run_lib.py:153] step: 336450, training_loss: 1.10550e+02
I1111 16:11:25.440866 140264174335808 run_lib.py:153] step: 336500, training_loss: 1.16300e+02
I1111 16:11:35.014303 140264174335808 run_lib.py:153] step: 336550, training_loss: 1.51284e+02
I1111 16:11:44.844623 140264174335808 run_lib.py:153] step: 336600, training_loss: 1.40014e+02
I1111 16:11:54.870214 140264174335808 run_lib.py:153] step: 336650, training_loss: 1.10661e+02
I1111 16:12:05.054267 140264174335808 run_lib.py:153] step: 336700, training_loss: 1.24377e+02
I1111 16:12:14.353895 140264174335808 run_lib.py:153] step: 336750, training_loss: 1.19786e+02
I1111 16:12:24.700911 140264174335808 run_lib.py:153] step: 336800, training_loss: 1.20927e+02
I1111 16:12:34.940185 140264174335808 run_lib.py:153] step: 336850, training_loss: 1.47353e+02
I1111 16:12:44.998082 140264174335808 run_lib.py:153] step: 336900, training_loss: 1.47522e+02
I1111 16:12:55.606462 140264174335808 run_lib.py:153] step: 336950, training_loss: 1.19316e+02
I1111 16:13:05.926711 140264174335808 run_lib.py:153] step: 337000, training_loss: 1.10531e+02
I1111 16:13:16.444087 140264174335808 run_lib.py:153] step: 337050, training_loss: 1.09316e+02
I1111 16:13:26.964267 140264174335808 run_lib.py:153] step: 337100, training_loss: 1.23793e+02
I1111 16:13:37.903450 140264174335808 run_lib.py:153] step: 337150, training_loss: 1.08061e+02
I1111 16:13:48.204017 140264174335808 run_lib.py:153] step: 337200, training_loss: 1.11823e+02
I1111 16:13:58.403761 140264174335808 run_lib.py:153] step: 337250, training_loss: 1.16016e+02
I1111 16:14:08.368534 140264174335808 run_lib.py:153] step: 337300, training_loss: 1.19691e+02
I1111 16:14:18.785358 140264174335808 run_lib.py:153] step: 337350, training_loss: 1.23315e+02
I1111 16:14:28.751876 140264174335808 run_lib.py:153] step: 337400, training_loss: 1.17374e+02
I1111 16:14:39.574488 140264174335808 run_lib.py:153] step: 337450, training_loss: 9.32713e+01
I1111 16:14:49.615175 140264174335808 run_lib.py:153] step: 337500, training_loss: 1.24938e+02
I1111 16:15:00.297862 140264174335808 run_lib.py:153] step: 337550, training_loss: 1.55676e+02
I1111 16:15:10.349978 140264174335808 run_lib.py:153] step: 337600, training_loss: 1.09997e+02
I1111 16:15:20.774970 140264174335808 run_lib.py:153] step: 337650, training_loss: 1.07496e+02
I1111 16:15:30.324507 140264174335808 run_lib.py:153] step: 337700, training_loss: 1.55378e+02
I1111 16:15:40.950064 140264174335808 run_lib.py:153] step: 337750, training_loss: 1.09199e+02
I1111 16:15:50.778082 140264174335808 run_lib.py:153] step: 337800, training_loss: 1.23444e+02
I1111 16:16:00.704589 140264174335808 run_lib.py:153] step: 337850, training_loss: 1.49225e+02
I1111 16:16:10.003830 140264174335808 run_lib.py:153] step: 337900, training_loss: 1.22999e+02
I1111 16:16:19.453409 140264174335808 run_lib.py:153] step: 337950, training_loss: 1.29095e+02
I1111 16:16:29.494009 140264174335808 run_lib.py:153] step: 338000, training_loss: 1.68444e+02
I1111 16:16:39.659185 140264174335808 run_lib.py:153] step: 338050, training_loss: 1.45411e+02
I1111 16:16:50.182690 140264174335808 run_lib.py:153] step: 338100, training_loss: 1.39631e+02
I1111 16:17:00.014566 140264174335808 run_lib.py:153] step: 338150, training_loss: 1.37656e+02
I1111 16:17:09.572559 140264174335808 run_lib.py:153] step: 338200, training_loss: 1.07442e+02
I1111 16:17:20.134799 140264174335808 run_lib.py:153] step: 338250, training_loss: 1.26851e+02
I1111 16:17:30.502931 140264174335808 run_lib.py:153] step: 338300, training_loss: 1.35095e+02
I1111 16:17:40.983230 140264174335808 run_lib.py:153] step: 338350, training_loss: 1.44401e+02
I1111 16:17:51.159139 140264174335808 run_lib.py:153] step: 338400, training_loss: 1.22661e+02
I1111 16:18:01.234186 140264174335808 run_lib.py:153] step: 338450, training_loss: 1.14189e+02
I1111 16:18:10.898718 140264174335808 run_lib.py:153] step: 338500, training_loss: 1.35422e+02
I1111 16:18:20.657334 140264174335808 run_lib.py:153] step: 338550, training_loss: 1.50682e+02
I1111 16:18:31.510928 140264174335808 run_lib.py:153] step: 338600, training_loss: 1.23810e+02
I1111 16:18:41.848320 140264174335808 run_lib.py:153] step: 338650, training_loss: 1.14973e+02
I1111 16:18:51.971332 140264174335808 run_lib.py:153] step: 338700, training_loss: 1.33351e+02
I1111 16:19:02.315473 140264174335808 run_lib.py:153] step: 338750, training_loss: 1.26105e+02
I1111 16:19:12.417233 140264174335808 run_lib.py:153] step: 338800, training_loss: 1.65348e+02
I1111 16:19:22.990041 140264174335808 run_lib.py:153] step: 338850, training_loss: 1.28215e+02
I1111 16:19:32.644567 140264174335808 run_lib.py:153] step: 338900, training_loss: 9.64149e+01
I1111 16:19:43.613716 140264174335808 run_lib.py:153] step: 338950, training_loss: 1.23841e+02
I1111 16:19:54.256194 140264174335808 run_lib.py:153] step: 339000, training_loss: 1.37394e+02
I1111 16:20:04.634995 140264174335808 run_lib.py:153] step: 339050, training_loss: 1.36962e+02
I1111 16:20:15.101944 140264174335808 run_lib.py:153] step: 339100, training_loss: 1.29632e+02
I1111 16:20:25.286679 140264174335808 run_lib.py:153] step: 339150, training_loss: 1.55583e+02
I1111 16:20:35.200301 140264174335808 run_lib.py:153] step: 339200, training_loss: 1.50655e+02
I1111 16:20:45.090042 140264174335808 run_lib.py:153] step: 339250, training_loss: 1.19045e+02
I1111 16:20:55.901971 140264174335808 run_lib.py:153] step: 339300, training_loss: 1.48292e+02
I1111 16:21:06.681902 140264174335808 run_lib.py:153] step: 339350, training_loss: 1.12099e+02
I1111 16:21:16.994437 140264174335808 run_lib.py:153] step: 339400, training_loss: 8.54663e+01
I1111 16:21:26.909713 140264174335808 run_lib.py:153] step: 339450, training_loss: 9.71555e+01
I1111 16:21:37.744109 140264174335808 run_lib.py:153] step: 339500, training_loss: 1.55294e+02
I1111 16:21:47.995001 140264174335808 run_lib.py:153] step: 339550, training_loss: 1.14357e+02
I1111 16:21:57.806625 140264174335808 run_lib.py:153] step: 339600, training_loss: 1.20938e+02
I1111 16:22:07.781808 140264174335808 run_lib.py:153] step: 339650, training_loss: 1.05029e+02
I1111 16:22:18.148953 140264174335808 run_lib.py:153] step: 339700, training_loss: 1.32425e+02
I1111 16:22:28.142532 140264174335808 run_lib.py:153] step: 339750, training_loss: 1.03855e+02
I1111 16:22:38.594730 140264174335808 run_lib.py:153] step: 339800, training_loss: 1.23225e+02
I1111 16:22:49.508127 140264174335808 run_lib.py:153] step: 339850, training_loss: 1.25149e+02
I1111 16:22:59.759685 140264174335808 run_lib.py:153] step: 339900, training_loss: 1.13302e+02
I1111 16:23:09.611176 140264174335808 run_lib.py:153] step: 339950, training_loss: 1.44021e+02
I1111 16:23:20.439845 140264174335808 run_lib.py:153] step: 340000, training_loss: 1.21255e+02
I1111 16:23:21.023447 140264174335808 run_lib.py:166] step: 340000, eval_loss: 1.29463e+02
I1111 16:23:31.390565 140264174335808 run_lib.py:153] step: 340050, training_loss: 1.05529e+02
I1111 16:23:41.698800 140264174335808 run_lib.py:153] step: 340100, training_loss: 1.37073e+02
I1111 16:23:51.993045 140264174335808 run_lib.py:153] step: 340150, training_loss: 1.42415e+02
I1111 16:24:02.233683 140264174335808 run_lib.py:153] step: 340200, training_loss: 1.16843e+02
I1111 16:24:11.617608 140264174335808 run_lib.py:153] step: 340250, training_loss: 1.19490e+02
I1111 16:24:21.513471 140264174335808 run_lib.py:153] step: 340300, training_loss: 1.29237e+02
I1111 16:24:31.616527 140264174335808 run_lib.py:153] step: 340350, training_loss: 1.23537e+02
I1111 16:24:41.915349 140264174335808 run_lib.py:153] step: 340400, training_loss: 1.19175e+02
I1111 16:24:52.357727 140264174335808 run_lib.py:153] step: 340450, training_loss: 1.35772e+02
I1111 16:25:02.701722 140264174335808 run_lib.py:153] step: 340500, training_loss: 1.35751e+02
I1111 16:25:12.751015 140264174335808 run_lib.py:153] step: 340550, training_loss: 1.16794e+02
I1111 16:25:22.997860 140264174335808 run_lib.py:153] step: 340600, training_loss: 1.16092e+02
I1111 16:25:33.285782 140264174335808 run_lib.py:153] step: 340650, training_loss: 1.37334e+02
I1111 16:25:43.851297 140264174335808 run_lib.py:153] step: 340700, training_loss: 1.10263e+02
I1111 16:25:53.623358 140264174335808 run_lib.py:153] step: 340750, training_loss: 1.05178e+02
I1111 16:26:03.744260 140264174335808 run_lib.py:153] step: 340800, training_loss: 1.15989e+02
I1111 16:26:13.860337 140264174335808 run_lib.py:153] step: 340850, training_loss: 1.10082e+02
I1111 16:26:23.669117 140264174335808 run_lib.py:153] step: 340900, training_loss: 1.35911e+02
I1111 16:26:34.006983 140264174335808 run_lib.py:153] step: 340950, training_loss: 1.45659e+02
I1111 16:26:44.038569 140264174335808 run_lib.py:153] step: 341000, training_loss: 1.08288e+02
I1111 16:26:54.233906 140264174335808 run_lib.py:153] step: 341050, training_loss: 1.37856e+02
I1111 16:27:04.411468 140264174335808 run_lib.py:153] step: 341100, training_loss: 1.21522e+02
I1111 16:27:14.054876 140264174335808 run_lib.py:153] step: 341150, training_loss: 1.04912e+02
I1111 16:27:24.008058 140264174335808 run_lib.py:153] step: 341200, training_loss: 1.38120e+02
I1111 16:27:34.186696 140264174335808 run_lib.py:153] step: 341250, training_loss: 1.21695e+02
I1111 16:27:44.419949 140264174335808 run_lib.py:153] step: 341300, training_loss: 1.11140e+02
I1111 16:27:54.857301 140264174335808 run_lib.py:153] step: 341350, training_loss: 9.03711e+01
I1111 16:28:05.843533 140264174335808 run_lib.py:153] step: 341400, training_loss: 1.04345e+02
I1111 16:28:16.189710 140264174335808 run_lib.py:153] step: 341450, training_loss: 1.54612e+02
I1111 16:28:26.446743 140264174335808 run_lib.py:153] step: 341500, training_loss: 1.34097e+02
I1111 16:28:36.230707 140264174335808 run_lib.py:153] step: 341550, training_loss: 1.50078e+02
I1111 16:28:46.543671 140264174335808 run_lib.py:153] step: 341600, training_loss: 1.07632e+02
I1111 16:28:55.897263 140264174335808 run_lib.py:153] step: 341650, training_loss: 1.22167e+02
I1111 16:29:05.519679 140264174335808 run_lib.py:153] step: 341700, training_loss: 1.21587e+02
I1111 16:29:16.020393 140264174335808 run_lib.py:153] step: 341750, training_loss: 1.16068e+02
I1111 16:29:25.979914 140264174335808 run_lib.py:153] step: 341800, training_loss: 1.12594e+02
I1111 16:29:36.851568 140264174335808 run_lib.py:153] step: 341850, training_loss: 1.35305e+02
I1111 16:29:47.231321 140264174335808 run_lib.py:153] step: 341900, training_loss: 1.31469e+02
I1111 16:29:57.336428 140264174335808 run_lib.py:153] step: 341950, training_loss: 1.00256e+02
I1111 16:30:07.197930 140264174335808 run_lib.py:153] step: 342000, training_loss: 1.40157e+02
I1111 16:30:16.968906 140264174335808 run_lib.py:153] step: 342050, training_loss: 1.46068e+02
I1111 16:30:27.162480 140264174335808 run_lib.py:153] step: 342100, training_loss: 1.35544e+02
I1111 16:30:37.757082 140264174335808 run_lib.py:153] step: 342150, training_loss: 1.30867e+02
I1111 16:30:47.733521 140264174335808 run_lib.py:153] step: 342200, training_loss: 1.16832e+02
I1111 16:30:58.153553 140264174335808 run_lib.py:153] step: 342250, training_loss: 1.30685e+02
I1111 16:31:08.600173 140264174335808 run_lib.py:153] step: 342300, training_loss: 1.35420e+02
I1111 16:31:19.092543 140264174335808 run_lib.py:153] step: 342350, training_loss: 1.57703e+02
I1111 16:31:28.343499 140264174335808 run_lib.py:153] step: 342400, training_loss: 1.11653e+02
I1111 16:31:39.051370 140264174335808 run_lib.py:153] step: 342450, training_loss: 1.19242e+02
I1111 16:31:49.354175 140264174335808 run_lib.py:153] step: 342500, training_loss: 1.07379e+02
I1111 16:31:59.148236 140264174335808 run_lib.py:153] step: 342550, training_loss: 9.89509e+01
I1111 16:32:09.081425 140264174335808 run_lib.py:153] step: 342600, training_loss: 1.12669e+02
I1111 16:32:19.253478 140264174335808 run_lib.py:153] step: 342650, training_loss: 1.32811e+02
I1111 16:32:28.817338 140264174335808 run_lib.py:153] step: 342700, training_loss: 1.12097e+02
I1111 16:32:38.718570 140264174335808 run_lib.py:153] step: 342750, training_loss: 1.39432e+02
I1111 16:32:48.489728 140264174335808 run_lib.py:153] step: 342800, training_loss: 1.12036e+02
I1111 16:32:58.526392 140264174335808 run_lib.py:153] step: 342850, training_loss: 1.26506e+02
I1111 16:33:08.220624 140264174335808 run_lib.py:153] step: 342900, training_loss: 1.05691e+02
I1111 16:33:18.331340 140264174335808 run_lib.py:153] step: 342950, training_loss: 1.37737e+02
I1111 16:33:28.277398 140264174335808 run_lib.py:153] step: 343000, training_loss: 1.51148e+02
I1111 16:33:38.246941 140264174335808 run_lib.py:153] step: 343050, training_loss: 1.39080e+02
I1111 16:33:48.437839 140264174335808 run_lib.py:153] step: 343100, training_loss: 1.51097e+02
I1111 16:33:57.860607 140264174335808 run_lib.py:153] step: 343150, training_loss: 1.43784e+02
I1111 16:34:07.821486 140264174335808 run_lib.py:153] step: 343200, training_loss: 1.19120e+02
I1111 16:34:17.576163 140264174335808 run_lib.py:153] step: 343250, training_loss: 1.22797e+02
I1111 16:34:27.304273 140264174335808 run_lib.py:153] step: 343300, training_loss: 1.34650e+02
I1111 16:34:36.615536 140264174335808 run_lib.py:153] step: 343350, training_loss: 1.44014e+02
I1111 16:34:46.198423 140264174335808 run_lib.py:153] step: 343400, training_loss: 1.16150e+02
I1111 16:34:56.091238 140264174335808 run_lib.py:153] step: 343450, training_loss: 1.42163e+02
I1111 16:35:05.977104 140264174335808 run_lib.py:153] step: 343500, training_loss: 1.05470e+02
I1111 16:35:15.901123 140264174335808 run_lib.py:153] step: 343550, training_loss: 1.20070e+02
I1111 16:35:26.327574 140264174335808 run_lib.py:153] step: 343600, training_loss: 1.26957e+02
I1111 16:35:35.858222 140264174335808 run_lib.py:153] step: 343650, training_loss: 1.16732e+02
I1111 16:35:45.449943 140264174335808 run_lib.py:153] step: 343700, training_loss: 1.43146e+02
I1111 16:35:55.169358 140264174335808 run_lib.py:153] step: 343750, training_loss: 1.44834e+02
I1111 16:36:04.789355 140264174335808 run_lib.py:153] step: 343800, training_loss: 1.23875e+02
I1111 16:36:14.323114 140264174335808 run_lib.py:153] step: 343850, training_loss: 1.26253e+02
I1111 16:36:24.640225 140264174335808 run_lib.py:153] step: 343900, training_loss: 1.32677e+02
I1111 16:36:34.496244 140264174335808 run_lib.py:153] step: 343950, training_loss: 1.45752e+02
I1111 16:36:44.286067 140264174335808 run_lib.py:153] step: 344000, training_loss: 1.39811e+02
I1111 16:36:54.635124 140264174335808 run_lib.py:153] step: 344050, training_loss: 9.96769e+01
I1111 16:37:04.769817 140264174335808 run_lib.py:153] step: 344100, training_loss: 1.25640e+02
I1111 16:37:15.230026 140264174335808 run_lib.py:153] step: 344150, training_loss: 1.18168e+02
I1111 16:37:25.898126 140264174335808 run_lib.py:153] step: 344200, training_loss: 1.32088e+02
I1111 16:37:35.174620 140264174335808 run_lib.py:153] step: 344250, training_loss: 1.26343e+02
I1111 16:37:44.909144 140264174335808 run_lib.py:153] step: 344300, training_loss: 1.14982e+02
I1111 16:37:54.757392 140264174335808 run_lib.py:153] step: 344350, training_loss: 1.62539e+02
I1111 16:38:05.049753 140264174335808 run_lib.py:153] step: 344400, training_loss: 1.27578e+02
I1111 16:38:14.906811 140264174335808 run_lib.py:153] step: 344450, training_loss: 1.10534e+02
I1111 16:38:24.418638 140264174335808 run_lib.py:153] step: 344500, training_loss: 1.22991e+02
I1111 16:38:34.291875 140264174335808 run_lib.py:153] step: 344550, training_loss: 1.06416e+02
I1111 16:38:44.453881 140264174335808 run_lib.py:153] step: 344600, training_loss: 1.37985e+02
I1111 16:38:54.501016 140264174335808 run_lib.py:153] step: 344650, training_loss: 1.27414e+02
I1111 16:39:04.735489 140264174335808 run_lib.py:153] step: 344700, training_loss: 1.21671e+02
I1111 16:39:14.830588 140264174335808 run_lib.py:153] step: 344750, training_loss: 1.09528e+02
I1111 16:39:24.413863 140264174335808 run_lib.py:153] step: 344800, training_loss: 1.19073e+02
I1111 16:39:34.423903 140264174335808 run_lib.py:153] step: 344850, training_loss: 1.07306e+02
I1111 16:39:44.505466 140264174335808 run_lib.py:153] step: 344900, training_loss: 1.22685e+02
I1111 16:39:54.601937 140264174335808 run_lib.py:153] step: 344950, training_loss: 1.25466e+02
I1111 16:40:04.335211 140264174335808 run_lib.py:153] step: 345000, training_loss: 1.33646e+02
I1111 16:40:04.436087 140264174335808 run_lib.py:166] step: 345000, eval_loss: 1.32444e+02
I1111 16:40:14.457230 140264174335808 run_lib.py:153] step: 345050, training_loss: 1.02808e+02
I1111 16:40:24.194651 140264174335808 run_lib.py:153] step: 345100, training_loss: 1.24735e+02
I1111 16:40:33.847110 140264174335808 run_lib.py:153] step: 345150, training_loss: 1.18087e+02
I1111 16:40:44.038850 140264174335808 run_lib.py:153] step: 345200, training_loss: 1.15787e+02
I1111 16:40:54.541417 140264174335808 run_lib.py:153] step: 345250, training_loss: 1.29034e+02
I1111 16:41:04.734260 140264174335808 run_lib.py:153] step: 345300, training_loss: 1.23011e+02
I1111 16:41:14.915959 140264174335808 run_lib.py:153] step: 345350, training_loss: 1.35331e+02
I1111 16:41:25.233419 140264174335808 run_lib.py:153] step: 345400, training_loss: 1.33526e+02
I1111 16:41:35.202081 140264174335808 run_lib.py:153] step: 345450, training_loss: 1.13365e+02
I1111 16:41:45.049486 140264174335808 run_lib.py:153] step: 345500, training_loss: 1.42011e+02
I1111 16:41:55.286658 140264174335808 run_lib.py:153] step: 345550, training_loss: 1.11876e+02
I1111 16:42:05.641543 140264174335808 run_lib.py:153] step: 345600, training_loss: 1.32621e+02
I1111 16:42:16.071970 140264174335808 run_lib.py:153] step: 345650, training_loss: 1.09414e+02
I1111 16:42:25.938071 140264174335808 run_lib.py:153] step: 345700, training_loss: 1.36258e+02
I1111 16:42:36.023842 140264174335808 run_lib.py:153] step: 345750, training_loss: 1.15234e+02
I1111 16:42:45.950227 140264174335808 run_lib.py:153] step: 345800, training_loss: 1.00507e+02
I1111 16:42:56.313496 140264174335808 run_lib.py:153] step: 345850, training_loss: 1.08479e+02
I1111 16:43:06.649121 140264174335808 run_lib.py:153] step: 345900, training_loss: 1.35706e+02
I1111 16:43:17.516092 140264174335808 run_lib.py:153] step: 345950, training_loss: 1.78135e+02
I1111 16:43:27.459284 140264174335808 run_lib.py:153] step: 346000, training_loss: 1.39193e+02
I1111 16:43:37.308136 140264174335808 run_lib.py:153] step: 346050, training_loss: 1.30006e+02
I1111 16:43:47.151211 140264174335808 run_lib.py:153] step: 346100, training_loss: 1.03633e+02
I1111 16:43:57.685066 140264174335808 run_lib.py:153] step: 346150, training_loss: 1.27885e+02
I1111 16:44:07.923247 140264174335808 run_lib.py:153] step: 346200, training_loss: 1.33279e+02
I1111 16:44:17.430123 140264174335808 run_lib.py:153] step: 346250, training_loss: 1.63461e+02
I1111 16:44:27.161857 140264174335808 run_lib.py:153] step: 346300, training_loss: 1.29188e+02
I1111 16:44:37.420810 140264174335808 run_lib.py:153] step: 346350, training_loss: 1.27832e+02
I1111 16:44:47.752203 140264174335808 run_lib.py:153] step: 346400, training_loss: 1.36528e+02
I1111 16:44:58.026560 140264174335808 run_lib.py:153] step: 346450, training_loss: 1.49522e+02
I1111 16:45:08.720259 140264174335808 run_lib.py:153] step: 346500, training_loss: 1.25407e+02
I1111 16:45:18.950179 140264174335808 run_lib.py:153] step: 346550, training_loss: 1.21792e+02
I1111 16:45:29.949951 140264174335808 run_lib.py:153] step: 346600, training_loss: 7.56680e+01
I1111 16:45:40.777062 140264174335808 run_lib.py:153] step: 346650, training_loss: 1.06328e+02
I1111 16:45:50.539163 140264174335808 run_lib.py:153] step: 346700, training_loss: 1.25713e+02
I1111 16:46:00.062009 140264174335808 run_lib.py:153] step: 346750, training_loss: 1.29198e+02
I1111 16:46:09.824678 140264174335808 run_lib.py:153] step: 346800, training_loss: 1.14097e+02
I1111 16:46:19.807165 140264174335808 run_lib.py:153] step: 346850, training_loss: 1.28767e+02
I1111 16:46:29.473487 140264174335808 run_lib.py:153] step: 346900, training_loss: 1.31799e+02
I1111 16:46:39.704560 140264174335808 run_lib.py:153] step: 346950, training_loss: 1.33889e+02
I1111 16:46:49.523611 140264174335808 run_lib.py:153] step: 347000, training_loss: 1.35468e+02
I1111 16:46:59.262710 140264174335808 run_lib.py:153] step: 347050, training_loss: 1.30168e+02
I1111 16:47:09.354526 140264174335808 run_lib.py:153] step: 347100, training_loss: 1.45887e+02
I1111 16:47:19.154644 140264174335808 run_lib.py:153] step: 347150, training_loss: 1.52208e+02
I1111 16:47:29.576330 140264174335808 run_lib.py:153] step: 347200, training_loss: 1.17668e+02
I1111 16:47:40.120669 140264174335808 run_lib.py:153] step: 347250, training_loss: 1.08799e+02
I1111 16:47:50.098341 140264174335808 run_lib.py:153] step: 347300, training_loss: 1.22537e+02
I1111 16:48:00.180050 140264174335808 run_lib.py:153] step: 347350, training_loss: 1.28307e+02
I1111 16:48:09.789292 140264174335808 run_lib.py:153] step: 347400, training_loss: 1.08554e+02
I1111 16:48:19.862005 140264174335808 run_lib.py:153] step: 347450, training_loss: 1.38340e+02
I1111 16:48:30.484812 140264174335808 run_lib.py:153] step: 347500, training_loss: 1.18080e+02
I1111 16:48:41.332677 140264174335808 run_lib.py:153] step: 347550, training_loss: 1.17647e+02
I1111 16:48:51.447563 140264174335808 run_lib.py:153] step: 347600, training_loss: 1.33628e+02
I1111 16:49:01.934334 140264174335808 run_lib.py:153] step: 347650, training_loss: 1.28701e+02
I1111 16:49:12.469144 140264174335808 run_lib.py:153] step: 347700, training_loss: 1.04968e+02
I1111 16:49:22.647090 140264174335808 run_lib.py:153] step: 347750, training_loss: 1.17978e+02
I1111 16:49:32.624070 140264174335808 run_lib.py:153] step: 347800, training_loss: 1.00996e+02
I1111 16:49:42.694168 140264174335808 run_lib.py:153] step: 347850, training_loss: 1.37071e+02
I1111 16:49:52.744503 140264174335808 run_lib.py:153] step: 347900, training_loss: 1.37507e+02
I1111 16:50:03.214702 140264174335808 run_lib.py:153] step: 347950, training_loss: 1.22483e+02
I1111 16:50:12.983511 140264174335808 run_lib.py:153] step: 348000, training_loss: 1.39628e+02
I1111 16:50:22.587917 140264174335808 run_lib.py:153] step: 348050, training_loss: 1.32224e+02
I1111 16:50:32.752138 140264174335808 run_lib.py:153] step: 348100, training_loss: 1.15977e+02
I1111 16:50:42.485250 140264174335808 run_lib.py:153] step: 348150, training_loss: 1.09853e+02
I1111 16:50:53.025445 140264174335808 run_lib.py:153] step: 348200, training_loss: 1.30821e+02
I1111 16:51:03.148463 140264174335808 run_lib.py:153] step: 348250, training_loss: 1.00825e+02
I1111 16:51:13.232625 140264174335808 run_lib.py:153] step: 348300, training_loss: 1.44062e+02
I1111 16:51:23.627857 140264174335808 run_lib.py:153] step: 348350, training_loss: 1.25011e+02
I1111 16:51:33.708967 140264174335808 run_lib.py:153] step: 348400, training_loss: 1.34587e+02
I1111 16:51:43.612998 140264174335808 run_lib.py:153] step: 348450, training_loss: 1.25703e+02
I1111 16:51:53.886580 140264174335808 run_lib.py:153] step: 348500, training_loss: 1.25746e+02
I1111 16:52:04.087037 140264174335808 run_lib.py:153] step: 348550, training_loss: 1.40331e+02
I1111 16:52:13.733533 140264174335808 run_lib.py:153] step: 348600, training_loss: 1.57487e+02
I1111 16:52:24.003308 140264174335808 run_lib.py:153] step: 348650, training_loss: 1.09937e+02
I1111 16:52:35.003961 140264174335808 run_lib.py:153] step: 348700, training_loss: 1.22775e+02
I1111 16:52:45.038155 140264174335808 run_lib.py:153] step: 348750, training_loss: 1.43534e+02
I1111 16:52:55.779712 140264174335808 run_lib.py:153] step: 348800, training_loss: 1.53770e+02
I1111 16:53:05.961539 140264174335808 run_lib.py:153] step: 348850, training_loss: 1.10957e+02
I1111 16:53:15.933537 140264174335808 run_lib.py:153] step: 348900, training_loss: 1.39049e+02
I1111 16:53:25.981075 140264174335808 run_lib.py:153] step: 348950, training_loss: 1.06203e+02
I1111 16:53:36.213696 140264174335808 run_lib.py:153] step: 349000, training_loss: 1.30164e+02
I1111 16:53:46.667258 140264174335808 run_lib.py:153] step: 349050, training_loss: 1.58348e+02
I1111 16:53:56.810590 140264174335808 run_lib.py:153] step: 349100, training_loss: 1.44988e+02
I1111 16:54:06.821592 140264174335808 run_lib.py:153] step: 349150, training_loss: 1.51371e+02
I1111 16:54:16.398030 140264174335808 run_lib.py:153] step: 349200, training_loss: 1.24411e+02
I1111 16:54:26.430170 140264174335808 run_lib.py:153] step: 349250, training_loss: 1.57839e+02
I1111 16:54:36.192099 140264174335808 run_lib.py:153] step: 349300, training_loss: 1.65979e+02
I1111 16:54:46.165551 140264174335808 run_lib.py:153] step: 349350, training_loss: 1.20662e+02
I1111 16:54:56.285736 140264174335808 run_lib.py:153] step: 349400, training_loss: 1.06644e+02
I1111 16:55:06.342067 140264174335808 run_lib.py:153] step: 349450, training_loss: 1.28451e+02
I1111 16:55:16.980156 140264174335808 run_lib.py:153] step: 349500, training_loss: 1.21778e+02
I1111 16:55:27.345210 140264174335808 run_lib.py:153] step: 349550, training_loss: 1.12076e+02
I1111 16:55:37.397412 140264174335808 run_lib.py:153] step: 349600, training_loss: 1.35221e+02
I1111 16:55:47.545411 140264174335808 run_lib.py:153] step: 349650, training_loss: 1.20188e+02
I1111 16:55:57.475616 140264174335808 run_lib.py:153] step: 349700, training_loss: 1.29975e+02
I1111 16:56:07.607932 140264174335808 run_lib.py:153] step: 349750, training_loss: 1.11930e+02
I1111 16:56:18.408595 140264174335808 run_lib.py:153] step: 349800, training_loss: 1.42227e+02
I1111 16:56:28.634724 140264174335808 run_lib.py:153] step: 349850, training_loss: 1.26184e+02
I1111 16:56:38.411403 140264174335808 run_lib.py:153] step: 349900, training_loss: 1.19855e+02
I1111 16:56:48.647330 140264174335808 run_lib.py:153] step: 349950, training_loss: 1.21345e+02
I1111 16:56:58.749708 140264174335808 run_lib.py:153] step: 350000, training_loss: 1.07118e+02
I1111 16:56:59.313787 140264174335808 run_lib.py:166] step: 350000, eval_loss: 1.33236e+02
I1111 16:57:09.651402 140264174335808 run_lib.py:153] step: 350050, training_loss: 1.19715e+02
I1111 16:57:19.829329 140264174335808 run_lib.py:153] step: 350100, training_loss: 1.30390e+02
I1111 16:57:30.301565 140264174335808 run_lib.py:153] step: 350150, training_loss: 1.35541e+02
I1111 16:57:40.085313 140264174335808 run_lib.py:153] step: 350200, training_loss: 1.42032e+02
I1111 16:57:50.520009 140264174335808 run_lib.py:153] step: 350250, training_loss: 1.35849e+02
I1111 16:58:00.962583 140264174335808 run_lib.py:153] step: 350300, training_loss: 1.35031e+02
I1111 16:58:11.420261 140264174335808 run_lib.py:153] step: 350350, training_loss: 1.41802e+02
I1111 16:58:21.614292 140264174335808 run_lib.py:153] step: 350400, training_loss: 1.44279e+02
I1111 16:58:32.105805 140264174335808 run_lib.py:153] step: 350450, training_loss: 1.28343e+02
I1111 16:58:41.734451 140264174335808 run_lib.py:153] step: 350500, training_loss: 1.08767e+02
I1111 16:58:51.627504 140264174335808 run_lib.py:153] step: 350550, training_loss: 1.19638e+02
I1111 16:59:02.091884 140264174335808 run_lib.py:153] step: 350600, training_loss: 1.48355e+02
I1111 16:59:12.150263 140264174335808 run_lib.py:153] step: 350650, training_loss: 1.15541e+02
I1111 16:59:23.024433 140264174335808 run_lib.py:153] step: 350700, training_loss: 1.59330e+02
I1111 16:59:33.832772 140264174335808 run_lib.py:153] step: 350750, training_loss: 1.07783e+02
I1111 16:59:44.180237 140264174335808 run_lib.py:153] step: 350800, training_loss: 1.30078e+02
I1111 16:59:54.582969 140264174335808 run_lib.py:153] step: 350850, training_loss: 1.43649e+02
I1111 17:00:05.333807 140264174335808 run_lib.py:153] step: 350900, training_loss: 1.29146e+02
I1111 17:00:15.167310 140264174335808 run_lib.py:153] step: 350950, training_loss: 1.13115e+02
I1111 17:00:25.577164 140264174335808 run_lib.py:153] step: 351000, training_loss: 9.08386e+01
I1111 17:00:35.646362 140264174335808 run_lib.py:153] step: 351050, training_loss: 1.22891e+02
I1111 17:00:46.214242 140264174335808 run_lib.py:153] step: 351100, training_loss: 1.19653e+02
I1111 17:00:56.603240 140264174335808 run_lib.py:153] step: 351150, training_loss: 1.32911e+02
I1111 17:01:06.916447 140264174335808 run_lib.py:153] step: 351200, training_loss: 1.57694e+02
I1111 17:01:17.276576 140264174335808 run_lib.py:153] step: 351250, training_loss: 1.25841e+02
I1111 17:01:27.847326 140264174335808 run_lib.py:153] step: 351300, training_loss: 1.01597e+02
I1111 17:01:38.585062 140264174335808 run_lib.py:153] step: 351350, training_loss: 1.55578e+02
I1111 17:01:48.895700 140264174335808 run_lib.py:153] step: 351400, training_loss: 1.34005e+02
I1111 17:01:59.024653 140264174335808 run_lib.py:153] step: 351450, training_loss: 1.32758e+02
I1111 17:02:09.727423 140264174335808 run_lib.py:153] step: 351500, training_loss: 1.35460e+02
I1111 17:02:19.563468 140264174335808 run_lib.py:153] step: 351550, training_loss: 1.20412e+02
I1111 17:02:29.887630 140264174335808 run_lib.py:153] step: 351600, training_loss: 1.32188e+02
I1111 17:02:40.321451 140264174335808 run_lib.py:153] step: 351650, training_loss: 1.25917e+02
I1111 17:02:50.584463 140264174335808 run_lib.py:153] step: 351700, training_loss: 9.92834e+01
I1111 17:03:00.392505 140264174335808 run_lib.py:153] step: 351750, training_loss: 1.14048e+02
I1111 17:03:10.534384 140264174335808 run_lib.py:153] step: 351800, training_loss: 1.46209e+02
I1111 17:03:20.902814 140264174335808 run_lib.py:153] step: 351850, training_loss: 1.39257e+02
I1111 17:03:31.367525 140264174335808 run_lib.py:153] step: 351900, training_loss: 1.17100e+02
I1111 17:03:41.332546 140264174335808 run_lib.py:153] step: 351950, training_loss: 1.05930e+02
I1111 17:03:50.841273 140264174335808 run_lib.py:153] step: 352000, training_loss: 1.27954e+02
I1111 17:04:00.607410 140264174335808 run_lib.py:153] step: 352050, training_loss: 9.34778e+01
I1111 17:04:10.715505 140264174335808 run_lib.py:153] step: 352100, training_loss: 1.02781e+02
I1111 17:04:20.544453 140264174335808 run_lib.py:153] step: 352150, training_loss: 1.52188e+02
I1111 17:04:30.041668 140264174335808 run_lib.py:153] step: 352200, training_loss: 1.26703e+02
I1111 17:04:39.956876 140264174335808 run_lib.py:153] step: 352250, training_loss: 1.21441e+02
I1111 17:04:50.492403 140264174335808 run_lib.py:153] step: 352300, training_loss: 1.12527e+02
I1111 17:05:01.041606 140264174335808 run_lib.py:153] step: 352350, training_loss: 1.18027e+02
I1111 17:05:11.714559 140264174335808 run_lib.py:153] step: 352400, training_loss: 1.14468e+02
I1111 17:05:22.558316 140264174335808 run_lib.py:153] step: 352450, training_loss: 1.37296e+02
I1111 17:05:32.845144 140264174335808 run_lib.py:153] step: 352500, training_loss: 1.24546e+02
I1111 17:05:42.964848 140264174335808 run_lib.py:153] step: 352550, training_loss: 1.36776e+02
I1111 17:05:53.135065 140264174335808 run_lib.py:153] step: 352600, training_loss: 1.41209e+02
I1111 17:06:03.220146 140264174335808 run_lib.py:153] step: 352650, training_loss: 1.62792e+02
I1111 17:06:13.862678 140264174335808 run_lib.py:153] step: 352700, training_loss: 1.58115e+02
I1111 17:06:24.507168 140264174335808 run_lib.py:153] step: 352750, training_loss: 1.23774e+02
I1111 17:06:34.628279 140264174335808 run_lib.py:153] step: 352800, training_loss: 1.13669e+02
I1111 17:06:44.105893 140264174335808 run_lib.py:153] step: 352850, training_loss: 1.21936e+02
I1111 17:06:54.613185 140264174335808 run_lib.py:153] step: 352900, training_loss: 1.04136e+02
I1111 17:07:05.088617 140264174335808 run_lib.py:153] step: 352950, training_loss: 1.22432e+02
I1111 17:07:14.906551 140264174335808 run_lib.py:153] step: 353000, training_loss: 1.47098e+02
I1111 17:07:25.340180 140264174335808 run_lib.py:153] step: 353050, training_loss: 1.15150e+02
I1111 17:07:34.772447 140264174335808 run_lib.py:153] step: 353100, training_loss: 9.79259e+01
I1111 17:07:44.651941 140264174335808 run_lib.py:153] step: 353150, training_loss: 1.42328e+02
I1111 17:07:54.587057 140264174335808 run_lib.py:153] step: 353200, training_loss: 1.42754e+02
I1111 17:08:04.918639 140264174335808 run_lib.py:153] step: 353250, training_loss: 1.22117e+02
I1111 17:08:15.162622 140264174335808 run_lib.py:153] step: 353300, training_loss: 1.19706e+02
I1111 17:08:24.698443 140264174335808 run_lib.py:153] step: 353350, training_loss: 1.11528e+02
I1111 17:08:35.120661 140264174335808 run_lib.py:153] step: 353400, training_loss: 1.16300e+02
I1111 17:08:45.564975 140264174335808 run_lib.py:153] step: 353450, training_loss: 1.15260e+02
I1111 17:08:55.793617 140264174335808 run_lib.py:153] step: 353500, training_loss: 1.45126e+02
I1111 17:09:05.544706 140264174335808 run_lib.py:153] step: 353550, training_loss: 1.36674e+02
I1111 17:09:15.741905 140264174335808 run_lib.py:153] step: 353600, training_loss: 1.36280e+02
I1111 17:09:25.300526 140264174335808 run_lib.py:153] step: 353650, training_loss: 1.18560e+02
I1111 17:09:35.300426 140264174335808 run_lib.py:153] step: 353700, training_loss: 1.39188e+02
I1111 17:09:45.963417 140264174335808 run_lib.py:153] step: 353750, training_loss: 1.09876e+02
I1111 17:09:56.633836 140264174335808 run_lib.py:153] step: 353800, training_loss: 1.56448e+02
I1111 17:10:06.886299 140264174335808 run_lib.py:153] step: 353850, training_loss: 1.08405e+02
I1111 17:10:16.947356 140264174335808 run_lib.py:153] step: 353900, training_loss: 1.26265e+02
I1111 17:10:26.944948 140264174335808 run_lib.py:153] step: 353950, training_loss: 1.08636e+02
I1111 17:10:37.177502 140264174335808 run_lib.py:153] step: 354000, training_loss: 1.24573e+02
I1111 17:10:47.642184 140264174335808 run_lib.py:153] step: 354050, training_loss: 1.24560e+02
I1111 17:10:57.406893 140264174335808 run_lib.py:153] step: 354100, training_loss: 1.36842e+02
I1111 17:11:08.124381 140264174335808 run_lib.py:153] step: 354150, training_loss: 1.20553e+02
I1111 17:11:18.356488 140264174335808 run_lib.py:153] step: 354200, training_loss: 1.46919e+02
I1111 17:11:28.321300 140264174335808 run_lib.py:153] step: 354250, training_loss: 1.35950e+02
I1111 17:11:38.441499 140264174335808 run_lib.py:153] step: 354300, training_loss: 1.25122e+02
I1111 17:11:48.379503 140264174335808 run_lib.py:153] step: 354350, training_loss: 1.48486e+02
I1111 17:11:58.907509 140264174335808 run_lib.py:153] step: 354400, training_loss: 1.29568e+02
I1111 17:12:08.725153 140264174335808 run_lib.py:153] step: 354450, training_loss: 1.24894e+02
I1111 17:12:19.345851 140264174335808 run_lib.py:153] step: 354500, training_loss: 9.75429e+01
I1111 17:12:29.736602 140264174335808 run_lib.py:153] step: 354550, training_loss: 1.46102e+02
I1111 17:12:40.121484 140264174335808 run_lib.py:153] step: 354600, training_loss: 1.06580e+02
I1111 17:12:50.431745 140264174335808 run_lib.py:153] step: 354650, training_loss: 1.19775e+02
I1111 17:13:00.958169 140264174335808 run_lib.py:153] step: 354700, training_loss: 1.40553e+02
I1111 17:13:10.735141 140264174335808 run_lib.py:153] step: 354750, training_loss: 1.24537e+02
I1111 17:13:20.612690 140264174335808 run_lib.py:153] step: 354800, training_loss: 1.23777e+02
I1111 17:13:30.541881 140264174335808 run_lib.py:153] step: 354850, training_loss: 1.15366e+02
I1111 17:13:40.482129 140264174335808 run_lib.py:153] step: 354900, training_loss: 1.29191e+02
I1111 17:13:50.064181 140264174335808 run_lib.py:153] step: 354950, training_loss: 1.13580e+02
I1111 17:14:00.238739 140264174335808 run_lib.py:153] step: 355000, training_loss: 1.12425e+02
I1111 17:14:00.375653 140264174335808 run_lib.py:166] step: 355000, eval_loss: 1.08121e+02
I1111 17:14:09.993024 140264174335808 run_lib.py:153] step: 355050, training_loss: 1.35525e+02
I1111 17:14:20.025209 140264174335808 run_lib.py:153] step: 355100, training_loss: 1.30643e+02
I1111 17:14:30.652248 140264174335808 run_lib.py:153] step: 355150, training_loss: 1.34596e+02
I1111 17:14:41.247692 140264174335808 run_lib.py:153] step: 355200, training_loss: 1.36345e+02
I1111 17:14:51.616356 140264174335808 run_lib.py:153] step: 355250, training_loss: 1.41447e+02
I1111 17:15:00.931587 140264174335808 run_lib.py:153] step: 355300, training_loss: 1.09921e+02
I1111 17:15:10.955809 140264174335808 run_lib.py:153] step: 355350, training_loss: 1.31321e+02
I1111 17:15:21.072180 140264174335808 run_lib.py:153] step: 355400, training_loss: 1.08804e+02
I1111 17:15:31.033863 140264174335808 run_lib.py:153] step: 355450, training_loss: 1.19558e+02
I1111 17:15:41.410640 140264174335808 run_lib.py:153] step: 355500, training_loss: 1.22380e+02
I1111 17:15:51.690826 140264174335808 run_lib.py:153] step: 355550, training_loss: 1.06990e+02
I1111 17:16:01.820708 140264174335808 run_lib.py:153] step: 355600, training_loss: 1.52910e+02
I1111 17:16:12.157063 140264174335808 run_lib.py:153] step: 355650, training_loss: 1.17117e+02
I1111 17:16:22.173172 140264174335808 run_lib.py:153] step: 355700, training_loss: 1.10197e+02
I1111 17:16:31.777513 140264174335808 run_lib.py:153] step: 355750, training_loss: 1.39189e+02
I1111 17:16:41.614036 140264174335808 run_lib.py:153] step: 355800, training_loss: 9.97947e+01
I1111 17:16:51.647009 140264174335808 run_lib.py:153] step: 355850, training_loss: 9.40450e+01
I1111 17:17:02.071673 140264174335808 run_lib.py:153] step: 355900, training_loss: 1.26725e+02
I1111 17:17:12.360189 140264174335808 run_lib.py:153] step: 355950, training_loss: 1.14610e+02
I1111 17:17:22.331526 140264174335808 run_lib.py:153] step: 356000, training_loss: 1.28252e+02
I1111 17:17:32.562214 140264174335808 run_lib.py:153] step: 356050, training_loss: 1.01673e+02
I1111 17:17:42.955078 140264174335808 run_lib.py:153] step: 356100, training_loss: 1.30485e+02
I1111 17:17:52.804623 140264174335808 run_lib.py:153] step: 356150, training_loss: 1.17087e+02
I1111 17:18:02.590198 140264174335808 run_lib.py:153] step: 356200, training_loss: 1.26346e+02
I1111 17:18:12.904118 140264174335808 run_lib.py:153] step: 356250, training_loss: 1.13983e+02
I1111 17:18:22.488474 140264174335808 run_lib.py:153] step: 356300, training_loss: 1.41071e+02
I1111 17:18:32.338011 140264174335808 run_lib.py:153] step: 356350, training_loss: 1.27414e+02
I1111 17:18:41.862384 140264174335808 run_lib.py:153] step: 356400, training_loss: 1.10251e+02
I1111 17:18:51.755707 140264174335808 run_lib.py:153] step: 356450, training_loss: 1.35029e+02
I1111 17:19:01.474887 140264174335808 run_lib.py:153] step: 356500, training_loss: 9.54595e+01
I1111 17:19:11.130418 140264174335808 run_lib.py:153] step: 356550, training_loss: 1.20699e+02
I1111 17:19:21.963528 140264174335808 run_lib.py:153] step: 356600, training_loss: 1.37654e+02
I1111 17:19:32.739171 140264174335808 run_lib.py:153] step: 356650, training_loss: 1.17720e+02
I1111 17:19:42.843636 140264174335808 run_lib.py:153] step: 356700, training_loss: 1.23255e+02
I1111 17:19:52.874524 140264174335808 run_lib.py:153] step: 356750, training_loss: 9.82980e+01
I1111 17:20:02.760134 140264174335808 run_lib.py:153] step: 356800, training_loss: 1.16514e+02
I1111 17:20:12.549796 140264174335808 run_lib.py:153] step: 356850, training_loss: 1.13512e+02
I1111 17:20:22.930181 140264174335808 run_lib.py:153] step: 356900, training_loss: 1.35086e+02
I1111 17:20:32.872155 140264174335808 run_lib.py:153] step: 356950, training_loss: 1.41875e+02
I1111 17:20:43.312011 140264174335808 run_lib.py:153] step: 357000, training_loss: 1.18296e+02
I1111 17:20:53.725152 140264174335808 run_lib.py:153] step: 357050, training_loss: 1.28188e+02
I1111 17:21:03.700835 140264174335808 run_lib.py:153] step: 357100, training_loss: 1.24330e+02
I1111 17:21:13.923584 140264174335808 run_lib.py:153] step: 357150, training_loss: 1.00007e+02
I1111 17:21:23.702188 140264174335808 run_lib.py:153] step: 357200, training_loss: 1.58906e+02
I1111 17:21:33.421205 140264174335808 run_lib.py:153] step: 357250, training_loss: 1.29208e+02
I1111 17:21:43.267588 140264174335808 run_lib.py:153] step: 357300, training_loss: 1.23820e+02
I1111 17:21:52.499388 140264174335808 run_lib.py:153] step: 357350, training_loss: 1.21589e+02
I1111 17:22:02.812143 140264174335808 run_lib.py:153] step: 357400, training_loss: 1.47371e+02
I1111 17:22:13.329869 140264174335808 run_lib.py:153] step: 357450, training_loss: 9.64663e+01
I1111 17:22:23.593714 140264174335808 run_lib.py:153] step: 357500, training_loss: 1.51407e+02
I1111 17:22:34.050473 140264174335808 run_lib.py:153] step: 357550, training_loss: 1.31351e+02
I1111 17:22:43.485777 140264174335808 run_lib.py:153] step: 357600, training_loss: 1.49089e+02
I1111 17:22:53.127358 140264174335808 run_lib.py:153] step: 357650, training_loss: 1.24873e+02
I1111 17:23:03.029132 140264174335808 run_lib.py:153] step: 357700, training_loss: 1.07577e+02
I1111 17:23:12.957761 140264174335808 run_lib.py:153] step: 357750, training_loss: 1.13852e+02
I1111 17:23:23.546565 140264174335808 run_lib.py:153] step: 357800, training_loss: 1.11645e+02
I1111 17:23:33.591943 140264174335808 run_lib.py:153] step: 357850, training_loss: 1.32145e+02
I1111 17:23:44.412916 140264174335808 run_lib.py:153] step: 357900, training_loss: 1.37794e+02
I1111 17:23:54.307162 140264174335808 run_lib.py:153] step: 357950, training_loss: 1.71076e+02
I1111 17:24:03.925017 140264174335808 run_lib.py:153] step: 358000, training_loss: 1.57030e+02
I1111 17:24:14.155994 140264174335808 run_lib.py:153] step: 358050, training_loss: 1.17734e+02
I1111 17:24:24.673826 140264174335808 run_lib.py:153] step: 358100, training_loss: 1.27584e+02
I1111 17:24:34.749032 140264174335808 run_lib.py:153] step: 358150, training_loss: 1.55473e+02
I1111 17:24:44.299011 140264174335808 run_lib.py:153] step: 358200, training_loss: 1.30934e+02
I1111 17:24:53.869118 140264174335808 run_lib.py:153] step: 358250, training_loss: 1.19455e+02
I1111 17:25:03.976451 140264174335808 run_lib.py:153] step: 358300, training_loss: 1.27144e+02
I1111 17:25:13.886758 140264174335808 run_lib.py:153] step: 358350, training_loss: 1.43854e+02
I1111 17:25:23.512069 140264174335808 run_lib.py:153] step: 358400, training_loss: 1.40491e+02
I1111 17:25:33.097503 140264174335808 run_lib.py:153] step: 358450, training_loss: 1.06462e+02
I1111 17:25:42.744617 140264174335808 run_lib.py:153] step: 358500, training_loss: 1.43077e+02
I1111 17:25:52.613850 140264174335808 run_lib.py:153] step: 358550, training_loss: 1.19967e+02
I1111 17:26:02.781872 140264174335808 run_lib.py:153] step: 358600, training_loss: 1.15562e+02
I1111 17:26:12.672868 140264174335808 run_lib.py:153] step: 358650, training_loss: 1.20436e+02
I1111 17:26:22.977625 140264174335808 run_lib.py:153] step: 358700, training_loss: 1.03130e+02
I1111 17:26:33.221711 140264174335808 run_lib.py:153] step: 358750, training_loss: 1.33013e+02
I1111 17:26:43.163003 140264174335808 run_lib.py:153] step: 358800, training_loss: 1.39781e+02
I1111 17:26:53.377704 140264174335808 run_lib.py:153] step: 358850, training_loss: 1.23496e+02
I1111 17:27:03.299638 140264174335808 run_lib.py:153] step: 358900, training_loss: 1.33876e+02
I1111 17:27:13.330251 140264174335808 run_lib.py:153] step: 358950, training_loss: 1.41080e+02
I1111 17:27:23.047282 140264174335808 run_lib.py:153] step: 359000, training_loss: 1.04301e+02
I1111 17:27:33.854702 140264174335808 run_lib.py:153] step: 359050, training_loss: 1.34776e+02
I1111 17:27:44.314532 140264174335808 run_lib.py:153] step: 359100, training_loss: 1.11787e+02
I1111 17:27:54.888439 140264174335808 run_lib.py:153] step: 359150, training_loss: 1.10395e+02
I1111 17:28:05.058447 140264174335808 run_lib.py:153] step: 359200, training_loss: 1.05891e+02
I1111 17:28:15.099675 140264174335808 run_lib.py:153] step: 359250, training_loss: 1.47244e+02
I1111 17:28:25.124973 140264174335808 run_lib.py:153] step: 359300, training_loss: 1.08650e+02
I1111 17:28:35.240041 140264174335808 run_lib.py:153] step: 359350, training_loss: 1.43180e+02
I1111 17:28:45.436719 140264174335808 run_lib.py:153] step: 359400, training_loss: 1.10844e+02
I1111 17:28:55.771321 140264174335808 run_lib.py:153] step: 359450, training_loss: 1.25952e+02
I1111 17:29:05.861779 140264174335808 run_lib.py:153] step: 359500, training_loss: 1.25774e+02
I1111 17:29:16.202737 140264174335808 run_lib.py:153] step: 359550, training_loss: 1.30973e+02
I1111 17:29:25.789431 140264174335808 run_lib.py:153] step: 359600, training_loss: 1.34005e+02
I1111 17:29:35.570420 140264174335808 run_lib.py:153] step: 359650, training_loss: 1.47347e+02
I1111 17:29:45.640752 140264174335808 run_lib.py:153] step: 359700, training_loss: 1.21349e+02
I1111 17:29:55.536667 140264174335808 run_lib.py:153] step: 359750, training_loss: 1.50606e+02
I1111 17:30:06.190573 140264174335808 run_lib.py:153] step: 359800, training_loss: 9.02712e+01
I1111 17:30:17.023112 140264174335808 run_lib.py:153] step: 359850, training_loss: 1.12455e+02
I1111 17:30:27.502670 140264174335808 run_lib.py:153] step: 359900, training_loss: 1.40754e+02
I1111 17:30:37.754862 140264174335808 run_lib.py:153] step: 359950, training_loss: 1.08538e+02
I1111 17:30:47.279821 140264174335808 run_lib.py:153] step: 360000, training_loss: 1.44178e+02
I1111 17:30:47.859081 140264174335808 run_lib.py:166] step: 360000, eval_loss: 1.25193e+02
I1111 17:30:57.861465 140264174335808 run_lib.py:153] step: 360050, training_loss: 1.30267e+02
I1111 17:31:08.217983 140264174335808 run_lib.py:153] step: 360100, training_loss: 1.70556e+02
I1111 17:31:18.324933 140264174335808 run_lib.py:153] step: 360150, training_loss: 1.25939e+02
I1111 17:31:28.408173 140264174335808 run_lib.py:153] step: 360200, training_loss: 1.03787e+02
I1111 17:31:38.427292 140264174335808 run_lib.py:153] step: 360250, training_loss: 1.24023e+02
I1111 17:31:48.825983 140264174335808 run_lib.py:153] step: 360300, training_loss: 1.28267e+02
I1111 17:31:59.259460 140264174335808 run_lib.py:153] step: 360350, training_loss: 1.14902e+02
I1111 17:32:09.772972 140264174335808 run_lib.py:153] step: 360400, training_loss: 1.35073e+02
I1111 17:32:20.260233 140264174335808 run_lib.py:153] step: 360450, training_loss: 1.77342e+02
I1111 17:32:30.268818 140264174335808 run_lib.py:153] step: 360500, training_loss: 1.59057e+02
I1111 17:32:40.348879 140264174335808 run_lib.py:153] step: 360550, training_loss: 1.08395e+02
I1111 17:32:50.427183 140264174335808 run_lib.py:153] step: 360600, training_loss: 1.15094e+02
I1111 17:33:00.599635 140264174335808 run_lib.py:153] step: 360650, training_loss: 1.21818e+02
I1111 17:33:10.207187 140264174335808 run_lib.py:153] step: 360700, training_loss: 1.04696e+02
I1111 17:33:20.036466 140264174335808 run_lib.py:153] step: 360750, training_loss: 1.23639e+02
I1111 17:33:30.597445 140264174335808 run_lib.py:153] step: 360800, training_loss: 1.01287e+02
I1111 17:33:41.436239 140264174335808 run_lib.py:153] step: 360850, training_loss: 1.22637e+02
I1111 17:33:51.588565 140264174335808 run_lib.py:153] step: 360900, training_loss: 1.20837e+02
I1111 17:34:01.979527 140264174335808 run_lib.py:153] step: 360950, training_loss: 1.33940e+02
I1111 17:34:11.614555 140264174335808 run_lib.py:153] step: 361000, training_loss: 1.30652e+02
I1111 17:34:21.616073 140264174335808 run_lib.py:153] step: 361050, training_loss: 1.11876e+02
I1111 17:34:31.989976 140264174335808 run_lib.py:153] step: 361100, training_loss: 1.47846e+02
I1111 17:34:41.561508 140264174335808 run_lib.py:153] step: 361150, training_loss: 1.20415e+02
I1111 17:34:52.210596 140264174335808 run_lib.py:153] step: 361200, training_loss: 1.31933e+02
I1111 17:35:02.140522 140264174335808 run_lib.py:153] step: 361250, training_loss: 1.55888e+02
I1111 17:35:12.598053 140264174335808 run_lib.py:153] step: 361300, training_loss: 1.20222e+02
I1111 17:35:22.737545 140264174335808 run_lib.py:153] step: 361350, training_loss: 1.14141e+02
I1111 17:35:32.836593 140264174335808 run_lib.py:153] step: 361400, training_loss: 1.13822e+02
I1111 17:35:43.611788 140264174335808 run_lib.py:153] step: 361450, training_loss: 1.33873e+02
I1111 17:35:53.750932 140264174335808 run_lib.py:153] step: 361500, training_loss: 1.22370e+02
I1111 17:36:03.899571 140264174335808 run_lib.py:153] step: 361550, training_loss: 1.33696e+02
I1111 17:36:14.198735 140264174335808 run_lib.py:153] step: 361600, training_loss: 1.43840e+02
I1111 17:36:24.521683 140264174335808 run_lib.py:153] step: 361650, training_loss: 1.18739e+02
I1111 17:36:34.919792 140264174335808 run_lib.py:153] step: 361700, training_loss: 1.12702e+02
I1111 17:36:45.546509 140264174335808 run_lib.py:153] step: 361750, training_loss: 1.25631e+02
I1111 17:36:56.676176 140264174335808 run_lib.py:153] step: 361800, training_loss: 1.30468e+02
I1111 17:37:07.396820 140264174335808 run_lib.py:153] step: 361850, training_loss: 1.08703e+02
I1111 17:37:17.636015 140264174335808 run_lib.py:153] step: 361900, training_loss: 1.09690e+02
I1111 17:37:28.160774 140264174335808 run_lib.py:153] step: 361950, training_loss: 1.20612e+02
I1111 17:37:38.589644 140264174335808 run_lib.py:153] step: 362000, training_loss: 1.23870e+02
I1111 17:37:48.967369 140264174335808 run_lib.py:153] step: 362050, training_loss: 1.13023e+02
I1111 17:37:58.770097 140264174335808 run_lib.py:153] step: 362100, training_loss: 1.35990e+02
I1111 17:38:09.332165 140264174335808 run_lib.py:153] step: 362150, training_loss: 1.22825e+02
I1111 17:38:19.873305 140264174335808 run_lib.py:153] step: 362200, training_loss: 1.39540e+02
I1111 17:38:29.323298 140264174335808 run_lib.py:153] step: 362250, training_loss: 1.37448e+02
I1111 17:38:39.291768 140264174335808 run_lib.py:153] step: 362300, training_loss: 1.57131e+02
I1111 17:38:49.672207 140264174335808 run_lib.py:153] step: 362350, training_loss: 1.25034e+02
I1111 17:38:59.894138 140264174335808 run_lib.py:153] step: 362400, training_loss: 1.15164e+02
I1111 17:39:10.386009 140264174335808 run_lib.py:153] step: 362450, training_loss: 1.22662e+02
I1111 17:39:20.978615 140264174335808 run_lib.py:153] step: 362500, training_loss: 1.28775e+02
I1111 17:39:31.511814 140264174335808 run_lib.py:153] step: 362550, training_loss: 1.52735e+02
I1111 17:39:41.492708 140264174335808 run_lib.py:153] step: 362600, training_loss: 1.48260e+02
I1111 17:39:52.230448 140264174335808 run_lib.py:153] step: 362650, training_loss: 1.25453e+02
I1111 17:40:03.514604 140264174335808 run_lib.py:153] step: 362700, training_loss: 1.20551e+02
I1111 17:40:14.780729 140264174335808 run_lib.py:153] step: 362750, training_loss: 1.26017e+02
I1111 17:40:24.516899 140264174335808 run_lib.py:153] step: 362800, training_loss: 1.12316e+02
I1111 17:40:34.886910 140264174335808 run_lib.py:153] step: 362850, training_loss: 1.11536e+02
I1111 17:40:44.769147 140264174335808 run_lib.py:153] step: 362900, training_loss: 1.56571e+02
I1111 17:40:54.847731 140264174335808 run_lib.py:153] step: 362950, training_loss: 1.15992e+02
I1111 17:41:04.784340 140264174335808 run_lib.py:153] step: 363000, training_loss: 1.22695e+02
I1111 17:41:14.599832 140264174335808 run_lib.py:153] step: 363050, training_loss: 1.38550e+02
I1111 17:41:24.419830 140264174335808 run_lib.py:153] step: 363100, training_loss: 1.12968e+02
I1111 17:41:34.476345 140264174335808 run_lib.py:153] step: 363150, training_loss: 1.52083e+02
I1111 17:41:44.518617 140264174335808 run_lib.py:153] step: 363200, training_loss: 1.28104e+02
I1111 17:41:54.321798 140264174335808 run_lib.py:153] step: 363250, training_loss: 1.22112e+02
I1111 17:42:03.897459 140264174335808 run_lib.py:153] step: 363300, training_loss: 1.46115e+02
I1111 17:42:14.019450 140264174335808 run_lib.py:153] step: 363350, training_loss: 1.15163e+02
I1111 17:42:24.923103 140264174335808 run_lib.py:153] step: 363400, training_loss: 1.19058e+02
I1111 17:42:35.875982 140264174335808 run_lib.py:153] step: 363450, training_loss: 1.37846e+02
I1111 17:42:46.307613 140264174335808 run_lib.py:153] step: 363500, training_loss: 1.27048e+02
I1111 17:42:56.279392 140264174335808 run_lib.py:153] step: 363550, training_loss: 1.67469e+02
I1111 17:43:06.474857 140264174335808 run_lib.py:153] step: 363600, training_loss: 1.15230e+02
I1111 17:43:16.333219 140264174335808 run_lib.py:153] step: 363650, training_loss: 1.13942e+02
I1111 17:43:26.742227 140264174335808 run_lib.py:153] step: 363700, training_loss: 1.45397e+02
I1111 17:43:37.011230 140264174335808 run_lib.py:153] step: 363750, training_loss: 1.12234e+02
I1111 17:43:47.430789 140264174335808 run_lib.py:153] step: 363800, training_loss: 1.24244e+02
I1111 17:43:58.008172 140264174335808 run_lib.py:153] step: 363850, training_loss: 1.15960e+02
I1111 17:44:08.720812 140264174335808 run_lib.py:153] step: 363900, training_loss: 1.29123e+02
I1111 17:44:18.841789 140264174335808 run_lib.py:153] step: 363950, training_loss: 1.52458e+02
I1111 17:44:28.743908 140264174335808 run_lib.py:153] step: 364000, training_loss: 1.44918e+02
I1111 17:44:38.444146 140264174335808 run_lib.py:153] step: 364050, training_loss: 1.08484e+02
I1111 17:44:48.983235 140264174335808 run_lib.py:153] step: 364100, training_loss: 1.39167e+02
I1111 17:44:59.563067 140264174335808 run_lib.py:153] step: 364150, training_loss: 1.59482e+02
I1111 17:45:09.218943 140264174335808 run_lib.py:153] step: 364200, training_loss: 1.05961e+02
I1111 17:45:19.964127 140264174335808 run_lib.py:153] step: 364250, training_loss: 1.16817e+02
I1111 17:45:30.814347 140264174335808 run_lib.py:153] step: 364300, training_loss: 8.77994e+01
I1111 17:45:41.492869 140264174335808 run_lib.py:153] step: 364350, training_loss: 1.37019e+02
I1111 17:45:52.060401 140264174335808 run_lib.py:153] step: 364400, training_loss: 1.13919e+02
I1111 17:46:02.004878 140264174335808 run_lib.py:153] step: 364450, training_loss: 1.23646e+02
I1111 17:46:12.442425 140264174335808 run_lib.py:153] step: 364500, training_loss: 1.12435e+02
I1111 17:46:22.770942 140264174335808 run_lib.py:153] step: 364550, training_loss: 9.59458e+01
I1111 17:46:33.285638 140264174335808 run_lib.py:153] step: 364600, training_loss: 1.23686e+02
I1111 17:46:43.218896 140264174335808 run_lib.py:153] step: 364650, training_loss: 1.27243e+02
I1111 17:46:53.027026 140264174335808 run_lib.py:153] step: 364700, training_loss: 1.29081e+02
I1111 17:47:03.614755 140264174335808 run_lib.py:153] step: 364750, training_loss: 1.30867e+02
I1111 17:47:14.619337 140264174335808 run_lib.py:153] step: 364800, training_loss: 9.98877e+01
I1111 17:47:24.640347 140264174335808 run_lib.py:153] step: 364850, training_loss: 1.35388e+02
I1111 17:47:35.861063 140264174335808 run_lib.py:153] step: 364900, training_loss: 1.30584e+02
I1111 17:47:45.974191 140264174335808 run_lib.py:153] step: 364950, training_loss: 1.26186e+02
I1111 17:47:55.802408 140264174335808 run_lib.py:153] step: 365000, training_loss: 1.33236e+02
I1111 17:47:55.942126 140264174335808 run_lib.py:166] step: 365000, eval_loss: 1.18397e+02
I1111 17:48:06.652895 140264174335808 run_lib.py:153] step: 365050, training_loss: 1.37803e+02
I1111 17:48:16.533790 140264174335808 run_lib.py:153] step: 365100, training_loss: 1.29038e+02
I1111 17:48:27.715947 140264174335808 run_lib.py:153] step: 365150, training_loss: 1.03959e+02
I1111 17:48:38.946341 140264174335808 run_lib.py:153] step: 365200, training_loss: 1.33100e+02
I1111 17:48:50.139728 140264174335808 run_lib.py:153] step: 365250, training_loss: 1.23781e+02
I1111 17:49:01.248228 140264174335808 run_lib.py:153] step: 365300, training_loss: 1.20983e+02
I1111 17:49:12.401059 140264174335808 run_lib.py:153] step: 365350, training_loss: 1.21184e+02
I1111 17:49:23.431027 140264174335808 run_lib.py:153] step: 365400, training_loss: 1.21076e+02
I1111 17:49:34.418209 140264174335808 run_lib.py:153] step: 365450, training_loss: 1.33594e+02
I1111 17:49:45.629805 140264174335808 run_lib.py:153] step: 365500, training_loss: 1.28654e+02
I1111 17:49:58.042258 140264174335808 run_lib.py:153] step: 365550, training_loss: 1.22238e+02
I1111 17:50:10.891508 140264174335808 run_lib.py:153] step: 365600, training_loss: 1.23863e+02
I1111 17:50:22.213753 140264174335808 run_lib.py:153] step: 365650, training_loss: 1.37578e+02
I1111 17:50:32.634234 140264174335808 run_lib.py:153] step: 365700, training_loss: 1.64119e+02
I1111 17:50:43.638653 140264174335808 run_lib.py:153] step: 365750, training_loss: 1.25395e+02
I1111 17:50:53.793623 140264174335808 run_lib.py:153] step: 365800, training_loss: 1.49950e+02
I1111 17:51:03.383174 140264174335808 run_lib.py:153] step: 365850, training_loss: 1.33248e+02
I1111 17:51:14.065469 140264174335808 run_lib.py:153] step: 365900, training_loss: 1.18368e+02
I1111 17:51:24.417392 140264174335808 run_lib.py:153] step: 365950, training_loss: 1.21226e+02
I1111 17:51:34.583925 140264174335808 run_lib.py:153] step: 366000, training_loss: 1.24293e+02
I1111 17:51:45.001554 140264174335808 run_lib.py:153] step: 366050, training_loss: 1.22580e+02
I1111 17:51:54.776807 140264174335808 run_lib.py:153] step: 366100, training_loss: 1.48835e+02
I1111 17:52:05.081463 140264174335808 run_lib.py:153] step: 366150, training_loss: 1.57266e+02
I1111 17:52:14.743377 140264174335808 run_lib.py:153] step: 366200, training_loss: 1.50935e+02
I1111 17:52:25.590142 140264174335808 run_lib.py:153] step: 366250, training_loss: 1.20591e+02
I1111 17:52:35.728219 140264174335808 run_lib.py:153] step: 366300, training_loss: 1.35929e+02
I1111 17:52:46.259459 140264174335808 run_lib.py:153] step: 366350, training_loss: 1.28373e+02
I1111 17:52:56.269435 140264174335808 run_lib.py:153] step: 366400, training_loss: 1.26837e+02
I1111 17:53:06.762272 140264174335808 run_lib.py:153] step: 366450, training_loss: 1.20555e+02
I1111 17:53:16.704404 140264174335808 run_lib.py:153] step: 366500, training_loss: 1.29916e+02
I1111 17:53:27.246769 140264174335808 run_lib.py:153] step: 366550, training_loss: 1.14104e+02
I1111 17:53:38.292903 140264174335808 run_lib.py:153] step: 366600, training_loss: 1.33568e+02
I1111 17:53:49.263058 140264174335808 run_lib.py:153] step: 366650, training_loss: 1.38658e+02
I1111 17:54:00.602440 140264174335808 run_lib.py:153] step: 366700, training_loss: 1.30507e+02
I1111 17:54:11.747667 140264174335808 run_lib.py:153] step: 366750, training_loss: 1.23878e+02
I1111 17:54:22.819708 140264174335808 run_lib.py:153] step: 366800, training_loss: 1.27191e+02
I1111 17:54:34.006992 140264174335808 run_lib.py:153] step: 366850, training_loss: 1.28429e+02
I1111 17:54:44.738793 140264174335808 run_lib.py:153] step: 366900, training_loss: 1.44583e+02
I1111 17:54:56.079552 140264174335808 run_lib.py:153] step: 366950, training_loss: 1.30290e+02
I1111 17:55:06.545703 140264174335808 run_lib.py:153] step: 367000, training_loss: 1.44159e+02
I1111 17:55:20.192178 140264174335808 run_lib.py:153] step: 367050, training_loss: 1.76995e+02
I1111 17:55:32.225510 140264174335808 run_lib.py:153] step: 367100, training_loss: 1.25824e+02
I1111 17:55:43.796764 140264174335808 run_lib.py:153] step: 367150, training_loss: 1.41031e+02
I1111 17:55:55.470139 140264174335808 run_lib.py:153] step: 367200, training_loss: 1.28394e+02
I1111 17:56:07.540086 140264174335808 run_lib.py:153] step: 367250, training_loss: 1.37157e+02
I1111 17:56:19.397330 140264174335808 run_lib.py:153] step: 367300, training_loss: 1.04195e+02
I1111 17:56:31.271444 140264174335808 run_lib.py:153] step: 367350, training_loss: 1.14093e+02
I1111 17:56:43.039107 140264174335808 run_lib.py:153] step: 367400, training_loss: 1.40808e+02
I1111 17:56:54.564338 140264174335808 run_lib.py:153] step: 367450, training_loss: 1.34986e+02
I1111 17:57:06.253256 140264174335808 run_lib.py:153] step: 367500, training_loss: 1.26884e+02
I1111 17:57:18.002326 140264174335808 run_lib.py:153] step: 367550, training_loss: 1.17910e+02
I1111 17:57:29.864326 140264174335808 run_lib.py:153] step: 367600, training_loss: 1.40204e+02
I1111 17:57:41.805285 140264174335808 run_lib.py:153] step: 367650, training_loss: 1.06363e+02
I1111 17:57:53.586096 140264174335808 run_lib.py:153] step: 367700, training_loss: 1.57627e+02
I1111 17:58:05.620084 140264174335808 run_lib.py:153] step: 367750, training_loss: 1.18038e+02
I1111 17:58:17.485264 140264174335808 run_lib.py:153] step: 367800, training_loss: 1.30925e+02
I1111 17:58:29.254475 140264174335808 run_lib.py:153] step: 367850, training_loss: 1.25327e+02
I1111 17:58:41.186148 140264174335808 run_lib.py:153] step: 367900, training_loss: 1.42222e+02
I1111 17:58:52.315597 140264174335808 run_lib.py:153] step: 367950, training_loss: 1.42879e+02
I1111 17:59:03.463505 140264174335808 run_lib.py:153] step: 368000, training_loss: 1.53846e+02
I1111 17:59:14.686952 140264174335808 run_lib.py:153] step: 368050, training_loss: 1.61325e+02
I1111 17:59:25.508528 140264174335808 run_lib.py:153] step: 368100, training_loss: 1.03676e+02
I1111 17:59:36.450697 140264174335808 run_lib.py:153] step: 368150, training_loss: 1.31432e+02
I1111 17:59:47.988296 140264174335808 run_lib.py:153] step: 368200, training_loss: 1.43044e+02
I1111 17:59:59.538431 140264174335808 run_lib.py:153] step: 368250, training_loss: 1.25400e+02
I1111 18:00:10.897132 140264174335808 run_lib.py:153] step: 368300, training_loss: 1.32439e+02
I1111 18:00:22.125479 140264174335808 run_lib.py:153] step: 368350, training_loss: 1.03644e+02
I1111 18:00:33.396459 140264174335808 run_lib.py:153] step: 368400, training_loss: 1.14619e+02
I1111 18:00:44.905680 140264174335808 run_lib.py:153] step: 368450, training_loss: 1.23313e+02
I1111 18:00:55.869953 140264174335808 run_lib.py:153] step: 368500, training_loss: 1.59550e+02
I1111 18:01:07.033344 140264174335808 run_lib.py:153] step: 368550, training_loss: 9.38966e+01
I1111 18:01:17.585900 140264174335808 run_lib.py:153] step: 368600, training_loss: 9.52112e+01
I1111 18:01:28.025927 140264174335808 run_lib.py:153] step: 368650, training_loss: 1.42493e+02
I1111 18:01:38.631489 140264174335808 run_lib.py:153] step: 368700, training_loss: 1.14620e+02
I1111 18:01:49.363147 140264174335808 run_lib.py:153] step: 368750, training_loss: 1.12918e+02
I1111 18:01:59.604404 140264174335808 run_lib.py:153] step: 368800, training_loss: 1.36788e+02
I1111 18:02:10.185331 140264174335808 run_lib.py:153] step: 368850, training_loss: 1.55877e+02
I1111 18:02:20.176862 140264174335808 run_lib.py:153] step: 368900, training_loss: 1.28934e+02
I1111 18:02:30.158015 140264174335808 run_lib.py:153] step: 368950, training_loss: 1.41441e+02
I1111 18:02:40.231066 140264174335808 run_lib.py:153] step: 369000, training_loss: 1.42897e+02
I1111 18:02:50.177148 140264174335808 run_lib.py:153] step: 369050, training_loss: 1.28055e+02
I1111 18:03:00.932555 140264174335808 run_lib.py:153] step: 369100, training_loss: 1.03350e+02
I1111 18:03:11.087195 140264174335808 run_lib.py:153] step: 369150, training_loss: 1.30004e+02
I1111 18:03:21.010703 140264174335808 run_lib.py:153] step: 369200, training_loss: 1.16771e+02
I1111 18:03:31.348133 140264174335808 run_lib.py:153] step: 369250, training_loss: 1.37858e+02
I1111 18:03:41.937044 140264174335808 run_lib.py:153] step: 369300, training_loss: 1.32246e+02
I1111 18:03:52.132856 140264174335808 run_lib.py:153] step: 369350, training_loss: 1.28857e+02
I1111 18:04:02.275841 140264174335808 run_lib.py:153] step: 369400, training_loss: 1.24298e+02
I1111 18:04:11.934108 140264174335808 run_lib.py:153] step: 369450, training_loss: 1.24584e+02
I1111 18:04:21.908754 140264174335808 run_lib.py:153] step: 369500, training_loss: 1.23057e+02
I1111 18:04:31.843298 140264174335808 run_lib.py:153] step: 369550, training_loss: 1.49487e+02
I1111 18:04:42.263943 140264174335808 run_lib.py:153] step: 369600, training_loss: 1.48323e+02
I1111 18:04:52.637671 140264174335808 run_lib.py:153] step: 369650, training_loss: 1.35096e+02
I1111 18:05:03.380879 140264174335808 run_lib.py:153] step: 369700, training_loss: 1.31330e+02
I1111 18:05:13.607786 140264174335808 run_lib.py:153] step: 369750, training_loss: 1.25272e+02
I1111 18:05:23.792667 140264174335808 run_lib.py:153] step: 369800, training_loss: 1.53854e+02
I1111 18:05:33.858384 140264174335808 run_lib.py:153] step: 369850, training_loss: 1.29043e+02
I1111 18:05:44.602498 140264174335808 run_lib.py:153] step: 369900, training_loss: 1.37030e+02
I1111 18:05:54.722115 140264174335808 run_lib.py:153] step: 369950, training_loss: 1.17733e+02
I1111 18:06:05.182239 140264174335808 run_lib.py:153] step: 370000, training_loss: 1.20933e+02
I1111 18:06:05.725094 140264174335808 run_lib.py:166] step: 370000, eval_loss: 1.34792e+02
I1111 18:06:15.152231 140264174335808 run_lib.py:153] step: 370050, training_loss: 1.35074e+02
I1111 18:06:25.451675 140264174335808 run_lib.py:153] step: 370100, training_loss: 1.35966e+02
I1111 18:06:35.516230 140264174335808 run_lib.py:153] step: 370150, training_loss: 1.29253e+02
I1111 18:06:45.337381 140264174335808 run_lib.py:153] step: 370200, training_loss: 1.42842e+02
I1111 18:06:54.968266 140264174335808 run_lib.py:153] step: 370250, training_loss: 1.38711e+02
I1111 18:07:05.310921 140264174335808 run_lib.py:153] step: 370300, training_loss: 1.09397e+02
I1111 18:07:15.398110 140264174335808 run_lib.py:153] step: 370350, training_loss: 1.30363e+02
I1111 18:07:25.221101 140264174335808 run_lib.py:153] step: 370400, training_loss: 1.58757e+02
I1111 18:07:35.642748 140264174335808 run_lib.py:153] step: 370450, training_loss: 1.54642e+02
I1111 18:07:45.194049 140264174335808 run_lib.py:153] step: 370500, training_loss: 9.52668e+01
I1111 18:07:55.289608 140264174335808 run_lib.py:153] step: 370550, training_loss: 1.22968e+02
I1111 18:08:05.572844 140264174335808 run_lib.py:153] step: 370600, training_loss: 1.29628e+02
I1111 18:08:15.878498 140264174335808 run_lib.py:153] step: 370650, training_loss: 1.29805e+02
I1111 18:08:25.862949 140264174335808 run_lib.py:153] step: 370700, training_loss: 1.27095e+02
I1111 18:08:36.752761 140264174335808 run_lib.py:153] step: 370750, training_loss: 1.22703e+02
I1111 18:08:46.935233 140264174335808 run_lib.py:153] step: 370800, training_loss: 1.16418e+02
I1111 18:08:56.469324 140264174335808 run_lib.py:153] step: 370850, training_loss: 1.25536e+02
I1111 18:09:06.734067 140264174335808 run_lib.py:153] step: 370900, training_loss: 1.65370e+02
I1111 18:09:17.300449 140264174335808 run_lib.py:153] step: 370950, training_loss: 1.37404e+02
I1111 18:09:27.581618 140264174335808 run_lib.py:153] step: 371000, training_loss: 1.40224e+02
I1111 18:09:37.521403 140264174335808 run_lib.py:153] step: 371050, training_loss: 1.36685e+02
I1111 18:09:47.400301 140264174335808 run_lib.py:153] step: 371100, training_loss: 1.33020e+02
I1111 18:09:57.021099 140264174335808 run_lib.py:153] step: 371150, training_loss: 1.44332e+02
I1111 18:10:07.215383 140264174335808 run_lib.py:153] step: 371200, training_loss: 1.35181e+02
I1111 18:10:17.751452 140264174335808 run_lib.py:153] step: 371250, training_loss: 9.88582e+01
I1111 18:10:27.983647 140264174335808 run_lib.py:153] step: 371300, training_loss: 1.54589e+02
I1111 18:10:38.498623 140264174335808 run_lib.py:153] step: 371350, training_loss: 1.57241e+02
I1111 18:10:48.259728 140264174335808 run_lib.py:153] step: 371400, training_loss: 1.24750e+02
I1111 18:10:58.723447 140264174335808 run_lib.py:153] step: 371450, training_loss: 1.19176e+02
I1111 18:11:08.724141 140264174335808 run_lib.py:153] step: 371500, training_loss: 1.18279e+02
I1111 18:11:18.278627 140264174335808 run_lib.py:153] step: 371550, training_loss: 1.30103e+02
I1111 18:11:28.514952 140264174335808 run_lib.py:153] step: 371600, training_loss: 1.41470e+02
I1111 18:11:38.204328 140264174335808 run_lib.py:153] step: 371650, training_loss: 1.19190e+02
I1111 18:11:47.600538 140264174335808 run_lib.py:153] step: 371700, training_loss: 1.40406e+02
I1111 18:11:57.340312 140264174335808 run_lib.py:153] step: 371750, training_loss: 1.39290e+02
I1111 18:12:07.716308 140264174335808 run_lib.py:153] step: 371800, training_loss: 1.28838e+02
I1111 18:12:17.991546 140264174335808 run_lib.py:153] step: 371850, training_loss: 1.10481e+02
I1111 18:12:27.983299 140264174335808 run_lib.py:153] step: 371900, training_loss: 1.34112e+02
I1111 18:12:38.318423 140264174335808 run_lib.py:153] step: 371950, training_loss: 1.44040e+02
I1111 18:12:48.463318 140264174335808 run_lib.py:153] step: 372000, training_loss: 1.48838e+02
I1111 18:12:58.827557 140264174335808 run_lib.py:153] step: 372050, training_loss: 1.31187e+02
I1111 18:13:08.595291 140264174335808 run_lib.py:153] step: 372100, training_loss: 1.20945e+02
I1111 18:13:19.051291 140264174335808 run_lib.py:153] step: 372150, training_loss: 1.42217e+02
I1111 18:13:29.110088 140264174335808 run_lib.py:153] step: 372200, training_loss: 1.21573e+02
I1111 18:13:39.226607 140264174335808 run_lib.py:153] step: 372250, training_loss: 1.32446e+02
I1111 18:13:49.274501 140264174335808 run_lib.py:153] step: 372300, training_loss: 1.47980e+02
I1111 18:14:00.273869 140264174335808 run_lib.py:153] step: 372350, training_loss: 1.08853e+02
I1111 18:14:11.092347 140264174335808 run_lib.py:153] step: 372400, training_loss: 1.29340e+02
I1111 18:14:21.336677 140264174335808 run_lib.py:153] step: 372450, training_loss: 1.61043e+02
I1111 18:14:31.047998 140264174335808 run_lib.py:153] step: 372500, training_loss: 1.37270e+02
I1111 18:14:41.828014 140264174335808 run_lib.py:153] step: 372550, training_loss: 1.35998e+02
I1111 18:14:52.107798 140264174335808 run_lib.py:153] step: 372600, training_loss: 1.39698e+02
I1111 18:15:01.860910 140264174335808 run_lib.py:153] step: 372650, training_loss: 1.35523e+02
I1111 18:15:12.493062 140264174335808 run_lib.py:153] step: 372700, training_loss: 1.55231e+02
I1111 18:15:21.861966 140264174335808 run_lib.py:153] step: 372750, training_loss: 1.27967e+02
I1111 18:15:32.197198 140264174335808 run_lib.py:153] step: 372800, training_loss: 1.38430e+02
I1111 18:15:42.786521 140264174335808 run_lib.py:153] step: 372850, training_loss: 1.40820e+02
I1111 18:15:53.359606 140264174335808 run_lib.py:153] step: 372900, training_loss: 1.35765e+02
I1111 18:16:04.048328 140264174335808 run_lib.py:153] step: 372950, training_loss: 1.17570e+02
I1111 18:16:14.459557 140264174335808 run_lib.py:153] step: 373000, training_loss: 1.62225e+02
I1111 18:16:24.843935 140264174335808 run_lib.py:153] step: 373050, training_loss: 1.28409e+02
I1111 18:16:34.752963 140264174335808 run_lib.py:153] step: 373100, training_loss: 1.28979e+02
I1111 18:16:45.063141 140264174335808 run_lib.py:153] step: 373150, training_loss: 1.15499e+02
I1111 18:16:55.243053 140264174335808 run_lib.py:153] step: 373200, training_loss: 1.46624e+02
I1111 18:17:05.189603 140264174335808 run_lib.py:153] step: 373250, training_loss: 1.13810e+02
I1111 18:17:15.161845 140264174335808 run_lib.py:153] step: 373300, training_loss: 1.03840e+02
I1111 18:17:24.506357 140264174335808 run_lib.py:153] step: 373350, training_loss: 1.26368e+02
I1111 18:17:34.642535 140264174335808 run_lib.py:153] step: 373400, training_loss: 9.08549e+01
I1111 18:17:45.016743 140264174335808 run_lib.py:153] step: 373450, training_loss: 1.19423e+02
I1111 18:17:55.458671 140264174335808 run_lib.py:153] step: 373500, training_loss: 1.53949e+02
I1111 18:18:06.749962 140264174335808 run_lib.py:153] step: 373550, training_loss: 1.23248e+02
I1111 18:18:16.795539 140264174335808 run_lib.py:153] step: 373600, training_loss: 1.43921e+02
I1111 18:18:27.373625 140264174335808 run_lib.py:153] step: 373650, training_loss: 1.36334e+02
I1111 18:18:36.993598 140264174335808 run_lib.py:153] step: 373700, training_loss: 1.28759e+02
I1111 18:18:47.397699 140264174335808 run_lib.py:153] step: 373750, training_loss: 9.38718e+01
I1111 18:18:57.794205 140264174335808 run_lib.py:153] step: 373800, training_loss: 1.21188e+02
I1111 18:19:08.344006 140264174335808 run_lib.py:153] step: 373850, training_loss: 1.32342e+02
I1111 18:19:19.174121 140264174335808 run_lib.py:153] step: 373900, training_loss: 1.32240e+02
I1111 18:19:29.201696 140264174335808 run_lib.py:153] step: 373950, training_loss: 1.29757e+02
I1111 18:19:38.865342 140264174335808 run_lib.py:153] step: 374000, training_loss: 1.45780e+02
I1111 18:19:48.627817 140264174335808 run_lib.py:153] step: 374050, training_loss: 1.34870e+02
I1111 18:19:58.170792 140264174335808 run_lib.py:153] step: 374100, training_loss: 1.56752e+02
I1111 18:20:08.344163 140264174335808 run_lib.py:153] step: 374150, training_loss: 1.02014e+02
I1111 18:20:18.056847 140264174335808 run_lib.py:153] step: 374200, training_loss: 1.62348e+02
I1111 18:20:27.948287 140264174335808 run_lib.py:153] step: 374250, training_loss: 1.33478e+02
I1111 18:20:37.823956 140264174335808 run_lib.py:153] step: 374300, training_loss: 1.34268e+02
I1111 18:20:47.643517 140264174335808 run_lib.py:153] step: 374350, training_loss: 1.48004e+02
I1111 18:20:57.627492 140264174335808 run_lib.py:153] step: 374400, training_loss: 1.37273e+02
I1111 18:21:07.310293 140264174335808 run_lib.py:153] step: 374450, training_loss: 1.34568e+02
I1111 18:21:16.733973 140264174335808 run_lib.py:153] step: 374500, training_loss: 1.20345e+02
I1111 18:21:26.456482 140264174335808 run_lib.py:153] step: 374550, training_loss: 1.13204e+02
I1111 18:21:35.688482 140264174335808 run_lib.py:153] step: 374600, training_loss: 1.20017e+02
I1111 18:21:45.357223 140264174335808 run_lib.py:153] step: 374650, training_loss: 1.51833e+02
I1111 18:21:55.326879 140264174335808 run_lib.py:153] step: 374700, training_loss: 1.17742e+02
I1111 18:22:04.973196 140264174335808 run_lib.py:153] step: 374750, training_loss: 1.27794e+02
I1111 18:22:15.055740 140264174335808 run_lib.py:153] step: 374800, training_loss: 1.24663e+02
I1111 18:22:25.543569 140264174335808 run_lib.py:153] step: 374850, training_loss: 1.24954e+02
I1111 18:22:35.733026 140264174335808 run_lib.py:153] step: 374900, training_loss: 1.36520e+02
I1111 18:22:45.447786 140264174335808 run_lib.py:153] step: 374950, training_loss: 1.49073e+02
I1111 18:22:56.532942 140264174335808 run_lib.py:153] step: 375000, training_loss: 1.55051e+02
I1111 18:22:56.674004 140264174335808 run_lib.py:166] step: 375000, eval_loss: 1.13409e+02
I1111 18:23:06.506005 140264174335808 run_lib.py:153] step: 375050, training_loss: 1.16125e+02
I1111 18:23:16.418178 140264174335808 run_lib.py:153] step: 375100, training_loss: 1.34639e+02
I1111 18:23:27.330549 140264174335808 run_lib.py:153] step: 375150, training_loss: 1.27637e+02
I1111 18:23:38.567644 140264174335808 run_lib.py:153] step: 375200, training_loss: 1.33617e+02
I1111 18:23:48.507358 140264174335808 run_lib.py:153] step: 375250, training_loss: 1.49009e+02
I1111 18:23:58.609257 140264174335808 run_lib.py:153] step: 375300, training_loss: 1.20329e+02
I1111 18:24:08.997317 140264174335808 run_lib.py:153] step: 375350, training_loss: 9.06893e+01
I1111 18:24:19.545280 140264174335808 run_lib.py:153] step: 375400, training_loss: 1.09785e+02
I1111 18:24:29.834299 140264174335808 run_lib.py:153] step: 375450, training_loss: 1.05027e+02
I1111 18:24:40.257692 140264174335808 run_lib.py:153] step: 375500, training_loss: 1.27710e+02
I1111 18:24:49.792511 140264174335808 run_lib.py:153] step: 375550, training_loss: 1.35077e+02
I1111 18:24:59.203495 140264174335808 run_lib.py:153] step: 375600, training_loss: 1.18597e+02
I1111 18:25:08.720783 140264174335808 run_lib.py:153] step: 375650, training_loss: 1.15079e+02
I1111 18:25:19.017850 140264174335808 run_lib.py:153] step: 375700, training_loss: 1.31945e+02
I1111 18:25:28.851447 140264174335808 run_lib.py:153] step: 375750, training_loss: 1.26524e+02
I1111 18:25:38.287870 140264174335808 run_lib.py:153] step: 375800, training_loss: 1.60801e+02
I1111 18:25:47.923317 140264174335808 run_lib.py:153] step: 375850, training_loss: 1.40230e+02
I1111 18:25:58.175823 140264174335808 run_lib.py:153] step: 375900, training_loss: 1.48675e+02
I1111 18:26:07.891555 140264174335808 run_lib.py:153] step: 375950, training_loss: 1.22619e+02
I1111 18:26:17.601858 140264174335808 run_lib.py:153] step: 376000, training_loss: 1.06537e+02
I1111 18:26:27.284329 140264174335808 run_lib.py:153] step: 376050, training_loss: 1.19525e+02
I1111 18:26:37.003093 140264174335808 run_lib.py:153] step: 376100, training_loss: 1.46711e+02
I1111 18:26:46.618543 140264174335808 run_lib.py:153] step: 376150, training_loss: 1.23913e+02
I1111 18:26:56.214148 140264174335808 run_lib.py:153] step: 376200, training_loss: 1.16047e+02
I1111 18:27:05.724381 140264174335808 run_lib.py:153] step: 376250, training_loss: 1.19942e+02
I1111 18:27:15.584721 140264174335808 run_lib.py:153] step: 376300, training_loss: 1.46315e+02
I1111 18:27:25.407521 140264174335808 run_lib.py:153] step: 376350, training_loss: 1.13200e+02
I1111 18:27:35.418197 140264174335808 run_lib.py:153] step: 376400, training_loss: 1.35768e+02
I1111 18:27:45.363152 140264174335808 run_lib.py:153] step: 376450, training_loss: 1.40963e+02
I1111 18:27:55.249927 140264174335808 run_lib.py:153] step: 376500, training_loss: 1.27192e+02
I1111 18:28:05.019268 140264174335808 run_lib.py:153] step: 376550, training_loss: 1.16915e+02
I1111 18:28:15.439160 140264174335808 run_lib.py:153] step: 376600, training_loss: 1.11606e+02
I1111 18:28:25.909165 140264174335808 run_lib.py:153] step: 376650, training_loss: 1.41684e+02
I1111 18:28:35.926604 140264174335808 run_lib.py:153] step: 376700, training_loss: 1.41243e+02
I1111 18:28:45.358769 140264174335808 run_lib.py:153] step: 376750, training_loss: 1.09698e+02
I1111 18:28:55.162400 140264174335808 run_lib.py:153] step: 376800, training_loss: 1.34851e+02
I1111 18:29:05.406314 140264174335808 run_lib.py:153] step: 376850, training_loss: 1.20587e+02
I1111 18:29:15.257184 140264174335808 run_lib.py:153] step: 376900, training_loss: 1.22208e+02
I1111 18:29:25.292846 140264174335808 run_lib.py:153] step: 376950, training_loss: 1.35754e+02
I1111 18:29:35.056755 140264174335808 run_lib.py:153] step: 377000, training_loss: 1.08559e+02
I1111 18:29:45.508667 140264174335808 run_lib.py:153] step: 377050, training_loss: 1.17374e+02
I1111 18:29:55.505790 140264174335808 run_lib.py:153] step: 377100, training_loss: 1.51725e+02
I1111 18:30:05.422745 140264174335808 run_lib.py:153] step: 377150, training_loss: 1.22074e+02
I1111 18:30:14.969955 140264174335808 run_lib.py:153] step: 377200, training_loss: 1.44032e+02
I1111 18:30:24.796017 140264174335808 run_lib.py:153] step: 377250, training_loss: 1.22514e+02
I1111 18:30:34.472584 140264174335808 run_lib.py:153] step: 377300, training_loss: 1.49275e+02
I1111 18:30:44.692793 140264174335808 run_lib.py:153] step: 377350, training_loss: 1.40173e+02
I1111 18:30:54.654806 140264174335808 run_lib.py:153] step: 377400, training_loss: 9.57262e+01
I1111 18:31:04.022025 140264174335808 run_lib.py:153] step: 377450, training_loss: 1.20341e+02
I1111 18:31:13.379411 140264174335808 run_lib.py:153] step: 377500, training_loss: 1.47863e+02
I1111 18:31:23.356522 140264174335808 run_lib.py:153] step: 377550, training_loss: 1.70730e+02
I1111 18:31:33.048177 140264174335808 run_lib.py:153] step: 377600, training_loss: 1.17216e+02
I1111 18:31:42.917259 140264174335808 run_lib.py:153] step: 377650, training_loss: 1.13712e+02
I1111 18:31:52.643848 140264174335808 run_lib.py:153] step: 377700, training_loss: 1.30052e+02
I1111 18:32:02.618181 140264174335808 run_lib.py:153] step: 377750, training_loss: 1.22935e+02
I1111 18:32:12.470463 140264174335808 run_lib.py:153] step: 377800, training_loss: 1.13364e+02
I1111 18:32:21.821513 140264174335808 run_lib.py:153] step: 377850, training_loss: 1.09016e+02
I1111 18:32:31.349625 140264174335808 run_lib.py:153] step: 377900, training_loss: 1.21933e+02
I1111 18:32:40.872645 140264174335808 run_lib.py:153] step: 377950, training_loss: 1.22068e+02
I1111 18:32:50.378042 140264174335808 run_lib.py:153] step: 378000, training_loss: 1.14085e+02
I1111 18:32:59.577874 140264174335808 run_lib.py:153] step: 378050, training_loss: 1.21244e+02
I1111 18:33:09.722557 140264174335808 run_lib.py:153] step: 378100, training_loss: 1.02404e+02
I1111 18:33:20.475641 140264174335808 run_lib.py:153] step: 378150, training_loss: 1.27008e+02
I1111 18:33:30.361692 140264174335808 run_lib.py:153] step: 378200, training_loss: 1.45310e+02
I1111 18:33:40.312116 140264174335808 run_lib.py:153] step: 378250, training_loss: 9.43620e+01
I1111 18:33:50.601878 140264174335808 run_lib.py:153] step: 378300, training_loss: 1.16786e+02
I1111 18:34:01.012717 140264174335808 run_lib.py:153] step: 378350, training_loss: 1.20918e+02
I1111 18:34:11.320729 140264174335808 run_lib.py:153] step: 378400, training_loss: 1.27635e+02
I1111 18:34:20.671590 140264174335808 run_lib.py:153] step: 378450, training_loss: 1.31867e+02
I1111 18:34:30.554211 140264174335808 run_lib.py:153] step: 378500, training_loss: 1.16482e+02
I1111 18:34:40.341126 140264174335808 run_lib.py:153] step: 378550, training_loss: 1.44054e+02
I1111 18:34:50.274014 140264174335808 run_lib.py:153] step: 378600, training_loss: 1.21878e+02
I1111 18:35:00.513549 140264174335808 run_lib.py:153] step: 378650, training_loss: 1.17766e+02
I1111 18:35:11.047485 140264174335808 run_lib.py:153] step: 378700, training_loss: 1.03817e+02
I1111 18:35:21.392694 140264174335808 run_lib.py:153] step: 378750, training_loss: 1.45099e+02
I1111 18:35:31.136151 140264174335808 run_lib.py:153] step: 378800, training_loss: 1.31529e+02
I1111 18:35:41.901395 140264174335808 run_lib.py:153] step: 378850, training_loss: 1.31762e+02
I1111 18:35:52.474710 140264174335808 run_lib.py:153] step: 378900, training_loss: 9.86963e+01
I1111 18:36:02.951887 140264174335808 run_lib.py:153] step: 378950, training_loss: 1.46788e+02
I1111 18:36:13.839738 140264174335808 run_lib.py:153] step: 379000, training_loss: 1.15724e+02
I1111 18:36:23.138844 140264174335808 run_lib.py:153] step: 379050, training_loss: 1.34883e+02
I1111 18:36:33.224416 140264174335808 run_lib.py:153] step: 379100, training_loss: 1.42869e+02
I1111 18:36:43.718888 140264174335808 run_lib.py:153] step: 379150, training_loss: 1.24052e+02
I1111 18:36:54.377127 140264174335808 run_lib.py:153] step: 379200, training_loss: 1.00668e+02
I1111 18:37:04.188252 140264174335808 run_lib.py:153] step: 379250, training_loss: 1.27624e+02
I1111 18:37:14.425108 140264174335808 run_lib.py:153] step: 379300, training_loss: 1.60443e+02
I1111 18:37:24.525391 140264174335808 run_lib.py:153] step: 379350, training_loss: 1.47017e+02
I1111 18:37:34.493752 140264174335808 run_lib.py:153] step: 379400, training_loss: 1.23137e+02
I1111 18:37:44.247148 140264174335808 run_lib.py:153] step: 379450, training_loss: 1.15363e+02
I1111 18:37:53.884900 140264174335808 run_lib.py:153] step: 379500, training_loss: 9.91876e+01
I1111 18:38:03.996053 140264174335808 run_lib.py:153] step: 379550, training_loss: 1.10474e+02
I1111 18:38:14.609174 140264174335808 run_lib.py:153] step: 379600, training_loss: 1.32152e+02
I1111 18:38:24.704802 140264174335808 run_lib.py:153] step: 379650, training_loss: 1.21875e+02
I1111 18:38:34.670887 140264174335808 run_lib.py:153] step: 379700, training_loss: 1.34815e+02
I1111 18:38:44.901939 140264174335808 run_lib.py:153] step: 379750, training_loss: 1.23075e+02
I1111 18:38:54.947524 140264174335808 run_lib.py:153] step: 379800, training_loss: 1.04194e+02
I1111 18:39:04.799190 140264174335808 run_lib.py:153] step: 379850, training_loss: 1.10210e+02
I1111 18:39:15.176239 140264174335808 run_lib.py:153] step: 379900, training_loss: 1.20486e+02
I1111 18:39:25.889925 140264174335808 run_lib.py:153] step: 379950, training_loss: 1.55055e+02
I1111 18:39:35.584821 140264174335808 run_lib.py:153] step: 380000, training_loss: 1.24225e+02
I1111 18:39:36.207911 140264174335808 run_lib.py:166] step: 380000, eval_loss: 1.36509e+02
I1111 18:39:45.722091 140264174335808 run_lib.py:153] step: 380050, training_loss: 1.21463e+02
I1111 18:39:56.049771 140264174335808 run_lib.py:153] step: 380100, training_loss: 1.28541e+02
I1111 18:40:06.308799 140264174335808 run_lib.py:153] step: 380150, training_loss: 1.38551e+02
I1111 18:40:15.712941 140264174335808 run_lib.py:153] step: 380200, training_loss: 1.54024e+02
I1111 18:40:26.107457 140264174335808 run_lib.py:153] step: 380250, training_loss: 9.52764e+01
I1111 18:40:36.266177 140264174335808 run_lib.py:153] step: 380300, training_loss: 9.98896e+01
I1111 18:40:46.997826 140264174335808 run_lib.py:153] step: 380350, training_loss: 1.15551e+02
I1111 18:40:57.125951 140264174335808 run_lib.py:153] step: 380400, training_loss: 1.32452e+02
I1111 18:41:07.696469 140264174335808 run_lib.py:153] step: 380450, training_loss: 1.22780e+02
I1111 18:41:18.230133 140264174335808 run_lib.py:153] step: 380500, training_loss: 1.40181e+02
I1111 18:41:28.323255 140264174335808 run_lib.py:153] step: 380550, training_loss: 1.55360e+02
I1111 18:41:38.583201 140264174335808 run_lib.py:153] step: 380600, training_loss: 1.21336e+02
I1111 18:41:48.641888 140264174335808 run_lib.py:153] step: 380650, training_loss: 1.04872e+02
I1111 18:41:58.688218 140264174335808 run_lib.py:153] step: 380700, training_loss: 1.11863e+02
I1111 18:42:08.821685 140264174335808 run_lib.py:153] step: 380750, training_loss: 1.47058e+02
I1111 18:42:18.722643 140264174335808 run_lib.py:153] step: 380800, training_loss: 1.10266e+02
I1111 18:42:28.704749 140264174335808 run_lib.py:153] step: 380850, training_loss: 1.12654e+02
I1111 18:42:38.614478 140264174335808 run_lib.py:153] step: 380900, training_loss: 1.39634e+02
I1111 18:42:48.049500 140264174335808 run_lib.py:153] step: 380950, training_loss: 1.32668e+02
I1111 18:42:57.882881 140264174335808 run_lib.py:153] step: 381000, training_loss: 1.09381e+02
I1111 18:43:07.764921 140264174335808 run_lib.py:153] step: 381050, training_loss: 1.34396e+02
I1111 18:43:17.503110 140264174335808 run_lib.py:153] step: 381100, training_loss: 8.22398e+01
I1111 18:43:28.122541 140264174335808 run_lib.py:153] step: 381150, training_loss: 1.10511e+02
I1111 18:43:39.553847 140264174335808 run_lib.py:153] step: 381200, training_loss: 1.62419e+02
I1111 18:43:49.455343 140264174335808 run_lib.py:153] step: 381250, training_loss: 1.37221e+02
I1111 18:43:59.280289 140264174335808 run_lib.py:153] step: 381300, training_loss: 1.18574e+02
I1111 18:44:09.294324 140264174335808 run_lib.py:153] step: 381350, training_loss: 1.34654e+02
I1111 18:44:18.877561 140264174335808 run_lib.py:153] step: 381400, training_loss: 1.23950e+02
I1111 18:44:28.611521 140264174335808 run_lib.py:153] step: 381450, training_loss: 1.08166e+02
I1111 18:44:38.248927 140264174335808 run_lib.py:153] step: 381500, training_loss: 1.15806e+02
I1111 18:44:48.148290 140264174335808 run_lib.py:153] step: 381550, training_loss: 1.27707e+02
I1111 18:44:57.494941 140264174335808 run_lib.py:153] step: 381600, training_loss: 1.41197e+02
I1111 18:45:07.239457 140264174335808 run_lib.py:153] step: 381650, training_loss: 1.56869e+02
I1111 18:45:16.765123 140264174335808 run_lib.py:153] step: 381700, training_loss: 1.37041e+02
I1111 18:45:26.780219 140264174335808 run_lib.py:153] step: 381750, training_loss: 1.28700e+02
I1111 18:45:36.783046 140264174335808 run_lib.py:153] step: 381800, training_loss: 1.53659e+02
I1111 18:45:46.498824 140264174335808 run_lib.py:153] step: 381850, training_loss: 1.50967e+02
I1111 18:45:56.415980 140264174335808 run_lib.py:153] step: 381900, training_loss: 1.27495e+02
I1111 18:46:06.610324 140264174335808 run_lib.py:153] step: 381950, training_loss: 1.08747e+02
I1111 18:46:17.051258 140264174335808 run_lib.py:153] step: 382000, training_loss: 1.22690e+02
I1111 18:46:27.278622 140264174335808 run_lib.py:153] step: 382050, training_loss: 1.35925e+02
I1111 18:46:37.535725 140264174335808 run_lib.py:153] step: 382100, training_loss: 1.22484e+02
I1111 18:46:47.999732 140264174335808 run_lib.py:153] step: 382150, training_loss: 1.11239e+02
I1111 18:46:58.090884 140264174335808 run_lib.py:153] step: 382200, training_loss: 1.29042e+02
I1111 18:47:07.498045 140264174335808 run_lib.py:153] step: 382250, training_loss: 1.24181e+02
I1111 18:47:16.712591 140264174335808 run_lib.py:153] step: 382300, training_loss: 1.42170e+02
I1111 18:47:26.302140 140264174335808 run_lib.py:153] step: 382350, training_loss: 1.11123e+02
I1111 18:47:36.250714 140264174335808 run_lib.py:153] step: 382400, training_loss: 1.00644e+02
I1111 18:47:46.234922 140264174335808 run_lib.py:153] step: 382450, training_loss: 1.32831e+02
I1111 18:47:55.973631 140264174335808 run_lib.py:153] step: 382500, training_loss: 1.26422e+02
I1111 18:48:06.010218 140264174335808 run_lib.py:153] step: 382550, training_loss: 1.17588e+02
I1111 18:48:16.083019 140264174335808 run_lib.py:153] step: 382600, training_loss: 1.14208e+02
I1111 18:48:25.501149 140264174335808 run_lib.py:153] step: 382650, training_loss: 1.21001e+02
I1111 18:48:35.564508 140264174335808 run_lib.py:153] step: 382700, training_loss: 1.08163e+02
I1111 18:48:45.712172 140264174335808 run_lib.py:153] step: 382750, training_loss: 1.56708e+02
I1111 18:48:56.557633 140264174335808 run_lib.py:153] step: 382800, training_loss: 1.39769e+02
I1111 18:49:06.685196 140264174335808 run_lib.py:153] step: 382850, training_loss: 1.29416e+02
I1111 18:49:16.472535 140264174335808 run_lib.py:153] step: 382900, training_loss: 1.55747e+02
I1111 18:49:26.966277 140264174335808 run_lib.py:153] step: 382950, training_loss: 1.23298e+02
I1111 18:49:37.287759 140264174335808 run_lib.py:153] step: 383000, training_loss: 1.22683e+02
I1111 18:49:47.318172 140264174335808 run_lib.py:153] step: 383050, training_loss: 1.19058e+02
I1111 18:49:57.363303 140264174335808 run_lib.py:153] step: 383100, training_loss: 1.46509e+02
I1111 18:50:07.031203 140264174335808 run_lib.py:153] step: 383150, training_loss: 1.29669e+02
I1111 18:50:17.291483 140264174335808 run_lib.py:153] step: 383200, training_loss: 1.22220e+02
I1111 18:50:27.626422 140264174335808 run_lib.py:153] step: 383250, training_loss: 1.32131e+02
I1111 18:50:37.411016 140264174335808 run_lib.py:153] step: 383300, training_loss: 1.03573e+02
I1111 18:50:47.861678 140264174335808 run_lib.py:153] step: 383350, training_loss: 1.24394e+02
I1111 18:50:58.057533 140264174335808 run_lib.py:153] step: 383400, training_loss: 1.26751e+02
I1111 18:51:07.771355 140264174335808 run_lib.py:153] step: 383450, training_loss: 1.11839e+02
I1111 18:51:18.220662 140264174335808 run_lib.py:153] step: 383500, training_loss: 1.38494e+02
I1111 18:51:28.249554 140264174335808 run_lib.py:153] step: 383550, training_loss: 1.18835e+02
I1111 18:51:38.563302 140264174335808 run_lib.py:153] step: 383600, training_loss: 1.20319e+02
I1111 18:51:48.144286 140264174335808 run_lib.py:153] step: 383650, training_loss: 1.32727e+02
I1111 18:51:57.481368 140264174335808 run_lib.py:153] step: 383700, training_loss: 1.18278e+02
I1111 18:52:07.465372 140264174335808 run_lib.py:153] step: 383750, training_loss: 1.17214e+02
I1111 18:52:17.672795 140264174335808 run_lib.py:153] step: 383800, training_loss: 1.31561e+02
I1111 18:52:27.909234 140264174335808 run_lib.py:153] step: 383850, training_loss: 1.34167e+02
I1111 18:52:38.169273 140264174335808 run_lib.py:153] step: 383900, training_loss: 1.20704e+02
I1111 18:52:47.667381 140264174335808 run_lib.py:153] step: 383950, training_loss: 1.21334e+02
I1111 18:52:57.851645 140264174335808 run_lib.py:153] step: 384000, training_loss: 1.29034e+02
I1111 18:53:07.247873 140264174335808 run_lib.py:153] step: 384050, training_loss: 1.02908e+02
I1111 18:53:17.152740 140264174335808 run_lib.py:153] step: 384100, training_loss: 1.08839e+02
I1111 18:53:27.675026 140264174335808 run_lib.py:153] step: 384150, training_loss: 8.54194e+01
I1111 18:53:37.504696 140264174335808 run_lib.py:153] step: 384200, training_loss: 1.13689e+02
I1111 18:53:48.090831 140264174335808 run_lib.py:153] step: 384250, training_loss: 1.06805e+02
I1111 18:53:58.758221 140264174335808 run_lib.py:153] step: 384300, training_loss: 1.25036e+02
I1111 18:54:08.787290 140264174335808 run_lib.py:153] step: 384350, training_loss: 1.34029e+02
I1111 18:54:19.299179 140264174335808 run_lib.py:153] step: 384400, training_loss: 1.22937e+02
I1111 18:54:29.658254 140264174335808 run_lib.py:153] step: 384450, training_loss: 1.15662e+02
I1111 18:54:39.788134 140264174335808 run_lib.py:153] step: 384500, training_loss: 1.44763e+02
I1111 18:54:49.814483 140264174335808 run_lib.py:153] step: 384550, training_loss: 1.24391e+02
I1111 18:54:59.563096 140264174335808 run_lib.py:153] step: 384600, training_loss: 1.55611e+02
I1111 18:55:09.201103 140264174335808 run_lib.py:153] step: 384650, training_loss: 1.10678e+02
I1111 18:55:20.279321 140264174335808 run_lib.py:153] step: 384700, training_loss: 1.31784e+02
I1111 18:55:30.171408 140264174335808 run_lib.py:153] step: 384750, training_loss: 1.42959e+02
I1111 18:55:40.215191 140264174335808 run_lib.py:153] step: 384800, training_loss: 1.26182e+02
I1111 18:55:50.012935 140264174335808 run_lib.py:153] step: 384850, training_loss: 1.45142e+02
I1111 18:56:01.149354 140264174335808 run_lib.py:153] step: 384900, training_loss: 1.20997e+02
I1111 18:56:11.097149 140264174335808 run_lib.py:153] step: 384950, training_loss: 1.47445e+02
I1111 18:56:21.439049 140264174335808 run_lib.py:153] step: 385000, training_loss: 9.41290e+01
I1111 18:56:21.540187 140264174335808 run_lib.py:166] step: 385000, eval_loss: 1.37607e+02
I1111 18:56:32.204076 140264174335808 run_lib.py:153] step: 385050, training_loss: 1.10817e+02
I1111 18:56:41.907070 140264174335808 run_lib.py:153] step: 385100, training_loss: 1.52741e+02
I1111 18:56:51.540100 140264174335808 run_lib.py:153] step: 385150, training_loss: 1.10071e+02
I1111 18:57:01.653833 140264174335808 run_lib.py:153] step: 385200, training_loss: 1.32203e+02
I1111 18:57:11.110817 140264174335808 run_lib.py:153] step: 385250, training_loss: 1.19516e+02
I1111 18:57:21.047333 140264174335808 run_lib.py:153] step: 385300, training_loss: 1.20089e+02
I1111 18:57:31.460218 140264174335808 run_lib.py:153] step: 385350, training_loss: 1.37030e+02
I1111 18:57:41.954910 140264174335808 run_lib.py:153] step: 385400, training_loss: 1.28901e+02
I1111 18:57:51.887053 140264174335808 run_lib.py:153] step: 385450, training_loss: 1.31796e+02
I1111 18:58:02.014976 140264174335808 run_lib.py:153] step: 385500, training_loss: 1.45264e+02
I1111 18:58:11.342452 140264174335808 run_lib.py:153] step: 385550, training_loss: 1.26097e+02
I1111 18:58:21.063220 140264174335808 run_lib.py:153] step: 385600, training_loss: 1.18104e+02
I1111 18:58:30.938918 140264174335808 run_lib.py:153] step: 385650, training_loss: 1.28063e+02
I1111 18:58:40.673280 140264174335808 run_lib.py:153] step: 385700, training_loss: 1.24459e+02
I1111 18:58:51.322974 140264174335808 run_lib.py:153] step: 385750, training_loss: 1.26342e+02
I1111 18:59:01.438089 140264174335808 run_lib.py:153] step: 385800, training_loss: 1.18614e+02
I1111 18:59:11.678421 140264174335808 run_lib.py:153] step: 385850, training_loss: 1.00076e+02
I1111 18:59:21.509863 140264174335808 run_lib.py:153] step: 385900, training_loss: 1.53689e+02
I1111 18:59:32.035836 140264174335808 run_lib.py:153] step: 385950, training_loss: 1.39206e+02
I1111 18:59:42.394250 140264174335808 run_lib.py:153] step: 386000, training_loss: 1.19352e+02
I1111 18:59:52.516754 140264174335808 run_lib.py:153] step: 386050, training_loss: 1.27304e+02
I1111 19:00:02.217123 140264174335808 run_lib.py:153] step: 386100, training_loss: 1.50575e+02
I1111 19:00:11.880957 140264174335808 run_lib.py:153] step: 386150, training_loss: 1.47744e+02
I1111 19:00:22.395925 140264174335808 run_lib.py:153] step: 386200, training_loss: 1.28248e+02
I1111 19:00:32.006084 140264174335808 run_lib.py:153] step: 386250, training_loss: 1.30891e+02
I1111 19:00:42.547589 140264174335808 run_lib.py:153] step: 386300, training_loss: 1.08120e+02
I1111 19:00:52.588847 140264174335808 run_lib.py:153] step: 386350, training_loss: 1.26524e+02
I1111 19:01:02.755199 140264174335808 run_lib.py:153] step: 386400, training_loss: 1.22967e+02
I1111 19:01:12.181154 140264174335808 run_lib.py:153] step: 386450, training_loss: 1.23000e+02
I1111 19:01:21.694846 140264174335808 run_lib.py:153] step: 386500, training_loss: 1.45390e+02
I1111 19:01:31.326572 140264174335808 run_lib.py:153] step: 386550, training_loss: 1.40101e+02
I1111 19:01:41.260635 140264174335808 run_lib.py:153] step: 386600, training_loss: 1.25927e+02
I1111 19:01:51.108875 140264174335808 run_lib.py:153] step: 386650, training_loss: 1.41377e+02
I1111 19:02:01.489731 140264174335808 run_lib.py:153] step: 386700, training_loss: 1.13852e+02
I1111 19:02:11.565441 140264174335808 run_lib.py:153] step: 386750, training_loss: 1.29166e+02
I1111 19:02:21.436045 140264174335808 run_lib.py:153] step: 386800, training_loss: 1.50761e+02
I1111 19:02:30.941599 140264174335808 run_lib.py:153] step: 386850, training_loss: 1.39946e+02
I1111 19:02:40.468390 140264174335808 run_lib.py:153] step: 386900, training_loss: 1.29439e+02
I1111 19:02:50.000584 140264174335808 run_lib.py:153] step: 386950, training_loss: 1.20325e+02
I1111 19:02:59.973423 140264174335808 run_lib.py:153] step: 387000, training_loss: 1.11944e+02
I1111 19:03:09.412984 140264174335808 run_lib.py:153] step: 387050, training_loss: 1.10998e+02
I1111 19:03:19.043599 140264174335808 run_lib.py:153] step: 387100, training_loss: 1.29793e+02
I1111 19:03:29.129738 140264174335808 run_lib.py:153] step: 387150, training_loss: 1.38567e+02
I1111 19:03:39.250740 140264174335808 run_lib.py:153] step: 387200, training_loss: 1.20795e+02
I1111 19:03:49.377440 140264174335808 run_lib.py:153] step: 387250, training_loss: 1.14890e+02
I1111 19:03:59.977670 140264174335808 run_lib.py:153] step: 387300, training_loss: 1.33437e+02
I1111 19:04:10.203018 140264174335808 run_lib.py:153] step: 387350, training_loss: 1.32708e+02
I1111 19:04:20.188445 140264174335808 run_lib.py:153] step: 387400, training_loss: 9.63604e+01
I1111 19:04:30.510657 140264174335808 run_lib.py:153] step: 387450, training_loss: 1.25759e+02
I1111 19:04:40.766751 140264174335808 run_lib.py:153] step: 387500, training_loss: 1.40358e+02
I1111 19:04:50.688027 140264174335808 run_lib.py:153] step: 387550, training_loss: 1.60306e+02
I1111 19:05:00.098206 140264174335808 run_lib.py:153] step: 387600, training_loss: 1.24394e+02
I1111 19:05:10.446795 140264174335808 run_lib.py:153] step: 387650, training_loss: 1.24934e+02
I1111 19:05:20.930364 140264174335808 run_lib.py:153] step: 387700, training_loss: 1.00838e+02
I1111 19:05:31.713114 140264174335808 run_lib.py:153] step: 387750, training_loss: 1.15349e+02
I1111 19:05:41.925378 140264174335808 run_lib.py:153] step: 387800, training_loss: 1.22167e+02
I1111 19:05:52.269093 140264174335808 run_lib.py:153] step: 387850, training_loss: 1.08248e+02
I1111 19:06:01.842116 140264174335808 run_lib.py:153] step: 387900, training_loss: 1.41550e+02
I1111 19:06:11.712610 140264174335808 run_lib.py:153] step: 387950, training_loss: 1.47648e+02
I1111 19:06:21.654766 140264174335808 run_lib.py:153] step: 388000, training_loss: 1.13365e+02
I1111 19:06:32.101943 140264174335808 run_lib.py:153] step: 388050, training_loss: 1.52542e+02
I1111 19:06:42.674501 140264174335808 run_lib.py:153] step: 388100, training_loss: 1.37120e+02
I1111 19:06:53.424399 140264174335808 run_lib.py:153] step: 388150, training_loss: 1.39824e+02
I1111 19:07:02.682821 140264174335808 run_lib.py:153] step: 388200, training_loss: 1.21382e+02
I1111 19:07:13.629397 140264174335808 run_lib.py:153] step: 388250, training_loss: 1.09165e+02
I1111 19:07:23.563247 140264174335808 run_lib.py:153] step: 388300, training_loss: 1.31864e+02
I1111 19:07:34.358426 140264174335808 run_lib.py:153] step: 388350, training_loss: 1.35100e+02
I1111 19:07:44.881432 140264174335808 run_lib.py:153] step: 388400, training_loss: 1.11334e+02
I1111 19:07:54.989005 140264174335808 run_lib.py:153] step: 388450, training_loss: 1.08435e+02
I1111 19:08:04.361337 140264174335808 run_lib.py:153] step: 388500, training_loss: 1.31080e+02
I1111 19:08:13.872138 140264174335808 run_lib.py:153] step: 388550, training_loss: 1.26895e+02
I1111 19:08:24.463935 140264174335808 run_lib.py:153] step: 388600, training_loss: 1.18877e+02
I1111 19:08:34.049133 140264174335808 run_lib.py:153] step: 388650, training_loss: 1.03002e+02
I1111 19:08:44.337568 140264174335808 run_lib.py:153] step: 388700, training_loss: 1.12237e+02
I1111 19:08:54.365589 140264174335808 run_lib.py:153] step: 388750, training_loss: 1.20585e+02
I1111 19:09:04.871629 140264174335808 run_lib.py:153] step: 388800, training_loss: 1.61891e+02
I1111 19:09:14.634665 140264174335808 run_lib.py:153] step: 388850, training_loss: 1.35563e+02
I1111 19:09:24.835204 140264174335808 run_lib.py:153] step: 388900, training_loss: 1.14908e+02
I1111 19:09:34.771047 140264174335808 run_lib.py:153] step: 388950, training_loss: 1.01877e+02
I1111 19:09:45.793492 140264174335808 run_lib.py:153] step: 389000, training_loss: 1.31108e+02
I1111 19:09:55.238421 140264174335808 run_lib.py:153] step: 389050, training_loss: 1.20080e+02
I1111 19:10:05.047253 140264174335808 run_lib.py:153] step: 389100, training_loss: 1.40412e+02
I1111 19:10:15.149492 140264174335808 run_lib.py:153] step: 389150, training_loss: 1.36031e+02
I1111 19:10:24.872800 140264174335808 run_lib.py:153] step: 389200, training_loss: 1.51875e+02
I1111 19:10:34.808355 140264174335808 run_lib.py:153] step: 389250, training_loss: 1.24477e+02
I1111 19:10:44.826322 140264174335808 run_lib.py:153] step: 389300, training_loss: 1.60223e+02
I1111 19:10:55.394413 140264174335808 run_lib.py:153] step: 389350, training_loss: 1.23133e+02
I1111 19:11:05.839493 140264174335808 run_lib.py:153] step: 389400, training_loss: 1.41601e+02
I1111 19:11:15.661302 140264174335808 run_lib.py:153] step: 389450, training_loss: 1.07624e+02
I1111 19:11:25.827368 140264174335808 run_lib.py:153] step: 389500, training_loss: 1.08183e+02
I1111 19:11:35.270658 140264174335808 run_lib.py:153] step: 389550, training_loss: 1.26854e+02
I1111 19:11:44.785130 140264174335808 run_lib.py:153] step: 389600, training_loss: 1.15034e+02
I1111 19:11:55.236413 140264174335808 run_lib.py:153] step: 389650, training_loss: 1.23411e+02
I1111 19:12:05.450441 140264174335808 run_lib.py:153] step: 389700, training_loss: 1.34994e+02
I1111 19:12:15.922805 140264174335808 run_lib.py:153] step: 389750, training_loss: 1.24209e+02
I1111 19:12:25.932811 140264174335808 run_lib.py:153] step: 389800, training_loss: 1.22892e+02
I1111 19:12:36.622986 140264174335808 run_lib.py:153] step: 389850, training_loss: 1.16771e+02
I1111 19:12:46.766921 140264174335808 run_lib.py:153] step: 389900, training_loss: 1.21736e+02
I1111 19:12:56.469020 140264174335808 run_lib.py:153] step: 389950, training_loss: 1.16479e+02
I1111 19:13:06.608061 140264174335808 run_lib.py:153] step: 390000, training_loss: 1.30917e+02
I1111 19:13:07.187294 140264174335808 run_lib.py:166] step: 390000, eval_loss: 1.26729e+02
I1111 19:13:16.930143 140264174335808 run_lib.py:153] step: 390050, training_loss: 9.06973e+01
I1111 19:13:26.587304 140264174335808 run_lib.py:153] step: 390100, training_loss: 1.08648e+02
I1111 19:13:36.386404 140264174335808 run_lib.py:153] step: 390150, training_loss: 1.20511e+02
I1111 19:13:46.064022 140264174335808 run_lib.py:153] step: 390200, training_loss: 1.10609e+02
I1111 19:13:56.420770 140264174335808 run_lib.py:153] step: 390250, training_loss: 9.08858e+01
I1111 19:14:06.163306 140264174335808 run_lib.py:153] step: 390300, training_loss: 1.08602e+02
I1111 19:14:16.440142 140264174335808 run_lib.py:153] step: 390350, training_loss: 1.38665e+02
I1111 19:14:26.552094 140264174335808 run_lib.py:153] step: 390400, training_loss: 1.26580e+02
I1111 19:14:36.877730 140264174335808 run_lib.py:153] step: 390450, training_loss: 1.41757e+02
I1111 19:14:47.294586 140264174335808 run_lib.py:153] step: 390500, training_loss: 1.41289e+02
I1111 19:14:57.440597 140264174335808 run_lib.py:153] step: 390550, training_loss: 1.17079e+02
I1111 19:15:07.544026 140264174335808 run_lib.py:153] step: 390600, training_loss: 1.41914e+02
I1111 19:15:17.375357 140264174335808 run_lib.py:153] step: 390650, training_loss: 1.40375e+02
I1111 19:15:27.070424 140264174335808 run_lib.py:153] step: 390700, training_loss: 1.20397e+02
I1111 19:15:36.945142 140264174335808 run_lib.py:153] step: 390750, training_loss: 1.67152e+02
I1111 19:15:46.545291 140264174335808 run_lib.py:153] step: 390800, training_loss: 1.27713e+02
I1111 19:15:56.137748 140264174335808 run_lib.py:153] step: 390850, training_loss: 1.17172e+02
I1111 19:16:06.793531 140264174335808 run_lib.py:153] step: 390900, training_loss: 1.00880e+02
I1111 19:16:17.740156 140264174335808 run_lib.py:153] step: 390950, training_loss: 1.23812e+02
I1111 19:16:27.980992 140264174335808 run_lib.py:153] step: 391000, training_loss: 1.24809e+02
I1111 19:16:38.206689 140264174335808 run_lib.py:153] step: 391050, training_loss: 1.28844e+02
I1111 19:16:48.721195 140264174335808 run_lib.py:153] step: 391100, training_loss: 1.17491e+02
I1111 19:16:58.558481 140264174335808 run_lib.py:153] step: 391150, training_loss: 1.23289e+02
I1111 19:17:08.424332 140264174335808 run_lib.py:153] step: 391200, training_loss: 1.17802e+02
I1111 19:17:18.523849 140264174335808 run_lib.py:153] step: 391250, training_loss: 1.40150e+02
I1111 19:17:28.327828 140264174335808 run_lib.py:153] step: 391300, training_loss: 1.21565e+02
I1111 19:17:38.804905 140264174335808 run_lib.py:153] step: 391350, training_loss: 1.32071e+02
I1111 19:17:49.221625 140264174335808 run_lib.py:153] step: 391400, training_loss: 1.36760e+02
I1111 19:17:58.966490 140264174335808 run_lib.py:153] step: 391450, training_loss: 1.02402e+02
I1111 19:18:08.222018 140264174335808 run_lib.py:153] step: 391500, training_loss: 1.47563e+02
I1111 19:18:17.591602 140264174335808 run_lib.py:153] step: 391550, training_loss: 1.25461e+02
I1111 19:18:27.027002 140264174335808 run_lib.py:153] step: 391600, training_loss: 1.38804e+02
I1111 19:18:36.836535 140264174335808 run_lib.py:153] step: 391650, training_loss: 1.22896e+02
I1111 19:18:46.977682 140264174335808 run_lib.py:153] step: 391700, training_loss: 1.15015e+02
I1111 19:18:57.156715 140264174335808 run_lib.py:153] step: 391750, training_loss: 1.28438e+02
I1111 19:19:07.030473 140264174335808 run_lib.py:153] step: 391800, training_loss: 1.58067e+02
I1111 19:19:17.217110 140264174335808 run_lib.py:153] step: 391850, training_loss: 1.11875e+02
I1111 19:19:27.342259 140264174335808 run_lib.py:153] step: 391900, training_loss: 1.33490e+02
I1111 19:19:37.832386 140264174335808 run_lib.py:153] step: 391950, training_loss: 1.09585e+02
I1111 19:19:47.517518 140264174335808 run_lib.py:153] step: 392000, training_loss: 1.32977e+02
I1111 19:19:57.507759 140264174335808 run_lib.py:153] step: 392050, training_loss: 1.45526e+02
I1111 19:20:07.332654 140264174335808 run_lib.py:153] step: 392100, training_loss: 1.28464e+02
I1111 19:20:17.326516 140264174335808 run_lib.py:153] step: 392150, training_loss: 1.73604e+02
I1111 19:20:28.096511 140264174335808 run_lib.py:153] step: 392200, training_loss: 1.29572e+02
I1111 19:20:38.496465 140264174335808 run_lib.py:153] step: 392250, training_loss: 1.19177e+02
I1111 19:20:48.542195 140264174335808 run_lib.py:153] step: 392300, training_loss: 1.31962e+02
I1111 19:20:59.456515 140264174335808 run_lib.py:153] step: 392350, training_loss: 1.23283e+02
I1111 19:21:09.847794 140264174335808 run_lib.py:153] step: 392400, training_loss: 1.36221e+02
I1111 19:21:19.982041 140264174335808 run_lib.py:153] step: 392450, training_loss: 1.47380e+02
I1111 19:21:29.844582 140264174335808 run_lib.py:153] step: 392500, training_loss: 1.37984e+02
I1111 19:21:39.522047 140264174335808 run_lib.py:153] step: 392550, training_loss: 1.19034e+02
I1111 19:21:49.352537 140264174335808 run_lib.py:153] step: 392600, training_loss: 1.19965e+02
I1111 19:21:59.319709 140264174335808 run_lib.py:153] step: 392650, training_loss: 1.09757e+02
I1111 19:22:08.885128 140264174335808 run_lib.py:153] step: 392700, training_loss: 1.14107e+02
I1111 19:22:19.071810 140264174335808 run_lib.py:153] step: 392750, training_loss: 1.25189e+02
I1111 19:22:29.118690 140264174335808 run_lib.py:153] step: 392800, training_loss: 1.37345e+02
I1111 19:22:38.885175 140264174335808 run_lib.py:153] step: 392850, training_loss: 1.12607e+02
I1111 19:22:48.862158 140264174335808 run_lib.py:153] step: 392900, training_loss: 9.54037e+01
I1111 19:22:58.887649 140264174335808 run_lib.py:153] step: 392950, training_loss: 1.12984e+02
I1111 19:23:09.169638 140264174335808 run_lib.py:153] step: 393000, training_loss: 1.55239e+02
I1111 19:23:19.258716 140264174335808 run_lib.py:153] step: 393050, training_loss: 1.19561e+02
I1111 19:23:28.793419 140264174335808 run_lib.py:153] step: 393100, training_loss: 1.19780e+02
I1111 19:23:38.140771 140264174335808 run_lib.py:153] step: 393150, training_loss: 1.43144e+02
I1111 19:23:47.605876 140264174335808 run_lib.py:153] step: 393200, training_loss: 1.41479e+02
I1111 19:23:57.437965 140264174335808 run_lib.py:153] step: 393250, training_loss: 1.37456e+02
I1111 19:24:07.100696 140264174335808 run_lib.py:153] step: 393300, training_loss: 1.13716e+02
I1111 19:24:16.945728 140264174335808 run_lib.py:153] step: 393350, training_loss: 1.56020e+02
I1111 19:24:26.382724 140264174335808 run_lib.py:153] step: 393400, training_loss: 1.01911e+02
I1111 19:24:36.127806 140264174335808 run_lib.py:153] step: 393450, training_loss: 1.06397e+02
I1111 19:24:46.459379 140264174335808 run_lib.py:153] step: 393500, training_loss: 1.26117e+02
I1111 19:24:57.218161 140264174335808 run_lib.py:153] step: 393550, training_loss: 1.34921e+02
I1111 19:25:07.332219 140264174335808 run_lib.py:153] step: 393600, training_loss: 1.51743e+02
I1111 19:25:17.402086 140264174335808 run_lib.py:153] step: 393650, training_loss: 1.65611e+02
I1111 19:25:27.179695 140264174335808 run_lib.py:153] step: 393700, training_loss: 1.34900e+02
I1111 19:25:37.058014 140264174335808 run_lib.py:153] step: 393750, training_loss: 1.35588e+02
I1111 19:25:47.676630 140264174335808 run_lib.py:153] step: 393800, training_loss: 1.26858e+02
I1111 19:25:57.978553 140264174335808 run_lib.py:153] step: 393850, training_loss: 1.33378e+02
I1111 19:26:08.409215 140264174335808 run_lib.py:153] step: 393900, training_loss: 1.19209e+02
I1111 19:26:18.011573 140264174335808 run_lib.py:153] step: 393950, training_loss: 1.40134e+02
I1111 19:26:28.171989 140264174335808 run_lib.py:153] step: 394000, training_loss: 1.32753e+02
I1111 19:26:38.282933 140264174335808 run_lib.py:153] step: 394050, training_loss: 1.39245e+02
I1111 19:26:48.430214 140264174335808 run_lib.py:153] step: 394100, training_loss: 1.32469e+02
I1111 19:26:58.151044 140264174335808 run_lib.py:153] step: 394150, training_loss: 1.36187e+02
I1111 19:27:08.327595 140264174335808 run_lib.py:153] step: 394200, training_loss: 1.22633e+02
I1111 19:27:18.856048 140264174335808 run_lib.py:153] step: 394250, training_loss: 1.34841e+02
I1111 19:27:29.206471 140264174335808 run_lib.py:153] step: 394300, training_loss: 1.42381e+02
I1111 19:27:38.912686 140264174335808 run_lib.py:153] step: 394350, training_loss: 1.09363e+02
I1111 19:27:48.874852 140264174335808 run_lib.py:153] step: 394400, training_loss: 1.33890e+02
I1111 19:27:58.440720 140264174335808 run_lib.py:153] step: 394450, training_loss: 1.13478e+02
I1111 19:28:08.041796 140264174335808 run_lib.py:153] step: 394500, training_loss: 1.20463e+02
I1111 19:28:17.880643 140264174335808 run_lib.py:153] step: 394550, training_loss: 1.43334e+02
I1111 19:28:27.341188 140264174335808 run_lib.py:153] step: 394600, training_loss: 1.31950e+02
I1111 19:28:37.853882 140264174335808 run_lib.py:153] step: 394650, training_loss: 1.21422e+02
I1111 19:28:48.004291 140264174335808 run_lib.py:153] step: 394700, training_loss: 1.41404e+02
I1111 19:28:58.379411 140264174335808 run_lib.py:153] step: 394750, training_loss: 1.06061e+02
I1111 19:29:08.992415 140264174335808 run_lib.py:153] step: 394800, training_loss: 9.12570e+01
I1111 19:29:19.338782 140264174335808 run_lib.py:153] step: 394850, training_loss: 1.46605e+02
I1111 19:29:29.050633 140264174335808 run_lib.py:153] step: 394900, training_loss: 1.06560e+02
I1111 19:29:39.309604 140264174335808 run_lib.py:153] step: 394950, training_loss: 1.55083e+02
I1111 19:29:49.018380 140264174335808 run_lib.py:153] step: 395000, training_loss: 1.57251e+02
I1111 19:29:49.119812 140264174335808 run_lib.py:166] step: 395000, eval_loss: 1.26794e+02
I1111 19:29:59.317088 140264174335808 run_lib.py:153] step: 395050, training_loss: 1.00158e+02
I1111 19:30:09.596411 140264174335808 run_lib.py:153] step: 395100, training_loss: 1.05763e+02
I1111 19:30:19.730584 140264174335808 run_lib.py:153] step: 395150, training_loss: 1.32532e+02
I1111 19:30:30.093259 140264174335808 run_lib.py:153] step: 395200, training_loss: 1.26790e+02
I1111 19:30:40.404912 140264174335808 run_lib.py:153] step: 395250, training_loss: 1.10040e+02
I1111 19:30:50.692574 140264174335808 run_lib.py:153] step: 395300, training_loss: 1.10792e+02
I1111 19:31:00.824219 140264174335808 run_lib.py:153] step: 395350, training_loss: 1.66833e+02
I1111 19:31:10.466902 140264174335808 run_lib.py:153] step: 395400, training_loss: 1.09695e+02
I1111 19:31:20.375051 140264174335808 run_lib.py:153] step: 395450, training_loss: 1.48580e+02
I1111 19:31:31.451152 140264174335808 run_lib.py:153] step: 395500, training_loss: 1.16852e+02
I1111 19:31:41.433553 140264174335808 run_lib.py:153] step: 395550, training_loss: 1.08227e+02
I1111 19:31:50.805087 140264174335808 run_lib.py:153] step: 395600, training_loss: 1.46755e+02
I1111 19:32:00.764517 140264174335808 run_lib.py:153] step: 395650, training_loss: 1.29495e+02
I1111 19:32:10.679923 140264174335808 run_lib.py:153] step: 395700, training_loss: 1.30957e+02
I1111 19:32:20.863205 140264174335808 run_lib.py:153] step: 395750, training_loss: 1.09371e+02
I1111 19:32:31.434457 140264174335808 run_lib.py:153] step: 395800, training_loss: 1.50440e+02
I1111 19:32:41.376522 140264174335808 run_lib.py:153] step: 395850, training_loss: 1.26315e+02
I1111 19:32:51.783310 140264174335808 run_lib.py:153] step: 395900, training_loss: 1.06243e+02
I1111 19:33:01.853777 140264174335808 run_lib.py:153] step: 395950, training_loss: 1.45775e+02
I1111 19:33:11.345977 140264174335808 run_lib.py:153] step: 396000, training_loss: 1.38300e+02
I1111 19:33:20.957250 140264174335808 run_lib.py:153] step: 396050, training_loss: 1.12388e+02
I1111 19:33:31.023543 140264174335808 run_lib.py:153] step: 396100, training_loss: 1.18023e+02
I1111 19:33:40.622111 140264174335808 run_lib.py:153] step: 396150, training_loss: 1.40810e+02
I1111 19:33:50.603884 140264174335808 run_lib.py:153] step: 396200, training_loss: 1.20188e+02
I1111 19:34:00.719626 140264174335808 run_lib.py:153] step: 396250, training_loss: 1.20693e+02
I1111 19:34:10.345811 140264174335808 run_lib.py:153] step: 396300, training_loss: 1.29324e+02
I1111 19:34:20.191294 140264174335808 run_lib.py:153] step: 396350, training_loss: 1.19012e+02
I1111 19:34:29.531116 140264174335808 run_lib.py:153] step: 396400, training_loss: 1.35030e+02
I1111 19:34:39.341803 140264174335808 run_lib.py:153] step: 396450, training_loss: 1.78194e+02
I1111 19:34:48.541227 140264174335808 run_lib.py:153] step: 396500, training_loss: 1.24796e+02
I1111 19:34:58.727835 140264174335808 run_lib.py:153] step: 396550, training_loss: 1.18399e+02
I1111 19:35:08.967892 140264174335808 run_lib.py:153] step: 396600, training_loss: 1.73368e+02
I1111 19:35:19.157810 140264174335808 run_lib.py:153] step: 396650, training_loss: 1.21151e+02
I1111 19:35:28.712532 140264174335808 run_lib.py:153] step: 396700, training_loss: 1.16241e+02
I1111 19:35:39.013357 140264174335808 run_lib.py:153] step: 396750, training_loss: 1.35565e+02
I1111 19:35:48.939122 140264174335808 run_lib.py:153] step: 396800, training_loss: 9.45378e+01
I1111 19:35:58.512101 140264174335808 run_lib.py:153] step: 396850, training_loss: 1.52189e+02
I1111 19:36:08.099175 140264174335808 run_lib.py:153] step: 396900, training_loss: 1.40979e+02
I1111 19:36:18.027504 140264174335808 run_lib.py:153] step: 396950, training_loss: 1.15068e+02
I1111 19:36:27.677976 140264174335808 run_lib.py:153] step: 397000, training_loss: 1.34836e+02
I1111 19:36:38.423637 140264174335808 run_lib.py:153] step: 397050, training_loss: 1.36599e+02
I1111 19:36:48.745550 140264174335808 run_lib.py:153] step: 397100, training_loss: 1.65213e+02
I1111 19:36:59.435674 140264174335808 run_lib.py:153] step: 397150, training_loss: 1.03509e+02
I1111 19:37:09.643472 140264174335808 run_lib.py:153] step: 397200, training_loss: 1.14064e+02
I1111 19:37:19.931182 140264174335808 run_lib.py:153] step: 397250, training_loss: 1.14411e+02
I1111 19:37:30.571775 140264174335808 run_lib.py:153] step: 397300, training_loss: 1.37229e+02
I1111 19:37:41.130146 140264174335808 run_lib.py:153] step: 397350, training_loss: 1.01927e+02
I1111 19:37:50.510050 140264174335808 run_lib.py:153] step: 397400, training_loss: 1.00745e+02
I1111 19:38:00.137226 140264174335808 run_lib.py:153] step: 397450, training_loss: 1.17718e+02
I1111 19:38:10.018830 140264174335808 run_lib.py:153] step: 397500, training_loss: 1.43116e+02
I1111 19:38:21.092744 140264174335808 run_lib.py:153] step: 397550, training_loss: 1.29488e+02
I1111 19:38:31.879896 140264174335808 run_lib.py:153] step: 397600, training_loss: 1.06314e+02
I1111 19:38:42.514431 140264174335808 run_lib.py:153] step: 397650, training_loss: 1.36010e+02
I1111 19:38:52.856863 140264174335808 run_lib.py:153] step: 397700, training_loss: 1.28557e+02
I1111 19:39:02.592373 140264174335808 run_lib.py:153] step: 397750, training_loss: 1.44793e+02
I1111 19:39:12.326394 140264174335808 run_lib.py:153] step: 397800, training_loss: 1.18568e+02
I1111 19:39:21.675949 140264174335808 run_lib.py:153] step: 397850, training_loss: 1.55467e+02
I1111 19:39:31.331218 140264174335808 run_lib.py:153] step: 397900, training_loss: 1.13652e+02
I1111 19:39:41.489284 140264174335808 run_lib.py:153] step: 397950, training_loss: 1.25485e+02
I1111 19:39:51.538123 140264174335808 run_lib.py:153] step: 398000, training_loss: 9.77909e+01
I1111 19:40:01.176841 140264174335808 run_lib.py:153] step: 398050, training_loss: 1.11692e+02
I1111 19:40:11.129570 140264174335808 run_lib.py:153] step: 398100, training_loss: 1.59293e+02
I1111 19:40:20.866308 140264174335808 run_lib.py:153] step: 398150, training_loss: 1.29390e+02
I1111 19:40:30.346273 140264174335808 run_lib.py:153] step: 398200, training_loss: 1.14197e+02
I1111 19:40:40.464364 140264174335808 run_lib.py:153] step: 398250, training_loss: 1.43293e+02
I1111 19:40:50.263200 140264174335808 run_lib.py:153] step: 398300, training_loss: 1.12777e+02
I1111 19:41:00.049177 140264174335808 run_lib.py:153] step: 398350, training_loss: 1.05329e+02
I1111 19:41:09.900166 140264174335808 run_lib.py:153] step: 398400, training_loss: 1.32782e+02
I1111 19:41:19.255215 140264174335808 run_lib.py:153] step: 398450, training_loss: 1.41941e+02
I1111 19:41:28.571832 140264174335808 run_lib.py:153] step: 398500, training_loss: 1.23917e+02
I1111 19:41:39.255194 140264174335808 run_lib.py:153] step: 398550, training_loss: 1.18925e+02
I1111 19:41:48.979685 140264174335808 run_lib.py:153] step: 398600, training_loss: 1.13246e+02
I1111 19:41:59.157331 140264174335808 run_lib.py:153] step: 398650, training_loss: 1.29701e+02
I1111 19:42:08.853818 140264174335808 run_lib.py:153] step: 398700, training_loss: 1.39006e+02
I1111 19:42:19.110360 140264174335808 run_lib.py:153] step: 398750, training_loss: 1.50700e+02
I1111 19:42:29.477654 140264174335808 run_lib.py:153] step: 398800, training_loss: 1.19800e+02
I1111 19:42:40.205585 140264174335808 run_lib.py:153] step: 398850, training_loss: 1.05939e+02
I1111 19:42:50.222023 140264174335808 run_lib.py:153] step: 398900, training_loss: 1.15482e+02
I1111 19:42:59.731435 140264174335808 run_lib.py:153] step: 398950, training_loss: 1.53591e+02
I1111 19:43:09.497168 140264174335808 run_lib.py:153] step: 399000, training_loss: 1.34101e+02
I1111 19:43:18.990866 140264174335808 run_lib.py:153] step: 399050, training_loss: 1.59047e+02
I1111 19:43:29.331614 140264174335808 run_lib.py:153] step: 399100, training_loss: 1.43569e+02
I1111 19:43:39.227444 140264174335808 run_lib.py:153] step: 399150, training_loss: 1.08269e+02
I1111 19:43:49.083519 140264174335808 run_lib.py:153] step: 399200, training_loss: 1.46361e+02
I1111 19:43:58.762876 140264174335808 run_lib.py:153] step: 399250, training_loss: 1.13101e+02
I1111 19:44:08.510794 140264174335808 run_lib.py:153] step: 399300, training_loss: 1.20264e+02
I1111 19:44:18.570569 140264174335808 run_lib.py:153] step: 399350, training_loss: 1.19557e+02
I1111 19:44:28.198342 140264174335808 run_lib.py:153] step: 399400, training_loss: 1.46389e+02
I1111 19:44:37.881652 140264174335808 run_lib.py:153] step: 399450, training_loss: 1.44834e+02
I1111 19:44:47.648281 140264174335808 run_lib.py:153] step: 399500, training_loss: 9.89416e+01
I1111 19:44:57.061761 140264174335808 run_lib.py:153] step: 399550, training_loss: 1.25339e+02
I1111 19:45:07.197707 140264174335808 run_lib.py:153] step: 399600, training_loss: 1.19119e+02
I1111 19:45:17.048424 140264174335808 run_lib.py:153] step: 399650, training_loss: 1.31750e+02
I1111 19:45:27.245177 140264174335808 run_lib.py:153] step: 399700, training_loss: 1.33111e+02
I1111 19:45:37.170139 140264174335808 run_lib.py:153] step: 399750, training_loss: 1.20973e+02
I1111 19:45:46.838009 140264174335808 run_lib.py:153] step: 399800, training_loss: 1.19885e+02
I1111 19:45:56.118815 140264174335808 run_lib.py:153] step: 399850, training_loss: 1.47793e+02
I1111 19:46:05.920705 140264174335808 run_lib.py:153] step: 399900, training_loss: 1.41568e+02
I1111 19:46:15.185974 140264174335808 run_lib.py:153] step: 399950, training_loss: 1.31135e+02
I1111 19:46:24.536324 140264174335808 run_lib.py:153] step: 400000, training_loss: 1.44706e+02
I1111 19:46:25.054344 140264174335808 run_lib.py:166] step: 400000, eval_loss: 1.04086e+02
I1111 19:46:34.883183 140264174335808 run_lib.py:153] step: 400050, training_loss: 1.36583e+02
I1111 19:46:44.573467 140264174335808 run_lib.py:153] step: 400100, training_loss: 1.35074e+02
I1111 19:46:53.978322 140264174335808 run_lib.py:153] step: 400150, training_loss: 1.07905e+02
I1111 19:47:03.355408 140264174335808 run_lib.py:153] step: 400200, training_loss: 1.32007e+02
I1111 19:47:12.800746 140264174335808 run_lib.py:153] step: 400250, training_loss: 1.48676e+02
I1111 19:47:22.450866 140264174335808 run_lib.py:153] step: 400300, training_loss: 1.19038e+02
I1111 19:47:31.736867 140264174335808 run_lib.py:153] step: 400350, training_loss: 1.04508e+02
I1111 19:47:41.597054 140264174335808 run_lib.py:153] step: 400400, training_loss: 1.13787e+02
I1111 19:47:51.352773 140264174335808 run_lib.py:153] step: 400450, training_loss: 1.24841e+02
I1111 19:48:01.224283 140264174335808 run_lib.py:153] step: 400500, training_loss: 1.08065e+02
I1111 19:48:11.852631 140264174335808 run_lib.py:153] step: 400550, training_loss: 1.18613e+02
I1111 19:48:21.387954 140264174335808 run_lib.py:153] step: 400600, training_loss: 1.01387e+02
I1111 19:48:31.203829 140264174335808 run_lib.py:153] step: 400650, training_loss: 1.36799e+02
I1111 19:48:40.752029 140264174335808 run_lib.py:153] step: 400700, training_loss: 1.22709e+02
I1111 19:48:50.205791 140264174335808 run_lib.py:153] step: 400750, training_loss: 1.46270e+02
I1111 19:49:00.238466 140264174335808 run_lib.py:153] step: 400800, training_loss: 1.49968e+02
I1111 19:49:10.009426 140264174335808 run_lib.py:153] step: 400850, training_loss: 1.57432e+02
I1111 19:49:19.523530 140264174335808 run_lib.py:153] step: 400900, training_loss: 1.20060e+02
I1111 19:49:28.769921 140264174335808 run_lib.py:153] step: 400950, training_loss: 1.25630e+02
I1111 19:49:38.902229 140264174335808 run_lib.py:153] step: 401000, training_loss: 1.31768e+02
I1111 19:49:48.334818 140264174335808 run_lib.py:153] step: 401050, training_loss: 9.81189e+01
I1111 19:49:58.197275 140264174335808 run_lib.py:153] step: 401100, training_loss: 1.71239e+02
I1111 19:50:07.719285 140264174335808 run_lib.py:153] step: 401150, training_loss: 1.30697e+02
I1111 19:50:17.425393 140264174335808 run_lib.py:153] step: 401200, training_loss: 1.18667e+02
I1111 19:50:27.720093 140264174335808 run_lib.py:153] step: 401250, training_loss: 1.29754e+02
I1111 19:50:37.486735 140264174335808 run_lib.py:153] step: 401300, training_loss: 1.49511e+02
I1111 19:50:47.100653 140264174335808 run_lib.py:153] step: 401350, training_loss: 1.43615e+02
I1111 19:50:56.688097 140264174335808 run_lib.py:153] step: 401400, training_loss: 1.09331e+02
I1111 19:51:06.884698 140264174335808 run_lib.py:153] step: 401450, training_loss: 1.65747e+02
I1111 19:51:17.314415 140264174335808 run_lib.py:153] step: 401500, training_loss: 1.47625e+02
I1111 19:51:27.165343 140264174335808 run_lib.py:153] step: 401550, training_loss: 1.59633e+02
I1111 19:51:36.899591 140264174335808 run_lib.py:153] step: 401600, training_loss: 1.26317e+02
I1111 19:51:46.136958 140264174335808 run_lib.py:153] step: 401650, training_loss: 1.30863e+02
I1111 19:51:55.958172 140264174335808 run_lib.py:153] step: 401700, training_loss: 1.23130e+02
I1111 19:52:05.871628 140264174335808 run_lib.py:153] step: 401750, training_loss: 1.44644e+02
I1111 19:52:15.946652 140264174335808 run_lib.py:153] step: 401800, training_loss: 1.42333e+02
I1111 19:52:26.145433 140264174335808 run_lib.py:153] step: 401850, training_loss: 1.41096e+02
I1111 19:52:35.630418 140264174335808 run_lib.py:153] step: 401900, training_loss: 1.68849e+02
I1111 19:52:45.440085 140264174335808 run_lib.py:153] step: 401950, training_loss: 1.11201e+02
I1111 19:52:54.967758 140264174335808 run_lib.py:153] step: 402000, training_loss: 1.42388e+02
I1111 19:53:04.523611 140264174335808 run_lib.py:153] step: 402050, training_loss: 1.33763e+02
I1111 19:53:13.984541 140264174335808 run_lib.py:153] step: 402100, training_loss: 1.39916e+02
I1111 19:53:23.785367 140264174335808 run_lib.py:153] step: 402150, training_loss: 1.11430e+02
I1111 19:53:33.779458 140264174335808 run_lib.py:153] step: 402200, training_loss: 1.19514e+02
I1111 19:53:43.398834 140264174335808 run_lib.py:153] step: 402250, training_loss: 1.17191e+02
I1111 19:53:53.053380 140264174335808 run_lib.py:153] step: 402300, training_loss: 1.15056e+02
I1111 19:54:02.633329 140264174335808 run_lib.py:153] step: 402350, training_loss: 1.23912e+02
I1111 19:54:12.425858 140264174335808 run_lib.py:153] step: 402400, training_loss: 1.18905e+02
I1111 19:54:21.917451 140264174335808 run_lib.py:153] step: 402450, training_loss: 1.26157e+02
I1111 19:54:31.275693 140264174335808 run_lib.py:153] step: 402500, training_loss: 1.38916e+02
I1111 19:54:41.297982 140264174335808 run_lib.py:153] step: 402550, training_loss: 1.33314e+02
I1111 19:54:50.994454 140264174335808 run_lib.py:153] step: 402600, training_loss: 9.82547e+01
I1111 19:55:01.386468 140264174335808 run_lib.py:153] step: 402650, training_loss: 1.21789e+02
I1111 19:55:11.497206 140264174335808 run_lib.py:153] step: 402700, training_loss: 1.16235e+02
I1111 19:55:21.513938 140264174335808 run_lib.py:153] step: 402750, training_loss: 1.50118e+02
I1111 19:55:32.050185 140264174335808 run_lib.py:153] step: 402800, training_loss: 1.51846e+02
I1111 19:55:42.709953 140264174335808 run_lib.py:153] step: 402850, training_loss: 1.23523e+02
I1111 19:55:52.678864 140264174335808 run_lib.py:153] step: 402900, training_loss: 1.38961e+02
I1111 19:56:02.643764 140264174335808 run_lib.py:153] step: 402950, training_loss: 1.26938e+02
I1111 19:56:12.341305 140264174335808 run_lib.py:153] step: 403000, training_loss: 1.38543e+02
I1111 19:56:21.848315 140264174335808 run_lib.py:153] step: 403050, training_loss: 1.27602e+02
I1111 19:56:32.037666 140264174335808 run_lib.py:153] step: 403100, training_loss: 1.48598e+02
I1111 19:56:41.362804 140264174335808 run_lib.py:153] step: 403150, training_loss: 1.14028e+02
I1111 19:56:50.843904 140264174335808 run_lib.py:153] step: 403200, training_loss: 1.33776e+02
I1111 19:57:00.100746 140264174335808 run_lib.py:153] step: 403250, training_loss: 1.22949e+02
I1111 19:57:09.590653 140264174335808 run_lib.py:153] step: 403300, training_loss: 1.23168e+02
I1111 19:57:19.057073 140264174335808 run_lib.py:153] step: 403350, training_loss: 1.45124e+02
I1111 19:57:28.810468 140264174335808 run_lib.py:153] step: 403400, training_loss: 1.22262e+02
I1111 19:57:38.966187 140264174335808 run_lib.py:153] step: 403450, training_loss: 1.65016e+02
I1111 19:57:48.698844 140264174335808 run_lib.py:153] step: 403500, training_loss: 1.38495e+02
I1111 19:57:58.629118 140264174335808 run_lib.py:153] step: 403550, training_loss: 1.22228e+02
I1111 19:58:09.026752 140264174335808 run_lib.py:153] step: 403600, training_loss: 1.27832e+02
I1111 19:58:18.776212 140264174335808 run_lib.py:153] step: 403650, training_loss: 1.46382e+02
I1111 19:58:28.929365 140264174335808 run_lib.py:153] step: 403700, training_loss: 1.21220e+02
I1111 19:58:39.205304 140264174335808 run_lib.py:153] step: 403750, training_loss: 9.69827e+01
I1111 19:58:48.469887 140264174335808 run_lib.py:153] step: 403800, training_loss: 1.42960e+02
I1111 19:58:58.014575 140264174335808 run_lib.py:153] step: 403850, training_loss: 1.32278e+02
I1111 19:59:08.142620 140264174335808 run_lib.py:153] step: 403900, training_loss: 1.06361e+02
I1111 19:59:19.100251 140264174335808 run_lib.py:153] step: 403950, training_loss: 1.25799e+02
I1111 19:59:29.462397 140264174335808 run_lib.py:153] step: 404000, training_loss: 1.29757e+02
I1111 19:59:39.685142 140264174335808 run_lib.py:153] step: 404050, training_loss: 1.24823e+02
I1111 19:59:49.647937 140264174335808 run_lib.py:153] step: 404100, training_loss: 1.26313e+02
I1111 20:00:00.311449 140264174335808 run_lib.py:153] step: 404150, training_loss: 1.32887e+02
I1111 20:00:10.434065 140264174335808 run_lib.py:153] step: 404200, training_loss: 1.34854e+02
I1111 20:00:21.278986 140264174335808 run_lib.py:153] step: 404250, training_loss: 1.32590e+02
I1111 20:00:31.784261 140264174335808 run_lib.py:153] step: 404300, training_loss: 1.36927e+02
I1111 20:00:42.704732 140264174335808 run_lib.py:153] step: 404350, training_loss: 1.07843e+02
I1111 20:00:52.673554 140264174335808 run_lib.py:153] step: 404400, training_loss: 1.34236e+02
I1111 20:01:02.202862 140264174335808 run_lib.py:153] step: 404450, training_loss: 1.35113e+02
I1111 20:01:11.837746 140264174335808 run_lib.py:153] step: 404500, training_loss: 1.12048e+02
I1111 20:01:21.540502 140264174335808 run_lib.py:153] step: 404550, training_loss: 1.08897e+02
I1111 20:01:30.845956 140264174335808 run_lib.py:153] step: 404600, training_loss: 1.30703e+02
I1111 20:01:41.321134 140264174335808 run_lib.py:153] step: 404650, training_loss: 1.25851e+02
I1111 20:01:52.035415 140264174335808 run_lib.py:153] step: 404700, training_loss: 1.09898e+02
I1111 20:02:02.152873 140264174335808 run_lib.py:153] step: 404750, training_loss: 1.23758e+02
I1111 20:02:12.044955 140264174335808 run_lib.py:153] step: 404800, training_loss: 1.22926e+02
I1111 20:02:21.646861 140264174335808 run_lib.py:153] step: 404850, training_loss: 9.89524e+01
I1111 20:02:31.207881 140264174335808 run_lib.py:153] step: 404900, training_loss: 1.31564e+02
I1111 20:02:41.661855 140264174335808 run_lib.py:153] step: 404950, training_loss: 1.41614e+02
I1111 20:02:51.979682 140264174335808 run_lib.py:153] step: 405000, training_loss: 1.12861e+02
I1111 20:02:52.116587 140264174335808 run_lib.py:166] step: 405000, eval_loss: 1.36421e+02
I1111 20:03:02.079069 140264174335808 run_lib.py:153] step: 405050, training_loss: 1.22874e+02
I1111 20:03:12.041921 140264174335808 run_lib.py:153] step: 405100, training_loss: 1.19969e+02
I1111 20:03:22.559767 140264174335808 run_lib.py:153] step: 405150, training_loss: 1.18040e+02
I1111 20:03:33.358839 140264174335808 run_lib.py:153] step: 405200, training_loss: 1.03801e+02
I1111 20:03:43.717057 140264174335808 run_lib.py:153] step: 405250, training_loss: 1.22791e+02
I1111 20:03:53.763833 140264174335808 run_lib.py:153] step: 405300, training_loss: 1.28848e+02
I1111 20:04:03.490075 140264174335808 run_lib.py:153] step: 405350, training_loss: 1.40558e+02
I1111 20:04:14.022664 140264174335808 run_lib.py:153] step: 405400, training_loss: 9.60139e+01
I1111 20:04:23.792468 140264174335808 run_lib.py:153] step: 405450, training_loss: 1.30287e+02
I1111 20:04:33.321650 140264174335808 run_lib.py:153] step: 405500, training_loss: 1.21154e+02
I1111 20:04:42.705172 140264174335808 run_lib.py:153] step: 405550, training_loss: 8.63158e+01
I1111 20:04:52.490787 140264174335808 run_lib.py:153] step: 405600, training_loss: 1.50479e+02
I1111 20:05:01.934033 140264174335808 run_lib.py:153] step: 405650, training_loss: 1.26909e+02
I1111 20:05:11.462024 140264174335808 run_lib.py:153] step: 405700, training_loss: 1.37406e+02
I1111 20:05:21.190751 140264174335808 run_lib.py:153] step: 405750, training_loss: 1.12260e+02
I1111 20:05:30.943021 140264174335808 run_lib.py:153] step: 405800, training_loss: 1.26263e+02
I1111 20:05:41.789451 140264174335808 run_lib.py:153] step: 405850, training_loss: 1.33054e+02
I1111 20:05:52.765780 140264174335808 run_lib.py:153] step: 405900, training_loss: 1.43369e+02
I1111 20:06:02.908089 140264174335808 run_lib.py:153] step: 405950, training_loss: 1.34517e+02
I1111 20:06:13.118321 140264174335808 run_lib.py:153] step: 406000, training_loss: 1.18095e+02
I1111 20:06:22.814722 140264174335808 run_lib.py:153] step: 406050, training_loss: 1.12790e+02
I1111 20:06:32.580170 140264174335808 run_lib.py:153] step: 406100, training_loss: 1.39250e+02
I1111 20:06:42.215890 140264174335808 run_lib.py:153] step: 406150, training_loss: 1.53138e+02
I1111 20:06:52.011002 140264174335808 run_lib.py:153] step: 406200, training_loss: 1.31589e+02
I1111 20:07:01.794201 140264174335808 run_lib.py:153] step: 406250, training_loss: 1.36060e+02
I1111 20:07:11.715614 140264174335808 run_lib.py:153] step: 406300, training_loss: 1.02592e+02
I1111 20:07:21.266985 140264174335808 run_lib.py:153] step: 406350, training_loss: 1.28423e+02
I1111 20:07:30.952995 140264174335808 run_lib.py:153] step: 406400, training_loss: 1.43595e+02
I1111 20:07:40.657075 140264174335808 run_lib.py:153] step: 406450, training_loss: 1.29654e+02
I1111 20:07:50.930851 140264174335808 run_lib.py:153] step: 406500, training_loss: 1.33717e+02
I1111 20:08:01.337768 140264174335808 run_lib.py:153] step: 406550, training_loss: 1.18426e+02
I1111 20:08:11.006036 140264174335808 run_lib.py:153] step: 406600, training_loss: 1.35452e+02
I1111 20:08:21.218415 140264174335808 run_lib.py:153] step: 406650, training_loss: 1.16440e+02
I1111 20:08:31.674742 140264174335808 run_lib.py:153] step: 406700, training_loss: 1.05328e+02
I1111 20:08:41.412957 140264174335808 run_lib.py:153] step: 406750, training_loss: 1.28652e+02
I1111 20:08:51.637978 140264174335808 run_lib.py:153] step: 406800, training_loss: 1.09798e+02
I1111 20:09:01.915546 140264174335808 run_lib.py:153] step: 406850, training_loss: 1.07556e+02
I1111 20:09:12.527492 140264174335808 run_lib.py:153] step: 406900, training_loss: 1.45503e+02
I1111 20:09:23.244335 140264174335808 run_lib.py:153] step: 406950, training_loss: 9.88295e+01
I1111 20:09:33.483621 140264174335808 run_lib.py:153] step: 407000, training_loss: 1.28613e+02
I1111 20:09:44.118484 140264174335808 run_lib.py:153] step: 407050, training_loss: 1.27226e+02
I1111 20:09:55.150484 140264174335808 run_lib.py:153] step: 407100, training_loss: 1.30559e+02
I1111 20:10:05.650245 140264174335808 run_lib.py:153] step: 407150, training_loss: 1.46460e+02
I1111 20:10:15.649631 140264174335808 run_lib.py:153] step: 407200, training_loss: 1.10629e+02
I1111 20:10:25.267928 140264174335808 run_lib.py:153] step: 407250, training_loss: 1.49118e+02
I1111 20:10:35.553637 140264174335808 run_lib.py:153] step: 407300, training_loss: 1.29401e+02
I1111 20:10:46.010053 140264174335808 run_lib.py:153] step: 407350, training_loss: 1.37454e+02
I1111 20:10:55.903909 140264174335808 run_lib.py:153] step: 407400, training_loss: 1.28771e+02
I1111 20:11:05.274845 140264174335808 run_lib.py:153] step: 407450, training_loss: 1.45445e+02
I1111 20:11:16.059308 140264174335808 run_lib.py:153] step: 407500, training_loss: 1.13173e+02
I1111 20:11:25.711030 140264174335808 run_lib.py:153] step: 407550, training_loss: 1.18071e+02
I1111 20:11:36.057611 140264174335808 run_lib.py:153] step: 407600, training_loss: 1.28216e+02
I1111 20:11:46.209629 140264174335808 run_lib.py:153] step: 407650, training_loss: 1.33565e+02
I1111 20:11:55.993428 140264174335808 run_lib.py:153] step: 407700, training_loss: 1.37661e+02
I1111 20:12:06.079874 140264174335808 run_lib.py:153] step: 407750, training_loss: 1.02480e+02
I1111 20:12:15.472033 140264174335808 run_lib.py:153] step: 407800, training_loss: 1.32493e+02
I1111 20:12:24.947461 140264174335808 run_lib.py:153] step: 407850, training_loss: 1.34458e+02
I1111 20:12:34.800621 140264174335808 run_lib.py:153] step: 407900, training_loss: 1.26072e+02
I1111 20:12:44.783147 140264174335808 run_lib.py:153] step: 407950, training_loss: 1.42486e+02
I1111 20:12:54.587181 140264174335808 run_lib.py:153] step: 408000, training_loss: 1.25023e+02
I1111 20:13:05.060827 140264174335808 run_lib.py:153] step: 408050, training_loss: 1.31750e+02
I1111 20:13:15.267785 140264174335808 run_lib.py:153] step: 408100, training_loss: 1.49619e+02
I1111 20:13:25.086848 140264174335808 run_lib.py:153] step: 408150, training_loss: 1.37701e+02
I1111 20:13:35.325059 140264174335808 run_lib.py:153] step: 408200, training_loss: 1.30058e+02
I1111 20:13:45.217915 140264174335808 run_lib.py:153] step: 408250, training_loss: 1.49398e+02
I1111 20:13:54.792370 140264174335808 run_lib.py:153] step: 408300, training_loss: 1.05560e+02
I1111 20:14:05.014874 140264174335808 run_lib.py:153] step: 408350, training_loss: 1.36889e+02
I1111 20:14:15.009065 140264174335808 run_lib.py:153] step: 408400, training_loss: 1.33244e+02
I1111 20:14:25.171145 140264174335808 run_lib.py:153] step: 408450, training_loss: 1.04890e+02
I1111 20:14:34.963583 140264174335808 run_lib.py:153] step: 408500, training_loss: 1.30046e+02
I1111 20:14:45.294684 140264174335808 run_lib.py:153] step: 408550, training_loss: 1.22513e+02
I1111 20:14:55.307227 140264174335808 run_lib.py:153] step: 408600, training_loss: 1.38810e+02
I1111 20:15:05.097014 140264174335808 run_lib.py:153] step: 408650, training_loss: 1.15845e+02
I1111 20:15:15.192700 140264174335808 run_lib.py:153] step: 408700, training_loss: 8.95406e+01
I1111 20:15:24.847897 140264174335808 run_lib.py:153] step: 408750, training_loss: 1.22299e+02
I1111 20:15:34.908038 140264174335808 run_lib.py:153] step: 408800, training_loss: 1.29511e+02
I1111 20:15:44.492182 140264174335808 run_lib.py:153] step: 408850, training_loss: 1.28378e+02
I1111 20:15:55.101699 140264174335808 run_lib.py:153] step: 408900, training_loss: 1.22010e+02
I1111 20:16:04.947612 140264174335808 run_lib.py:153] step: 408950, training_loss: 1.24803e+02
I1111 20:16:14.997812 140264174335808 run_lib.py:153] step: 409000, training_loss: 1.09909e+02
I1111 20:16:24.609196 140264174335808 run_lib.py:153] step: 409050, training_loss: 1.04652e+02
I1111 20:16:35.185211 140264174335808 run_lib.py:153] step: 409100, training_loss: 1.31671e+02
I1111 20:16:45.223084 140264174335808 run_lib.py:153] step: 409150, training_loss: 1.28025e+02
I1111 20:16:54.556845 140264174335808 run_lib.py:153] step: 409200, training_loss: 1.11746e+02
I1111 20:17:04.303089 140264174335808 run_lib.py:153] step: 409250, training_loss: 1.24356e+02
I1111 20:17:13.889551 140264174335808 run_lib.py:153] step: 409300, training_loss: 1.23505e+02
I1111 20:17:23.513575 140264174335808 run_lib.py:153] step: 409350, training_loss: 1.28015e+02
I1111 20:17:33.926470 140264174335808 run_lib.py:153] step: 409400, training_loss: 1.35014e+02
I1111 20:17:44.314149 140264174335808 run_lib.py:153] step: 409450, training_loss: 9.46968e+01
I1111 20:17:54.148299 140264174335808 run_lib.py:153] step: 409500, training_loss: 1.06477e+02
I1111 20:18:03.841299 140264174335808 run_lib.py:153] step: 409550, training_loss: 1.12788e+02
I1111 20:18:13.591185 140264174335808 run_lib.py:153] step: 409600, training_loss: 1.36079e+02
I1111 20:18:23.134147 140264174335808 run_lib.py:153] step: 409650, training_loss: 1.31026e+02
I1111 20:18:32.864602 140264174335808 run_lib.py:153] step: 409700, training_loss: 1.35851e+02
I1111 20:18:43.556773 140264174335808 run_lib.py:153] step: 409750, training_loss: 1.07218e+02
I1111 20:18:53.512910 140264174335808 run_lib.py:153] step: 409800, training_loss: 1.29028e+02
I1111 20:19:03.526301 140264174335808 run_lib.py:153] step: 409850, training_loss: 1.30495e+02
I1111 20:19:13.319740 140264174335808 run_lib.py:153] step: 409900, training_loss: 1.08989e+02
I1111 20:19:23.198524 140264174335808 run_lib.py:153] step: 409950, training_loss: 1.35222e+02
I1111 20:19:33.865952 140264174335808 run_lib.py:153] step: 410000, training_loss: 9.52779e+01
I1111 20:19:34.419651 140264174335808 run_lib.py:166] step: 410000, eval_loss: 1.31280e+02
I1111 20:19:44.574776 140264174335808 run_lib.py:153] step: 410050, training_loss: 9.75920e+01
I1111 20:19:54.943031 140264174335808 run_lib.py:153] step: 410100, training_loss: 1.39430e+02
I1111 20:20:04.588827 140264174335808 run_lib.py:153] step: 410150, training_loss: 1.47692e+02
I1111 20:20:14.405358 140264174335808 run_lib.py:153] step: 410200, training_loss: 1.16891e+02
I1111 20:20:24.542506 140264174335808 run_lib.py:153] step: 410250, training_loss: 1.27104e+02
I1111 20:20:34.813704 140264174335808 run_lib.py:153] step: 410300, training_loss: 1.37775e+02
I1111 20:20:45.299188 140264174335808 run_lib.py:153] step: 410350, training_loss: 1.11606e+02
I1111 20:20:55.707914 140264174335808 run_lib.py:153] step: 410400, training_loss: 1.27285e+02
I1111 20:21:06.059118 140264174335808 run_lib.py:153] step: 410450, training_loss: 1.18036e+02
I1111 20:21:15.725791 140264174335808 run_lib.py:153] step: 410500, training_loss: 1.29940e+02
I1111 20:21:26.497721 140264174335808 run_lib.py:153] step: 410550, training_loss: 1.46134e+02
I1111 20:21:36.564507 140264174335808 run_lib.py:153] step: 410600, training_loss: 1.25917e+02
I1111 20:21:47.501833 140264174335808 run_lib.py:153] step: 410650, training_loss: 1.68485e+02
I1111 20:21:58.161646 140264174335808 run_lib.py:153] step: 410700, training_loss: 1.23000e+02
I1111 20:22:07.950041 140264174335808 run_lib.py:153] step: 410750, training_loss: 1.21818e+02
I1111 20:22:17.815382 140264174335808 run_lib.py:153] step: 410800, training_loss: 1.19370e+02
I1111 20:22:27.666679 140264174335808 run_lib.py:153] step: 410850, training_loss: 9.63157e+01
I1111 20:22:37.524805 140264174335808 run_lib.py:153] step: 410900, training_loss: 9.82668e+01
I1111 20:22:47.451896 140264174335808 run_lib.py:153] step: 410950, training_loss: 1.22017e+02
I1111 20:22:56.902073 140264174335808 run_lib.py:153] step: 411000, training_loss: 1.13190e+02
I1111 20:23:06.625939 140264174335808 run_lib.py:153] step: 411050, training_loss: 1.22086e+02
I1111 20:23:16.560179 140264174335808 run_lib.py:153] step: 411100, training_loss: 1.38624e+02
I1111 20:23:26.522666 140264174335808 run_lib.py:153] step: 411150, training_loss: 1.15382e+02
I1111 20:23:36.735538 140264174335808 run_lib.py:153] step: 411200, training_loss: 1.22402e+02
I1111 20:23:46.342849 140264174335808 run_lib.py:153] step: 411250, training_loss: 1.19685e+02
I1111 20:23:56.296795 140264174335808 run_lib.py:153] step: 411300, training_loss: 1.11433e+02
I1111 20:24:06.288163 140264174335808 run_lib.py:153] step: 411350, training_loss: 1.47049e+02
I1111 20:24:16.441178 140264174335808 run_lib.py:153] step: 411400, training_loss: 1.13069e+02
I1111 20:24:26.640586 140264174335808 run_lib.py:153] step: 411450, training_loss: 1.28226e+02
I1111 20:24:36.852267 140264174335808 run_lib.py:153] step: 411500, training_loss: 1.57486e+02
I1111 20:24:46.252540 140264174335808 run_lib.py:153] step: 411550, training_loss: 1.24538e+02
I1111 20:24:56.027919 140264174335808 run_lib.py:153] step: 411600, training_loss: 1.40304e+02
I1111 20:25:05.747356 140264174335808 run_lib.py:153] step: 411650, training_loss: 1.17916e+02
I1111 20:25:15.107785 140264174335808 run_lib.py:153] step: 411700, training_loss: 1.53309e+02
I1111 20:25:24.918306 140264174335808 run_lib.py:153] step: 411750, training_loss: 1.07655e+02
I1111 20:25:34.807683 140264174335808 run_lib.py:153] step: 411800, training_loss: 1.14047e+02
I1111 20:25:44.776059 140264174335808 run_lib.py:153] step: 411850, training_loss: 1.25845e+02
I1111 20:25:54.590000 140264174335808 run_lib.py:153] step: 411900, training_loss: 1.36862e+02
I1111 20:26:04.973325 140264174335808 run_lib.py:153] step: 411950, training_loss: 1.18267e+02
I1111 20:26:15.147021 140264174335808 run_lib.py:153] step: 412000, training_loss: 1.36169e+02
I1111 20:26:24.856571 140264174335808 run_lib.py:153] step: 412050, training_loss: 1.15997e+02
I1111 20:26:34.189425 140264174335808 run_lib.py:153] step: 412100, training_loss: 1.37760e+02
I1111 20:26:44.961559 140264174335808 run_lib.py:153] step: 412150, training_loss: 1.41636e+02
I1111 20:26:55.379243 140264174335808 run_lib.py:153] step: 412200, training_loss: 1.35984e+02
I1111 20:27:05.738607 140264174335808 run_lib.py:153] step: 412250, training_loss: 1.48055e+02
I1111 20:27:15.806055 140264174335808 run_lib.py:153] step: 412300, training_loss: 1.37200e+02
I1111 20:27:25.408823 140264174335808 run_lib.py:153] step: 412350, training_loss: 1.53948e+02
I1111 20:27:34.935707 140264174335808 run_lib.py:153] step: 412400, training_loss: 1.30941e+02
I1111 20:27:44.889410 140264174335808 run_lib.py:153] step: 412450, training_loss: 1.03840e+02
I1111 20:27:54.972905 140264174335808 run_lib.py:153] step: 412500, training_loss: 1.36085e+02
I1111 20:28:05.054256 140264174335808 run_lib.py:153] step: 412550, training_loss: 9.76023e+01
I1111 20:28:14.841967 140264174335808 run_lib.py:153] step: 412600, training_loss: 1.35001e+02
I1111 20:28:25.378557 140264174335808 run_lib.py:153] step: 412650, training_loss: 1.47275e+02
I1111 20:28:35.210498 140264174335808 run_lib.py:153] step: 412700, training_loss: 1.40731e+02
I1111 20:28:44.901511 140264174335808 run_lib.py:153] step: 412750, training_loss: 1.28110e+02
I1111 20:28:54.685023 140264174335808 run_lib.py:153] step: 412800, training_loss: 1.07755e+02
I1111 20:29:04.974507 140264174335808 run_lib.py:153] step: 412850, training_loss: 1.21632e+02
I1111 20:29:14.946643 140264174335808 run_lib.py:153] step: 412900, training_loss: 1.15527e+02
I1111 20:29:25.815212 140264174335808 run_lib.py:153] step: 412950, training_loss: 1.24156e+02
I1111 20:29:35.187164 140264174335808 run_lib.py:153] step: 413000, training_loss: 1.33317e+02
I1111 20:29:44.677982 140264174335808 run_lib.py:153] step: 413050, training_loss: 1.51318e+02
I1111 20:29:54.472081 140264174335808 run_lib.py:153] step: 413100, training_loss: 1.74199e+02
I1111 20:30:04.596263 140264174335808 run_lib.py:153] step: 413150, training_loss: 1.22877e+02
I1111 20:30:15.216834 140264174335808 run_lib.py:153] step: 413200, training_loss: 1.37712e+02
I1111 20:30:25.253812 140264174335808 run_lib.py:153] step: 413250, training_loss: 1.24432e+02
I1111 20:30:36.071070 140264174335808 run_lib.py:153] step: 413300, training_loss: 1.11428e+02
I1111 20:30:46.568279 140264174335808 run_lib.py:153] step: 413350, training_loss: 1.25686e+02
I1111 20:30:56.407323 140264174335808 run_lib.py:153] step: 413400, training_loss: 1.10995e+02
I1111 20:31:06.487100 140264174335808 run_lib.py:153] step: 413450, training_loss: 1.24700e+02
I1111 20:31:16.354084 140264174335808 run_lib.py:153] step: 413500, training_loss: 1.20234e+02
I1111 20:31:26.142290 140264174335808 run_lib.py:153] step: 413550, training_loss: 1.47179e+02
I1111 20:31:36.977888 140264174335808 run_lib.py:153] step: 413600, training_loss: 1.19198e+02
I1111 20:31:46.666622 140264174335808 run_lib.py:153] step: 413650, training_loss: 1.36196e+02
I1111 20:31:56.735906 140264174335808 run_lib.py:153] step: 413700, training_loss: 1.05493e+02
I1111 20:32:06.873950 140264174335808 run_lib.py:153] step: 413750, training_loss: 1.33192e+02
I1111 20:32:17.118163 140264174335808 run_lib.py:153] step: 413800, training_loss: 1.34732e+02
I1111 20:32:27.159392 140264174335808 run_lib.py:153] step: 413850, training_loss: 1.51508e+02
I1111 20:32:37.693782 140264174335808 run_lib.py:153] step: 413900, training_loss: 1.19330e+02
I1111 20:32:47.987544 140264174335808 run_lib.py:153] step: 413950, training_loss: 1.03468e+02
I1111 20:32:58.272277 140264174335808 run_lib.py:153] step: 414000, training_loss: 1.24411e+02
I1111 20:33:08.061000 140264174335808 run_lib.py:153] step: 414050, training_loss: 1.18356e+02
I1111 20:33:17.969223 140264174335808 run_lib.py:153] step: 414100, training_loss: 1.11945e+02
I1111 20:33:27.552688 140264174335808 run_lib.py:153] step: 414150, training_loss: 1.63560e+02
I1111 20:33:38.195923 140264174335808 run_lib.py:153] step: 414200, training_loss: 1.21497e+02
I1111 20:33:48.028419 140264174335808 run_lib.py:153] step: 414250, training_loss: 1.04902e+02
I1111 20:33:57.840821 140264174335808 run_lib.py:153] step: 414300, training_loss: 1.43000e+02
I1111 20:34:08.256281 140264174335808 run_lib.py:153] step: 414350, training_loss: 1.06933e+02
I1111 20:34:17.615846 140264174335808 run_lib.py:153] step: 414400, training_loss: 1.14778e+02
I1111 20:34:26.920917 140264174335808 run_lib.py:153] step: 414450, training_loss: 1.37020e+02
I1111 20:34:37.448307 140264174335808 run_lib.py:153] step: 414500, training_loss: 1.11047e+02
I1111 20:34:48.214589 140264174335808 run_lib.py:153] step: 414550, training_loss: 9.88729e+01
I1111 20:34:57.906660 140264174335808 run_lib.py:153] step: 414600, training_loss: 1.57156e+02
I1111 20:35:07.529283 140264174335808 run_lib.py:153] step: 414650, training_loss: 1.45105e+02
I1111 20:35:17.285201 140264174335808 run_lib.py:153] step: 414700, training_loss: 1.43169e+02
I1111 20:35:27.372856 140264174335808 run_lib.py:153] step: 414750, training_loss: 1.41537e+02
I1111 20:35:37.013256 140264174335808 run_lib.py:153] step: 414800, training_loss: 1.27878e+02
I1111 20:35:47.145658 140264174335808 run_lib.py:153] step: 414850, training_loss: 1.26000e+02
I1111 20:35:56.863314 140264174335808 run_lib.py:153] step: 414900, training_loss: 1.23587e+02
I1111 20:36:06.402764 140264174335808 run_lib.py:153] step: 414950, training_loss: 1.20694e+02
I1111 20:36:17.573780 140264174335808 run_lib.py:153] step: 415000, training_loss: 1.30687e+02
I1111 20:36:17.719806 140264174335808 run_lib.py:166] step: 415000, eval_loss: 1.31331e+02
I1111 20:36:27.987786 140264174335808 run_lib.py:153] step: 415050, training_loss: 1.11739e+02
I1111 20:36:37.736036 140264174335808 run_lib.py:153] step: 415100, training_loss: 1.34142e+02
I1111 20:36:47.319659 140264174335808 run_lib.py:153] step: 415150, training_loss: 1.37555e+02
I1111 20:36:57.854307 140264174335808 run_lib.py:153] step: 415200, training_loss: 9.34339e+01
I1111 20:37:07.796661 140264174335808 run_lib.py:153] step: 415250, training_loss: 1.42767e+02
I1111 20:37:17.992765 140264174335808 run_lib.py:153] step: 415300, training_loss: 1.31921e+02
I1111 20:37:28.917487 140264174335808 run_lib.py:153] step: 415350, training_loss: 1.20220e+02
I1111 20:37:39.457143 140264174335808 run_lib.py:153] step: 415400, training_loss: 1.43738e+02
I1111 20:37:49.613608 140264174335808 run_lib.py:153] step: 415450, training_loss: 1.35796e+02
I1111 20:37:59.981559 140264174335808 run_lib.py:153] step: 415500, training_loss: 1.15493e+02
I1111 20:38:09.992314 140264174335808 run_lib.py:153] step: 415550, training_loss: 1.52427e+02
I1111 20:38:20.373510 140264174335808 run_lib.py:153] step: 415600, training_loss: 1.34540e+02
I1111 20:38:30.571738 140264174335808 run_lib.py:153] step: 415650, training_loss: 1.03468e+02
I1111 20:38:41.221512 140264174335808 run_lib.py:153] step: 415700, training_loss: 1.29424e+02
I1111 20:38:51.168675 140264174335808 run_lib.py:153] step: 415750, training_loss: 1.09095e+02
I1111 20:39:01.169866 140264174335808 run_lib.py:153] step: 415800, training_loss: 1.55442e+02
I1111 20:39:11.054930 140264174335808 run_lib.py:153] step: 415850, training_loss: 1.16540e+02
I1111 20:39:20.664805 140264174335808 run_lib.py:153] step: 415900, training_loss: 1.22678e+02
I1111 20:39:30.531892 140264174335808 run_lib.py:153] step: 415950, training_loss: 1.24480e+02
I1111 20:39:40.874717 140264174335808 run_lib.py:153] step: 416000, training_loss: 1.37166e+02
I1111 20:39:51.397006 140264174335808 run_lib.py:153] step: 416050, training_loss: 1.40235e+02
I1111 20:40:01.782313 140264174335808 run_lib.py:153] step: 416100, training_loss: 1.51903e+02
I1111 20:40:12.359881 140264174335808 run_lib.py:153] step: 416150, training_loss: 1.30484e+02
I1111 20:40:22.683343 140264174335808 run_lib.py:153] step: 416200, training_loss: 1.08604e+02
I1111 20:40:32.638489 140264174335808 run_lib.py:153] step: 416250, training_loss: 1.10313e+02
I1111 20:40:42.283720 140264174335808 run_lib.py:153] step: 416300, training_loss: 1.30425e+02
I1111 20:40:52.135263 140264174335808 run_lib.py:153] step: 416350, training_loss: 1.07542e+02
I1111 20:41:02.752551 140264174335808 run_lib.py:153] step: 416400, training_loss: 1.32349e+02
I1111 20:41:13.015723 140264174335808 run_lib.py:153] step: 416450, training_loss: 1.06391e+02
I1111 20:41:23.513329 140264174335808 run_lib.py:153] step: 416500, training_loss: 1.47764e+02
I1111 20:41:33.709024 140264174335808 run_lib.py:153] step: 416550, training_loss: 1.17873e+02
I1111 20:41:43.658428 140264174335808 run_lib.py:153] step: 416600, training_loss: 1.21854e+02
I1111 20:41:54.011294 140264174335808 run_lib.py:153] step: 416650, training_loss: 1.28445e+02
I1111 20:42:03.799402 140264174335808 run_lib.py:153] step: 416700, training_loss: 1.22369e+02
I1111 20:42:13.461898 140264174335808 run_lib.py:153] step: 416750, training_loss: 1.32857e+02
I1111 20:42:24.621529 140264174335808 run_lib.py:153] step: 416800, training_loss: 1.21648e+02
I1111 20:42:34.812796 140264174335808 run_lib.py:153] step: 416850, training_loss: 1.20455e+02
I1111 20:42:44.741456 140264174335808 run_lib.py:153] step: 416900, training_loss: 1.47299e+02
I1111 20:42:54.859227 140264174335808 run_lib.py:153] step: 416950, training_loss: 1.20568e+02
I1111 20:43:05.175371 140264174335808 run_lib.py:153] step: 417000, training_loss: 1.43143e+02
I1111 20:43:15.467489 140264174335808 run_lib.py:153] step: 417050, training_loss: 1.52895e+02
I1111 20:43:25.482691 140264174335808 run_lib.py:153] step: 417100, training_loss: 1.22801e+02
I1111 20:43:35.725736 140264174335808 run_lib.py:153] step: 417150, training_loss: 1.26182e+02
I1111 20:43:45.400010 140264174335808 run_lib.py:153] step: 417200, training_loss: 1.13684e+02
I1111 20:43:55.960593 140264174335808 run_lib.py:153] step: 417250, training_loss: 8.97097e+01
I1111 20:44:05.665200 140264174335808 run_lib.py:153] step: 417300, training_loss: 1.12917e+02
I1111 20:44:15.989293 140264174335808 run_lib.py:153] step: 417350, training_loss: 1.13889e+02
I1111 20:44:25.388014 140264174335808 run_lib.py:153] step: 417400, training_loss: 1.13220e+02
I1111 20:44:34.752317 140264174335808 run_lib.py:153] step: 417450, training_loss: 1.14615e+02
I1111 20:44:44.069260 140264174335808 run_lib.py:153] step: 417500, training_loss: 1.39555e+02
I1111 20:44:53.877476 140264174335808 run_lib.py:153] step: 417550, training_loss: 1.26420e+02
I1111 20:45:03.092468 140264174335808 run_lib.py:153] step: 417600, training_loss: 1.15920e+02
I1111 20:45:13.724715 140264174335808 run_lib.py:153] step: 417650, training_loss: 1.35593e+02
I1111 20:45:23.635319 140264174335808 run_lib.py:153] step: 417700, training_loss: 9.48960e+01
I1111 20:45:33.744180 140264174335808 run_lib.py:153] step: 417750, training_loss: 1.31581e+02
I1111 20:45:43.773670 140264174335808 run_lib.py:153] step: 417800, training_loss: 1.20877e+02
I1111 20:45:53.940990 140264174335808 run_lib.py:153] step: 417850, training_loss: 1.54889e+02
I1111 20:46:04.229098 140264174335808 run_lib.py:153] step: 417900, training_loss: 1.32009e+02
I1111 20:46:14.491139 140264174335808 run_lib.py:153] step: 417950, training_loss: 1.48624e+02
I1111 20:46:24.517678 140264174335808 run_lib.py:153] step: 418000, training_loss: 1.25135e+02
I1111 20:46:34.082064 140264174335808 run_lib.py:153] step: 418050, training_loss: 1.27834e+02
I1111 20:46:43.437920 140264174335808 run_lib.py:153] step: 418100, training_loss: 1.62403e+02
I1111 20:46:52.967961 140264174335808 run_lib.py:153] step: 418150, training_loss: 1.24259e+02
I1111 20:47:02.396968 140264174335808 run_lib.py:153] step: 418200, training_loss: 1.56966e+02
I1111 20:47:12.360903 140264174335808 run_lib.py:153] step: 418250, training_loss: 1.28766e+02
I1111 20:47:22.166005 140264174335808 run_lib.py:153] step: 418300, training_loss: 1.22146e+02
I1111 20:47:32.949820 140264174335808 run_lib.py:153] step: 418350, training_loss: 1.07593e+02
I1111 20:47:43.289045 140264174335808 run_lib.py:153] step: 418400, training_loss: 1.45635e+02
I1111 20:47:53.811367 140264174335808 run_lib.py:153] step: 418450, training_loss: 1.10119e+02
I1111 20:48:04.038488 140264174335808 run_lib.py:153] step: 418500, training_loss: 1.42047e+02
I1111 20:48:14.240075 140264174335808 run_lib.py:153] step: 418550, training_loss: 1.38646e+02
I1111 20:48:23.940524 140264174335808 run_lib.py:153] step: 418600, training_loss: 1.56296e+02
I1111 20:48:33.826648 140264174335808 run_lib.py:153] step: 418650, training_loss: 9.71317e+01
I1111 20:48:44.102825 140264174335808 run_lib.py:153] step: 418700, training_loss: 1.14118e+02
I1111 20:48:54.321640 140264174335808 run_lib.py:153] step: 418750, training_loss: 1.03652e+02
I1111 20:49:03.984911 140264174335808 run_lib.py:153] step: 418800, training_loss: 1.07275e+02
I1111 20:49:13.912413 140264174335808 run_lib.py:153] step: 418850, training_loss: 1.42542e+02
I1111 20:49:24.363216 140264174335808 run_lib.py:153] step: 418900, training_loss: 1.61484e+02
I1111 20:49:34.320902 140264174335808 run_lib.py:153] step: 418950, training_loss: 1.56192e+02
I1111 20:49:44.649351 140264174335808 run_lib.py:153] step: 419000, training_loss: 1.12861e+02
I1111 20:49:54.925669 140264174335808 run_lib.py:153] step: 419050, training_loss: 1.15611e+02
I1111 20:50:05.476605 140264174335808 run_lib.py:153] step: 419100, training_loss: 1.17208e+02
I1111 20:50:16.397079 140264174335808 run_lib.py:153] step: 419150, training_loss: 1.22425e+02
I1111 20:50:26.784829 140264174335808 run_lib.py:153] step: 419200, training_loss: 1.16714e+02
I1111 20:50:37.418024 140264174335808 run_lib.py:153] step: 419250, training_loss: 1.54715e+02
I1111 20:50:46.914767 140264174335808 run_lib.py:153] step: 419300, training_loss: 1.16362e+02
I1111 20:50:57.181005 140264174335808 run_lib.py:153] step: 419350, training_loss: 1.49803e+02
I1111 20:51:07.114617 140264174335808 run_lib.py:153] step: 419400, training_loss: 1.25944e+02
I1111 20:51:17.379343 140264174335808 run_lib.py:153] step: 419450, training_loss: 1.21910e+02
I1111 20:51:27.234056 140264174335808 run_lib.py:153] step: 419500, training_loss: 1.09069e+02
I1111 20:51:37.343792 140264174335808 run_lib.py:153] step: 419550, training_loss: 1.26494e+02
I1111 20:51:47.626546 140264174335808 run_lib.py:153] step: 419600, training_loss: 1.52248e+02
I1111 20:51:58.361677 140264174335808 run_lib.py:153] step: 419650, training_loss: 1.50316e+02
I1111 20:52:07.963523 140264174335808 run_lib.py:153] step: 419700, training_loss: 1.11078e+02
I1111 20:52:17.175492 140264174335808 run_lib.py:153] step: 419750, training_loss: 1.52994e+02
I1111 20:52:27.347674 140264174335808 run_lib.py:153] step: 419800, training_loss: 1.34032e+02
I1111 20:52:37.530251 140264174335808 run_lib.py:153] step: 419850, training_loss: 1.24261e+02
I1111 20:52:48.068330 140264174335808 run_lib.py:153] step: 419900, training_loss: 1.19604e+02
I1111 20:52:58.582334 140264174335808 run_lib.py:153] step: 419950, training_loss: 1.11498e+02
I1111 20:53:08.171427 140264174335808 run_lib.py:153] step: 420000, training_loss: 1.17200e+02
I1111 20:53:08.760769 140264174335808 run_lib.py:166] step: 420000, eval_loss: 1.22514e+02
I1111 20:53:18.050088 140264174335808 run_lib.py:153] step: 420050, training_loss: 1.10308e+02
I1111 20:53:28.459682 140264174335808 run_lib.py:153] step: 420100, training_loss: 1.27069e+02
I1111 20:53:37.903809 140264174335808 run_lib.py:153] step: 420150, training_loss: 1.64134e+02
I1111 20:53:47.465721 140264174335808 run_lib.py:153] step: 420200, training_loss: 9.78241e+01
I1111 20:53:57.060333 140264174335808 run_lib.py:153] step: 420250, training_loss: 1.39300e+02
I1111 20:54:06.765606 140264174335808 run_lib.py:153] step: 420300, training_loss: 1.50800e+02
I1111 20:54:17.118430 140264174335808 run_lib.py:153] step: 420350, training_loss: 1.31487e+02
I1111 20:54:26.832215 140264174335808 run_lib.py:153] step: 420400, training_loss: 1.43419e+02
I1111 20:54:37.759580 140264174335808 run_lib.py:153] step: 420450, training_loss: 1.32360e+02
I1111 20:54:47.598476 140264174335808 run_lib.py:153] step: 420500, training_loss: 1.35932e+02
I1111 20:54:57.658934 140264174335808 run_lib.py:153] step: 420550, training_loss: 9.71114e+01
I1111 20:55:07.650473 140264174335808 run_lib.py:153] step: 420600, training_loss: 1.19422e+02
I1111 20:55:18.244970 140264174335808 run_lib.py:153] step: 420650, training_loss: 1.22731e+02
I1111 20:55:28.120357 140264174335808 run_lib.py:153] step: 420700, training_loss: 1.39729e+02
I1111 20:55:38.029431 140264174335808 run_lib.py:153] step: 420750, training_loss: 1.16269e+02
I1111 20:55:48.606570 140264174335808 run_lib.py:153] step: 420800, training_loss: 1.59809e+02
I1111 20:55:58.936538 140264174335808 run_lib.py:153] step: 420850, training_loss: 1.21753e+02
I1111 20:56:08.845364 140264174335808 run_lib.py:153] step: 420900, training_loss: 1.23936e+02
I1111 20:56:18.083180 140264174335808 run_lib.py:153] step: 420950, training_loss: 1.30006e+02
I1111 20:56:28.288714 140264174335808 run_lib.py:153] step: 421000, training_loss: 1.28149e+02
I1111 20:56:38.572702 140264174335808 run_lib.py:153] step: 421050, training_loss: 1.25281e+02
I1111 20:56:48.815182 140264174335808 run_lib.py:153] step: 421100, training_loss: 1.21811e+02
I1111 20:56:59.742246 140264174335808 run_lib.py:153] step: 421150, training_loss: 1.04306e+02
I1111 20:57:10.486916 140264174335808 run_lib.py:153] step: 421200, training_loss: 1.18437e+02
I1111 20:57:20.698903 140264174335808 run_lib.py:153] step: 421250, training_loss: 1.05763e+02
I1111 20:57:30.686222 140264174335808 run_lib.py:153] step: 421300, training_loss: 1.37567e+02
I1111 20:57:41.029949 140264174335808 run_lib.py:153] step: 421350, training_loss: 1.37563e+02
I1111 20:57:51.015838 140264174335808 run_lib.py:153] step: 421400, training_loss: 1.20987e+02
I1111 20:58:01.445593 140264174335808 run_lib.py:153] step: 421450, training_loss: 1.28612e+02
I1111 20:58:11.562207 140264174335808 run_lib.py:153] step: 421500, training_loss: 1.02408e+02
I1111 20:58:21.358016 140264174335808 run_lib.py:153] step: 421550, training_loss: 1.22337e+02
I1111 20:58:31.749866 140264174335808 run_lib.py:153] step: 421600, training_loss: 1.28746e+02
I1111 20:58:41.781405 140264174335808 run_lib.py:153] step: 421650, training_loss: 1.39088e+02
I1111 20:58:51.549779 140264174335808 run_lib.py:153] step: 421700, training_loss: 1.32746e+02
I1111 20:59:01.717310 140264174335808 run_lib.py:153] step: 421750, training_loss: 1.22993e+02
I1111 20:59:11.931529 140264174335808 run_lib.py:153] step: 421800, training_loss: 1.22492e+02
I1111 20:59:22.294827 140264174335808 run_lib.py:153] step: 421850, training_loss: 1.33531e+02
I1111 20:59:32.052081 140264174335808 run_lib.py:153] step: 421900, training_loss: 1.27190e+02
I1111 20:59:42.014727 140264174335808 run_lib.py:153] step: 421950, training_loss: 1.28702e+02
I1111 20:59:51.565338 140264174335808 run_lib.py:153] step: 422000, training_loss: 1.18112e+02
I1111 21:00:01.305403 140264174335808 run_lib.py:153] step: 422050, training_loss: 9.39153e+01
I1111 21:00:11.856811 140264174335808 run_lib.py:153] step: 422100, training_loss: 1.26092e+02
I1111 21:00:22.058798 140264174335808 run_lib.py:153] step: 422150, training_loss: 1.56081e+02
I1111 21:00:31.957936 140264174335808 run_lib.py:153] step: 422200, training_loss: 1.23232e+02
I1111 21:00:42.048171 140264174335808 run_lib.py:153] step: 422250, training_loss: 1.12930e+02
I1111 21:00:52.436110 140264174335808 run_lib.py:153] step: 422300, training_loss: 1.52676e+02
I1111 21:01:03.159203 140264174335808 run_lib.py:153] step: 422350, training_loss: 1.26178e+02
I1111 21:01:13.486745 140264174335808 run_lib.py:153] step: 422400, training_loss: 9.88049e+01
I1111 21:01:23.284962 140264174335808 run_lib.py:153] step: 422450, training_loss: 1.23158e+02
I1111 21:01:33.795801 140264174335808 run_lib.py:153] step: 422500, training_loss: 1.36977e+02
I1111 21:01:43.893869 140264174335808 run_lib.py:153] step: 422550, training_loss: 1.38702e+02
I1111 21:01:53.646687 140264174335808 run_lib.py:153] step: 422600, training_loss: 1.11251e+02
I1111 21:02:04.345598 140264174335808 run_lib.py:153] step: 422650, training_loss: 1.23283e+02
I1111 21:02:14.588128 140264174335808 run_lib.py:153] step: 422700, training_loss: 1.29382e+02
I1111 21:02:24.462420 140264174335808 run_lib.py:153] step: 422750, training_loss: 1.23858e+02
I1111 21:02:34.269910 140264174335808 run_lib.py:153] step: 422800, training_loss: 1.39311e+02
I1111 21:02:45.525745 140264174335808 run_lib.py:153] step: 422850, training_loss: 1.40185e+02
I1111 21:02:56.211451 140264174335808 run_lib.py:153] step: 422900, training_loss: 1.18796e+02
I1111 21:03:05.813951 140264174335808 run_lib.py:153] step: 422950, training_loss: 1.58186e+02
I1111 21:03:16.534128 140264174335808 run_lib.py:153] step: 423000, training_loss: 1.11602e+02
I1111 21:03:26.532116 140264174335808 run_lib.py:153] step: 423050, training_loss: 1.24349e+02
I1111 21:03:35.998093 140264174335808 run_lib.py:153] step: 423100, training_loss: 1.11238e+02
I1111 21:03:46.075989 140264174335808 run_lib.py:153] step: 423150, training_loss: 1.25194e+02
I1111 21:03:56.196186 140264174335808 run_lib.py:153] step: 423200, training_loss: 1.26902e+02
I1111 21:04:05.433476 140264174335808 run_lib.py:153] step: 423250, training_loss: 1.42730e+02
I1111 21:04:15.639816 140264174335808 run_lib.py:153] step: 423300, training_loss: 1.29664e+02
I1111 21:04:26.189883 140264174335808 run_lib.py:153] step: 423350, training_loss: 1.33199e+02
I1111 21:04:36.879495 140264174335808 run_lib.py:153] step: 423400, training_loss: 1.18320e+02
I1111 21:04:46.903087 140264174335808 run_lib.py:153] step: 423450, training_loss: 1.24291e+02
I1111 21:04:56.557542 140264174335808 run_lib.py:153] step: 423500, training_loss: 1.33478e+02
I1111 21:05:06.652320 140264174335808 run_lib.py:153] step: 423550, training_loss: 1.64027e+02
I1111 21:05:16.400475 140264174335808 run_lib.py:153] step: 423600, training_loss: 1.19721e+02
I1111 21:05:25.844699 140264174335808 run_lib.py:153] step: 423650, training_loss: 1.54867e+02
I1111 21:05:35.463031 140264174335808 run_lib.py:153] step: 423700, training_loss: 1.42412e+02
I1111 21:05:45.032404 140264174335808 run_lib.py:153] step: 423750, training_loss: 1.20518e+02
I1111 21:05:55.368389 140264174335808 run_lib.py:153] step: 423800, training_loss: 1.38527e+02
I1111 21:06:05.193542 140264174335808 run_lib.py:153] step: 423850, training_loss: 1.33589e+02
I1111 21:06:14.905275 140264174335808 run_lib.py:153] step: 423900, training_loss: 1.40370e+02
I1111 21:06:24.630761 140264174335808 run_lib.py:153] step: 423950, training_loss: 1.50473e+02
I1111 21:06:33.897059 140264174335808 run_lib.py:153] step: 424000, training_loss: 1.18017e+02
I1111 21:06:43.768576 140264174335808 run_lib.py:153] step: 424050, training_loss: 1.03621e+02
I1111 21:06:53.286504 140264174335808 run_lib.py:153] step: 424100, training_loss: 1.51133e+02
I1111 21:07:03.271596 140264174335808 run_lib.py:153] step: 424150, training_loss: 1.18750e+02
I1111 21:07:13.009099 140264174335808 run_lib.py:153] step: 424200, training_loss: 1.48214e+02
I1111 21:07:23.511004 140264174335808 run_lib.py:153] step: 424250, training_loss: 9.83576e+01
I1111 21:07:33.202350 140264174335808 run_lib.py:153] step: 424300, training_loss: 1.18696e+02
I1111 21:07:43.298210 140264174335808 run_lib.py:153] step: 424350, training_loss: 1.11542e+02
I1111 21:07:52.754780 140264174335808 run_lib.py:153] step: 424400, training_loss: 1.08906e+02
I1111 21:08:02.676989 140264174335808 run_lib.py:153] step: 424450, training_loss: 1.29423e+02
I1111 21:08:12.709289 140264174335808 run_lib.py:153] step: 424500, training_loss: 1.35376e+02
I1111 21:08:23.611837 140264174335808 run_lib.py:153] step: 424550, training_loss: 1.36559e+02
I1111 21:08:33.692065 140264174335808 run_lib.py:153] step: 424600, training_loss: 1.09973e+02
I1111 21:08:43.075838 140264174335808 run_lib.py:153] step: 424650, training_loss: 1.28354e+02
I1111 21:08:54.001056 140264174335808 run_lib.py:153] step: 424700, training_loss: 9.83303e+01
I1111 21:09:03.993449 140264174335808 run_lib.py:153] step: 424750, training_loss: 1.28535e+02
I1111 21:09:13.413236 140264174335808 run_lib.py:153] step: 424800, training_loss: 1.13312e+02
I1111 21:09:23.867410 140264174335808 run_lib.py:153] step: 424850, training_loss: 1.14105e+02
I1111 21:09:34.583990 140264174335808 run_lib.py:153] step: 424900, training_loss: 1.53287e+02
I1111 21:09:44.317929 140264174335808 run_lib.py:153] step: 424950, training_loss: 1.14696e+02
I1111 21:09:53.986492 140264174335808 run_lib.py:153] step: 425000, training_loss: 1.23302e+02
I1111 21:09:54.089463 140264174335808 run_lib.py:166] step: 425000, eval_loss: 1.40864e+02
I1111 21:10:03.845636 140264174335808 run_lib.py:153] step: 425050, training_loss: 1.39487e+02
I1111 21:10:13.585874 140264174335808 run_lib.py:153] step: 425100, training_loss: 1.39398e+02
I1111 21:10:23.834581 140264174335808 run_lib.py:153] step: 425150, training_loss: 1.21688e+02
I1111 21:10:33.853903 140264174335808 run_lib.py:153] step: 425200, training_loss: 1.33561e+02
I1111 21:10:44.142755 140264174335808 run_lib.py:153] step: 425250, training_loss: 1.40562e+02
I1111 21:10:53.892544 140264174335808 run_lib.py:153] step: 425300, training_loss: 1.15530e+02
I1111 21:11:03.383931 140264174335808 run_lib.py:153] step: 425350, training_loss: 1.20188e+02
I1111 21:11:14.025148 140264174335808 run_lib.py:153] step: 425400, training_loss: 1.18127e+02
I1111 21:11:24.559540 140264174335808 run_lib.py:153] step: 425450, training_loss: 1.09637e+02
I1111 21:11:34.904597 140264174335808 run_lib.py:153] step: 425500, training_loss: 1.22226e+02
I1111 21:11:45.270156 140264174335808 run_lib.py:153] step: 425550, training_loss: 1.39137e+02
I1111 21:11:55.568529 140264174335808 run_lib.py:153] step: 425600, training_loss: 1.33755e+02
I1111 21:12:05.764498 140264174335808 run_lib.py:153] step: 425650, training_loss: 1.12796e+02
I1111 21:12:15.746837 140264174335808 run_lib.py:153] step: 425700, training_loss: 1.43551e+02
I1111 21:12:26.467611 140264174335808 run_lib.py:153] step: 425750, training_loss: 1.18091e+02
I1111 21:12:36.105514 140264174335808 run_lib.py:153] step: 425800, training_loss: 1.61435e+02
I1111 21:12:46.965484 140264174335808 run_lib.py:153] step: 425850, training_loss: 1.30482e+02
I1111 21:12:57.335162 140264174335808 run_lib.py:153] step: 425900, training_loss: 1.31799e+02
I1111 21:13:07.533088 140264174335808 run_lib.py:153] step: 425950, training_loss: 1.40980e+02
I1111 21:13:18.733607 140264174335808 run_lib.py:153] step: 426000, training_loss: 1.28821e+02
I1111 21:13:28.598319 140264174335808 run_lib.py:153] step: 426050, training_loss: 1.14000e+02
I1111 21:13:39.544027 140264174335808 run_lib.py:153] step: 426100, training_loss: 1.24445e+02
I1111 21:13:49.313223 140264174335808 run_lib.py:153] step: 426150, training_loss: 1.46939e+02
I1111 21:13:59.658754 140264174335808 run_lib.py:153] step: 426200, training_loss: 9.70620e+01
I1111 21:14:10.428765 140264174335808 run_lib.py:153] step: 426250, training_loss: 1.16976e+02
I1111 21:14:20.410502 140264174335808 run_lib.py:153] step: 426300, training_loss: 1.17755e+02
I1111 21:14:30.760921 140264174335808 run_lib.py:153] step: 426350, training_loss: 1.39968e+02
I1111 21:14:40.160917 140264174335808 run_lib.py:153] step: 426400, training_loss: 1.21925e+02
I1111 21:14:50.150424 140264174335808 run_lib.py:153] step: 426450, training_loss: 1.20935e+02
I1111 21:15:00.015611 140264174335808 run_lib.py:153] step: 426500, training_loss: 1.21778e+02
I1111 21:15:09.737893 140264174335808 run_lib.py:153] step: 426550, training_loss: 1.40166e+02
I1111 21:15:19.653219 140264174335808 run_lib.py:153] step: 426600, training_loss: 1.25316e+02
I1111 21:15:30.199517 140264174335808 run_lib.py:153] step: 426650, training_loss: 1.12545e+02
I1111 21:15:40.217896 140264174335808 run_lib.py:153] step: 426700, training_loss: 1.31586e+02
I1111 21:15:50.538242 140264174335808 run_lib.py:153] step: 426750, training_loss: 1.40332e+02
I1111 21:16:01.112670 140264174335808 run_lib.py:153] step: 426800, training_loss: 1.42044e+02
I1111 21:16:11.236158 140264174335808 run_lib.py:153] step: 426850, training_loss: 1.07963e+02
I1111 21:16:21.511372 140264174335808 run_lib.py:153] step: 426900, training_loss: 1.03638e+02
I1111 21:16:31.902222 140264174335808 run_lib.py:153] step: 426950, training_loss: 1.17834e+02
I1111 21:16:41.957877 140264174335808 run_lib.py:153] step: 427000, training_loss: 1.12683e+02
I1111 21:16:51.760162 140264174335808 run_lib.py:153] step: 427050, training_loss: 1.15961e+02
I1111 21:17:01.829272 140264174335808 run_lib.py:153] step: 427100, training_loss: 9.66324e+01
I1111 21:17:11.915817 140264174335808 run_lib.py:153] step: 427150, training_loss: 1.33864e+02
I1111 21:17:21.654797 140264174335808 run_lib.py:153] step: 427200, training_loss: 1.37048e+02
I1111 21:17:32.025221 140264174335808 run_lib.py:153] step: 427250, training_loss: 1.25498e+02
I1111 21:17:42.289223 140264174335808 run_lib.py:153] step: 427300, training_loss: 1.04553e+02
I1111 21:17:52.673145 140264174335808 run_lib.py:153] step: 427350, training_loss: 1.34317e+02
I1111 21:18:02.406323 140264174335808 run_lib.py:153] step: 427400, training_loss: 1.45218e+02
I1111 21:18:12.358672 140264174335808 run_lib.py:153] step: 427450, training_loss: 1.22189e+02
I1111 21:18:22.567152 140264174335808 run_lib.py:153] step: 427500, training_loss: 1.15989e+02
I1111 21:18:32.640512 140264174335808 run_lib.py:153] step: 427550, training_loss: 1.40297e+02
I1111 21:18:42.765977 140264174335808 run_lib.py:153] step: 427600, training_loss: 1.35045e+02
I1111 21:18:53.057097 140264174335808 run_lib.py:153] step: 427650, training_loss: 1.34503e+02
I1111 21:19:03.549295 140264174335808 run_lib.py:153] step: 427700, training_loss: 1.18921e+02
I1111 21:19:14.180098 140264174335808 run_lib.py:153] step: 427750, training_loss: 1.38566e+02
I1111 21:19:24.343741 140264174335808 run_lib.py:153] step: 427800, training_loss: 8.24265e+01
I1111 21:19:34.573858 140264174335808 run_lib.py:153] step: 427850, training_loss: 1.37275e+02
I1111 21:19:44.780639 140264174335808 run_lib.py:153] step: 427900, training_loss: 1.30586e+02
I1111 21:19:55.056905 140264174335808 run_lib.py:153] step: 427950, training_loss: 1.08188e+02
I1111 21:20:05.233605 140264174335808 run_lib.py:153] step: 428000, training_loss: 1.23612e+02
I1111 21:20:14.961037 140264174335808 run_lib.py:153] step: 428050, training_loss: 1.13879e+02
I1111 21:20:24.911822 140264174335808 run_lib.py:153] step: 428100, training_loss: 1.48878e+02
I1111 21:20:35.498466 140264174335808 run_lib.py:153] step: 428150, training_loss: 1.12285e+02
I1111 21:20:45.660272 140264174335808 run_lib.py:153] step: 428200, training_loss: 1.40906e+02
I1111 21:20:55.614309 140264174335808 run_lib.py:153] step: 428250, training_loss: 1.47038e+02
I1111 21:21:05.173815 140264174335808 run_lib.py:153] step: 428300, training_loss: 1.41864e+02
I1111 21:21:15.029651 140264174335808 run_lib.py:153] step: 428350, training_loss: 1.21702e+02
I1111 21:21:24.955784 140264174335808 run_lib.py:153] step: 428400, training_loss: 1.33300e+02
I1111 21:21:34.738198 140264174335808 run_lib.py:153] step: 428450, training_loss: 9.68422e+01
I1111 21:21:45.607354 140264174335808 run_lib.py:153] step: 428500, training_loss: 1.18539e+02
I1111 21:21:56.279927 140264174335808 run_lib.py:153] step: 428550, training_loss: 1.33625e+02
I1111 21:22:06.121818 140264174335808 run_lib.py:153] step: 428600, training_loss: 1.35994e+02
I1111 21:22:16.524053 140264174335808 run_lib.py:153] step: 428650, training_loss: 1.23462e+02
I1111 21:22:26.246001 140264174335808 run_lib.py:153] step: 428700, training_loss: 1.17630e+02
I1111 21:22:36.472308 140264174335808 run_lib.py:153] step: 428750, training_loss: 1.28554e+02
I1111 21:22:46.616148 140264174335808 run_lib.py:153] step: 428800, training_loss: 1.27271e+02
I1111 21:22:57.251293 140264174335808 run_lib.py:153] step: 428850, training_loss: 1.15447e+02
I1111 21:23:06.962868 140264174335808 run_lib.py:153] step: 428900, training_loss: 1.19848e+02
I1111 21:23:17.570060 140264174335808 run_lib.py:153] step: 428950, training_loss: 1.14447e+02
I1111 21:23:27.166931 140264174335808 run_lib.py:153] step: 429000, training_loss: 1.43296e+02
I1111 21:23:36.855158 140264174335808 run_lib.py:153] step: 429050, training_loss: 1.13588e+02
I1111 21:23:47.582792 140264174335808 run_lib.py:153] step: 429100, training_loss: 1.08944e+02
I1111 21:23:57.533007 140264174335808 run_lib.py:153] step: 429150, training_loss: 1.25190e+02
I1111 21:24:07.314330 140264174335808 run_lib.py:153] step: 429200, training_loss: 1.12208e+02
I1111 21:24:17.638931 140264174335808 run_lib.py:153] step: 429250, training_loss: 1.46742e+02
I1111 21:24:28.357431 140264174335808 run_lib.py:153] step: 429300, training_loss: 1.27237e+02
I1111 21:24:39.062259 140264174335808 run_lib.py:153] step: 429350, training_loss: 1.14677e+02
I1111 21:24:49.151146 140264174335808 run_lib.py:153] step: 429400, training_loss: 1.35436e+02
I1111 21:24:59.835438 140264174335808 run_lib.py:153] step: 429450, training_loss: 1.21391e+02
I1111 21:25:09.870399 140264174335808 run_lib.py:153] step: 429500, training_loss: 1.09032e+02
I1111 21:25:20.665908 140264174335808 run_lib.py:153] step: 429550, training_loss: 1.35118e+02
I1111 21:25:31.329270 140264174335808 run_lib.py:153] step: 429600, training_loss: 1.51058e+02
I1111 21:25:41.035470 140264174335808 run_lib.py:153] step: 429650, training_loss: 1.16535e+02
I1111 21:25:51.061440 140264174335808 run_lib.py:153] step: 429700, training_loss: 1.32312e+02
I1111 21:26:01.104368 140264174335808 run_lib.py:153] step: 429750, training_loss: 1.29160e+02
I1111 21:26:10.639079 140264174335808 run_lib.py:153] step: 429800, training_loss: 9.86430e+01
I1111 21:26:20.246850 140264174335808 run_lib.py:153] step: 429850, training_loss: 1.42968e+02
I1111 21:26:30.800001 140264174335808 run_lib.py:153] step: 429900, training_loss: 1.38923e+02
I1111 21:26:41.000818 140264174335808 run_lib.py:153] step: 429950, training_loss: 1.06853e+02
I1111 21:26:50.983919 140264174335808 run_lib.py:153] step: 430000, training_loss: 1.20175e+02
I1111 21:26:51.566788 140264174335808 run_lib.py:166] step: 430000, eval_loss: 9.09575e+01
I1111 21:27:02.131644 140264174335808 run_lib.py:153] step: 430050, training_loss: 1.53757e+02
I1111 21:27:13.331811 140264174335808 run_lib.py:153] step: 430100, training_loss: 1.23886e+02
I1111 21:27:23.539725 140264174335808 run_lib.py:153] step: 430150, training_loss: 1.39672e+02
I1111 21:27:33.712262 140264174335808 run_lib.py:153] step: 430200, training_loss: 1.50041e+02
I1111 21:27:43.326754 140264174335808 run_lib.py:153] step: 430250, training_loss: 1.23696e+02
I1111 21:27:53.470057 140264174335808 run_lib.py:153] step: 430300, training_loss: 1.27372e+02
I1111 21:28:03.475538 140264174335808 run_lib.py:153] step: 430350, training_loss: 1.16300e+02
I1111 21:28:13.945123 140264174335808 run_lib.py:153] step: 430400, training_loss: 1.47742e+02
I1111 21:28:23.845635 140264174335808 run_lib.py:153] step: 430450, training_loss: 1.15809e+02
I1111 21:28:33.258147 140264174335808 run_lib.py:153] step: 430500, training_loss: 1.17664e+02
I1111 21:28:43.303013 140264174335808 run_lib.py:153] step: 430550, training_loss: 1.31211e+02
I1111 21:28:53.880995 140264174335808 run_lib.py:153] step: 430600, training_loss: 1.07871e+02
I1111 21:29:04.387824 140264174335808 run_lib.py:153] step: 430650, training_loss: 1.27300e+02
I1111 21:29:15.418689 140264174335808 run_lib.py:153] step: 430700, training_loss: 1.35043e+02
I1111 21:29:25.840603 140264174335808 run_lib.py:153] step: 430750, training_loss: 1.46241e+02
I1111 21:29:35.543964 140264174335808 run_lib.py:153] step: 430800, training_loss: 1.09594e+02
I1111 21:29:46.014775 140264174335808 run_lib.py:153] step: 430850, training_loss: 1.07131e+02
I1111 21:29:56.246479 140264174335808 run_lib.py:153] step: 430900, training_loss: 1.25714e+02
I1111 21:30:06.211894 140264174335808 run_lib.py:153] step: 430950, training_loss: 1.36821e+02
I1111 21:30:16.867480 140264174335808 run_lib.py:153] step: 431000, training_loss: 1.40252e+02
I1111 21:30:27.132035 140264174335808 run_lib.py:153] step: 431050, training_loss: 1.49787e+02
I1111 21:30:37.369327 140264174335808 run_lib.py:153] step: 431100, training_loss: 1.33366e+02
I1111 21:30:47.775652 140264174335808 run_lib.py:153] step: 431150, training_loss: 1.17809e+02
I1111 21:30:57.206006 140264174335808 run_lib.py:153] step: 431200, training_loss: 1.32919e+02
I1111 21:31:07.511017 140264174335808 run_lib.py:153] step: 431250, training_loss: 1.15994e+02
I1111 21:31:17.471670 140264174335808 run_lib.py:153] step: 431300, training_loss: 1.03079e+02
I1111 21:31:27.430282 140264174335808 run_lib.py:153] step: 431350, training_loss: 1.20901e+02
I1111 21:31:36.933908 140264174335808 run_lib.py:153] step: 431400, training_loss: 1.34311e+02
I1111 21:31:46.514227 140264174335808 run_lib.py:153] step: 431450, training_loss: 1.31156e+02
I1111 21:31:57.323283 140264174335808 run_lib.py:153] step: 431500, training_loss: 1.59203e+02
I1111 21:32:07.779310 140264174335808 run_lib.py:153] step: 431550, training_loss: 1.22553e+02
I1111 21:32:17.596140 140264174335808 run_lib.py:153] step: 431600, training_loss: 1.30957e+02
I1111 21:32:28.035027 140264174335808 run_lib.py:153] step: 431650, training_loss: 1.26043e+02
I1111 21:32:38.314282 140264174335808 run_lib.py:153] step: 431700, training_loss: 1.00952e+02
I1111 21:32:48.374759 140264174335808 run_lib.py:153] step: 431750, training_loss: 1.50834e+02
I1111 21:32:57.934595 140264174335808 run_lib.py:153] step: 431800, training_loss: 1.44447e+02
I1111 21:33:07.649645 140264174335808 run_lib.py:153] step: 431850, training_loss: 1.29291e+02
I1111 21:33:17.396881 140264174335808 run_lib.py:153] step: 431900, training_loss: 9.81308e+01
I1111 21:33:26.892073 140264174335808 run_lib.py:153] step: 431950, training_loss: 1.15216e+02
I1111 21:33:37.367810 140264174335808 run_lib.py:153] step: 432000, training_loss: 1.16213e+02
I1111 21:33:47.908205 140264174335808 run_lib.py:153] step: 432050, training_loss: 1.03741e+02
I1111 21:33:58.056381 140264174335808 run_lib.py:153] step: 432100, training_loss: 1.06321e+02
I1111 21:34:07.847967 140264174335808 run_lib.py:153] step: 432150, training_loss: 1.65469e+02
I1111 21:34:18.052444 140264174335808 run_lib.py:153] step: 432200, training_loss: 1.02682e+02
I1111 21:34:28.591055 140264174335808 run_lib.py:153] step: 432250, training_loss: 1.14651e+02
I1111 21:34:38.884733 140264174335808 run_lib.py:153] step: 432300, training_loss: 1.56023e+02
I1111 21:34:49.060603 140264174335808 run_lib.py:153] step: 432350, training_loss: 1.47210e+02
I1111 21:35:00.519322 140264174335808 run_lib.py:153] step: 432400, training_loss: 1.13641e+02
I1111 21:35:10.563900 140264174335808 run_lib.py:153] step: 432450, training_loss: 1.58005e+02
I1111 21:35:20.258464 140264174335808 run_lib.py:153] step: 432500, training_loss: 1.26551e+02
I1111 21:35:29.912266 140264174335808 run_lib.py:153] step: 432550, training_loss: 1.02425e+02
I1111 21:35:39.586481 140264174335808 run_lib.py:153] step: 432600, training_loss: 1.41238e+02
I1111 21:35:49.238824 140264174335808 run_lib.py:153] step: 432650, training_loss: 9.65104e+01
I1111 21:35:59.018176 140264174335808 run_lib.py:153] step: 432700, training_loss: 1.26148e+02
I1111 21:36:08.380428 140264174335808 run_lib.py:153] step: 432750, training_loss: 1.26154e+02
I1111 21:36:18.124034 140264174335808 run_lib.py:153] step: 432800, training_loss: 1.27386e+02
I1111 21:36:27.665477 140264174335808 run_lib.py:153] step: 432850, training_loss: 1.22181e+02
I1111 21:36:37.275325 140264174335808 run_lib.py:153] step: 432900, training_loss: 1.36334e+02
I1111 21:36:47.604947 140264174335808 run_lib.py:153] step: 432950, training_loss: 1.44666e+02
I1111 21:36:57.629749 140264174335808 run_lib.py:153] step: 433000, training_loss: 1.01896e+02
I1111 21:37:07.277607 140264174335808 run_lib.py:153] step: 433050, training_loss: 1.31907e+02
I1111 21:37:17.158628 140264174335808 run_lib.py:153] step: 433100, training_loss: 1.14432e+02
I1111 21:37:27.391589 140264174335808 run_lib.py:153] step: 433150, training_loss: 1.35585e+02
I1111 21:37:37.802135 140264174335808 run_lib.py:153] step: 433200, training_loss: 1.27090e+02
I1111 21:37:47.614312 140264174335808 run_lib.py:153] step: 433250, training_loss: 1.43124e+02
I1111 21:37:57.264220 140264174335808 run_lib.py:153] step: 433300, training_loss: 1.24005e+02
I1111 21:38:07.721893 140264174335808 run_lib.py:153] step: 433350, training_loss: 1.45628e+02
I1111 21:38:18.339912 140264174335808 run_lib.py:153] step: 433400, training_loss: 1.32644e+02
I1111 21:38:27.808887 140264174335808 run_lib.py:153] step: 433450, training_loss: 1.34260e+02
I1111 21:38:37.463029 140264174335808 run_lib.py:153] step: 433500, training_loss: 1.39946e+02
I1111 21:38:46.877640 140264174335808 run_lib.py:153] step: 433550, training_loss: 1.13356e+02
I1111 21:38:57.495379 140264174335808 run_lib.py:153] step: 433600, training_loss: 1.18192e+02
I1111 21:39:07.428441 140264174335808 run_lib.py:153] step: 433650, training_loss: 1.26968e+02
I1111 21:39:17.014150 140264174335808 run_lib.py:153] step: 433700, training_loss: 1.04750e+02
I1111 21:39:26.505547 140264174335808 run_lib.py:153] step: 433750, training_loss: 1.29255e+02
I1111 21:39:36.349898 140264174335808 run_lib.py:153] step: 433800, training_loss: 1.30148e+02
I1111 21:39:45.966999 140264174335808 run_lib.py:153] step: 433850, training_loss: 1.01357e+02
I1111 21:39:56.683913 140264174335808 run_lib.py:153] step: 433900, training_loss: 1.55876e+02
I1111 21:40:06.370014 140264174335808 run_lib.py:153] step: 433950, training_loss: 1.36337e+02
I1111 21:40:16.089827 140264174335808 run_lib.py:153] step: 434000, training_loss: 1.30363e+02
I1111 21:40:25.725506 140264174335808 run_lib.py:153] step: 434050, training_loss: 1.22510e+02
I1111 21:40:35.125767 140264174335808 run_lib.py:153] step: 434100, training_loss: 1.44569e+02
I1111 21:40:45.404823 140264174335808 run_lib.py:153] step: 434150, training_loss: 1.28211e+02
I1111 21:40:55.459809 140264174335808 run_lib.py:153] step: 434200, training_loss: 1.16196e+02
I1111 21:41:05.462756 140264174335808 run_lib.py:153] step: 434250, training_loss: 1.16494e+02
I1111 21:41:15.348067 140264174335808 run_lib.py:153] step: 434300, training_loss: 1.41732e+02
I1111 21:41:25.207618 140264174335808 run_lib.py:153] step: 434350, training_loss: 1.24203e+02
I1111 21:41:35.435291 140264174335808 run_lib.py:153] step: 434400, training_loss: 1.75896e+02
I1111 21:41:45.522956 140264174335808 run_lib.py:153] step: 434450, training_loss: 1.42981e+02
I1111 21:41:56.097533 140264174335808 run_lib.py:153] step: 434500, training_loss: 1.58004e+02
I1111 21:42:07.130780 140264174335808 run_lib.py:153] step: 434550, training_loss: 1.16705e+02
I1111 21:42:17.205788 140264174335808 run_lib.py:153] step: 434600, training_loss: 1.37172e+02
I1111 21:42:27.285319 140264174335808 run_lib.py:153] step: 434650, training_loss: 1.16595e+02
I1111 21:42:37.263958 140264174335808 run_lib.py:153] step: 434700, training_loss: 1.15419e+02
I1111 21:42:46.813797 140264174335808 run_lib.py:153] step: 434750, training_loss: 1.22463e+02
I1111 21:42:56.653769 140264174335808 run_lib.py:153] step: 434800, training_loss: 1.34589e+02
I1111 21:43:06.813920 140264174335808 run_lib.py:153] step: 434850, training_loss: 1.40738e+02
I1111 21:43:16.813452 140264174335808 run_lib.py:153] step: 434900, training_loss: 1.68039e+02
I1111 21:43:26.981726 140264174335808 run_lib.py:153] step: 434950, training_loss: 1.39718e+02
I1111 21:43:36.945142 140264174335808 run_lib.py:153] step: 435000, training_loss: 1.33803e+02
I1111 21:43:37.047779 140264174335808 run_lib.py:166] step: 435000, eval_loss: 1.64493e+02
I1111 21:43:46.797863 140264174335808 run_lib.py:153] step: 435050, training_loss: 1.49040e+02
I1111 21:43:56.058744 140264174335808 run_lib.py:153] step: 435100, training_loss: 1.16159e+02
I1111 21:44:06.014927 140264174335808 run_lib.py:153] step: 435150, training_loss: 1.41509e+02
I1111 21:44:16.037586 140264174335808 run_lib.py:153] step: 435200, training_loss: 1.65704e+02
I1111 21:44:26.386344 140264174335808 run_lib.py:153] step: 435250, training_loss: 1.02312e+02
I1111 21:44:36.000794 140264174335808 run_lib.py:153] step: 435300, training_loss: 1.00308e+02
I1111 21:44:46.203822 140264174335808 run_lib.py:153] step: 435350, training_loss: 1.19060e+02
I1111 21:44:55.970924 140264174335808 run_lib.py:153] step: 435400, training_loss: 1.31324e+02
I1111 21:45:06.029161 140264174335808 run_lib.py:153] step: 435450, training_loss: 1.59310e+02
I1111 21:45:16.412650 140264174335808 run_lib.py:153] step: 435500, training_loss: 1.54042e+02
I1111 21:45:25.887265 140264174335808 run_lib.py:153] step: 435550, training_loss: 1.20119e+02
I1111 21:45:36.076848 140264174335808 run_lib.py:153] step: 435600, training_loss: 1.51871e+02
I1111 21:45:45.833217 140264174335808 run_lib.py:153] step: 435650, training_loss: 1.21692e+02
I1111 21:45:55.638384 140264174335808 run_lib.py:153] step: 435700, training_loss: 9.52897e+01
I1111 21:46:05.958720 140264174335808 run_lib.py:153] step: 435750, training_loss: 1.26090e+02
I1111 21:46:15.717113 140264174335808 run_lib.py:153] step: 435800, training_loss: 1.27599e+02
I1111 21:46:25.356599 140264174335808 run_lib.py:153] step: 435850, training_loss: 1.43751e+02
I1111 21:46:35.446481 140264174335808 run_lib.py:153] step: 435900, training_loss: 9.02130e+01
I1111 21:46:45.116464 140264174335808 run_lib.py:153] step: 435950, training_loss: 1.26410e+02
I1111 21:46:55.038653 140264174335808 run_lib.py:153] step: 436000, training_loss: 1.12894e+02
I1111 21:47:04.419976 140264174335808 run_lib.py:153] step: 436050, training_loss: 1.06790e+02
I1111 21:47:14.927101 140264174335808 run_lib.py:153] step: 436100, training_loss: 1.23518e+02
I1111 21:47:24.731591 140264174335808 run_lib.py:153] step: 436150, training_loss: 1.28420e+02
I1111 21:47:34.671582 140264174335808 run_lib.py:153] step: 436200, training_loss: 1.30191e+02
I1111 21:47:44.600146 140264174335808 run_lib.py:153] step: 436250, training_loss: 1.49081e+02
I1111 21:47:55.304352 140264174335808 run_lib.py:153] step: 436300, training_loss: 1.46992e+02
I1111 21:48:05.250520 140264174335808 run_lib.py:153] step: 436350, training_loss: 1.15136e+02
I1111 21:48:14.989204 140264174335808 run_lib.py:153] step: 436400, training_loss: 1.36430e+02
I1111 21:48:24.829591 140264174335808 run_lib.py:153] step: 436450, training_loss: 1.22368e+02
I1111 21:48:35.362707 140264174335808 run_lib.py:153] step: 436500, training_loss: 1.32503e+02
I1111 21:48:45.495910 140264174335808 run_lib.py:153] step: 436550, training_loss: 1.33300e+02
I1111 21:48:55.791213 140264174335808 run_lib.py:153] step: 436600, training_loss: 1.38014e+02
I1111 21:49:06.363018 140264174335808 run_lib.py:153] step: 436650, training_loss: 1.15592e+02
I1111 21:49:16.186890 140264174335808 run_lib.py:153] step: 436700, training_loss: 1.18613e+02
I1111 21:49:25.619374 140264174335808 run_lib.py:153] step: 436750, training_loss: 1.42127e+02
I1111 21:49:35.203938 140264174335808 run_lib.py:153] step: 436800, training_loss: 1.57319e+02
I1111 21:49:44.980335 140264174335808 run_lib.py:153] step: 436850, training_loss: 1.06068e+02
I1111 21:49:54.727165 140264174335808 run_lib.py:153] step: 436900, training_loss: 1.54663e+02
I1111 21:50:05.333933 140264174335808 run_lib.py:153] step: 436950, training_loss: 1.35194e+02
I1111 21:50:15.294419 140264174335808 run_lib.py:153] step: 437000, training_loss: 1.16331e+02
I1111 21:50:25.316541 140264174335808 run_lib.py:153] step: 437050, training_loss: 1.40848e+02
I1111 21:50:35.035825 140264174335808 run_lib.py:153] step: 437100, training_loss: 1.17779e+02
I1111 21:50:46.032100 140264174335808 run_lib.py:153] step: 437150, training_loss: 9.44747e+01
I1111 21:50:56.323495 140264174335808 run_lib.py:153] step: 437200, training_loss: 1.51631e+02
I1111 21:51:06.287553 140264174335808 run_lib.py:153] step: 437250, training_loss: 9.45145e+01
I1111 21:51:15.955223 140264174335808 run_lib.py:153] step: 437300, training_loss: 1.09947e+02
I1111 21:51:26.168872 140264174335808 run_lib.py:153] step: 437350, training_loss: 1.23727e+02
I1111 21:51:36.694467 140264174335808 run_lib.py:153] step: 437400, training_loss: 1.31166e+02
I1111 21:51:46.390951 140264174335808 run_lib.py:153] step: 437450, training_loss: 1.19176e+02
I1111 21:51:56.170423 140264174335808 run_lib.py:153] step: 437500, training_loss: 1.42813e+02
I1111 21:52:05.926579 140264174335808 run_lib.py:153] step: 437550, training_loss: 1.31234e+02
I1111 21:52:16.456309 140264174335808 run_lib.py:153] step: 437600, training_loss: 1.04454e+02
I1111 21:52:25.873694 140264174335808 run_lib.py:153] step: 437650, training_loss: 1.15574e+02
I1111 21:52:36.132374 140264174335808 run_lib.py:153] step: 437700, training_loss: 1.33277e+02
I1111 21:52:45.946654 140264174335808 run_lib.py:153] step: 437750, training_loss: 1.32016e+02
I1111 21:52:56.002981 140264174335808 run_lib.py:153] step: 437800, training_loss: 1.36305e+02
I1111 21:53:06.349755 140264174335808 run_lib.py:153] step: 437850, training_loss: 1.19362e+02
I1111 21:53:16.678788 140264174335808 run_lib.py:153] step: 437900, training_loss: 1.27925e+02
I1111 21:53:27.240845 140264174335808 run_lib.py:153] step: 437950, training_loss: 1.23192e+02
I1111 21:53:37.702754 140264174335808 run_lib.py:153] step: 438000, training_loss: 1.19212e+02
I1111 21:53:47.763263 140264174335808 run_lib.py:153] step: 438050, training_loss: 1.07358e+02
I1111 21:53:57.701838 140264174335808 run_lib.py:153] step: 438100, training_loss: 1.25695e+02
I1111 21:54:07.386864 140264174335808 run_lib.py:153] step: 438150, training_loss: 1.34908e+02
I1111 21:54:17.637086 140264174335808 run_lib.py:153] step: 438200, training_loss: 1.15159e+02
I1111 21:54:28.050312 140264174335808 run_lib.py:153] step: 438250, training_loss: 1.19473e+02
I1111 21:54:37.915036 140264174335808 run_lib.py:153] step: 438300, training_loss: 1.36760e+02
I1111 21:54:48.108632 140264174335808 run_lib.py:153] step: 438350, training_loss: 1.17027e+02
I1111 21:54:58.502157 140264174335808 run_lib.py:153] step: 438400, training_loss: 1.29227e+02
I1111 21:55:09.093471 140264174335808 run_lib.py:153] step: 438450, training_loss: 1.22864e+02
I1111 21:55:19.030574 140264174335808 run_lib.py:153] step: 438500, training_loss: 1.16885e+02
I1111 21:55:28.510997 140264174335808 run_lib.py:153] step: 438550, training_loss: 1.42658e+02
I1111 21:55:38.731391 140264174335808 run_lib.py:153] step: 438600, training_loss: 1.19582e+02
I1111 21:55:48.807081 140264174335808 run_lib.py:153] step: 438650, training_loss: 9.98678e+01
I1111 21:55:58.805162 140264174335808 run_lib.py:153] step: 438700, training_loss: 1.29706e+02
I1111 21:56:08.420718 140264174335808 run_lib.py:153] step: 438750, training_loss: 1.23974e+02
I1111 21:56:18.104727 140264174335808 run_lib.py:153] step: 438800, training_loss: 1.57801e+02
I1111 21:56:28.306184 140264174335808 run_lib.py:153] step: 438850, training_loss: 1.30359e+02
I1111 21:56:38.119215 140264174335808 run_lib.py:153] step: 438900, training_loss: 1.22044e+02
I1111 21:56:47.642834 140264174335808 run_lib.py:153] step: 438950, training_loss: 1.62321e+02
I1111 21:56:57.867084 140264174335808 run_lib.py:153] step: 439000, training_loss: 1.07568e+02
I1111 21:57:07.819078 140264174335808 run_lib.py:153] step: 439050, training_loss: 1.24301e+02
I1111 21:57:17.036856 140264174335808 run_lib.py:153] step: 439100, training_loss: 1.49208e+02
I1111 21:57:27.252959 140264174335808 run_lib.py:153] step: 439150, training_loss: 1.26602e+02
I1111 21:57:37.470732 140264174335808 run_lib.py:153] step: 439200, training_loss: 1.20109e+02
I1111 21:57:47.843907 140264174335808 run_lib.py:153] step: 439250, training_loss: 1.33289e+02
I1111 21:57:57.584952 140264174335808 run_lib.py:153] step: 439300, training_loss: 1.09208e+02
I1111 21:58:07.796915 140264174335808 run_lib.py:153] step: 439350, training_loss: 1.27351e+02
I1111 21:58:18.113483 140264174335808 run_lib.py:153] step: 439400, training_loss: 1.45123e+02
I1111 21:58:28.183142 140264174335808 run_lib.py:153] step: 439450, training_loss: 1.52306e+02
I1111 21:58:37.912007 140264174335808 run_lib.py:153] step: 439500, training_loss: 1.25915e+02
I1111 21:58:48.342104 140264174335808 run_lib.py:153] step: 439550, training_loss: 1.23172e+02
I1111 21:58:59.071475 140264174335808 run_lib.py:153] step: 439600, training_loss: 1.24927e+02
I1111 21:59:09.399556 140264174335808 run_lib.py:153] step: 439650, training_loss: 1.21684e+02
I1111 21:59:19.185897 140264174335808 run_lib.py:153] step: 439700, training_loss: 1.34848e+02
I1111 21:59:29.223626 140264174335808 run_lib.py:153] step: 439750, training_loss: 1.38241e+02
I1111 21:59:38.798932 140264174335808 run_lib.py:153] step: 439800, training_loss: 1.22685e+02
I1111 21:59:49.282576 140264174335808 run_lib.py:153] step: 439850, training_loss: 1.27258e+02
I1111 22:00:00.090055 140264174335808 run_lib.py:153] step: 439900, training_loss: 1.40195e+02
I1111 22:00:10.919042 140264174335808 run_lib.py:153] step: 439950, training_loss: 1.23823e+02
I1111 22:00:20.799290 140264174335808 run_lib.py:153] step: 440000, training_loss: 1.25657e+02
I1111 22:00:21.376948 140264174335808 run_lib.py:166] step: 440000, eval_loss: 1.15312e+02
I1111 22:00:31.640001 140264174335808 run_lib.py:153] step: 440050, training_loss: 1.50352e+02
I1111 22:00:41.245130 140264174335808 run_lib.py:153] step: 440100, training_loss: 1.28219e+02
I1111 22:00:51.676289 140264174335808 run_lib.py:153] step: 440150, training_loss: 1.37364e+02
I1111 22:01:01.626927 140264174335808 run_lib.py:153] step: 440200, training_loss: 1.19063e+02
I1111 22:01:11.222683 140264174335808 run_lib.py:153] step: 440250, training_loss: 1.23401e+02
I1111 22:01:20.874820 140264174335808 run_lib.py:153] step: 440300, training_loss: 1.25354e+02
I1111 22:01:31.312170 140264174335808 run_lib.py:153] step: 440350, training_loss: 1.05291e+02
I1111 22:01:42.045662 140264174335808 run_lib.py:153] step: 440400, training_loss: 1.22142e+02
I1111 22:01:52.265101 140264174335808 run_lib.py:153] step: 440450, training_loss: 1.21269e+02
I1111 22:02:02.221618 140264174335808 run_lib.py:153] step: 440500, training_loss: 1.07431e+02
I1111 22:02:11.794781 140264174335808 run_lib.py:153] step: 440550, training_loss: 1.28386e+02
I1111 22:02:22.208907 140264174335808 run_lib.py:153] step: 440600, training_loss: 1.06913e+02
I1111 22:02:32.274019 140264174335808 run_lib.py:153] step: 440650, training_loss: 1.50067e+02
I1111 22:02:42.502655 140264174335808 run_lib.py:153] step: 440700, training_loss: 1.66244e+02
I1111 22:02:51.995228 140264174335808 run_lib.py:153] step: 440750, training_loss: 9.68584e+01
I1111 22:03:02.126498 140264174335808 run_lib.py:153] step: 440800, training_loss: 1.38349e+02
I1111 22:03:11.938726 140264174335808 run_lib.py:153] step: 440850, training_loss: 1.25759e+02
I1111 22:03:21.617421 140264174335808 run_lib.py:153] step: 440900, training_loss: 1.29844e+02
I1111 22:03:32.052147 140264174335808 run_lib.py:153] step: 440950, training_loss: 1.48293e+02
I1111 22:03:42.187532 140264174335808 run_lib.py:153] step: 441000, training_loss: 1.16261e+02
I1111 22:03:52.690174 140264174335808 run_lib.py:153] step: 441050, training_loss: 1.34036e+02
I1111 22:04:02.820760 140264174335808 run_lib.py:153] step: 441100, training_loss: 1.19158e+02
I1111 22:04:12.813341 140264174335808 run_lib.py:153] step: 441150, training_loss: 1.12484e+02
I1111 22:04:23.157723 140264174335808 run_lib.py:153] step: 441200, training_loss: 1.13476e+02
I1111 22:04:33.312409 140264174335808 run_lib.py:153] step: 441250, training_loss: 1.04982e+02
I1111 22:04:43.611802 140264174335808 run_lib.py:153] step: 441300, training_loss: 1.35717e+02
I1111 22:04:53.409796 140264174335808 run_lib.py:153] step: 441350, training_loss: 1.20348e+02
I1111 22:05:03.262597 140264174335808 run_lib.py:153] step: 441400, training_loss: 1.11848e+02
I1111 22:05:13.350461 140264174335808 run_lib.py:153] step: 441450, training_loss: 1.40905e+02
I1111 22:05:22.951211 140264174335808 run_lib.py:153] step: 441500, training_loss: 1.12210e+02
I1111 22:05:32.382889 140264174335808 run_lib.py:153] step: 441550, training_loss: 1.16741e+02
I1111 22:05:42.234238 140264174335808 run_lib.py:153] step: 441600, training_loss: 1.23537e+02
I1111 22:05:52.385311 140264174335808 run_lib.py:153] step: 441650, training_loss: 8.15971e+01
I1111 22:06:02.165352 140264174335808 run_lib.py:153] step: 441700, training_loss: 1.40094e+02
I1111 22:06:13.084479 140264174335808 run_lib.py:153] step: 441750, training_loss: 1.13953e+02
I1111 22:06:23.556811 140264174335808 run_lib.py:153] step: 441800, training_loss: 1.24030e+02
I1111 22:06:33.789973 140264174335808 run_lib.py:153] step: 441850, training_loss: 1.07035e+02
I1111 22:06:44.073472 140264174335808 run_lib.py:153] step: 441900, training_loss: 1.05948e+02
I1111 22:06:54.837151 140264174335808 run_lib.py:153] step: 441950, training_loss: 1.41110e+02
I1111 22:07:05.217608 140264174335808 run_lib.py:153] step: 442000, training_loss: 1.27013e+02
I1111 22:07:15.185179 140264174335808 run_lib.py:153] step: 442050, training_loss: 1.08993e+02
I1111 22:07:24.909665 140264174335808 run_lib.py:153] step: 442100, training_loss: 1.28794e+02
I1111 22:07:35.384925 140264174335808 run_lib.py:153] step: 442150, training_loss: 1.37898e+02
I1111 22:07:45.087661 140264174335808 run_lib.py:153] step: 442200, training_loss: 1.16452e+02
I1111 22:07:54.830056 140264174335808 run_lib.py:153] step: 442250, training_loss: 1.12821e+02
I1111 22:08:05.032728 140264174335808 run_lib.py:153] step: 442300, training_loss: 1.35498e+02
I1111 22:08:14.948503 140264174335808 run_lib.py:153] step: 442350, training_loss: 1.24664e+02
I1111 22:08:24.969377 140264174335808 run_lib.py:153] step: 442400, training_loss: 1.26390e+02
I1111 22:08:34.855738 140264174335808 run_lib.py:153] step: 442450, training_loss: 1.06326e+02
I1111 22:08:44.559982 140264174335808 run_lib.py:153] step: 442500, training_loss: 1.26100e+02
I1111 22:08:54.857026 140264174335808 run_lib.py:153] step: 442550, training_loss: 1.23657e+02
I1111 22:09:05.195269 140264174335808 run_lib.py:153] step: 442600, training_loss: 1.42956e+02
I1111 22:09:15.505822 140264174335808 run_lib.py:153] step: 442650, training_loss: 1.33674e+02
I1111 22:09:24.970700 140264174335808 run_lib.py:153] step: 442700, training_loss: 1.05238e+02
I1111 22:09:34.358591 140264174335808 run_lib.py:153] step: 442750, training_loss: 1.37761e+02
I1111 22:09:43.629416 140264174335808 run_lib.py:153] step: 442800, training_loss: 1.24066e+02
I1111 22:09:52.864802 140264174335808 run_lib.py:153] step: 442850, training_loss: 1.28784e+02
I1111 22:10:02.800070 140264174335808 run_lib.py:153] step: 442900, training_loss: 1.17596e+02
I1111 22:10:12.852216 140264174335808 run_lib.py:153] step: 442950, training_loss: 1.47174e+02
I1111 22:10:22.639617 140264174335808 run_lib.py:153] step: 443000, training_loss: 1.16459e+02
I1111 22:10:32.679348 140264174335808 run_lib.py:153] step: 443050, training_loss: 1.25361e+02
I1111 22:10:42.938566 140264174335808 run_lib.py:153] step: 443100, training_loss: 1.29601e+02
I1111 22:10:52.482723 140264174335808 run_lib.py:153] step: 443150, training_loss: 1.41385e+02
I1111 22:11:01.882415 140264174335808 run_lib.py:153] step: 443200, training_loss: 1.29701e+02
I1111 22:11:12.082100 140264174335808 run_lib.py:153] step: 443250, training_loss: 1.32843e+02
I1111 22:11:22.315569 140264174335808 run_lib.py:153] step: 443300, training_loss: 1.65136e+02
I1111 22:11:32.795601 140264174335808 run_lib.py:153] step: 443350, training_loss: 1.42069e+02
I1111 22:11:42.881597 140264174335808 run_lib.py:153] step: 443400, training_loss: 1.26003e+02
I1111 22:11:53.383201 140264174335808 run_lib.py:153] step: 443450, training_loss: 1.19719e+02
I1111 22:12:04.050345 140264174335808 run_lib.py:153] step: 443500, training_loss: 1.46932e+02
I1111 22:12:14.157470 140264174335808 run_lib.py:153] step: 443550, training_loss: 1.22573e+02
I1111 22:12:25.388230 140264174335808 run_lib.py:153] step: 443600, training_loss: 1.21309e+02
I1111 22:12:35.850591 140264174335808 run_lib.py:153] step: 443650, training_loss: 1.45619e+02
I1111 22:12:45.498164 140264174335808 run_lib.py:153] step: 443700, training_loss: 1.25533e+02
I1111 22:12:55.434825 140264174335808 run_lib.py:153] step: 443750, training_loss: 9.39222e+01
I1111 22:13:05.167243 140264174335808 run_lib.py:153] step: 443800, training_loss: 1.26800e+02
I1111 22:13:15.808600 140264174335808 run_lib.py:153] step: 443850, training_loss: 1.48471e+02
I1111 22:13:26.355894 140264174335808 run_lib.py:153] step: 443900, training_loss: 1.29836e+02
I1111 22:13:36.676329 140264174335808 run_lib.py:153] step: 443950, training_loss: 1.34265e+02
I1111 22:13:46.675684 140264174335808 run_lib.py:153] step: 444000, training_loss: 1.39200e+02
I1111 22:13:56.907910 140264174335808 run_lib.py:153] step: 444050, training_loss: 1.28204e+02
I1111 22:14:08.011369 140264174335808 run_lib.py:153] step: 444100, training_loss: 1.58587e+02
I1111 22:14:18.376673 140264174335808 run_lib.py:153] step: 444150, training_loss: 1.30867e+02
I1111 22:14:28.220929 140264174335808 run_lib.py:153] step: 444200, training_loss: 1.14558e+02
I1111 22:14:37.622995 140264174335808 run_lib.py:153] step: 444250, training_loss: 1.17361e+02
I1111 22:14:47.895677 140264174335808 run_lib.py:153] step: 444300, training_loss: 1.30976e+02
I1111 22:14:57.658881 140264174335808 run_lib.py:153] step: 444350, training_loss: 1.20741e+02
I1111 22:15:07.634797 140264174335808 run_lib.py:153] step: 444400, training_loss: 1.32730e+02
I1111 22:15:17.361660 140264174335808 run_lib.py:153] step: 444450, training_loss: 1.09894e+02
I1111 22:15:27.474753 140264174335808 run_lib.py:153] step: 444500, training_loss: 1.21853e+02
I1111 22:15:37.775156 140264174335808 run_lib.py:153] step: 444550, training_loss: 9.79606e+01
I1111 22:15:47.414904 140264174335808 run_lib.py:153] step: 444600, training_loss: 1.17869e+02
I1111 22:15:56.898431 140264174335808 run_lib.py:153] step: 444650, training_loss: 1.02517e+02
I1111 22:16:07.315532 140264174335808 run_lib.py:153] step: 444700, training_loss: 1.44547e+02
I1111 22:16:17.389013 140264174335808 run_lib.py:153] step: 444750, training_loss: 9.94790e+01
I1111 22:16:27.194571 140264174335808 run_lib.py:153] step: 444800, training_loss: 1.65289e+02
I1111 22:16:36.961922 140264174335808 run_lib.py:153] step: 444850, training_loss: 1.48972e+02
I1111 22:16:46.774302 140264174335808 run_lib.py:153] step: 444900, training_loss: 1.31857e+02
I1111 22:16:56.881044 140264174335808 run_lib.py:153] step: 444950, training_loss: 1.20800e+02
I1111 22:17:07.148901 140264174335808 run_lib.py:153] step: 445000, training_loss: 1.35850e+02
I1111 22:17:07.286686 140264174335808 run_lib.py:166] step: 445000, eval_loss: 1.27163e+02
I1111 22:17:17.094870 140264174335808 run_lib.py:153] step: 445050, training_loss: 1.04312e+02
I1111 22:17:26.805506 140264174335808 run_lib.py:153] step: 445100, training_loss: 1.15259e+02
I1111 22:17:36.614838 140264174335808 run_lib.py:153] step: 445150, training_loss: 1.68716e+02
I1111 22:17:46.579825 140264174335808 run_lib.py:153] step: 445200, training_loss: 1.49887e+02
I1111 22:17:57.069393 140264174335808 run_lib.py:153] step: 445250, training_loss: 1.19250e+02
I1111 22:18:07.215161 140264174335808 run_lib.py:153] step: 445300, training_loss: 1.15527e+02
I1111 22:18:17.124699 140264174335808 run_lib.py:153] step: 445350, training_loss: 1.17678e+02
I1111 22:18:27.810686 140264174335808 run_lib.py:153] step: 445400, training_loss: 1.31027e+02
I1111 22:18:38.337382 140264174335808 run_lib.py:153] step: 445450, training_loss: 1.56445e+02
I1111 22:18:49.056433 140264174335808 run_lib.py:153] step: 445500, training_loss: 1.48937e+02
I1111 22:18:59.929738 140264174335808 run_lib.py:153] step: 445550, training_loss: 1.03396e+02
I1111 22:19:10.010370 140264174335808 run_lib.py:153] step: 445600, training_loss: 1.61605e+02
I1111 22:19:20.049169 140264174335808 run_lib.py:153] step: 445650, training_loss: 1.29258e+02
I1111 22:19:30.067968 140264174335808 run_lib.py:153] step: 445700, training_loss: 1.30649e+02
I1111 22:19:40.068775 140264174335808 run_lib.py:153] step: 445750, training_loss: 1.13791e+02
I1111 22:19:49.650243 140264174335808 run_lib.py:153] step: 445800, training_loss: 1.59298e+02
I1111 22:19:59.130306 140264174335808 run_lib.py:153] step: 445850, training_loss: 1.60344e+02
I1111 22:20:08.837356 140264174335808 run_lib.py:153] step: 445900, training_loss: 9.96570e+01
I1111 22:20:19.465708 140264174335808 run_lib.py:153] step: 445950, training_loss: 1.37932e+02
I1111 22:20:29.820206 140264174335808 run_lib.py:153] step: 446000, training_loss: 9.59320e+01
I1111 22:20:39.983480 140264174335808 run_lib.py:153] step: 446050, training_loss: 1.38696e+02
I1111 22:20:50.127693 140264174335808 run_lib.py:153] step: 446100, training_loss: 1.38361e+02
I1111 22:20:59.979839 140264174335808 run_lib.py:153] step: 446150, training_loss: 1.13008e+02
I1111 22:21:09.662168 140264174335808 run_lib.py:153] step: 446200, training_loss: 1.12484e+02
I1111 22:21:20.014506 140264174335808 run_lib.py:153] step: 446250, training_loss: 1.57790e+02
I1111 22:21:30.093105 140264174335808 run_lib.py:153] step: 446300, training_loss: 1.09264e+02
I1111 22:21:39.861207 140264174335808 run_lib.py:153] step: 446350, training_loss: 1.27651e+02
I1111 22:21:49.805538 140264174335808 run_lib.py:153] step: 446400, training_loss: 9.66951e+01
I1111 22:21:59.917505 140264174335808 run_lib.py:153] step: 446450, training_loss: 1.31196e+02
I1111 22:22:10.572760 140264174335808 run_lib.py:153] step: 446500, training_loss: 1.14485e+02
I1111 22:22:21.148517 140264174335808 run_lib.py:153] step: 446550, training_loss: 1.37988e+02
I1111 22:22:31.285775 140264174335808 run_lib.py:153] step: 446600, training_loss: 9.95084e+01
I1111 22:22:41.963474 140264174335808 run_lib.py:153] step: 446650, training_loss: 1.39474e+02
I1111 22:22:51.630492 140264174335808 run_lib.py:153] step: 446700, training_loss: 1.19092e+02
I1111 22:23:01.237125 140264174335808 run_lib.py:153] step: 446750, training_loss: 1.62330e+02
I1111 22:23:11.052210 140264174335808 run_lib.py:153] step: 446800, training_loss: 1.05678e+02
I1111 22:23:21.568978 140264174335808 run_lib.py:153] step: 446850, training_loss: 1.31294e+02
I1111 22:23:31.841995 140264174335808 run_lib.py:153] step: 446900, training_loss: 1.38738e+02
I1111 22:23:42.838889 140264174335808 run_lib.py:153] step: 446950, training_loss: 1.11248e+02
I1111 22:23:53.063143 140264174335808 run_lib.py:153] step: 447000, training_loss: 1.26213e+02
I1111 22:24:03.397328 140264174335808 run_lib.py:153] step: 447050, training_loss: 1.01744e+02
I1111 22:24:13.256612 140264174335808 run_lib.py:153] step: 447100, training_loss: 1.26483e+02
I1111 22:24:23.365651 140264174335808 run_lib.py:153] step: 447150, training_loss: 1.16861e+02
I1111 22:24:34.026212 140264174335808 run_lib.py:153] step: 447200, training_loss: 1.04014e+02
I1111 22:24:44.866115 140264174335808 run_lib.py:153] step: 447250, training_loss: 1.44688e+02
I1111 22:24:54.636004 140264174335808 run_lib.py:153] step: 447300, training_loss: 9.82888e+01
I1111 22:25:04.345276 140264174335808 run_lib.py:153] step: 447350, training_loss: 1.17409e+02
I1111 22:25:13.948073 140264174335808 run_lib.py:153] step: 447400, training_loss: 1.47872e+02
I1111 22:25:23.644210 140264174335808 run_lib.py:153] step: 447450, training_loss: 1.32908e+02
I1111 22:25:33.767266 140264174335808 run_lib.py:153] step: 447500, training_loss: 1.30480e+02
I1111 22:25:43.868126 140264174335808 run_lib.py:153] step: 447550, training_loss: 1.16691e+02
I1111 22:25:53.769320 140264174335808 run_lib.py:153] step: 447600, training_loss: 1.14801e+02
I1111 22:26:03.300581 140264174335808 run_lib.py:153] step: 447650, training_loss: 1.29294e+02
I1111 22:26:13.188674 140264174335808 run_lib.py:153] step: 447700, training_loss: 1.25850e+02
I1111 22:26:23.185342 140264174335808 run_lib.py:153] step: 447750, training_loss: 1.02077e+02
I1111 22:26:33.390583 140264174335808 run_lib.py:153] step: 447800, training_loss: 1.19615e+02
I1111 22:26:43.799913 140264174335808 run_lib.py:153] step: 447850, training_loss: 1.09000e+02
I1111 22:26:54.386615 140264174335808 run_lib.py:153] step: 447900, training_loss: 1.48646e+02
I1111 22:27:04.116210 140264174335808 run_lib.py:153] step: 447950, training_loss: 1.43541e+02
I1111 22:27:14.591127 140264174335808 run_lib.py:153] step: 448000, training_loss: 1.37398e+02
I1111 22:27:24.135684 140264174335808 run_lib.py:153] step: 448050, training_loss: 1.03455e+02
I1111 22:27:33.968345 140264174335808 run_lib.py:153] step: 448100, training_loss: 1.21958e+02
I1111 22:27:44.108511 140264174335808 run_lib.py:153] step: 448150, training_loss: 1.31145e+02
I1111 22:27:54.992178 140264174335808 run_lib.py:153] step: 448200, training_loss: 9.98161e+01
I1111 22:28:04.579182 140264174335808 run_lib.py:153] step: 448250, training_loss: 1.19598e+02
I1111 22:28:14.180090 140264174335808 run_lib.py:153] step: 448300, training_loss: 1.25736e+02
I1111 22:28:24.511237 140264174335808 run_lib.py:153] step: 448350, training_loss: 1.32543e+02
I1111 22:28:34.384021 140264174335808 run_lib.py:153] step: 448400, training_loss: 1.31560e+02
I1111 22:28:45.084341 140264174335808 run_lib.py:153] step: 448450, training_loss: 1.47397e+02
I1111 22:28:54.979861 140264174335808 run_lib.py:153] step: 448500, training_loss: 8.82005e+01
I1111 22:29:04.730147 140264174335808 run_lib.py:153] step: 448550, training_loss: 1.31605e+02
I1111 22:29:14.976876 140264174335808 run_lib.py:153] step: 448600, training_loss: 1.32177e+02
I1111 22:29:25.243770 140264174335808 run_lib.py:153] step: 448650, training_loss: 1.41489e+02
I1111 22:29:35.364123 140264174335808 run_lib.py:153] step: 448700, training_loss: 1.09697e+02
I1111 22:29:45.296376 140264174335808 run_lib.py:153] step: 448750, training_loss: 1.10858e+02
I1111 22:29:54.917129 140264174335808 run_lib.py:153] step: 448800, training_loss: 1.26874e+02
I1111 22:30:04.796004 140264174335808 run_lib.py:153] step: 448850, training_loss: 1.61129e+02
I1111 22:30:15.503288 140264174335808 run_lib.py:153] step: 448900, training_loss: 1.39815e+02
I1111 22:30:25.026680 140264174335808 run_lib.py:153] step: 448950, training_loss: 1.19889e+02
I1111 22:30:35.056699 140264174335808 run_lib.py:153] step: 449000, training_loss: 1.56380e+02
I1111 22:30:44.825756 140264174335808 run_lib.py:153] step: 449050, training_loss: 1.18825e+02
I1111 22:30:54.880407 140264174335808 run_lib.py:153] step: 449100, training_loss: 1.23049e+02
I1111 22:31:05.004770 140264174335808 run_lib.py:153] step: 449150, training_loss: 1.53913e+02
I1111 22:31:14.617762 140264174335808 run_lib.py:153] step: 449200, training_loss: 1.19038e+02
I1111 22:31:25.043488 140264174335808 run_lib.py:153] step: 449250, training_loss: 9.93808e+01
I1111 22:31:35.164549 140264174335808 run_lib.py:153] step: 449300, training_loss: 1.63368e+02
I1111 22:31:44.914813 140264174335808 run_lib.py:153] step: 449350, training_loss: 1.21932e+02
I1111 22:31:55.274322 140264174335808 run_lib.py:153] step: 449400, training_loss: 1.33507e+02
I1111 22:32:05.425832 140264174335808 run_lib.py:153] step: 449450, training_loss: 1.17103e+02
I1111 22:32:15.557129 140264174335808 run_lib.py:153] step: 449500, training_loss: 1.03635e+02
I1111 22:32:25.253386 140264174335808 run_lib.py:153] step: 449550, training_loss: 1.16733e+02
I1111 22:32:34.816142 140264174335808 run_lib.py:153] step: 449600, training_loss: 1.55364e+02
I1111 22:32:44.648073 140264174335808 run_lib.py:153] step: 449650, training_loss: 1.40965e+02
I1111 22:32:55.260679 140264174335808 run_lib.py:153] step: 449700, training_loss: 1.13854e+02
I1111 22:33:05.229789 140264174335808 run_lib.py:153] step: 449750, training_loss: 1.46798e+02
I1111 22:33:15.355615 140264174335808 run_lib.py:153] step: 449800, training_loss: 9.37781e+01
I1111 22:33:25.260445 140264174335808 run_lib.py:153] step: 449850, training_loss: 1.08874e+02
I1111 22:33:36.114937 140264174335808 run_lib.py:153] step: 449900, training_loss: 1.00940e+02
I1111 22:33:46.316308 140264174335808 run_lib.py:153] step: 449950, training_loss: 1.05778e+02
I1111 22:33:57.010089 140264174335808 run_lib.py:153] step: 450000, training_loss: 1.41124e+02
I1111 22:33:57.616788 140264174335808 run_lib.py:166] step: 450000, eval_loss: 1.45060e+02
I1111 22:34:08.178599 140264174335808 run_lib.py:153] step: 450050, training_loss: 9.60078e+01
I1111 22:34:18.079087 140264174335808 run_lib.py:153] step: 450100, training_loss: 9.81423e+01
I1111 22:34:27.840873 140264174335808 run_lib.py:153] step: 450150, training_loss: 1.26545e+02
I1111 22:34:37.904179 140264174335808 run_lib.py:153] step: 450200, training_loss: 9.36863e+01
I1111 22:34:47.993876 140264174335808 run_lib.py:153] step: 450250, training_loss: 1.16304e+02
I1111 22:34:57.416568 140264174335808 run_lib.py:153] step: 450300, training_loss: 1.35055e+02
I1111 22:35:07.412048 140264174335808 run_lib.py:153] step: 450350, training_loss: 1.20379e+02
I1111 22:35:17.509158 140264174335808 run_lib.py:153] step: 450400, training_loss: 1.15206e+02
I1111 22:35:27.922060 140264174335808 run_lib.py:153] step: 450450, training_loss: 1.37556e+02
I1111 22:35:37.673362 140264174335808 run_lib.py:153] step: 450500, training_loss: 1.25381e+02
I1111 22:35:47.585677 140264174335808 run_lib.py:153] step: 450550, training_loss: 1.13294e+02
I1111 22:35:57.900133 140264174335808 run_lib.py:153] step: 450600, training_loss: 1.25334e+02
I1111 22:36:07.704022 140264174335808 run_lib.py:153] step: 450650, training_loss: 1.34132e+02
I1111 22:36:17.772900 140264174335808 run_lib.py:153] step: 450700, training_loss: 1.13230e+02
I1111 22:36:27.404947 140264174335808 run_lib.py:153] step: 450750, training_loss: 1.06142e+02
I1111 22:36:37.257047 140264174335808 run_lib.py:153] step: 450800, training_loss: 1.14617e+02
I1111 22:36:47.005731 140264174335808 run_lib.py:153] step: 450850, training_loss: 1.30210e+02
I1111 22:36:56.977150 140264174335808 run_lib.py:153] step: 450900, training_loss: 1.38059e+02
I1111 22:37:08.145265 140264174335808 run_lib.py:153] step: 450950, training_loss: 1.07371e+02
I1111 22:37:18.190464 140264174335808 run_lib.py:153] step: 451000, training_loss: 1.38700e+02
I1111 22:37:28.239083 140264174335808 run_lib.py:153] step: 451050, training_loss: 9.95878e+01
I1111 22:37:39.094766 140264174335808 run_lib.py:153] step: 451100, training_loss: 1.18547e+02
I1111 22:37:48.696313 140264174335808 run_lib.py:153] step: 451150, training_loss: 1.28539e+02
I1111 22:37:58.446436 140264174335808 run_lib.py:153] step: 451200, training_loss: 1.21410e+02
I1111 22:38:08.965266 140264174335808 run_lib.py:153] step: 451250, training_loss: 1.50728e+02
I1111 22:38:19.391503 140264174335808 run_lib.py:153] step: 451300, training_loss: 1.14570e+02
I1111 22:38:30.458074 140264174335808 run_lib.py:153] step: 451350, training_loss: 1.46884e+02
I1111 22:38:40.798018 140264174335808 run_lib.py:153] step: 451400, training_loss: 1.16596e+02
I1111 22:38:50.535357 140264174335808 run_lib.py:153] step: 451450, training_loss: 1.25334e+02
I1111 22:39:01.086706 140264174335808 run_lib.py:153] step: 451500, training_loss: 1.49490e+02
I1111 22:39:11.352829 140264174335808 run_lib.py:153] step: 451550, training_loss: 1.22652e+02
I1111 22:39:21.514489 140264174335808 run_lib.py:153] step: 451600, training_loss: 1.21503e+02
I1111 22:39:31.728359 140264174335808 run_lib.py:153] step: 451650, training_loss: 1.23076e+02
I1111 22:39:41.476599 140264174335808 run_lib.py:153] step: 451700, training_loss: 1.15624e+02
I1111 22:39:51.367894 140264174335808 run_lib.py:153] step: 451750, training_loss: 1.44541e+02
I1111 22:40:00.972168 140264174335808 run_lib.py:153] step: 451800, training_loss: 1.45616e+02
I1111 22:40:11.856672 140264174335808 run_lib.py:153] step: 451850, training_loss: 1.35588e+02
I1111 22:40:21.995497 140264174335808 run_lib.py:153] step: 451900, training_loss: 1.32629e+02
I1111 22:40:31.593788 140264174335808 run_lib.py:153] step: 451950, training_loss: 1.31005e+02
I1111 22:40:42.134717 140264174335808 run_lib.py:153] step: 452000, training_loss: 9.00921e+01
I1111 22:40:52.523648 140264174335808 run_lib.py:153] step: 452050, training_loss: 1.10515e+02
I1111 22:41:02.453916 140264174335808 run_lib.py:153] step: 452100, training_loss: 1.17565e+02
I1111 22:41:12.725445 140264174335808 run_lib.py:153] step: 452150, training_loss: 1.21409e+02
I1111 22:41:22.889359 140264174335808 run_lib.py:153] step: 452200, training_loss: 1.25368e+02
I1111 22:41:32.316614 140264174335808 run_lib.py:153] step: 452250, training_loss: 1.52990e+02
I1111 22:41:42.746099 140264174335808 run_lib.py:153] step: 452300, training_loss: 1.35433e+02
I1111 22:41:52.204041 140264174335808 run_lib.py:153] step: 452350, training_loss: 1.36386e+02
I1111 22:42:02.963085 140264174335808 run_lib.py:153] step: 452400, training_loss: 1.06927e+02
I1111 22:42:12.875970 140264174335808 run_lib.py:153] step: 452450, training_loss: 1.44774e+02
I1111 22:42:22.689692 140264174335808 run_lib.py:153] step: 452500, training_loss: 1.19187e+02
I1111 22:42:32.559937 140264174335808 run_lib.py:153] step: 452550, training_loss: 1.67882e+02
I1111 22:42:42.756473 140264174335808 run_lib.py:153] step: 452600, training_loss: 1.38793e+02
I1111 22:42:52.873249 140264174335808 run_lib.py:153] step: 452650, training_loss: 1.51902e+02
I1111 22:43:04.054419 140264174335808 run_lib.py:153] step: 452700, training_loss: 1.14838e+02
I1111 22:43:14.027347 140264174335808 run_lib.py:153] step: 452750, training_loss: 1.20442e+02
I1111 22:43:23.801808 140264174335808 run_lib.py:153] step: 452800, training_loss: 1.10435e+02
I1111 22:43:35.190025 140264174335808 run_lib.py:153] step: 452850, training_loss: 1.28214e+02
I1111 22:43:46.100280 140264174335808 run_lib.py:153] step: 452900, training_loss: 1.18759e+02
I1111 22:43:57.028832 140264174335808 run_lib.py:153] step: 452950, training_loss: 1.16201e+02
I1111 22:44:07.779979 140264174335808 run_lib.py:153] step: 453000, training_loss: 1.22865e+02
I1111 22:44:18.113821 140264174335808 run_lib.py:153] step: 453050, training_loss: 1.28295e+02
I1111 22:44:28.297298 140264174335808 run_lib.py:153] step: 453100, training_loss: 1.56646e+02
I1111 22:44:38.105970 140264174335808 run_lib.py:153] step: 453150, training_loss: 1.23796e+02
I1111 22:44:47.941560 140264174335808 run_lib.py:153] step: 453200, training_loss: 1.18166e+02
I1111 22:44:58.393877 140264174335808 run_lib.py:153] step: 453250, training_loss: 1.37454e+02
I1111 22:45:08.810806 140264174335808 run_lib.py:153] step: 453300, training_loss: 1.20161e+02
I1111 22:45:18.573752 140264174335808 run_lib.py:153] step: 453350, training_loss: 1.10643e+02
I1111 22:45:28.321607 140264174335808 run_lib.py:153] step: 453400, training_loss: 1.22142e+02
I1111 22:45:38.667920 140264174335808 run_lib.py:153] step: 453450, training_loss: 1.06486e+02
I1111 22:45:49.537122 140264174335808 run_lib.py:153] step: 453500, training_loss: 1.39302e+02
I1111 22:45:59.543803 140264174335808 run_lib.py:153] step: 453550, training_loss: 1.55888e+02
I1111 22:46:09.889631 140264174335808 run_lib.py:153] step: 453600, training_loss: 1.29482e+02
I1111 22:46:19.855045 140264174335808 run_lib.py:153] step: 453650, training_loss: 1.29984e+02
I1111 22:46:30.286206 140264174335808 run_lib.py:153] step: 453700, training_loss: 1.13483e+02
I1111 22:46:39.905942 140264174335808 run_lib.py:153] step: 453750, training_loss: 1.02461e+02
I1111 22:46:49.481147 140264174335808 run_lib.py:153] step: 453800, training_loss: 1.14145e+02
I1111 22:46:59.548288 140264174335808 run_lib.py:153] step: 453850, training_loss: 1.24678e+02
I1111 22:47:09.787824 140264174335808 run_lib.py:153] step: 453900, training_loss: 1.19010e+02
I1111 22:47:20.354714 140264174335808 run_lib.py:153] step: 453950, training_loss: 1.34519e+02
I1111 22:47:30.083939 140264174335808 run_lib.py:153] step: 454000, training_loss: 1.09634e+02
I1111 22:47:39.941631 140264174335808 run_lib.py:153] step: 454050, training_loss: 1.15721e+02
I1111 22:47:50.719388 140264174335808 run_lib.py:153] step: 454100, training_loss: 1.38313e+02
I1111 22:48:00.397587 140264174335808 run_lib.py:153] step: 454150, training_loss: 1.29945e+02
I1111 22:48:10.888261 140264174335808 run_lib.py:153] step: 454200, training_loss: 1.27200e+02
I1111 22:48:21.253632 140264174335808 run_lib.py:153] step: 454250, training_loss: 1.19011e+02
I1111 22:48:31.537072 140264174335808 run_lib.py:153] step: 454300, training_loss: 1.28230e+02
I1111 22:48:41.109698 140264174335808 run_lib.py:153] step: 454350, training_loss: 1.26532e+02
I1111 22:48:51.079155 140264174335808 run_lib.py:153] step: 454400, training_loss: 1.10426e+02
I1111 22:49:01.194275 140264174335808 run_lib.py:153] step: 454450, training_loss: 1.19899e+02
I1111 22:49:12.279644 140264174335808 run_lib.py:153] step: 454500, training_loss: 1.44034e+02
I1111 22:49:22.943058 140264174335808 run_lib.py:153] step: 454550, training_loss: 1.27812e+02
I1111 22:49:33.869026 140264174335808 run_lib.py:153] step: 454600, training_loss: 1.30740e+02
I1111 22:49:43.929136 140264174335808 run_lib.py:153] step: 454650, training_loss: 1.33464e+02
I1111 22:49:54.724237 140264174335808 run_lib.py:153] step: 454700, training_loss: 9.77708e+01
I1111 22:50:05.161110 140264174335808 run_lib.py:153] step: 454750, training_loss: 1.40320e+02
I1111 22:50:14.946161 140264174335808 run_lib.py:153] step: 454800, training_loss: 1.45102e+02
I1111 22:50:25.419920 140264174335808 run_lib.py:153] step: 454850, training_loss: 1.05605e+02
I1111 22:50:35.041966 140264174335808 run_lib.py:153] step: 454900, training_loss: 1.01055e+02
I1111 22:50:44.984595 140264174335808 run_lib.py:153] step: 454950, training_loss: 1.31166e+02
I1111 22:50:55.309172 140264174335808 run_lib.py:153] step: 455000, training_loss: 1.16461e+02
I1111 22:50:55.413330 140264174335808 run_lib.py:166] step: 455000, eval_loss: 1.35023e+02
I1111 22:51:05.495633 140264174335808 run_lib.py:153] step: 455050, training_loss: 1.19901e+02
I1111 22:51:15.396376 140264174335808 run_lib.py:153] step: 455100, training_loss: 1.15234e+02
I1111 22:51:25.202212 140264174335808 run_lib.py:153] step: 455150, training_loss: 1.22731e+02
I1111 22:51:36.227974 140264174335808 run_lib.py:153] step: 455200, training_loss: 1.23423e+02
I1111 22:51:46.273950 140264174335808 run_lib.py:153] step: 455250, training_loss: 9.95323e+01
I1111 22:51:56.114088 140264174335808 run_lib.py:153] step: 455300, training_loss: 1.24916e+02
I1111 22:52:06.368011 140264174335808 run_lib.py:153] step: 455350, training_loss: 1.29029e+02
I1111 22:52:16.632335 140264174335808 run_lib.py:153] step: 455400, training_loss: 1.17714e+02
I1111 22:52:26.875669 140264174335808 run_lib.py:153] step: 455450, training_loss: 1.63894e+02
I1111 22:52:37.243224 140264174335808 run_lib.py:153] step: 455500, training_loss: 1.05025e+02
I1111 22:52:46.810986 140264174335808 run_lib.py:153] step: 455550, training_loss: 1.26949e+02
I1111 22:52:57.208037 140264174335808 run_lib.py:153] step: 455600, training_loss: 1.40871e+02
I1111 22:53:06.841716 140264174335808 run_lib.py:153] step: 455650, training_loss: 1.18693e+02
I1111 22:53:17.182697 140264174335808 run_lib.py:153] step: 455700, training_loss: 1.31841e+02
I1111 22:53:26.858647 140264174335808 run_lib.py:153] step: 455750, training_loss: 1.03296e+02
I1111 22:53:36.727358 140264174335808 run_lib.py:153] step: 455800, training_loss: 1.23963e+02
I1111 22:53:47.817750 140264174335808 run_lib.py:153] step: 455850, training_loss: 1.24222e+02
I1111 22:53:58.040632 140264174335808 run_lib.py:153] step: 455900, training_loss: 1.09959e+02
I1111 22:54:07.942662 140264174335808 run_lib.py:153] step: 455950, training_loss: 1.16669e+02
I1111 22:54:18.006568 140264174335808 run_lib.py:153] step: 456000, training_loss: 9.90527e+01
I1111 22:54:28.502954 140264174335808 run_lib.py:153] step: 456050, training_loss: 1.29408e+02
I1111 22:54:38.515023 140264174335808 run_lib.py:153] step: 456100, training_loss: 1.05833e+02
I1111 22:54:48.381648 140264174335808 run_lib.py:153] step: 456150, training_loss: 1.58237e+02
I1111 22:54:58.392457 140264174335808 run_lib.py:153] step: 456200, training_loss: 1.51390e+02
I1111 22:55:08.588426 140264174335808 run_lib.py:153] step: 456250, training_loss: 1.34404e+02
I1111 22:55:18.936254 140264174335808 run_lib.py:153] step: 456300, training_loss: 1.38018e+02
I1111 22:55:28.473436 140264174335808 run_lib.py:153] step: 456350, training_loss: 1.14660e+02
I1111 22:55:38.348184 140264174335808 run_lib.py:153] step: 456400, training_loss: 1.21664e+02
I1111 22:55:47.849623 140264174335808 run_lib.py:153] step: 456450, training_loss: 1.29276e+02
I1111 22:55:58.077085 140264174335808 run_lib.py:153] step: 456500, training_loss: 1.20692e+02
I1111 22:56:08.016973 140264174335808 run_lib.py:153] step: 456550, training_loss: 1.59954e+02
I1111 22:56:18.358842 140264174335808 run_lib.py:153] step: 456600, training_loss: 1.31633e+02
I1111 22:56:29.150611 140264174335808 run_lib.py:153] step: 456650, training_loss: 1.16305e+02
I1111 22:56:39.067887 140264174335808 run_lib.py:153] step: 456700, training_loss: 1.20630e+02
I1111 22:56:49.014526 140264174335808 run_lib.py:153] step: 456750, training_loss: 1.05719e+02
I1111 22:56:59.552377 140264174335808 run_lib.py:153] step: 456800, training_loss: 1.02785e+02
I1111 22:57:09.927883 140264174335808 run_lib.py:153] step: 456850, training_loss: 1.47874e+02
I1111 22:57:20.368164 140264174335808 run_lib.py:153] step: 456900, training_loss: 1.15698e+02
I1111 22:57:30.994715 140264174335808 run_lib.py:153] step: 456950, training_loss: 1.28703e+02
I1111 22:57:41.586147 140264174335808 run_lib.py:153] step: 457000, training_loss: 1.54083e+02
I1111 22:57:52.130567 140264174335808 run_lib.py:153] step: 457050, training_loss: 1.71466e+02
I1111 22:58:02.554774 140264174335808 run_lib.py:153] step: 457100, training_loss: 1.05283e+02
I1111 22:58:12.906991 140264174335808 run_lib.py:153] step: 457150, training_loss: 1.21290e+02
I1111 22:58:23.017158 140264174335808 run_lib.py:153] step: 457200, training_loss: 1.42078e+02
I1111 22:58:33.378548 140264174335808 run_lib.py:153] step: 457250, training_loss: 1.45619e+02
I1111 22:58:43.485168 140264174335808 run_lib.py:153] step: 457300, training_loss: 1.32003e+02
I1111 22:58:53.505505 140264174335808 run_lib.py:153] step: 457350, training_loss: 1.33760e+02
I1111 22:59:04.062889 140264174335808 run_lib.py:153] step: 457400, training_loss: 1.40619e+02
I1111 22:59:14.540106 140264174335808 run_lib.py:153] step: 457450, training_loss: 1.30955e+02
I1111 22:59:24.879404 140264174335808 run_lib.py:153] step: 457500, training_loss: 1.03213e+02
I1111 22:59:35.269827 140264174335808 run_lib.py:153] step: 457550, training_loss: 1.18209e+02
I1111 22:59:45.772973 140264174335808 run_lib.py:153] step: 457600, training_loss: 1.32710e+02
I1111 22:59:56.246948 140264174335808 run_lib.py:153] step: 457650, training_loss: 1.21172e+02
I1111 23:00:05.793643 140264174335808 run_lib.py:153] step: 457700, training_loss: 1.40491e+02
I1111 23:00:15.919191 140264174335808 run_lib.py:153] step: 457750, training_loss: 1.09672e+02
I1111 23:00:25.970422 140264174335808 run_lib.py:153] step: 457800, training_loss: 1.27820e+02
I1111 23:00:35.477857 140264174335808 run_lib.py:153] step: 457850, training_loss: 1.09643e+02
I1111 23:00:45.550076 140264174335808 run_lib.py:153] step: 457900, training_loss: 1.21727e+02
I1111 23:00:55.482582 140264174335808 run_lib.py:153] step: 457950, training_loss: 1.48090e+02
I1111 23:01:05.545179 140264174335808 run_lib.py:153] step: 458000, training_loss: 1.39891e+02
I1111 23:01:16.073798 140264174335808 run_lib.py:153] step: 458050, training_loss: 1.28316e+02
I1111 23:01:26.266264 140264174335808 run_lib.py:153] step: 458100, training_loss: 1.30830e+02
I1111 23:01:36.339282 140264174335808 run_lib.py:153] step: 458150, training_loss: 1.13668e+02
I1111 23:01:45.583966 140264174335808 run_lib.py:153] step: 458200, training_loss: 1.39270e+02
I1111 23:01:55.499604 140264174335808 run_lib.py:153] step: 458250, training_loss: 1.27028e+02
I1111 23:02:05.488888 140264174335808 run_lib.py:153] step: 458300, training_loss: 1.35248e+02
I1111 23:02:15.330131 140264174335808 run_lib.py:153] step: 458350, training_loss: 1.03399e+02
I1111 23:02:24.943309 140264174335808 run_lib.py:153] step: 458400, training_loss: 1.12950e+02
I1111 23:02:34.945183 140264174335808 run_lib.py:153] step: 458450, training_loss: 1.04553e+02
I1111 23:02:44.650875 140264174335808 run_lib.py:153] step: 458500, training_loss: 1.55011e+02
I1111 23:02:54.390670 140264174335808 run_lib.py:153] step: 458550, training_loss: 1.48218e+02
I1111 23:03:04.788512 140264174335808 run_lib.py:153] step: 458600, training_loss: 1.31636e+02
I1111 23:03:15.490852 140264174335808 run_lib.py:153] step: 458650, training_loss: 1.18676e+02
I1111 23:03:25.934832 140264174335808 run_lib.py:153] step: 458700, training_loss: 1.32724e+02
I1111 23:03:36.528972 140264174335808 run_lib.py:153] step: 458750, training_loss: 1.08898e+02
I1111 23:03:47.110664 140264174335808 run_lib.py:153] step: 458800, training_loss: 1.45335e+02
I1111 23:03:57.611799 140264174335808 run_lib.py:153] step: 458850, training_loss: 1.53490e+02
I1111 23:04:07.913668 140264174335808 run_lib.py:153] step: 458900, training_loss: 1.31948e+02
I1111 23:04:18.322600 140264174335808 run_lib.py:153] step: 458950, training_loss: 1.12975e+02
I1111 23:04:28.214668 140264174335808 run_lib.py:153] step: 459000, training_loss: 1.01894e+02
I1111 23:04:38.246571 140264174335808 run_lib.py:153] step: 459050, training_loss: 1.44118e+02
I1111 23:04:48.379816 140264174335808 run_lib.py:153] step: 459100, training_loss: 1.48939e+02
I1111 23:04:58.319271 140264174335808 run_lib.py:153] step: 459150, training_loss: 1.49808e+02
I1111 23:05:08.857895 140264174335808 run_lib.py:153] step: 459200, training_loss: 1.40155e+02
I1111 23:05:18.749009 140264174335808 run_lib.py:153] step: 459250, training_loss: 1.61267e+02
I1111 23:05:28.470964 140264174335808 run_lib.py:153] step: 459300, training_loss: 1.29011e+02
I1111 23:05:38.727401 140264174335808 run_lib.py:153] step: 459350, training_loss: 1.55814e+02
I1111 23:05:49.359719 140264174335808 run_lib.py:153] step: 459400, training_loss: 1.22057e+02
I1111 23:05:59.113273 140264174335808 run_lib.py:153] step: 459450, training_loss: 1.23509e+02
I1111 23:06:09.613785 140264174335808 run_lib.py:153] step: 459500, training_loss: 1.21058e+02
I1111 23:06:20.115834 140264174335808 run_lib.py:153] step: 459550, training_loss: 1.43040e+02
I1111 23:06:30.165352 140264174335808 run_lib.py:153] step: 459600, training_loss: 1.10891e+02
I1111 23:06:40.122574 140264174335808 run_lib.py:153] step: 459650, training_loss: 1.29870e+02
I1111 23:06:50.040335 140264174335808 run_lib.py:153] step: 459700, training_loss: 1.14138e+02
I1111 23:07:00.021682 140264174335808 run_lib.py:153] step: 459750, training_loss: 1.36292e+02
I1111 23:07:09.733580 140264174335808 run_lib.py:153] step: 459800, training_loss: 1.39761e+02
I1111 23:07:19.385330 140264174335808 run_lib.py:153] step: 459850, training_loss: 1.12647e+02
I1111 23:07:30.099216 140264174335808 run_lib.py:153] step: 459900, training_loss: 1.19316e+02
I1111 23:07:40.295082 140264174335808 run_lib.py:153] step: 459950, training_loss: 1.35451e+02
I1111 23:07:49.728698 140264174335808 run_lib.py:153] step: 460000, training_loss: 1.35985e+02
I1111 23:07:50.274043 140264174335808 run_lib.py:166] step: 460000, eval_loss: 1.04421e+02
I1111 23:07:59.892902 140264174335808 run_lib.py:153] step: 460050, training_loss: 1.02987e+02
I1111 23:08:09.983236 140264174335808 run_lib.py:153] step: 460100, training_loss: 1.20222e+02
I1111 23:08:19.798835 140264174335808 run_lib.py:153] step: 460150, training_loss: 1.34013e+02
I1111 23:08:29.285789 140264174335808 run_lib.py:153] step: 460200, training_loss: 1.18457e+02
I1111 23:08:38.973959 140264174335808 run_lib.py:153] step: 460250, training_loss: 8.98565e+01
I1111 23:08:48.844197 140264174335808 run_lib.py:153] step: 460300, training_loss: 1.20854e+02
I1111 23:08:58.414005 140264174335808 run_lib.py:153] step: 460350, training_loss: 1.42854e+02
I1111 23:09:08.490533 140264174335808 run_lib.py:153] step: 460400, training_loss: 1.10390e+02
I1111 23:09:17.960435 140264174335808 run_lib.py:153] step: 460450, training_loss: 1.03130e+02
I1111 23:09:27.980080 140264174335808 run_lib.py:153] step: 460500, training_loss: 1.19658e+02
I1111 23:09:37.977771 140264174335808 run_lib.py:153] step: 460550, training_loss: 1.17587e+02
I1111 23:09:48.437302 140264174335808 run_lib.py:153] step: 460600, training_loss: 9.82992e+01
I1111 23:09:58.687517 140264174335808 run_lib.py:153] step: 460650, training_loss: 1.35805e+02
I1111 23:10:08.484014 140264174335808 run_lib.py:153] step: 460700, training_loss: 1.15845e+02
I1111 23:10:18.342394 140264174335808 run_lib.py:153] step: 460750, training_loss: 1.30559e+02
I1111 23:10:28.134160 140264174335808 run_lib.py:153] step: 460800, training_loss: 1.14492e+02
I1111 23:10:37.786314 140264174335808 run_lib.py:153] step: 460850, training_loss: 1.44552e+02
I1111 23:10:48.553369 140264174335808 run_lib.py:153] step: 460900, training_loss: 1.10253e+02
I1111 23:10:58.352954 140264174335808 run_lib.py:153] step: 460950, training_loss: 1.39518e+02
I1111 23:11:08.112510 140264174335808 run_lib.py:153] step: 461000, training_loss: 1.15643e+02
I1111 23:11:17.751288 140264174335808 run_lib.py:153] step: 461050, training_loss: 1.45131e+02
I1111 23:11:27.737569 140264174335808 run_lib.py:153] step: 461100, training_loss: 1.25982e+02
I1111 23:11:37.733501 140264174335808 run_lib.py:153] step: 461150, training_loss: 1.25688e+02
I1111 23:11:48.173933 140264174335808 run_lib.py:153] step: 461200, training_loss: 1.16243e+02
I1111 23:11:57.804619 140264174335808 run_lib.py:153] step: 461250, training_loss: 1.41892e+02
I1111 23:12:07.763791 140264174335808 run_lib.py:153] step: 461300, training_loss: 1.32886e+02
I1111 23:12:17.234234 140264174335808 run_lib.py:153] step: 461350, training_loss: 1.36820e+02
I1111 23:12:26.699644 140264174335808 run_lib.py:153] step: 461400, training_loss: 1.45471e+02
I1111 23:12:36.844310 140264174335808 run_lib.py:153] step: 461450, training_loss: 1.51132e+02
I1111 23:12:46.355196 140264174335808 run_lib.py:153] step: 461500, training_loss: 1.02809e+02
I1111 23:12:56.904897 140264174335808 run_lib.py:153] step: 461550, training_loss: 1.32601e+02
I1111 23:13:06.464139 140264174335808 run_lib.py:153] step: 461600, training_loss: 1.10559e+02
I1111 23:13:16.515487 140264174335808 run_lib.py:153] step: 461650, training_loss: 1.01227e+02
I1111 23:13:26.811531 140264174335808 run_lib.py:153] step: 461700, training_loss: 1.33279e+02
I1111 23:13:36.994074 140264174335808 run_lib.py:153] step: 461750, training_loss: 9.92722e+01
I1111 23:13:47.012759 140264174335808 run_lib.py:153] step: 461800, training_loss: 1.42392e+02
I1111 23:13:56.902895 140264174335808 run_lib.py:153] step: 461850, training_loss: 1.29312e+02
I1111 23:14:06.248939 140264174335808 run_lib.py:153] step: 461900, training_loss: 1.25033e+02
I1111 23:14:15.871308 140264174335808 run_lib.py:153] step: 461950, training_loss: 1.26831e+02
I1111 23:14:25.413533 140264174335808 run_lib.py:153] step: 462000, training_loss: 1.25947e+02
I1111 23:14:35.538800 140264174335808 run_lib.py:153] step: 462050, training_loss: 1.44753e+02
I1111 23:14:45.014923 140264174335808 run_lib.py:153] step: 462100, training_loss: 1.19041e+02
I1111 23:14:55.578099 140264174335808 run_lib.py:153] step: 462150, training_loss: 1.07489e+02
I1111 23:15:05.908552 140264174335808 run_lib.py:153] step: 462200, training_loss: 1.22074e+02
I1111 23:15:15.778985 140264174335808 run_lib.py:153] step: 462250, training_loss: 1.43724e+02
I1111 23:15:26.210466 140264174335808 run_lib.py:153] step: 462300, training_loss: 1.01429e+02
I1111 23:15:36.537033 140264174335808 run_lib.py:153] step: 462350, training_loss: 1.10078e+02
I1111 23:15:46.441786 140264174335808 run_lib.py:153] step: 462400, training_loss: 1.10113e+02
I1111 23:15:56.156249 140264174335808 run_lib.py:153] step: 462450, training_loss: 1.41401e+02
I1111 23:16:05.713100 140264174335808 run_lib.py:153] step: 462500, training_loss: 1.29237e+02
I1111 23:16:15.949853 140264174335808 run_lib.py:153] step: 462550, training_loss: 1.07751e+02
I1111 23:16:25.810918 140264174335808 run_lib.py:153] step: 462600, training_loss: 8.39522e+01
I1111 23:16:35.241073 140264174335808 run_lib.py:153] step: 462650, training_loss: 1.30968e+02
I1111 23:16:45.244059 140264174335808 run_lib.py:153] step: 462700, training_loss: 1.43896e+02
I1111 23:16:55.245449 140264174335808 run_lib.py:153] step: 462750, training_loss: 1.53339e+02
I1111 23:17:05.195520 140264174335808 run_lib.py:153] step: 462800, training_loss: 1.07346e+02
I1111 23:17:15.517377 140264174335808 run_lib.py:153] step: 462850, training_loss: 1.31796e+02
I1111 23:17:26.102404 140264174335808 run_lib.py:153] step: 462900, training_loss: 1.21397e+02
I1111 23:17:36.988914 140264174335808 run_lib.py:153] step: 462950, training_loss: 1.25918e+02
I1111 23:17:46.239697 140264174335808 run_lib.py:153] step: 463000, training_loss: 1.36947e+02
I1111 23:17:55.983471 140264174335808 run_lib.py:153] step: 463050, training_loss: 1.24418e+02
I1111 23:18:05.759510 140264174335808 run_lib.py:153] step: 463100, training_loss: 1.38753e+02
I1111 23:18:16.203071 140264174335808 run_lib.py:153] step: 463150, training_loss: 1.36320e+02
I1111 23:18:26.542855 140264174335808 run_lib.py:153] step: 463200, training_loss: 1.29885e+02
I1111 23:18:35.814882 140264174335808 run_lib.py:153] step: 463250, training_loss: 1.40549e+02
I1111 23:18:46.351418 140264174335808 run_lib.py:153] step: 463300, training_loss: 1.27207e+02
I1111 23:18:57.429206 140264174335808 run_lib.py:153] step: 463350, training_loss: 1.28823e+02
I1111 23:19:07.763600 140264174335808 run_lib.py:153] step: 463400, training_loss: 1.28945e+02
I1111 23:19:18.080014 140264174335808 run_lib.py:153] step: 463450, training_loss: 1.55198e+02
I1111 23:19:28.236518 140264174335808 run_lib.py:153] step: 463500, training_loss: 1.28227e+02
I1111 23:19:37.456074 140264174335808 run_lib.py:153] step: 463550, training_loss: 1.42914e+02
I1111 23:19:47.800007 140264174335808 run_lib.py:153] step: 463600, training_loss: 1.07505e+02
I1111 23:19:57.952478 140264174335808 run_lib.py:153] step: 463650, training_loss: 1.28990e+02
I1111 23:20:08.489971 140264174335808 run_lib.py:153] step: 463700, training_loss: 1.56291e+02
I1111 23:20:18.591217 140264174335808 run_lib.py:153] step: 463750, training_loss: 1.36590e+02
I1111 23:20:28.366686 140264174335808 run_lib.py:153] step: 463800, training_loss: 1.42319e+02
I1111 23:20:38.184913 140264174335808 run_lib.py:153] step: 463850, training_loss: 1.34401e+02
I1111 23:20:48.231035 140264174335808 run_lib.py:153] step: 463900, training_loss: 1.31787e+02
I1111 23:20:57.905642 140264174335808 run_lib.py:153] step: 463950, training_loss: 1.19724e+02
I1111 23:21:07.468783 140264174335808 run_lib.py:153] step: 464000, training_loss: 1.22364e+02
I1111 23:21:18.076111 140264174335808 run_lib.py:153] step: 464050, training_loss: 1.15982e+02
I1111 23:21:28.401408 140264174335808 run_lib.py:153] step: 464100, training_loss: 1.41222e+02
I1111 23:21:39.110929 140264174335808 run_lib.py:153] step: 464150, training_loss: 9.92813e+01
I1111 23:21:48.587374 140264174335808 run_lib.py:153] step: 464200, training_loss: 1.09144e+02
I1111 23:21:58.292076 140264174335808 run_lib.py:153] step: 464250, training_loss: 1.57996e+02
I1111 23:22:08.485633 140264174335808 run_lib.py:153] step: 464300, training_loss: 1.27998e+02
I1111 23:22:18.044336 140264174335808 run_lib.py:153] step: 464350, training_loss: 1.22987e+02
I1111 23:22:27.880457 140264174335808 run_lib.py:153] step: 464400, training_loss: 1.54756e+02
I1111 23:22:37.475586 140264174335808 run_lib.py:153] step: 464450, training_loss: 1.20169e+02
I1111 23:22:47.220375 140264174335808 run_lib.py:153] step: 464500, training_loss: 1.32582e+02
I1111 23:22:57.564375 140264174335808 run_lib.py:153] step: 464550, training_loss: 1.13485e+02
I1111 23:23:07.660904 140264174335808 run_lib.py:153] step: 464600, training_loss: 1.30970e+02
I1111 23:23:17.903044 140264174335808 run_lib.py:153] step: 464650, training_loss: 1.30402e+02
I1111 23:23:28.302870 140264174335808 run_lib.py:153] step: 464700, training_loss: 1.32606e+02
I1111 23:23:38.192140 140264174335808 run_lib.py:153] step: 464750, training_loss: 1.25749e+02
I1111 23:23:48.029204 140264174335808 run_lib.py:153] step: 464800, training_loss: 1.50232e+02
I1111 23:23:58.549298 140264174335808 run_lib.py:153] step: 464850, training_loss: 1.12637e+02
I1111 23:24:08.590622 140264174335808 run_lib.py:153] step: 464900, training_loss: 1.16514e+02
I1111 23:24:18.330220 140264174335808 run_lib.py:153] step: 464950, training_loss: 1.30084e+02
I1111 23:24:28.464423 140264174335808 run_lib.py:153] step: 465000, training_loss: 1.33299e+02
I1111 23:24:28.567893 140264174335808 run_lib.py:166] step: 465000, eval_loss: 1.37833e+02
I1111 23:24:38.332880 140264174335808 run_lib.py:153] step: 465050, training_loss: 1.46741e+02
I1111 23:24:48.554851 140264174335808 run_lib.py:153] step: 465100, training_loss: 1.36613e+02
I1111 23:24:58.392609 140264174335808 run_lib.py:153] step: 465150, training_loss: 1.31032e+02
I1111 23:25:08.267959 140264174335808 run_lib.py:153] step: 465200, training_loss: 1.30126e+02
I1111 23:25:18.356634 140264174335808 run_lib.py:153] step: 465250, training_loss: 1.33583e+02
I1111 23:25:28.654304 140264174335808 run_lib.py:153] step: 465300, training_loss: 1.33275e+02
I1111 23:25:38.313061 140264174335808 run_lib.py:153] step: 465350, training_loss: 1.20833e+02
I1111 23:25:48.633344 140264174335808 run_lib.py:153] step: 465400, training_loss: 1.26470e+02
I1111 23:25:58.945198 140264174335808 run_lib.py:153] step: 465450, training_loss: 1.28618e+02
I1111 23:26:09.189272 140264174335808 run_lib.py:153] step: 465500, training_loss: 1.32649e+02
I1111 23:26:18.768454 140264174335808 run_lib.py:153] step: 465550, training_loss: 1.25005e+02
I1111 23:26:29.123474 140264174335808 run_lib.py:153] step: 465600, training_loss: 1.35210e+02
I1111 23:26:40.171724 140264174335808 run_lib.py:153] step: 465650, training_loss: 1.48803e+02
I1111 23:26:50.736120 140264174335808 run_lib.py:153] step: 465700, training_loss: 1.14546e+02
I1111 23:27:00.956725 140264174335808 run_lib.py:153] step: 465750, training_loss: 1.50294e+02
I1111 23:27:11.340934 140264174335808 run_lib.py:153] step: 465800, training_loss: 1.38436e+02
I1111 23:27:21.367240 140264174335808 run_lib.py:153] step: 465850, training_loss: 1.40197e+02
I1111 23:27:32.201188 140264174335808 run_lib.py:153] step: 465900, training_loss: 1.24506e+02
I1111 23:27:42.901003 140264174335808 run_lib.py:153] step: 465950, training_loss: 1.30092e+02
I1111 23:27:53.096873 140264174335808 run_lib.py:153] step: 466000, training_loss: 1.35821e+02
I1111 23:28:03.175252 140264174335808 run_lib.py:153] step: 466050, training_loss: 1.27668e+02
I1111 23:28:13.534503 140264174335808 run_lib.py:153] step: 466100, training_loss: 1.15895e+02
I1111 23:28:24.056849 140264174335808 run_lib.py:153] step: 466150, training_loss: 1.50766e+02
I1111 23:28:34.048137 140264174335808 run_lib.py:153] step: 466200, training_loss: 1.14586e+02
I1111 23:28:43.980448 140264174335808 run_lib.py:153] step: 466250, training_loss: 1.43692e+02
I1111 23:28:54.186210 140264174335808 run_lib.py:153] step: 466300, training_loss: 1.03863e+02
I1111 23:29:04.633956 140264174335808 run_lib.py:153] step: 466350, training_loss: 1.08742e+02
I1111 23:29:15.661772 140264174335808 run_lib.py:153] step: 466400, training_loss: 1.14639e+02
I1111 23:29:26.327949 140264174335808 run_lib.py:153] step: 466450, training_loss: 1.25951e+02
I1111 23:29:36.679388 140264174335808 run_lib.py:153] step: 466500, training_loss: 1.22631e+02
I1111 23:29:47.003960 140264174335808 run_lib.py:153] step: 466550, training_loss: 1.38475e+02
I1111 23:29:57.695789 140264174335808 run_lib.py:153] step: 466600, training_loss: 1.47209e+02
I1111 23:30:07.553917 140264174335808 run_lib.py:153] step: 466650, training_loss: 1.40792e+02
I1111 23:30:18.767222 140264174335808 run_lib.py:153] step: 466700, training_loss: 1.18933e+02
I1111 23:30:29.422557 140264174335808 run_lib.py:153] step: 466750, training_loss: 1.45128e+02
I1111 23:30:40.003618 140264174335808 run_lib.py:153] step: 466800, training_loss: 1.12424e+02
I1111 23:30:50.482958 140264174335808 run_lib.py:153] step: 466850, training_loss: 1.34876e+02
I1111 23:31:00.747243 140264174335808 run_lib.py:153] step: 466900, training_loss: 9.65792e+01
I1111 23:31:10.919459 140264174335808 run_lib.py:153] step: 466950, training_loss: 1.25266e+02
I1111 23:31:20.574805 140264174335808 run_lib.py:153] step: 467000, training_loss: 1.39917e+02
I1111 23:31:30.814404 140264174335808 run_lib.py:153] step: 467050, training_loss: 1.67816e+02
I1111 23:31:41.016731 140264174335808 run_lib.py:153] step: 467100, training_loss: 1.35323e+02
I1111 23:31:51.468028 140264174335808 run_lib.py:153] step: 467150, training_loss: 1.30924e+02
I1111 23:32:02.133451 140264174335808 run_lib.py:153] step: 467200, training_loss: 1.20907e+02
I1111 23:32:11.645673 140264174335808 run_lib.py:153] step: 467250, training_loss: 1.17934e+02
I1111 23:32:21.530009 140264174335808 run_lib.py:153] step: 467300, training_loss: 1.37711e+02
I1111 23:32:31.537559 140264174335808 run_lib.py:153] step: 467350, training_loss: 1.59014e+02
I1111 23:32:41.046280 140264174335808 run_lib.py:153] step: 467400, training_loss: 1.58652e+02
I1111 23:32:51.408861 140264174335808 run_lib.py:153] step: 467450, training_loss: 1.32962e+02
I1111 23:33:01.491807 140264174335808 run_lib.py:153] step: 467500, training_loss: 1.68269e+02
I1111 23:33:11.737639 140264174335808 run_lib.py:153] step: 467550, training_loss: 1.40163e+02
I1111 23:33:21.157484 140264174335808 run_lib.py:153] step: 467600, training_loss: 1.32813e+02
I1111 23:33:31.699666 140264174335808 run_lib.py:153] step: 467650, training_loss: 9.48550e+01
I1111 23:33:41.598079 140264174335808 run_lib.py:153] step: 467700, training_loss: 1.45919e+02
I1111 23:33:52.318370 140264174335808 run_lib.py:153] step: 467750, training_loss: 1.36248e+02
I1111 23:34:02.275727 140264174335808 run_lib.py:153] step: 467800, training_loss: 1.42777e+02
I1111 23:34:12.385694 140264174335808 run_lib.py:153] step: 467850, training_loss: 1.24881e+02
I1111 23:34:22.803165 140264174335808 run_lib.py:153] step: 467900, training_loss: 1.21040e+02
I1111 23:34:32.566949 140264174335808 run_lib.py:153] step: 467950, training_loss: 1.36473e+02
I1111 23:34:42.743917 140264174335808 run_lib.py:153] step: 468000, training_loss: 1.15528e+02
I1111 23:34:52.957252 140264174335808 run_lib.py:153] step: 468050, training_loss: 1.31166e+02
I1111 23:35:03.019409 140264174335808 run_lib.py:153] step: 468100, training_loss: 1.19039e+02
I1111 23:35:13.547584 140264174335808 run_lib.py:153] step: 468150, training_loss: 1.22558e+02
I1111 23:35:23.853306 140264174335808 run_lib.py:153] step: 468200, training_loss: 1.12366e+02
I1111 23:35:34.528962 140264174335808 run_lib.py:153] step: 468250, training_loss: 1.36638e+02
I1111 23:35:44.757452 140264174335808 run_lib.py:153] step: 468300, training_loss: 1.17291e+02
I1111 23:35:55.085261 140264174335808 run_lib.py:153] step: 468350, training_loss: 1.16759e+02
I1111 23:36:04.515706 140264174335808 run_lib.py:153] step: 468400, training_loss: 1.26463e+02
I1111 23:36:14.592713 140264174335808 run_lib.py:153] step: 468450, training_loss: 1.02670e+02
I1111 23:36:24.730257 140264174335808 run_lib.py:153] step: 468500, training_loss: 8.98612e+01
I1111 23:36:34.344161 140264174335808 run_lib.py:153] step: 468550, training_loss: 1.39734e+02
I1111 23:36:44.728315 140264174335808 run_lib.py:153] step: 468600, training_loss: 1.15353e+02
I1111 23:36:55.499281 140264174335808 run_lib.py:153] step: 468650, training_loss: 1.32603e+02
I1111 23:37:05.520514 140264174335808 run_lib.py:153] step: 468700, training_loss: 1.29322e+02
I1111 23:37:15.503547 140264174335808 run_lib.py:153] step: 468750, training_loss: 1.22321e+02
I1111 23:37:25.275438 140264174335808 run_lib.py:153] step: 468800, training_loss: 1.44092e+02
I1111 23:37:34.527240 140264174335808 run_lib.py:153] step: 468850, training_loss: 1.71933e+02
I1111 23:37:44.288434 140264174335808 run_lib.py:153] step: 468900, training_loss: 1.25608e+02
I1111 23:37:53.809051 140264174335808 run_lib.py:153] step: 468950, training_loss: 1.12535e+02
I1111 23:38:03.814899 140264174335808 run_lib.py:153] step: 469000, training_loss: 1.37814e+02
I1111 23:38:13.944451 140264174335808 run_lib.py:153] step: 469050, training_loss: 1.15362e+02
I1111 23:38:23.992263 140264174335808 run_lib.py:153] step: 469100, training_loss: 1.46487e+02
I1111 23:38:34.051648 140264174335808 run_lib.py:153] step: 469150, training_loss: 1.20652e+02
I1111 23:38:44.216767 140264174335808 run_lib.py:153] step: 469200, training_loss: 1.12444e+02
I1111 23:38:54.626743 140264174335808 run_lib.py:153] step: 469250, training_loss: 1.01234e+02
I1111 23:39:04.894180 140264174335808 run_lib.py:153] step: 469300, training_loss: 1.06552e+02
I1111 23:39:15.528235 140264174335808 run_lib.py:153] step: 469350, training_loss: 1.64840e+02
I1111 23:39:25.766955 140264174335808 run_lib.py:153] step: 469400, training_loss: 8.67868e+01
I1111 23:39:35.223556 140264174335808 run_lib.py:153] step: 469450, training_loss: 1.37538e+02
I1111 23:39:45.584965 140264174335808 run_lib.py:153] step: 469500, training_loss: 1.41221e+02
I1111 23:39:55.730107 140264174335808 run_lib.py:153] step: 469550, training_loss: 1.14363e+02
I1111 23:40:05.464993 140264174335808 run_lib.py:153] step: 469600, training_loss: 1.13844e+02
I1111 23:40:14.984422 140264174335808 run_lib.py:153] step: 469650, training_loss: 1.16259e+02
I1111 23:40:24.629084 140264174335808 run_lib.py:153] step: 469700, training_loss: 1.36527e+02
I1111 23:40:34.647614 140264174335808 run_lib.py:153] step: 469750, training_loss: 1.21096e+02
I1111 23:40:44.790699 140264174335808 run_lib.py:153] step: 469800, training_loss: 1.12455e+02
I1111 23:40:55.391670 140264174335808 run_lib.py:153] step: 469850, training_loss: 1.44896e+02
I1111 23:41:05.041111 140264174335808 run_lib.py:153] step: 469900, training_loss: 1.12845e+02
I1111 23:41:14.481642 140264174335808 run_lib.py:153] step: 469950, training_loss: 1.15125e+02
I1111 23:41:24.033355 140264174335808 run_lib.py:153] step: 470000, training_loss: 1.12587e+02
I1111 23:41:24.564531 140264174335808 run_lib.py:166] step: 470000, eval_loss: 1.34924e+02
I1111 23:41:34.548713 140264174335808 run_lib.py:153] step: 470050, training_loss: 1.08298e+02
I1111 23:41:44.614436 140264174335808 run_lib.py:153] step: 470100, training_loss: 1.20112e+02
I1111 23:41:54.407934 140264174335808 run_lib.py:153] step: 470150, training_loss: 1.38889e+02
I1111 23:42:04.684514 140264174335808 run_lib.py:153] step: 470200, training_loss: 1.28405e+02
I1111 23:42:14.748964 140264174335808 run_lib.py:153] step: 470250, training_loss: 9.69467e+01
I1111 23:42:24.752296 140264174335808 run_lib.py:153] step: 470300, training_loss: 1.71271e+02
I1111 23:42:34.673220 140264174335808 run_lib.py:153] step: 470350, training_loss: 1.40287e+02
I1111 23:42:44.628108 140264174335808 run_lib.py:153] step: 470400, training_loss: 1.29441e+02
I1111 23:42:54.357610 140264174335808 run_lib.py:153] step: 470450, training_loss: 1.18666e+02
I1111 23:43:04.481705 140264174335808 run_lib.py:153] step: 470500, training_loss: 1.06674e+02
I1111 23:43:14.151766 140264174335808 run_lib.py:153] step: 470550, training_loss: 1.56699e+02
I1111 23:43:23.549824 140264174335808 run_lib.py:153] step: 470600, training_loss: 1.37217e+02
I1111 23:43:33.159630 140264174335808 run_lib.py:153] step: 470650, training_loss: 1.32175e+02
I1111 23:43:42.707249 140264174335808 run_lib.py:153] step: 470700, training_loss: 1.33746e+02
I1111 23:43:52.685622 140264174335808 run_lib.py:153] step: 470750, training_loss: 1.10406e+02
I1111 23:44:02.254061 140264174335808 run_lib.py:153] step: 470800, training_loss: 1.10563e+02
I1111 23:44:12.247761 140264174335808 run_lib.py:153] step: 470850, training_loss: 1.18834e+02
I1111 23:44:22.064469 140264174335808 run_lib.py:153] step: 470900, training_loss: 1.24816e+02
I1111 23:44:32.334877 140264174335808 run_lib.py:153] step: 470950, training_loss: 1.48790e+02
I1111 23:44:42.310512 140264174335808 run_lib.py:153] step: 471000, training_loss: 1.20379e+02
I1111 23:44:51.993751 140264174335808 run_lib.py:153] step: 471050, training_loss: 1.25612e+02
I1111 23:45:01.681745 140264174335808 run_lib.py:153] step: 471100, training_loss: 1.54039e+02
I1111 23:45:11.417418 140264174335808 run_lib.py:153] step: 471150, training_loss: 1.26433e+02
I1111 23:45:21.772951 140264174335808 run_lib.py:153] step: 471200, training_loss: 1.09005e+02
I1111 23:45:31.428416 140264174335808 run_lib.py:153] step: 471250, training_loss: 1.63093e+02
I1111 23:45:41.497392 140264174335808 run_lib.py:153] step: 471300, training_loss: 1.45571e+02
I1111 23:45:51.906046 140264174335808 run_lib.py:153] step: 471350, training_loss: 1.33212e+02
I1111 23:46:01.539821 140264174335808 run_lib.py:153] step: 471400, training_loss: 1.28704e+02
I1111 23:46:10.884466 140264174335808 run_lib.py:153] step: 471450, training_loss: 1.60621e+02
I1111 23:46:21.072841 140264174335808 run_lib.py:153] step: 471500, training_loss: 1.03631e+02
I1111 23:46:31.619636 140264174335808 run_lib.py:153] step: 471550, training_loss: 1.57504e+02
I1111 23:46:41.311683 140264174335808 run_lib.py:153] step: 471600, training_loss: 1.36073e+02
I1111 23:46:51.252474 140264174335808 run_lib.py:153] step: 471650, training_loss: 1.35223e+02
I1111 23:47:01.478244 140264174335808 run_lib.py:153] step: 471700, training_loss: 1.31979e+02
I1111 23:47:11.047176 140264174335808 run_lib.py:153] step: 471750, training_loss: 1.08594e+02
I1111 23:47:21.478318 140264174335808 run_lib.py:153] step: 471800, training_loss: 9.07431e+01
I1111 23:47:31.915846 140264174335808 run_lib.py:153] step: 471850, training_loss: 1.18611e+02
I1111 23:47:41.555661 140264174335808 run_lib.py:153] step: 471900, training_loss: 1.20146e+02
I1111 23:47:51.069212 140264174335808 run_lib.py:153] step: 471950, training_loss: 1.07853e+02
I1111 23:48:00.440380 140264174335808 run_lib.py:153] step: 472000, training_loss: 1.26980e+02
I1111 23:48:10.639528 140264174335808 run_lib.py:153] step: 472050, training_loss: 1.19073e+02
I1111 23:48:20.883787 140264174335808 run_lib.py:153] step: 472100, training_loss: 1.24489e+02
I1111 23:48:30.772909 140264174335808 run_lib.py:153] step: 472150, training_loss: 9.92028e+01
I1111 23:48:40.872330 140264174335808 run_lib.py:153] step: 472200, training_loss: 1.52078e+02
I1111 23:48:50.960506 140264174335808 run_lib.py:153] step: 472250, training_loss: 1.24437e+02
I1111 23:49:00.958227 140264174335808 run_lib.py:153] step: 472300, training_loss: 1.28591e+02
I1111 23:49:11.389424 140264174335808 run_lib.py:153] step: 472350, training_loss: 1.46515e+02
I1111 23:49:21.353512 140264174335808 run_lib.py:153] step: 472400, training_loss: 9.10357e+01
I1111 23:49:32.137938 140264174335808 run_lib.py:153] step: 472450, training_loss: 1.60706e+02
I1111 23:49:41.529005 140264174335808 run_lib.py:153] step: 472500, training_loss: 1.40753e+02
I1111 23:49:52.000206 140264174335808 run_lib.py:153] step: 472550, training_loss: 1.38530e+02
I1111 23:50:01.658957 140264174335808 run_lib.py:153] step: 472600, training_loss: 1.50610e+02
I1111 23:50:11.060289 140264174335808 run_lib.py:153] step: 472650, training_loss: 1.07373e+02
I1111 23:50:20.720774 140264174335808 run_lib.py:153] step: 472700, training_loss: 1.04586e+02
I1111 23:50:30.722867 140264174335808 run_lib.py:153] step: 472750, training_loss: 1.23399e+02
I1111 23:50:40.238466 140264174335808 run_lib.py:153] step: 472800, training_loss: 1.13393e+02
I1111 23:50:49.977671 140264174335808 run_lib.py:153] step: 472850, training_loss: 1.55089e+02
I1111 23:50:59.528930 140264174335808 run_lib.py:153] step: 472900, training_loss: 1.22365e+02
I1111 23:51:08.812443 140264174335808 run_lib.py:153] step: 472950, training_loss: 1.28965e+02
I1111 23:51:19.102642 140264174335808 run_lib.py:153] step: 473000, training_loss: 1.11363e+02
I1111 23:51:28.343512 140264174335808 run_lib.py:153] step: 473050, training_loss: 1.09914e+02
I1111 23:51:37.861308 140264174335808 run_lib.py:153] step: 473100, training_loss: 1.53957e+02
I1111 23:51:48.281507 140264174335808 run_lib.py:153] step: 473150, training_loss: 1.45196e+02
I1111 23:51:57.851597 140264174335808 run_lib.py:153] step: 473200, training_loss: 1.49292e+02
I1111 23:52:07.273448 140264174335808 run_lib.py:153] step: 473250, training_loss: 1.16945e+02
I1111 23:52:17.451020 140264174335808 run_lib.py:153] step: 473300, training_loss: 1.44823e+02
I1111 23:52:27.349204 140264174335808 run_lib.py:153] step: 473350, training_loss: 1.09642e+02
I1111 23:52:37.209614 140264174335808 run_lib.py:153] step: 473400, training_loss: 1.62192e+02
I1111 23:52:47.014806 140264174335808 run_lib.py:153] step: 473450, training_loss: 1.27628e+02
I1111 23:52:57.542684 140264174335808 run_lib.py:153] step: 473500, training_loss: 1.28366e+02
I1111 23:53:07.064661 140264174335808 run_lib.py:153] step: 473550, training_loss: 1.23850e+02
I1111 23:53:16.759305 140264174335808 run_lib.py:153] step: 473600, training_loss: 1.30369e+02
I1111 23:53:26.195558 140264174335808 run_lib.py:153] step: 473650, training_loss: 1.48801e+02
I1111 23:53:35.718574 140264174335808 run_lib.py:153] step: 473700, training_loss: 1.17329e+02
I1111 23:53:45.192829 140264174335808 run_lib.py:153] step: 473750, training_loss: 1.12550e+02
I1111 23:53:54.713877 140264174335808 run_lib.py:153] step: 473800, training_loss: 1.19125e+02
I1111 23:54:04.654639 140264174335808 run_lib.py:153] step: 473850, training_loss: 1.15614e+02
I1111 23:54:15.216719 140264174335808 run_lib.py:153] step: 473900, training_loss: 1.48474e+02
I1111 23:54:25.263634 140264174335808 run_lib.py:153] step: 473950, training_loss: 1.47202e+02
I1111 23:54:35.149173 140264174335808 run_lib.py:153] step: 474000, training_loss: 1.34394e+02
I1111 23:54:44.836921 140264174335808 run_lib.py:153] step: 474050, training_loss: 1.24109e+02
I1111 23:54:54.487939 140264174335808 run_lib.py:153] step: 474100, training_loss: 1.48300e+02
I1111 23:55:04.633271 140264174335808 run_lib.py:153] step: 474150, training_loss: 1.61841e+02
I1111 23:55:13.826172 140264174335808 run_lib.py:153] step: 474200, training_loss: 1.36846e+02
I1111 23:55:24.413037 140264174335808 run_lib.py:153] step: 474250, training_loss: 1.54395e+02
I1111 23:55:34.383177 140264174335808 run_lib.py:153] step: 474300, training_loss: 1.05762e+02
I1111 23:55:44.039638 140264174335808 run_lib.py:153] step: 474350, training_loss: 1.49838e+02
I1111 23:55:53.344199 140264174335808 run_lib.py:153] step: 474400, training_loss: 9.63246e+01
I1111 23:56:03.562945 140264174335808 run_lib.py:153] step: 474450, training_loss: 1.15674e+02
I1111 23:56:14.411606 140264174335808 run_lib.py:153] step: 474500, training_loss: 1.47940e+02
I1111 23:56:24.570010 140264174335808 run_lib.py:153] step: 474550, training_loss: 1.19318e+02
I1111 23:56:34.149029 140264174335808 run_lib.py:153] step: 474600, training_loss: 1.19710e+02
I1111 23:56:44.610520 140264174335808 run_lib.py:153] step: 474650, training_loss: 1.45735e+02
I1111 23:56:54.855308 140264174335808 run_lib.py:153] step: 474700, training_loss: 1.17528e+02
I1111 23:57:04.873417 140264174335808 run_lib.py:153] step: 474750, training_loss: 1.37724e+02
I1111 23:57:14.297285 140264174335808 run_lib.py:153] step: 474800, training_loss: 1.63829e+02
I1111 23:57:24.465371 140264174335808 run_lib.py:153] step: 474850, training_loss: 1.38414e+02
I1111 23:57:34.524963 140264174335808 run_lib.py:153] step: 474900, training_loss: 1.23902e+02
I1111 23:57:44.652105 140264174335808 run_lib.py:153] step: 474950, training_loss: 1.21781e+02
I1111 23:57:55.057888 140264174335808 run_lib.py:153] step: 475000, training_loss: 1.21648e+02
I1111 23:57:55.194911 140264174335808 run_lib.py:166] step: 475000, eval_loss: 1.40801e+02
I1111 23:58:05.979027 140264174335808 run_lib.py:153] step: 475050, training_loss: 1.30333e+02
I1111 23:58:16.292058 140264174335808 run_lib.py:153] step: 475100, training_loss: 1.44531e+02
I1111 23:58:25.843689 140264174335808 run_lib.py:153] step: 475150, training_loss: 1.29858e+02
I1111 23:58:36.322902 140264174335808 run_lib.py:153] step: 475200, training_loss: 1.07659e+02
I1111 23:58:46.005733 140264174335808 run_lib.py:153] step: 475250, training_loss: 1.25347e+02
I1111 23:58:55.717437 140264174335808 run_lib.py:153] step: 475300, training_loss: 1.06158e+02
I1111 23:59:05.585214 140264174335808 run_lib.py:153] step: 475350, training_loss: 1.37729e+02
I1111 23:59:15.509746 140264174335808 run_lib.py:153] step: 475400, training_loss: 1.42657e+02
I1111 23:59:26.127558 140264174335808 run_lib.py:153] step: 475450, training_loss: 1.15180e+02
I1111 23:59:35.963051 140264174335808 run_lib.py:153] step: 475500, training_loss: 1.02931e+02
I1111 23:59:45.982510 140264174335808 run_lib.py:153] step: 475550, training_loss: 1.45634e+02
I1111 23:59:56.003325 140264174335808 run_lib.py:153] step: 475600, training_loss: 1.33563e+02
I1112 00:00:06.685468 140264174335808 run_lib.py:153] step: 475650, training_loss: 1.00696e+02
I1112 00:00:16.755535 140264174335808 run_lib.py:153] step: 475700, training_loss: 9.71550e+01
I1112 00:00:27.297953 140264174335808 run_lib.py:153] step: 475750, training_loss: 1.10194e+02
I1112 00:00:37.179075 140264174335808 run_lib.py:153] step: 475800, training_loss: 1.03339e+02
I1112 00:00:47.607759 140264174335808 run_lib.py:153] step: 475850, training_loss: 1.02680e+02
I1112 00:00:57.036839 140264174335808 run_lib.py:153] step: 475900, training_loss: 1.32982e+02
I1112 00:01:06.483182 140264174335808 run_lib.py:153] step: 475950, training_loss: 9.88384e+01
I1112 00:01:16.724089 140264174335808 run_lib.py:153] step: 476000, training_loss: 1.15128e+02
I1112 00:01:26.567482 140264174335808 run_lib.py:153] step: 476050, training_loss: 1.12734e+02
I1112 00:01:36.083702 140264174335808 run_lib.py:153] step: 476100, training_loss: 9.88507e+01
I1112 00:01:45.520473 140264174335808 run_lib.py:153] step: 476150, training_loss: 1.25925e+02
I1112 00:01:54.935737 140264174335808 run_lib.py:153] step: 476200, training_loss: 1.13873e+02
I1112 00:02:04.409855 140264174335808 run_lib.py:153] step: 476250, training_loss: 1.28788e+02
I1112 00:02:13.735392 140264174335808 run_lib.py:153] step: 476300, training_loss: 1.51223e+02
I1112 00:02:23.484365 140264174335808 run_lib.py:153] step: 476350, training_loss: 1.15668e+02
I1112 00:02:32.863429 140264174335808 run_lib.py:153] step: 476400, training_loss: 1.28131e+02
I1112 00:02:42.404028 140264174335808 run_lib.py:153] step: 476450, training_loss: 1.21776e+02
I1112 00:02:53.013213 140264174335808 run_lib.py:153] step: 476500, training_loss: 1.46710e+02
I1112 00:03:03.106298 140264174335808 run_lib.py:153] step: 476550, training_loss: 1.07018e+02
I1112 00:03:13.417110 140264174335808 run_lib.py:153] step: 476600, training_loss: 1.44371e+02
I1112 00:03:22.784112 140264174335808 run_lib.py:153] step: 476650, training_loss: 1.24174e+02
I1112 00:03:32.116657 140264174335808 run_lib.py:153] step: 476700, training_loss: 1.10262e+02
I1112 00:03:41.801000 140264174335808 run_lib.py:153] step: 476750, training_loss: 1.26012e+02
I1112 00:03:51.481520 140264174335808 run_lib.py:153] step: 476800, training_loss: 1.28666e+02
I1112 00:04:01.743130 140264174335808 run_lib.py:153] step: 476850, training_loss: 1.26822e+02
I1112 00:04:11.230409 140264174335808 run_lib.py:153] step: 476900, training_loss: 1.22519e+02
I1112 00:04:21.310983 140264174335808 run_lib.py:153] step: 476950, training_loss: 1.21876e+02
I1112 00:04:30.550430 140264174335808 run_lib.py:153] step: 477000, training_loss: 1.37538e+02
I1112 00:04:40.146909 140264174335808 run_lib.py:153] step: 477050, training_loss: 1.12596e+02
I1112 00:04:49.852082 140264174335808 run_lib.py:153] step: 477100, training_loss: 1.13910e+02
I1112 00:05:00.002052 140264174335808 run_lib.py:153] step: 477150, training_loss: 1.34067e+02
I1112 00:05:09.669079 140264174335808 run_lib.py:153] step: 477200, training_loss: 1.14041e+02
I1112 00:05:19.671477 140264174335808 run_lib.py:153] step: 477250, training_loss: 1.14401e+02
I1112 00:05:29.876140 140264174335808 run_lib.py:153] step: 477300, training_loss: 1.08475e+02
I1112 00:05:39.272222 140264174335808 run_lib.py:153] step: 477350, training_loss: 1.42744e+02
I1112 00:05:49.327350 140264174335808 run_lib.py:153] step: 477400, training_loss: 1.31522e+02
I1112 00:05:59.387526 140264174335808 run_lib.py:153] step: 477450, training_loss: 1.48805e+02
I1112 00:06:09.100247 140264174335808 run_lib.py:153] step: 477500, training_loss: 1.19786e+02
I1112 00:06:18.780327 140264174335808 run_lib.py:153] step: 477550, training_loss: 1.32051e+02
I1112 00:06:28.839942 140264174335808 run_lib.py:153] step: 477600, training_loss: 1.22270e+02
I1112 00:06:38.480807 140264174335808 run_lib.py:153] step: 477650, training_loss: 1.18390e+02
I1112 00:06:48.457319 140264174335808 run_lib.py:153] step: 477700, training_loss: 1.23770e+02
I1112 00:06:58.229120 140264174335808 run_lib.py:153] step: 477750, training_loss: 1.46940e+02
I1112 00:07:07.680891 140264174335808 run_lib.py:153] step: 477800, training_loss: 1.00800e+02
I1112 00:07:17.871899 140264174335808 run_lib.py:153] step: 477850, training_loss: 1.40681e+02
I1112 00:07:28.416156 140264174335808 run_lib.py:153] step: 477900, training_loss: 1.13827e+02
I1112 00:07:38.791672 140264174335808 run_lib.py:153] step: 477950, training_loss: 9.50053e+01
I1112 00:07:49.103246 140264174335808 run_lib.py:153] step: 478000, training_loss: 1.43802e+02
I1112 00:07:59.093938 140264174335808 run_lib.py:153] step: 478050, training_loss: 1.18012e+02
I1112 00:08:09.082046 140264174335808 run_lib.py:153] step: 478100, training_loss: 1.19555e+02
I1112 00:08:19.363492 140264174335808 run_lib.py:153] step: 478150, training_loss: 1.24913e+02
I1112 00:08:30.281824 140264174335808 run_lib.py:153] step: 478200, training_loss: 1.01447e+02
I1112 00:08:39.846798 140264174335808 run_lib.py:153] step: 478250, training_loss: 1.51149e+02
I1112 00:08:49.560983 140264174335808 run_lib.py:153] step: 478300, training_loss: 1.06364e+02
I1112 00:08:59.737841 140264174335808 run_lib.py:153] step: 478350, training_loss: 1.43009e+02
I1112 00:09:10.095152 140264174335808 run_lib.py:153] step: 478400, training_loss: 1.31908e+02
I1112 00:09:20.195691 140264174335808 run_lib.py:153] step: 478450, training_loss: 1.28351e+02
I1112 00:09:30.678207 140264174335808 run_lib.py:153] step: 478500, training_loss: 1.13858e+02
I1112 00:09:40.715450 140264174335808 run_lib.py:153] step: 478550, training_loss: 1.25741e+02
I1112 00:09:50.639326 140264174335808 run_lib.py:153] step: 478600, training_loss: 1.19839e+02
I1112 00:10:01.141146 140264174335808 run_lib.py:153] step: 478650, training_loss: 1.11251e+02
I1112 00:10:11.700802 140264174335808 run_lib.py:153] step: 478700, training_loss: 1.23943e+02
I1112 00:10:21.676748 140264174335808 run_lib.py:153] step: 478750, training_loss: 1.68331e+02
I1112 00:10:31.528051 140264174335808 run_lib.py:153] step: 478800, training_loss: 1.34054e+02
I1112 00:10:41.568333 140264174335808 run_lib.py:153] step: 478850, training_loss: 1.08389e+02
I1112 00:10:52.622936 140264174335808 run_lib.py:153] step: 478900, training_loss: 1.27206e+02
I1112 00:11:03.462343 140264174335808 run_lib.py:153] step: 478950, training_loss: 1.23508e+02
I1112 00:11:13.705010 140264174335808 run_lib.py:153] step: 479000, training_loss: 1.24586e+02
I1112 00:11:23.546155 140264174335808 run_lib.py:153] step: 479050, training_loss: 1.53445e+02
I1112 00:11:33.130358 140264174335808 run_lib.py:153] step: 479100, training_loss: 1.23178e+02
I1112 00:11:43.345036 140264174335808 run_lib.py:153] step: 479150, training_loss: 1.44588e+02
I1112 00:11:53.643203 140264174335808 run_lib.py:153] step: 479200, training_loss: 1.30614e+02
I1112 00:12:03.610847 140264174335808 run_lib.py:153] step: 479250, training_loss: 1.57116e+02
I1112 00:12:13.506572 140264174335808 run_lib.py:153] step: 479300, training_loss: 1.52322e+02
I1112 00:12:22.987286 140264174335808 run_lib.py:153] step: 479350, training_loss: 1.19090e+02
I1112 00:12:32.935515 140264174335808 run_lib.py:153] step: 479400, training_loss: 1.54112e+02
I1112 00:12:42.458237 140264174335808 run_lib.py:153] step: 479450, training_loss: 1.04061e+02
I1112 00:12:52.159494 140264174335808 run_lib.py:153] step: 479500, training_loss: 1.06647e+02
I1112 00:13:01.888856 140264174335808 run_lib.py:153] step: 479550, training_loss: 1.35135e+02
I1112 00:13:11.843235 140264174335808 run_lib.py:153] step: 479600, training_loss: 1.19831e+02
I1112 00:13:21.410905 140264174335808 run_lib.py:153] step: 479650, training_loss: 9.82014e+01
I1112 00:13:30.856424 140264174335808 run_lib.py:153] step: 479700, training_loss: 1.39080e+02
I1112 00:13:41.123422 140264174335808 run_lib.py:153] step: 479750, training_loss: 1.22828e+02
I1112 00:13:51.002819 140264174335808 run_lib.py:153] step: 479800, training_loss: 1.25486e+02
I1112 00:14:01.152703 140264174335808 run_lib.py:153] step: 479850, training_loss: 1.50351e+02
I1112 00:14:11.083739 140264174335808 run_lib.py:153] step: 479900, training_loss: 1.68995e+02
I1112 00:14:20.937922 140264174335808 run_lib.py:153] step: 479950, training_loss: 1.12640e+02
I1112 00:14:30.806972 140264174335808 run_lib.py:153] step: 480000, training_loss: 1.15978e+02
I1112 00:14:31.378156 140264174335808 run_lib.py:166] step: 480000, eval_loss: 1.28266e+02
I1112 00:14:41.601547 140264174335808 run_lib.py:153] step: 480050, training_loss: 1.20711e+02
I1112 00:14:52.005613 140264174335808 run_lib.py:153] step: 480100, training_loss: 1.78167e+02
I1112 00:15:02.488487 140264174335808 run_lib.py:153] step: 480150, training_loss: 1.20510e+02
I1112 00:15:12.919617 140264174335808 run_lib.py:153] step: 480200, training_loss: 1.18603e+02
I1112 00:15:22.508851 140264174335808 run_lib.py:153] step: 480250, training_loss: 1.25611e+02
I1112 00:15:32.278455 140264174335808 run_lib.py:153] step: 480300, training_loss: 1.17078e+02
I1112 00:15:42.850049 140264174335808 run_lib.py:153] step: 480350, training_loss: 1.00782e+02
I1112 00:15:52.840175 140264174335808 run_lib.py:153] step: 480400, training_loss: 1.25197e+02
I1112 00:16:02.253955 140264174335808 run_lib.py:153] step: 480450, training_loss: 1.54873e+02
I1112 00:16:11.847993 140264174335808 run_lib.py:153] step: 480500, training_loss: 1.26049e+02
I1112 00:16:21.329065 140264174335808 run_lib.py:153] step: 480550, training_loss: 1.09935e+02
I1112 00:16:31.659662 140264174335808 run_lib.py:153] step: 480600, training_loss: 1.34969e+02
I1112 00:16:42.562995 140264174335808 run_lib.py:153] step: 480650, training_loss: 1.17212e+02
I1112 00:16:52.965766 140264174335808 run_lib.py:153] step: 480700, training_loss: 1.20986e+02
I1112 00:17:02.594861 140264174335808 run_lib.py:153] step: 480750, training_loss: 1.21529e+02
I1112 00:17:13.005653 140264174335808 run_lib.py:153] step: 480800, training_loss: 1.55682e+02
I1112 00:17:22.485179 140264174335808 run_lib.py:153] step: 480850, training_loss: 1.04857e+02
I1112 00:17:31.764997 140264174335808 run_lib.py:153] step: 480900, training_loss: 1.32245e+02
I1112 00:17:41.244597 140264174335808 run_lib.py:153] step: 480950, training_loss: 1.21298e+02
I1112 00:17:50.522759 140264174335808 run_lib.py:153] step: 481000, training_loss: 1.06138e+02
I1112 00:18:00.387914 140264174335808 run_lib.py:153] step: 481050, training_loss: 1.33613e+02
I1112 00:18:10.150064 140264174335808 run_lib.py:153] step: 481100, training_loss: 1.42749e+02
I1112 00:18:20.494404 140264174335808 run_lib.py:153] step: 481150, training_loss: 1.33652e+02
I1112 00:18:30.974113 140264174335808 run_lib.py:153] step: 481200, training_loss: 1.44501e+02
I1112 00:18:40.651485 140264174335808 run_lib.py:153] step: 481250, training_loss: 1.18733e+02
I1112 00:18:49.928181 140264174335808 run_lib.py:153] step: 481300, training_loss: 1.29439e+02
I1112 00:18:59.763940 140264174335808 run_lib.py:153] step: 481350, training_loss: 1.02576e+02
I1112 00:19:09.637372 140264174335808 run_lib.py:153] step: 481400, training_loss: 1.28484e+02
I1112 00:19:19.510695 140264174335808 run_lib.py:153] step: 481450, training_loss: 1.13608e+02
I1112 00:19:29.297278 140264174335808 run_lib.py:153] step: 481500, training_loss: 1.63517e+02
I1112 00:19:39.247411 140264174335808 run_lib.py:153] step: 481550, training_loss: 1.35471e+02
I1112 00:19:48.774646 140264174335808 run_lib.py:153] step: 481600, training_loss: 1.39739e+02
I1112 00:19:58.799218 140264174335808 run_lib.py:153] step: 481650, training_loss: 1.06456e+02
I1112 00:20:09.000248 140264174335808 run_lib.py:153] step: 481700, training_loss: 1.23889e+02
I1112 00:20:20.142416 140264174335808 run_lib.py:153] step: 481750, training_loss: 1.26775e+02
I1112 00:20:30.382417 140264174335808 run_lib.py:153] step: 481800, training_loss: 1.28963e+02
I1112 00:20:40.579096 140264174335808 run_lib.py:153] step: 481850, training_loss: 1.70029e+02
I1112 00:20:51.329222 140264174335808 run_lib.py:153] step: 481900, training_loss: 1.42482e+02
I1112 00:21:01.786017 140264174335808 run_lib.py:153] step: 481950, training_loss: 1.29228e+02
I1112 00:21:11.775653 140264174335808 run_lib.py:153] step: 482000, training_loss: 1.67422e+02
I1112 00:21:21.842625 140264174335808 run_lib.py:153] step: 482050, training_loss: 1.24635e+02
I1112 00:21:31.998003 140264174335808 run_lib.py:153] step: 482100, training_loss: 1.13734e+02
I1112 00:21:42.530260 140264174335808 run_lib.py:153] step: 482150, training_loss: 1.21215e+02
I1112 00:21:52.802719 140264174335808 run_lib.py:153] step: 482200, training_loss: 1.11694e+02
I1112 00:22:02.387448 140264174335808 run_lib.py:153] step: 482250, training_loss: 1.36998e+02
I1112 00:22:11.899255 140264174335808 run_lib.py:153] step: 482300, training_loss: 1.26385e+02
I1112 00:22:21.618660 140264174335808 run_lib.py:153] step: 482350, training_loss: 1.23117e+02
I1112 00:22:32.672037 140264174335808 run_lib.py:153] step: 482400, training_loss: 1.39733e+02
I1112 00:22:43.681477 140264174335808 run_lib.py:153] step: 482450, training_loss: 1.11308e+02
I1112 00:22:53.648762 140264174335808 run_lib.py:153] step: 482500, training_loss: 1.29909e+02
I1112 00:23:03.437234 140264174335808 run_lib.py:153] step: 482550, training_loss: 1.17676e+02
I1112 00:23:14.328126 140264174335808 run_lib.py:153] step: 482600, training_loss: 1.41221e+02
I1112 00:23:24.183591 140264174335808 run_lib.py:153] step: 482650, training_loss: 1.20679e+02
I1112 00:23:34.372871 140264174335808 run_lib.py:153] step: 482700, training_loss: 1.64834e+02
I1112 00:23:44.225740 140264174335808 run_lib.py:153] step: 482750, training_loss: 1.20948e+02
I1112 00:23:53.959419 140264174335808 run_lib.py:153] step: 482800, training_loss: 1.35228e+02
I1112 00:24:03.846912 140264174335808 run_lib.py:153] step: 482850, training_loss: 1.34554e+02
I1112 00:24:13.922040 140264174335808 run_lib.py:153] step: 482900, training_loss: 9.62732e+01
I1112 00:24:24.308919 140264174335808 run_lib.py:153] step: 482950, training_loss: 1.21518e+02
I1112 00:24:34.450149 140264174335808 run_lib.py:153] step: 483000, training_loss: 1.19731e+02
I1112 00:24:45.195464 140264174335808 run_lib.py:153] step: 483050, training_loss: 1.26625e+02
I1112 00:24:55.501701 140264174335808 run_lib.py:153] step: 483100, training_loss: 1.05018e+02
I1112 00:25:05.162470 140264174335808 run_lib.py:153] step: 483150, training_loss: 1.28233e+02
I1112 00:25:15.197578 140264174335808 run_lib.py:153] step: 483200, training_loss: 1.34178e+02
I1112 00:25:24.656647 140264174335808 run_lib.py:153] step: 483250, training_loss: 1.47874e+02
I1112 00:25:34.214468 140264174335808 run_lib.py:153] step: 483300, training_loss: 1.31461e+02
I1112 00:25:43.869558 140264174335808 run_lib.py:153] step: 483350, training_loss: 1.41594e+02
I1112 00:25:54.476275 140264174335808 run_lib.py:153] step: 483400, training_loss: 1.05950e+02
I1112 00:26:05.154443 140264174335808 run_lib.py:153] step: 483450, training_loss: 1.33044e+02
I1112 00:26:15.764796 140264174335808 run_lib.py:153] step: 483500, training_loss: 1.20805e+02
I1112 00:26:26.081027 140264174335808 run_lib.py:153] step: 483550, training_loss: 9.91151e+01
I1112 00:26:36.327381 140264174335808 run_lib.py:153] step: 483600, training_loss: 1.16389e+02
I1112 00:26:46.732991 140264174335808 run_lib.py:153] step: 483650, training_loss: 1.23952e+02
I1112 00:26:57.216938 140264174335808 run_lib.py:153] step: 483700, training_loss: 1.17892e+02
I1112 00:27:07.108606 140264174335808 run_lib.py:153] step: 483750, training_loss: 1.01549e+02
I1112 00:27:17.428692 140264174335808 run_lib.py:153] step: 483800, training_loss: 1.78085e+02
I1112 00:27:27.383187 140264174335808 run_lib.py:153] step: 483850, training_loss: 1.17433e+02
I1112 00:27:37.516506 140264174335808 run_lib.py:153] step: 483900, training_loss: 1.42188e+02
I1112 00:27:47.635896 140264174335808 run_lib.py:153] step: 483950, training_loss: 1.10662e+02
I1112 00:27:58.130699 140264174335808 run_lib.py:153] step: 484000, training_loss: 1.07636e+02
I1112 00:28:08.158889 140264174335808 run_lib.py:153] step: 484050, training_loss: 9.00497e+01
I1112 00:28:18.600039 140264174335808 run_lib.py:153] step: 484100, training_loss: 1.27809e+02
I1112 00:28:28.663434 140264174335808 run_lib.py:153] step: 484150, training_loss: 9.35483e+01
I1112 00:28:38.496656 140264174335808 run_lib.py:153] step: 484200, training_loss: 1.22895e+02
I1112 00:28:48.204886 140264174335808 run_lib.py:153] step: 484250, training_loss: 1.14633e+02
I1112 00:28:57.619936 140264174335808 run_lib.py:153] step: 484300, training_loss: 1.23866e+02
I1112 00:29:07.362909 140264174335808 run_lib.py:153] step: 484350, training_loss: 1.53644e+02
I1112 00:29:16.977202 140264174335808 run_lib.py:153] step: 484400, training_loss: 1.39624e+02
I1112 00:29:26.668366 140264174335808 run_lib.py:153] step: 484450, training_loss: 1.11700e+02
I1112 00:29:37.617406 140264174335808 run_lib.py:153] step: 484500, training_loss: 1.24683e+02
I1112 00:29:48.776212 140264174335808 run_lib.py:153] step: 484550, training_loss: 8.25090e+01
I1112 00:29:59.130754 140264174335808 run_lib.py:153] step: 484600, training_loss: 9.77779e+01
I1112 00:30:08.856108 140264174335808 run_lib.py:153] step: 484650, training_loss: 1.42489e+02
I1112 00:30:19.407924 140264174335808 run_lib.py:153] step: 484700, training_loss: 1.31952e+02
I1112 00:30:30.259302 140264174335808 run_lib.py:153] step: 484750, training_loss: 1.30106e+02
I1112 00:30:40.138953 140264174335808 run_lib.py:153] step: 484800, training_loss: 1.59372e+02
I1112 00:30:50.202739 140264174335808 run_lib.py:153] step: 484850, training_loss: 1.37960e+02
I1112 00:31:00.206655 140264174335808 run_lib.py:153] step: 484900, training_loss: 1.30935e+02
I1112 00:31:09.782978 140264174335808 run_lib.py:153] step: 484950, training_loss: 1.13099e+02
I1112 00:31:19.223880 140264174335808 run_lib.py:153] step: 485000, training_loss: 1.07190e+02
I1112 00:31:19.327216 140264174335808 run_lib.py:166] step: 485000, eval_loss: 1.29973e+02
I1112 00:31:28.645820 140264174335808 run_lib.py:153] step: 485050, training_loss: 1.58509e+02
I1112 00:31:38.318296 140264174335808 run_lib.py:153] step: 485100, training_loss: 1.29262e+02
I1112 00:31:47.757463 140264174335808 run_lib.py:153] step: 485150, training_loss: 1.02315e+02
I1112 00:31:57.485944 140264174335808 run_lib.py:153] step: 485200, training_loss: 1.51754e+02
I1112 00:32:07.865066 140264174335808 run_lib.py:153] step: 485250, training_loss: 1.10908e+02
I1112 00:32:17.962546 140264174335808 run_lib.py:153] step: 485300, training_loss: 1.38769e+02
I1112 00:32:28.274298 140264174335808 run_lib.py:153] step: 485350, training_loss: 1.36711e+02
I1112 00:32:38.674564 140264174335808 run_lib.py:153] step: 485400, training_loss: 1.04132e+02
I1112 00:32:49.062807 140264174335808 run_lib.py:153] step: 485450, training_loss: 1.50349e+02
I1112 00:32:59.708330 140264174335808 run_lib.py:153] step: 485500, training_loss: 1.75826e+02
I1112 00:33:09.851639 140264174335808 run_lib.py:153] step: 485550, training_loss: 1.12647e+02
I1112 00:33:20.156003 140264174335808 run_lib.py:153] step: 485600, training_loss: 1.20014e+02
I1112 00:33:29.942326 140264174335808 run_lib.py:153] step: 485650, training_loss: 1.25513e+02
I1112 00:33:39.881775 140264174335808 run_lib.py:153] step: 485700, training_loss: 1.41136e+02
I1112 00:33:50.574986 140264174335808 run_lib.py:153] step: 485750, training_loss: 1.30230e+02
I1112 00:34:00.551371 140264174335808 run_lib.py:153] step: 485800, training_loss: 1.28641e+02
I1112 00:34:10.341221 140264174335808 run_lib.py:153] step: 485850, training_loss: 1.34203e+02
I1112 00:34:19.704277 140264174335808 run_lib.py:153] step: 485900, training_loss: 1.15715e+02
I1112 00:34:30.001590 140264174335808 run_lib.py:153] step: 485950, training_loss: 1.19563e+02
I1112 00:34:40.807680 140264174335808 run_lib.py:153] step: 486000, training_loss: 1.56746e+02
I1112 00:34:50.534271 140264174335808 run_lib.py:153] step: 486050, training_loss: 1.22572e+02
I1112 00:35:00.516090 140264174335808 run_lib.py:153] step: 486100, training_loss: 1.31953e+02
I1112 00:35:10.592392 140264174335808 run_lib.py:153] step: 486150, training_loss: 1.34314e+02
I1112 00:35:20.624353 140264174335808 run_lib.py:153] step: 486200, training_loss: 1.06849e+02
I1112 00:35:31.577937 140264174335808 run_lib.py:153] step: 486250, training_loss: 1.42281e+02
I1112 00:35:41.484533 140264174335808 run_lib.py:153] step: 486300, training_loss: 1.31660e+02
I1112 00:35:52.010734 140264174335808 run_lib.py:153] step: 486350, training_loss: 1.12965e+02
I1112 00:36:02.711431 140264174335808 run_lib.py:153] step: 486400, training_loss: 1.32270e+02
I1112 00:36:13.136295 140264174335808 run_lib.py:153] step: 486450, training_loss: 1.07015e+02
I1112 00:36:24.206101 140264174335808 run_lib.py:153] step: 486500, training_loss: 1.16263e+02
I1112 00:36:35.013938 140264174335808 run_lib.py:153] step: 486550, training_loss: 1.62891e+02
I1112 00:36:45.458795 140264174335808 run_lib.py:153] step: 486600, training_loss: 1.47198e+02
I1112 00:36:55.305466 140264174335808 run_lib.py:153] step: 486650, training_loss: 1.29155e+02
I1112 00:37:05.064854 140264174335808 run_lib.py:153] step: 486700, training_loss: 1.23345e+02
I1112 00:37:15.208610 140264174335808 run_lib.py:153] step: 486750, training_loss: 1.27098e+02
I1112 00:37:25.184612 140264174335808 run_lib.py:153] step: 486800, training_loss: 1.35017e+02
I1112 00:37:34.939839 140264174335808 run_lib.py:153] step: 486850, training_loss: 1.14411e+02
I1112 00:37:45.023673 140264174335808 run_lib.py:153] step: 486900, training_loss: 1.33187e+02
I1112 00:37:55.733460 140264174335808 run_lib.py:153] step: 486950, training_loss: 1.48428e+02
I1112 00:38:06.420468 140264174335808 run_lib.py:153] step: 487000, training_loss: 1.30845e+02
I1112 00:38:15.858764 140264174335808 run_lib.py:153] step: 487050, training_loss: 1.13196e+02
I1112 00:38:26.047323 140264174335808 run_lib.py:153] step: 487100, training_loss: 1.15391e+02
I1112 00:38:35.804731 140264174335808 run_lib.py:153] step: 487150, training_loss: 1.35918e+02
I1112 00:38:46.272904 140264174335808 run_lib.py:153] step: 487200, training_loss: 1.40099e+02
I1112 00:38:57.072871 140264174335808 run_lib.py:153] step: 487250, training_loss: 1.43473e+02
I1112 00:39:08.262390 140264174335808 run_lib.py:153] step: 487300, training_loss: 1.39590e+02
I1112 00:39:18.916221 140264174335808 run_lib.py:153] step: 487350, training_loss: 1.30449e+02
I1112 00:39:29.423673 140264174335808 run_lib.py:153] step: 487400, training_loss: 1.10809e+02
I1112 00:39:40.222940 140264174335808 run_lib.py:153] step: 487450, training_loss: 1.15707e+02
I1112 00:39:50.381638 140264174335808 run_lib.py:153] step: 487500, training_loss: 1.09178e+02
I1112 00:39:59.792375 140264174335808 run_lib.py:153] step: 487550, training_loss: 1.25577e+02
I1112 00:40:09.586050 140264174335808 run_lib.py:153] step: 487600, training_loss: 1.18710e+02
I1112 00:40:19.771872 140264174335808 run_lib.py:153] step: 487650, training_loss: 1.13965e+02
I1112 00:40:30.518093 140264174335808 run_lib.py:153] step: 487700, training_loss: 1.14843e+02
I1112 00:40:41.153236 140264174335808 run_lib.py:153] step: 487750, training_loss: 1.18206e+02
I1112 00:40:51.484982 140264174335808 run_lib.py:153] step: 487800, training_loss: 1.31865e+02
I1112 00:41:01.317264 140264174335808 run_lib.py:153] step: 487850, training_loss: 1.06295e+02
I1112 00:41:11.045576 140264174335808 run_lib.py:153] step: 487900, training_loss: 1.23363e+02
I1112 00:41:21.688002 140264174335808 run_lib.py:153] step: 487950, training_loss: 1.16538e+02
I1112 00:41:31.826339 140264174335808 run_lib.py:153] step: 488000, training_loss: 1.25757e+02
I1112 00:41:41.605214 140264174335808 run_lib.py:153] step: 488050, training_loss: 1.63631e+02
I1112 00:41:51.450536 140264174335808 run_lib.py:153] step: 488100, training_loss: 1.02864e+02
I1112 00:42:01.695962 140264174335808 run_lib.py:153] step: 488150, training_loss: 1.28565e+02
I1112 00:42:11.903610 140264174335808 run_lib.py:153] step: 488200, training_loss: 1.38183e+02
I1112 00:42:21.995045 140264174335808 run_lib.py:153] step: 488250, training_loss: 1.48203e+02
I1112 00:42:31.476137 140264174335808 run_lib.py:153] step: 488300, training_loss: 1.23729e+02
I1112 00:42:41.723004 140264174335808 run_lib.py:153] step: 488350, training_loss: 1.42040e+02
I1112 00:42:51.970295 140264174335808 run_lib.py:153] step: 488400, training_loss: 1.20082e+02
I1112 00:43:01.671895 140264174335808 run_lib.py:153] step: 488450, training_loss: 1.33557e+02
I1112 00:43:11.936015 140264174335808 run_lib.py:153] step: 488500, training_loss: 1.50539e+02
I1112 00:43:22.199429 140264174335808 run_lib.py:153] step: 488550, training_loss: 1.31818e+02
I1112 00:43:32.834521 140264174335808 run_lib.py:153] step: 488600, training_loss: 1.29583e+02
I1112 00:43:43.128427 140264174335808 run_lib.py:153] step: 488650, training_loss: 1.44994e+02
I1112 00:43:53.459265 140264174335808 run_lib.py:153] step: 488700, training_loss: 1.38580e+02
I1112 00:44:03.664588 140264174335808 run_lib.py:153] step: 488750, training_loss: 1.53587e+02
I1112 00:44:13.913573 140264174335808 run_lib.py:153] step: 488800, training_loss: 1.35700e+02
I1112 00:44:24.242161 140264174335808 run_lib.py:153] step: 488850, training_loss: 1.10607e+02
I1112 00:44:35.221776 140264174335808 run_lib.py:153] step: 488900, training_loss: 1.28857e+02
I1112 00:44:45.441850 140264174335808 run_lib.py:153] step: 488950, training_loss: 1.20808e+02
I1112 00:44:55.607439 140264174335808 run_lib.py:153] step: 489000, training_loss: 1.15655e+02
I1112 00:45:06.505936 140264174335808 run_lib.py:153] step: 489050, training_loss: 1.13022e+02
I1112 00:45:16.232247 140264174335808 run_lib.py:153] step: 489100, training_loss: 1.20605e+02
I1112 00:45:27.029033 140264174335808 run_lib.py:153] step: 489150, training_loss: 1.02797e+02
I1112 00:45:37.227970 140264174335808 run_lib.py:153] step: 489200, training_loss: 1.36757e+02
I1112 00:45:48.055896 140264174335808 run_lib.py:153] step: 489250, training_loss: 1.33721e+02
I1112 00:45:57.786046 140264174335808 run_lib.py:153] step: 489300, training_loss: 1.15963e+02
I1112 00:46:08.529820 140264174335808 run_lib.py:153] step: 489350, training_loss: 1.23221e+02
I1112 00:46:18.893684 140264174335808 run_lib.py:153] step: 489400, training_loss: 1.47287e+02
I1112 00:46:29.672954 140264174335808 run_lib.py:153] step: 489450, training_loss: 1.16443e+02
I1112 00:46:39.906890 140264174335808 run_lib.py:153] step: 489500, training_loss: 1.27867e+02
I1112 00:46:50.329624 140264174335808 run_lib.py:153] step: 489550, training_loss: 1.67157e+02
I1112 00:47:00.467388 140264174335808 run_lib.py:153] step: 489600, training_loss: 1.21371e+02
I1112 00:47:10.348590 140264174335808 run_lib.py:153] step: 489650, training_loss: 9.80381e+01
I1112 00:47:20.386628 140264174335808 run_lib.py:153] step: 489700, training_loss: 1.23717e+02
I1112 00:47:30.653599 140264174335808 run_lib.py:153] step: 489750, training_loss: 1.36487e+02
I1112 00:47:40.846151 140264174335808 run_lib.py:153] step: 489800, training_loss: 1.21803e+02
I1112 00:47:51.036333 140264174335808 run_lib.py:153] step: 489850, training_loss: 1.33073e+02
I1112 00:48:01.198069 140264174335808 run_lib.py:153] step: 489900, training_loss: 1.32435e+02
I1112 00:48:11.492998 140264174335808 run_lib.py:153] step: 489950, training_loss: 1.43042e+02
I1112 00:48:21.791330 140264174335808 run_lib.py:153] step: 490000, training_loss: 1.37754e+02
I1112 00:48:22.334902 140264174335808 run_lib.py:166] step: 490000, eval_loss: 1.26335e+02
I1112 00:48:31.611618 140264174335808 run_lib.py:153] step: 490050, training_loss: 1.16941e+02
I1112 00:48:41.081886 140264174335808 run_lib.py:153] step: 490100, training_loss: 1.05579e+02
I1112 00:48:51.798512 140264174335808 run_lib.py:153] step: 490150, training_loss: 1.26459e+02
I1112 00:49:01.432141 140264174335808 run_lib.py:153] step: 490200, training_loss: 1.25589e+02
I1112 00:49:11.085314 140264174335808 run_lib.py:153] step: 490250, training_loss: 1.33584e+02
I1112 00:49:21.322233 140264174335808 run_lib.py:153] step: 490300, training_loss: 1.35129e+02
I1112 00:49:30.549095 140264174335808 run_lib.py:153] step: 490350, training_loss: 1.39849e+02
I1112 00:49:40.674023 140264174335808 run_lib.py:153] step: 490400, training_loss: 1.26626e+02
I1112 00:49:50.627951 140264174335808 run_lib.py:153] step: 490450, training_loss: 1.08177e+02
I1112 00:50:00.784321 140264174335808 run_lib.py:153] step: 490500, training_loss: 1.49142e+02
I1112 00:50:11.147262 140264174335808 run_lib.py:153] step: 490550, training_loss: 1.22308e+02
I1112 00:50:21.624790 140264174335808 run_lib.py:153] step: 490600, training_loss: 1.29014e+02
I1112 00:50:31.710317 140264174335808 run_lib.py:153] step: 490650, training_loss: 1.33620e+02
I1112 00:50:42.363031 140264174335808 run_lib.py:153] step: 490700, training_loss: 1.14493e+02
I1112 00:50:52.266329 140264174335808 run_lib.py:153] step: 490750, training_loss: 1.10465e+02
I1112 00:51:02.074393 140264174335808 run_lib.py:153] step: 490800, training_loss: 1.25720e+02
I1112 00:51:12.602328 140264174335808 run_lib.py:153] step: 490850, training_loss: 8.68476e+01
I1112 00:51:22.373134 140264174335808 run_lib.py:153] step: 490900, training_loss: 1.35099e+02
I1112 00:51:32.849884 140264174335808 run_lib.py:153] step: 490950, training_loss: 1.12445e+02
I1112 00:51:42.261229 140264174335808 run_lib.py:153] step: 491000, training_loss: 1.49179e+02
I1112 00:51:51.618657 140264174335808 run_lib.py:153] step: 491050, training_loss: 1.14673e+02
I1112 00:52:01.681849 140264174335808 run_lib.py:153] step: 491100, training_loss: 1.18844e+02
I1112 00:52:11.823129 140264174335808 run_lib.py:153] step: 491150, training_loss: 1.17495e+02
I1112 00:52:22.121717 140264174335808 run_lib.py:153] step: 491200, training_loss: 1.23083e+02
I1112 00:52:31.651544 140264174335808 run_lib.py:153] step: 491250, training_loss: 1.22954e+02
I1112 00:52:42.489281 140264174335808 run_lib.py:153] step: 491300, training_loss: 1.39982e+02
I1112 00:52:51.914741 140264174335808 run_lib.py:153] step: 491350, training_loss: 1.18600e+02
I1112 00:53:01.194764 140264174335808 run_lib.py:153] step: 491400, training_loss: 1.42197e+02
I1112 00:53:11.721909 140264174335808 run_lib.py:153] step: 491450, training_loss: 1.38981e+02
I1112 00:53:21.663905 140264174335808 run_lib.py:153] step: 491500, training_loss: 1.39809e+02
I1112 00:53:31.549792 140264174335808 run_lib.py:153] step: 491550, training_loss: 1.25852e+02
I1112 00:53:42.087406 140264174335808 run_lib.py:153] step: 491600, training_loss: 1.44983e+02
I1112 00:53:52.491392 140264174335808 run_lib.py:153] step: 491650, training_loss: 1.27763e+02
I1112 00:54:03.539244 140264174335808 run_lib.py:153] step: 491700, training_loss: 1.15429e+02
I1112 00:54:14.249304 140264174335808 run_lib.py:153] step: 491750, training_loss: 1.13748e+02
I1112 00:54:23.724782 140264174335808 run_lib.py:153] step: 491800, training_loss: 1.54585e+02
I1112 00:54:33.381720 140264174335808 run_lib.py:153] step: 491850, training_loss: 1.23651e+02
I1112 00:54:43.220762 140264174335808 run_lib.py:153] step: 491900, training_loss: 1.11069e+02
I1112 00:54:52.906130 140264174335808 run_lib.py:153] step: 491950, training_loss: 1.17655e+02
I1112 00:55:02.586481 140264174335808 run_lib.py:153] step: 492000, training_loss: 1.50663e+02
I1112 00:55:12.536170 140264174335808 run_lib.py:153] step: 492050, training_loss: 1.19074e+02
I1112 00:55:22.015920 140264174335808 run_lib.py:153] step: 492100, training_loss: 1.46854e+02
I1112 00:55:32.863853 140264174335808 run_lib.py:153] step: 492150, training_loss: 1.13236e+02
I1112 00:55:43.449835 140264174335808 run_lib.py:153] step: 492200, training_loss: 1.34552e+02
I1112 00:55:54.563005 140264174335808 run_lib.py:153] step: 492250, training_loss: 8.55835e+01
I1112 00:56:04.389663 140264174335808 run_lib.py:153] step: 492300, training_loss: 1.18325e+02
I1112 00:56:14.637510 140264174335808 run_lib.py:153] step: 492350, training_loss: 1.23290e+02
I1112 00:56:25.001827 140264174335808 run_lib.py:153] step: 492400, training_loss: 1.23371e+02
I1112 00:56:34.743772 140264174335808 run_lib.py:153] step: 492450, training_loss: 1.50368e+02
I1112 00:56:45.690769 140264174335808 run_lib.py:153] step: 492500, training_loss: 1.31948e+02
I1112 00:56:55.189749 140264174335808 run_lib.py:153] step: 492550, training_loss: 1.33281e+02
I1112 00:57:04.723479 140264174335808 run_lib.py:153] step: 492600, training_loss: 1.15041e+02
I1112 00:57:14.480946 140264174335808 run_lib.py:153] step: 492650, training_loss: 9.87981e+01
I1112 00:57:24.633965 140264174335808 run_lib.py:153] step: 492700, training_loss: 1.18955e+02
I1112 00:57:34.716260 140264174335808 run_lib.py:153] step: 492750, training_loss: 1.37688e+02
I1112 00:57:44.520147 140264174335808 run_lib.py:153] step: 492800, training_loss: 1.00503e+02
I1112 00:57:54.659408 140264174335808 run_lib.py:153] step: 492850, training_loss: 1.20020e+02
I1112 00:58:04.406266 140264174335808 run_lib.py:153] step: 492900, training_loss: 1.25503e+02
I1112 00:58:15.534624 140264174335808 run_lib.py:153] step: 492950, training_loss: 1.30016e+02
I1112 00:58:26.090442 140264174335808 run_lib.py:153] step: 493000, training_loss: 1.19534e+02
I1112 00:58:35.676104 140264174335808 run_lib.py:153] step: 493050, training_loss: 1.34876e+02
I1112 00:58:45.251487 140264174335808 run_lib.py:153] step: 493100, training_loss: 1.36515e+02
I1112 00:58:55.619737 140264174335808 run_lib.py:153] step: 493150, training_loss: 1.57686e+02
I1112 00:59:05.126271 140264174335808 run_lib.py:153] step: 493200, training_loss: 1.05387e+02
I1112 00:59:15.213193 140264174335808 run_lib.py:153] step: 493250, training_loss: 9.69979e+01
I1112 00:59:25.715778 140264174335808 run_lib.py:153] step: 493300, training_loss: 1.51016e+02
I1112 00:59:35.628074 140264174335808 run_lib.py:153] step: 493350, training_loss: 1.37674e+02
I1112 00:59:45.469883 140264174335808 run_lib.py:153] step: 493400, training_loss: 1.26509e+02
I1112 00:59:54.832865 140264174335808 run_lib.py:153] step: 493450, training_loss: 1.39173e+02
I1112 01:00:04.790712 140264174335808 run_lib.py:153] step: 493500, training_loss: 1.26986e+02
I1112 01:00:15.086476 140264174335808 run_lib.py:153] step: 493550, training_loss: 1.25052e+02
I1112 01:00:25.278484 140264174335808 run_lib.py:153] step: 493600, training_loss: 1.16108e+02
I1112 01:00:34.673919 140264174335808 run_lib.py:153] step: 493650, training_loss: 1.68380e+02
I1112 01:00:44.244845 140264174335808 run_lib.py:153] step: 493700, training_loss: 1.47638e+02
I1112 01:00:54.585159 140264174335808 run_lib.py:153] step: 493750, training_loss: 1.20079e+02
I1112 01:01:04.213744 140264174335808 run_lib.py:153] step: 493800, training_loss: 1.39864e+02
I1112 01:01:14.288618 140264174335808 run_lib.py:153] step: 493850, training_loss: 1.20281e+02
I1112 01:01:25.276412 140264174335808 run_lib.py:153] step: 493900, training_loss: 1.24040e+02
I1112 01:01:35.998800 140264174335808 run_lib.py:153] step: 493950, training_loss: 1.14608e+02
I1112 01:01:45.860502 140264174335808 run_lib.py:153] step: 494000, training_loss: 1.33264e+02
I1112 01:01:55.323973 140264174335808 run_lib.py:153] step: 494050, training_loss: 1.34413e+02
I1112 01:02:05.854198 140264174335808 run_lib.py:153] step: 494100, training_loss: 8.94463e+01
I1112 01:02:15.873579 140264174335808 run_lib.py:153] step: 494150, training_loss: 1.28285e+02
I1112 01:02:25.527769 140264174335808 run_lib.py:153] step: 494200, training_loss: 1.34042e+02
I1112 01:02:35.762495 140264174335808 run_lib.py:153] step: 494250, training_loss: 1.55571e+02
I1112 01:02:45.606008 140264174335808 run_lib.py:153] step: 494300, training_loss: 1.24833e+02
I1112 01:02:56.689637 140264174335808 run_lib.py:153] step: 494350, training_loss: 1.14688e+02
I1112 01:03:06.970260 140264174335808 run_lib.py:153] step: 494400, training_loss: 1.08098e+02
I1112 01:03:17.981804 140264174335808 run_lib.py:153] step: 494450, training_loss: 1.14803e+02
I1112 01:03:28.278211 140264174335808 run_lib.py:153] step: 494500, training_loss: 1.45388e+02
I1112 01:03:38.851120 140264174335808 run_lib.py:153] step: 494550, training_loss: 1.42600e+02
I1112 01:03:48.584387 140264174335808 run_lib.py:153] step: 494600, training_loss: 1.32764e+02
I1112 01:03:59.361190 140264174335808 run_lib.py:153] step: 494650, training_loss: 1.28420e+02
I1112 01:04:09.589956 140264174335808 run_lib.py:153] step: 494700, training_loss: 1.15154e+02
I1112 01:04:19.834236 140264174335808 run_lib.py:153] step: 494750, training_loss: 1.20299e+02
I1112 01:04:30.325066 140264174335808 run_lib.py:153] step: 494800, training_loss: 1.19515e+02
I1112 01:04:40.356144 140264174335808 run_lib.py:153] step: 494850, training_loss: 1.22045e+02
I1112 01:04:50.678342 140264174335808 run_lib.py:153] step: 494900, training_loss: 1.14600e+02
I1112 01:05:01.258334 140264174335808 run_lib.py:153] step: 494950, training_loss: 9.69434e+01
I1112 01:05:11.671254 140264174335808 run_lib.py:153] step: 495000, training_loss: 1.34513e+02
I1112 01:05:11.810257 140264174335808 run_lib.py:166] step: 495000, eval_loss: 1.01071e+02
I1112 01:05:22.154738 140264174335808 run_lib.py:153] step: 495050, training_loss: 1.32363e+02
I1112 01:05:32.302293 140264174335808 run_lib.py:153] step: 495100, training_loss: 1.38145e+02
I1112 01:05:42.213638 140264174335808 run_lib.py:153] step: 495150, training_loss: 1.05958e+02
I1112 01:05:52.272985 140264174335808 run_lib.py:153] step: 495200, training_loss: 1.41826e+02
I1112 01:06:02.655124 140264174335808 run_lib.py:153] step: 495250, training_loss: 1.41716e+02
I1112 01:06:12.846158 140264174335808 run_lib.py:153] step: 495300, training_loss: 1.31754e+02
I1112 01:06:23.473122 140264174335808 run_lib.py:153] step: 495350, training_loss: 9.49808e+01
I1112 01:06:33.302799 140264174335808 run_lib.py:153] step: 495400, training_loss: 1.27838e+02
I1112 01:06:43.125268 140264174335808 run_lib.py:153] step: 495450, training_loss: 1.09648e+02
I1112 01:06:52.735826 140264174335808 run_lib.py:153] step: 495500, training_loss: 1.24075e+02
I1112 01:07:03.420578 140264174335808 run_lib.py:153] step: 495550, training_loss: 1.37681e+02
I1112 01:07:13.473396 140264174335808 run_lib.py:153] step: 495600, training_loss: 1.01295e+02
I1112 01:07:23.653282 140264174335808 run_lib.py:153] step: 495650, training_loss: 1.21059e+02
I1112 01:07:33.552695 140264174335808 run_lib.py:153] step: 495700, training_loss: 1.02642e+02
I1112 01:07:43.430676 140264174335808 run_lib.py:153] step: 495750, training_loss: 1.09157e+02
I1112 01:07:53.471779 140264174335808 run_lib.py:153] step: 495800, training_loss: 1.14960e+02
I1112 01:08:03.877214 140264174335808 run_lib.py:153] step: 495850, training_loss: 1.39805e+02
I1112 01:08:14.446563 140264174335808 run_lib.py:153] step: 495900, training_loss: 1.25264e+02
I1112 01:08:24.310683 140264174335808 run_lib.py:153] step: 495950, training_loss: 1.26272e+02
I1112 01:08:34.031126 140264174335808 run_lib.py:153] step: 496000, training_loss: 1.22489e+02
I1112 01:08:43.659099 140264174335808 run_lib.py:153] step: 496050, training_loss: 9.85554e+01
I1112 01:08:53.757026 140264174335808 run_lib.py:153] step: 496100, training_loss: 1.46876e+02
I1112 01:09:03.293408 140264174335808 run_lib.py:153] step: 496150, training_loss: 1.17019e+02
I1112 01:09:13.725728 140264174335808 run_lib.py:153] step: 496200, training_loss: 1.41913e+02
I1112 01:09:23.952697 140264174335808 run_lib.py:153] step: 496250, training_loss: 1.39044e+02
I1112 01:09:35.095461 140264174335808 run_lib.py:153] step: 496300, training_loss: 1.33019e+02
I1112 01:09:45.107600 140264174335808 run_lib.py:153] step: 496350, training_loss: 1.50682e+02
I1112 01:09:54.685926 140264174335808 run_lib.py:153] step: 496400, training_loss: 1.40364e+02
I1112 01:10:04.461139 140264174335808 run_lib.py:153] step: 496450, training_loss: 1.25968e+02
I1112 01:10:14.851090 140264174335808 run_lib.py:153] step: 496500, training_loss: 1.05602e+02
I1112 01:10:25.328577 140264174335808 run_lib.py:153] step: 496550, training_loss: 1.29142e+02
I1112 01:10:36.222310 140264174335808 run_lib.py:153] step: 496600, training_loss: 1.30942e+02
I1112 01:10:46.799820 140264174335808 run_lib.py:153] step: 496650, training_loss: 1.42908e+02
I1112 01:10:57.119337 140264174335808 run_lib.py:153] step: 496700, training_loss: 1.21521e+02
I1112 01:11:07.330829 140264174335808 run_lib.py:153] step: 496750, training_loss: 1.23515e+02
I1112 01:11:17.306527 140264174335808 run_lib.py:153] step: 496800, training_loss: 1.32568e+02
I1112 01:11:26.942451 140264174335808 run_lib.py:153] step: 496850, training_loss: 1.19592e+02
I1112 01:11:37.131937 140264174335808 run_lib.py:153] step: 496900, training_loss: 1.15176e+02
I1112 01:11:47.566132 140264174335808 run_lib.py:153] step: 496950, training_loss: 1.28291e+02
I1112 01:11:57.755026 140264174335808 run_lib.py:153] step: 497000, training_loss: 1.25764e+02
I1112 01:12:07.982086 140264174335808 run_lib.py:153] step: 497050, training_loss: 1.37979e+02
I1112 01:12:17.812021 140264174335808 run_lib.py:153] step: 497100, training_loss: 1.17593e+02
I1112 01:12:27.786923 140264174335808 run_lib.py:153] step: 497150, training_loss: 1.26324e+02
I1112 01:12:37.715592 140264174335808 run_lib.py:153] step: 497200, training_loss: 1.17299e+02
I1112 01:12:48.013180 140264174335808 run_lib.py:153] step: 497250, training_loss: 1.51669e+02
I1112 01:12:57.904713 140264174335808 run_lib.py:153] step: 497300, training_loss: 1.28838e+02
I1112 01:13:08.024036 140264174335808 run_lib.py:153] step: 497350, training_loss: 1.23068e+02
I1112 01:13:18.096503 140264174335808 run_lib.py:153] step: 497400, training_loss: 1.26141e+02
I1112 01:13:27.412132 140264174335808 run_lib.py:153] step: 497450, training_loss: 9.92557e+01
I1112 01:13:37.503374 140264174335808 run_lib.py:153] step: 497500, training_loss: 1.17559e+02
I1112 01:13:48.158144 140264174335808 run_lib.py:153] step: 497550, training_loss: 1.06829e+02
I1112 01:13:58.108741 140264174335808 run_lib.py:153] step: 497600, training_loss: 1.10946e+02
I1112 01:14:08.291439 140264174335808 run_lib.py:153] step: 497650, training_loss: 1.28458e+02
I1112 01:14:17.965099 140264174335808 run_lib.py:153] step: 497700, training_loss: 1.50449e+02
I1112 01:14:28.310472 140264174335808 run_lib.py:153] step: 497750, training_loss: 1.18206e+02
I1112 01:14:38.245370 140264174335808 run_lib.py:153] step: 497800, training_loss: 1.23741e+02
I1112 01:14:48.330319 140264174335808 run_lib.py:153] step: 497850, training_loss: 1.42149e+02
I1112 01:14:59.083283 140264174335808 run_lib.py:153] step: 497900, training_loss: 1.27901e+02
I1112 01:15:09.605172 140264174335808 run_lib.py:153] step: 497950, training_loss: 1.35961e+02
I1112 01:15:19.421238 140264174335808 run_lib.py:153] step: 498000, training_loss: 1.24687e+02
I1112 01:15:28.681540 140264174335808 run_lib.py:153] step: 498050, training_loss: 1.13910e+02
I1112 01:15:38.874344 140264174335808 run_lib.py:153] step: 498100, training_loss: 1.14386e+02
I1112 01:15:49.159337 140264174335808 run_lib.py:153] step: 498150, training_loss: 1.15273e+02
I1112 01:15:59.616750 140264174335808 run_lib.py:153] step: 498200, training_loss: 1.24679e+02
I1112 01:16:10.472109 140264174335808 run_lib.py:153] step: 498250, training_loss: 1.44413e+02
I1112 01:16:20.252552 140264174335808 run_lib.py:153] step: 498300, training_loss: 1.32837e+02
I1112 01:16:30.711699 140264174335808 run_lib.py:153] step: 498350, training_loss: 1.08134e+02
I1112 01:16:40.456032 140264174335808 run_lib.py:153] step: 498400, training_loss: 1.18423e+02
I1112 01:16:49.677612 140264174335808 run_lib.py:153] step: 498450, training_loss: 1.53424e+02
I1112 01:16:59.134545 140264174335808 run_lib.py:153] step: 498500, training_loss: 1.23467e+02
I1112 01:17:09.397486 140264174335808 run_lib.py:153] step: 498550, training_loss: 1.53061e+02
I1112 01:17:19.817415 140264174335808 run_lib.py:153] step: 498600, training_loss: 1.27127e+02
I1112 01:17:30.010592 140264174335808 run_lib.py:153] step: 498650, training_loss: 1.10307e+02
I1112 01:17:40.032846 140264174335808 run_lib.py:153] step: 498700, training_loss: 1.15817e+02
I1112 01:17:49.661865 140264174335808 run_lib.py:153] step: 498750, training_loss: 1.11079e+02
I1112 01:18:00.227737 140264174335808 run_lib.py:153] step: 498800, training_loss: 1.17710e+02
I1112 01:18:10.216177 140264174335808 run_lib.py:153] step: 498850, training_loss: 1.34986e+02
I1112 01:18:20.831342 140264174335808 run_lib.py:153] step: 498900, training_loss: 1.42557e+02
I1112 01:18:31.036467 140264174335808 run_lib.py:153] step: 498950, training_loss: 1.13746e+02
I1112 01:18:41.395812 140264174335808 run_lib.py:153] step: 499000, training_loss: 1.36168e+02
I1112 01:18:51.590327 140264174335808 run_lib.py:153] step: 499050, training_loss: 1.20395e+02
I1112 01:19:01.490868 140264174335808 run_lib.py:153] step: 499100, training_loss: 1.33456e+02
I1112 01:19:11.152263 140264174335808 run_lib.py:153] step: 499150, training_loss: 1.41651e+02
I1112 01:19:20.701038 140264174335808 run_lib.py:153] step: 499200, training_loss: 1.08158e+02
I1112 01:19:30.287629 140264174335808 run_lib.py:153] step: 499250, training_loss: 1.18628e+02
I1112 01:19:40.321586 140264174335808 run_lib.py:153] step: 499300, training_loss: 9.50976e+01
I1112 01:19:50.122331 140264174335808 run_lib.py:153] step: 499350, training_loss: 1.22533e+02
I1112 01:20:00.266599 140264174335808 run_lib.py:153] step: 499400, training_loss: 1.35448e+02
I1112 01:20:09.882317 140264174335808 run_lib.py:153] step: 499450, training_loss: 1.30321e+02
I1112 01:20:19.819459 140264174335808 run_lib.py:153] step: 499500, training_loss: 1.31318e+02
I1112 01:20:29.036172 140264174335808 run_lib.py:153] step: 499550, training_loss: 1.33003e+02
I1112 01:20:38.484443 140264174335808 run_lib.py:153] step: 499600, training_loss: 1.21742e+02
I1112 01:20:48.148629 140264174335808 run_lib.py:153] step: 499650, training_loss: 1.19700e+02
I1112 01:20:58.206061 140264174335808 run_lib.py:153] step: 499700, training_loss: 9.98875e+01
I1112 01:21:09.117302 140264174335808 run_lib.py:153] step: 499750, training_loss: 1.23532e+02
I1112 01:21:19.526323 140264174335808 run_lib.py:153] step: 499800, training_loss: 1.21930e+02
I1112 01:21:29.581683 140264174335808 run_lib.py:153] step: 499850, training_loss: 1.23939e+02
I1112 01:21:40.172446 140264174335808 run_lib.py:153] step: 499900, training_loss: 1.56060e+02
I1112 01:21:50.886497 140264174335808 run_lib.py:153] step: 499950, training_loss: 1.17289e+02
I1112 01:22:00.834060 140264174335808 run_lib.py:153] step: 500000, training_loss: 1.15850e+02
I1112 01:22:01.470046 140264174335808 run_lib.py:166] step: 500000, eval_loss: 1.28775e+02
I1112 01:22:11.940449 140264174335808 run_lib.py:153] step: 500050, training_loss: 1.49033e+02
I1112 01:22:22.100649 140264174335808 run_lib.py:153] step: 500100, training_loss: 9.52846e+01
I1112 01:22:31.954747 140264174335808 run_lib.py:153] step: 500150, training_loss: 9.99726e+01
I1112 01:22:42.394224 140264174335808 run_lib.py:153] step: 500200, training_loss: 1.26956e+02
I1112 01:22:52.232710 140264174335808 run_lib.py:153] step: 500250, training_loss: 1.38566e+02
I1112 01:23:01.780933 140264174335808 run_lib.py:153] step: 500300, training_loss: 1.49315e+02
I1112 01:23:11.413252 140264174335808 run_lib.py:153] step: 500350, training_loss: 1.09854e+02
I1112 01:23:20.989864 140264174335808 run_lib.py:153] step: 500400, training_loss: 1.43478e+02
I1112 01:23:31.528167 140264174335808 run_lib.py:153] step: 500450, training_loss: 1.24552e+02
I1112 01:23:41.812458 140264174335808 run_lib.py:153] step: 500500, training_loss: 1.01313e+02
I1112 01:23:52.616522 140264174335808 run_lib.py:153] step: 500550, training_loss: 1.00806e+02
I1112 01:24:02.638692 140264174335808 run_lib.py:153] step: 500600, training_loss: 1.43477e+02
I1112 01:24:13.227405 140264174335808 run_lib.py:153] step: 500650, training_loss: 1.30176e+02
I1112 01:24:23.485924 140264174335808 run_lib.py:153] step: 500700, training_loss: 1.28146e+02
I1112 01:24:33.075995 140264174335808 run_lib.py:153] step: 500750, training_loss: 1.59420e+02
I1112 01:24:42.897710 140264174335808 run_lib.py:153] step: 500800, training_loss: 1.50910e+02
I1112 01:24:52.881859 140264174335808 run_lib.py:153] step: 500850, training_loss: 1.21053e+02
I1112 01:25:02.453504 140264174335808 run_lib.py:153] step: 500900, training_loss: 1.26995e+02
I1112 01:25:12.861421 140264174335808 run_lib.py:153] step: 500950, training_loss: 1.23267e+02
I1112 01:25:22.470392 140264174335808 run_lib.py:153] step: 501000, training_loss: 1.20631e+02
I1112 01:25:32.333541 140264174335808 run_lib.py:153] step: 501050, training_loss: 1.55940e+02
I1112 01:25:42.914596 140264174335808 run_lib.py:153] step: 501100, training_loss: 1.27604e+02
I1112 01:25:53.083817 140264174335808 run_lib.py:153] step: 501150, training_loss: 1.65860e+02
I1112 01:26:04.109637 140264174335808 run_lib.py:153] step: 501200, training_loss: 1.27199e+02
I1112 01:26:15.044058 140264174335808 run_lib.py:153] step: 501250, training_loss: 1.33784e+02
I1112 01:26:25.061203 140264174335808 run_lib.py:153] step: 501300, training_loss: 1.34890e+02
I1112 01:26:35.955168 140264174335808 run_lib.py:153] step: 501350, training_loss: 1.18997e+02
I1112 01:26:46.314858 140264174335808 run_lib.py:153] step: 501400, training_loss: 1.09353e+02
I1112 01:26:56.209370 140264174335808 run_lib.py:153] step: 501450, training_loss: 1.39023e+02
I1112 01:27:06.338257 140264174335808 run_lib.py:153] step: 501500, training_loss: 1.11896e+02
I1112 01:27:16.766955 140264174335808 run_lib.py:153] step: 501550, training_loss: 1.21232e+02
I1112 01:27:27.237125 140264174335808 run_lib.py:153] step: 501600, training_loss: 1.17414e+02
I1112 01:27:38.266865 140264174335808 run_lib.py:153] step: 501650, training_loss: 1.31058e+02
I1112 01:27:49.057216 140264174335808 run_lib.py:153] step: 501700, training_loss: 1.11186e+02
I1112 01:27:58.875303 140264174335808 run_lib.py:153] step: 501750, training_loss: 1.43303e+02
I1112 01:28:09.099608 140264174335808 run_lib.py:153] step: 501800, training_loss: 1.28777e+02
I1112 01:28:20.041383 140264174335808 run_lib.py:153] step: 501850, training_loss: 1.40566e+02
I1112 01:28:30.594879 140264174335808 run_lib.py:153] step: 501900, training_loss: 1.03185e+02
I1112 01:28:41.076890 140264174335808 run_lib.py:153] step: 501950, training_loss: 1.32677e+02
I1112 01:28:51.661701 140264174335808 run_lib.py:153] step: 502000, training_loss: 9.75490e+01
I1112 01:29:01.752425 140264174335808 run_lib.py:153] step: 502050, training_loss: 1.17736e+02
I1112 01:29:12.203911 140264174335808 run_lib.py:153] step: 502100, training_loss: 8.91825e+01
I1112 01:29:22.216150 140264174335808 run_lib.py:153] step: 502150, training_loss: 1.18898e+02
I1112 01:29:32.324111 140264174335808 run_lib.py:153] step: 502200, training_loss: 1.09244e+02
I1112 01:29:42.442692 140264174335808 run_lib.py:153] step: 502250, training_loss: 1.28252e+02
I1112 01:29:52.900009 140264174335808 run_lib.py:153] step: 502300, training_loss: 1.40723e+02
I1112 01:30:02.865928 140264174335808 run_lib.py:153] step: 502350, training_loss: 1.01455e+02
I1112 01:30:13.510046 140264174335808 run_lib.py:153] step: 502400, training_loss: 1.11119e+02
I1112 01:30:24.033923 140264174335808 run_lib.py:153] step: 502450, training_loss: 1.33926e+02
I1112 01:30:34.039798 140264174335808 run_lib.py:153] step: 502500, training_loss: 1.50901e+02
I1112 01:30:44.070271 140264174335808 run_lib.py:153] step: 502550, training_loss: 1.39285e+02
I1112 01:30:54.585470 140264174335808 run_lib.py:153] step: 502600, training_loss: 1.42284e+02
I1112 01:31:04.855808 140264174335808 run_lib.py:153] step: 502650, training_loss: 1.12684e+02
I1112 01:31:14.527151 140264174335808 run_lib.py:153] step: 502700, training_loss: 1.19907e+02
I1112 01:31:24.679946 140264174335808 run_lib.py:153] step: 502750, training_loss: 1.23618e+02
I1112 01:31:35.018383 140264174335808 run_lib.py:153] step: 502800, training_loss: 1.37139e+02
I1112 01:31:45.133041 140264174335808 run_lib.py:153] step: 502850, training_loss: 1.25962e+02
I1112 01:31:54.383583 140264174335808 run_lib.py:153] step: 502900, training_loss: 1.21886e+02
I1112 01:32:04.303401 140264174335808 run_lib.py:153] step: 502950, training_loss: 1.04794e+02
I1112 01:32:14.131453 140264174335808 run_lib.py:153] step: 503000, training_loss: 1.28931e+02
I1112 01:32:23.742937 140264174335808 run_lib.py:153] step: 503050, training_loss: 1.35572e+02
I1112 01:32:34.361222 140264174335808 run_lib.py:153] step: 503100, training_loss: 1.34779e+02
I1112 01:32:44.125561 140264174335808 run_lib.py:153] step: 503150, training_loss: 1.23461e+02
I1112 01:32:53.891385 140264174335808 run_lib.py:153] step: 503200, training_loss: 1.37315e+02
I1112 01:33:03.713889 140264174335808 run_lib.py:153] step: 503250, training_loss: 1.15548e+02
I1112 01:33:13.619883 140264174335808 run_lib.py:153] step: 503300, training_loss: 1.24969e+02
I1112 01:33:23.440131 140264174335808 run_lib.py:153] step: 503350, training_loss: 1.33450e+02
I1112 01:33:33.305123 140264174335808 run_lib.py:153] step: 503400, training_loss: 1.47981e+02
I1112 01:33:43.699910 140264174335808 run_lib.py:153] step: 503450, training_loss: 1.01264e+02
I1112 01:33:54.202841 140264174335808 run_lib.py:153] step: 503500, training_loss: 1.09788e+02
I1112 01:34:04.310694 140264174335808 run_lib.py:153] step: 503550, training_loss: 1.32253e+02
I1112 01:34:14.700504 140264174335808 run_lib.py:153] step: 503600, training_loss: 1.35962e+02
I1112 01:34:25.133682 140264174335808 run_lib.py:153] step: 503650, training_loss: 1.43015e+02
I1112 01:34:35.127700 140264174335808 run_lib.py:153] step: 503700, training_loss: 1.37452e+02
I1112 01:34:45.753238 140264174335808 run_lib.py:153] step: 503750, training_loss: 1.28024e+02
I1112 01:34:55.988000 140264174335808 run_lib.py:153] step: 503800, training_loss: 1.31167e+02
I1112 01:35:07.140132 140264174335808 run_lib.py:153] step: 503850, training_loss: 1.28680e+02
I1112 01:35:17.402331 140264174335808 run_lib.py:153] step: 503900, training_loss: 1.12543e+02
I1112 01:35:27.188411 140264174335808 run_lib.py:153] step: 503950, training_loss: 9.41356e+01
I1112 01:35:36.911358 140264174335808 run_lib.py:153] step: 504000, training_loss: 1.30059e+02
I1112 01:35:47.072076 140264174335808 run_lib.py:153] step: 504050, training_loss: 1.17389e+02
I1112 01:35:56.661701 140264174335808 run_lib.py:153] step: 504100, training_loss: 1.22727e+02
I1112 01:36:06.492684 140264174335808 run_lib.py:153] step: 504150, training_loss: 1.22864e+02
I1112 01:36:16.451091 140264174335808 run_lib.py:153] step: 504200, training_loss: 1.21009e+02
I1112 01:36:26.220905 140264174335808 run_lib.py:153] step: 504250, training_loss: 1.05933e+02
I1112 01:36:36.719749 140264174335808 run_lib.py:153] step: 504300, training_loss: 1.15439e+02
I1112 01:36:46.740765 140264174335808 run_lib.py:153] step: 504350, training_loss: 1.30062e+02
I1112 01:36:57.168278 140264174335808 run_lib.py:153] step: 504400, training_loss: 1.28406e+02
I1112 01:37:07.514668 140264174335808 run_lib.py:153] step: 504450, training_loss: 1.28225e+02
I1112 01:37:17.329578 140264174335808 run_lib.py:153] step: 504500, training_loss: 1.26170e+02
I1112 01:37:27.402038 140264174335808 run_lib.py:153] step: 504550, training_loss: 1.28675e+02
I1112 01:37:37.592355 140264174335808 run_lib.py:153] step: 504600, training_loss: 1.13657e+02
I1112 01:37:47.283502 140264174335808 run_lib.py:153] step: 504650, training_loss: 1.57502e+02
I1112 01:37:57.303487 140264174335808 run_lib.py:153] step: 504700, training_loss: 1.59813e+02
I1112 01:38:07.770556 140264174335808 run_lib.py:153] step: 504750, training_loss: 1.26362e+02
I1112 01:38:17.697711 140264174335808 run_lib.py:153] step: 504800, training_loss: 1.30057e+02
I1112 01:38:27.773484 140264174335808 run_lib.py:153] step: 504850, training_loss: 1.27557e+02
I1112 01:38:38.435337 140264174335808 run_lib.py:153] step: 504900, training_loss: 1.26776e+02
I1112 01:38:49.206798 140264174335808 run_lib.py:153] step: 504950, training_loss: 1.12697e+02
I1112 01:38:59.732574 140264174335808 run_lib.py:153] step: 505000, training_loss: 1.26819e+02
I1112 01:38:59.834676 140264174335808 run_lib.py:166] step: 505000, eval_loss: 1.40364e+02
I1112 01:39:09.887258 140264174335808 run_lib.py:153] step: 505050, training_loss: 9.51311e+01
I1112 01:39:19.258903 140264174335808 run_lib.py:153] step: 505100, training_loss: 1.17408e+02
I1112 01:39:28.801848 140264174335808 run_lib.py:153] step: 505150, training_loss: 1.30226e+02
I1112 01:39:38.540280 140264174335808 run_lib.py:153] step: 505200, training_loss: 1.63336e+02
I1112 01:39:47.988888 140264174335808 run_lib.py:153] step: 505250, training_loss: 1.18035e+02
I1112 01:39:58.074940 140264174335808 run_lib.py:153] step: 505300, training_loss: 9.88287e+01
I1112 01:40:08.059290 140264174335808 run_lib.py:153] step: 505350, training_loss: 1.30471e+02
I1112 01:40:17.786108 140264174335808 run_lib.py:153] step: 505400, training_loss: 1.55003e+02
I1112 01:40:27.607889 140264174335808 run_lib.py:153] step: 505450, training_loss: 1.48082e+02
I1112 01:40:37.378783 140264174335808 run_lib.py:153] step: 505500, training_loss: 1.34766e+02
I1112 01:40:46.832727 140264174335808 run_lib.py:153] step: 505550, training_loss: 1.45373e+02
I1112 01:40:56.064598 140264174335808 run_lib.py:153] step: 505600, training_loss: 1.30350e+02
I1112 01:41:05.999736 140264174335808 run_lib.py:153] step: 505650, training_loss: 1.06861e+02
I1112 01:41:15.378967 140264174335808 run_lib.py:153] step: 505700, training_loss: 1.36761e+02
I1112 01:41:24.624909 140264174335808 run_lib.py:153] step: 505750, training_loss: 1.15439e+02
I1112 01:41:33.964281 140264174335808 run_lib.py:153] step: 505800, training_loss: 1.28137e+02
I1112 01:41:43.448589 140264174335808 run_lib.py:153] step: 505850, training_loss: 9.02486e+01
I1112 01:41:53.714673 140264174335808 run_lib.py:153] step: 505900, training_loss: 1.30503e+02
I1112 01:42:03.419977 140264174335808 run_lib.py:153] step: 505950, training_loss: 1.62845e+02
I1112 01:42:13.099344 140264174335808 run_lib.py:153] step: 506000, training_loss: 1.33131e+02
I1112 01:42:22.513643 140264174335808 run_lib.py:153] step: 506050, training_loss: 1.02506e+02
I1112 01:42:32.557389 140264174335808 run_lib.py:153] step: 506100, training_loss: 1.17136e+02
I1112 01:42:43.135658 140264174335808 run_lib.py:153] step: 506150, training_loss: 1.47879e+02
I1112 01:42:53.368147 140264174335808 run_lib.py:153] step: 506200, training_loss: 1.04750e+02
I1112 01:43:02.821183 140264174335808 run_lib.py:153] step: 506250, training_loss: 1.24169e+02
I1112 01:43:12.877037 140264174335808 run_lib.py:153] step: 506300, training_loss: 1.32462e+02
I1112 01:43:22.205305 140264174335808 run_lib.py:153] step: 506350, training_loss: 1.50237e+02
I1112 01:43:31.668941 140264174335808 run_lib.py:153] step: 506400, training_loss: 1.29133e+02
I1112 01:43:40.923115 140264174335808 run_lib.py:153] step: 506450, training_loss: 1.26799e+02
I1112 01:43:50.149671 140264174335808 run_lib.py:153] step: 506500, training_loss: 9.07034e+01
I1112 01:43:59.592275 140264174335808 run_lib.py:153] step: 506550, training_loss: 1.27622e+02
I1112 01:44:09.369520 140264174335808 run_lib.py:153] step: 506600, training_loss: 1.21330e+02
I1112 01:44:19.050855 140264174335808 run_lib.py:153] step: 506650, training_loss: 1.28499e+02
I1112 01:44:29.576536 140264174335808 run_lib.py:153] step: 506700, training_loss: 1.37853e+02
I1112 01:44:39.433347 140264174335808 run_lib.py:153] step: 506750, training_loss: 1.30684e+02
I1112 01:44:49.767737 140264174335808 run_lib.py:153] step: 506800, training_loss: 1.24309e+02
I1112 01:44:59.959340 140264174335808 run_lib.py:153] step: 506850, training_loss: 1.16036e+02
I1112 01:45:10.055310 140264174335808 run_lib.py:153] step: 506900, training_loss: 1.22556e+02
I1112 01:45:20.541356 140264174335808 run_lib.py:153] step: 506950, training_loss: 1.06771e+02
I1112 01:45:30.170635 140264174335808 run_lib.py:153] step: 507000, training_loss: 1.23794e+02
I1112 01:45:40.203413 140264174335808 run_lib.py:153] step: 507050, training_loss: 1.51936e+02
I1112 01:45:49.769284 140264174335808 run_lib.py:153] step: 507100, training_loss: 1.10919e+02
I1112 01:45:59.432902 140264174335808 run_lib.py:153] step: 507150, training_loss: 1.50472e+02
I1112 01:46:09.177207 140264174335808 run_lib.py:153] step: 507200, training_loss: 1.22223e+02
I1112 01:46:18.914589 140264174335808 run_lib.py:153] step: 507250, training_loss: 1.34245e+02
I1112 01:46:28.669332 140264174335808 run_lib.py:153] step: 507300, training_loss: 1.31841e+02
I1112 01:46:38.821934 140264174335808 run_lib.py:153] step: 507350, training_loss: 1.12361e+02
I1112 01:46:49.561413 140264174335808 run_lib.py:153] step: 507400, training_loss: 9.95308e+01
I1112 01:46:59.649704 140264174335808 run_lib.py:153] step: 507450, training_loss: 1.24104e+02
I1112 01:47:09.873398 140264174335808 run_lib.py:153] step: 507500, training_loss: 1.49820e+02
I1112 01:47:19.855584 140264174335808 run_lib.py:153] step: 507550, training_loss: 1.01991e+02
I1112 01:47:29.849823 140264174335808 run_lib.py:153] step: 507600, training_loss: 1.02580e+02
I1112 01:47:39.399528 140264174335808 run_lib.py:153] step: 507650, training_loss: 1.25954e+02
I1112 01:47:49.379814 140264174335808 run_lib.py:153] step: 507700, training_loss: 1.19528e+02
I1112 01:47:59.920088 140264174335808 run_lib.py:153] step: 507750, training_loss: 1.24160e+02
I1112 01:48:10.449849 140264174335808 run_lib.py:153] step: 507800, training_loss: 1.37907e+02
I1112 01:48:20.656553 140264174335808 run_lib.py:153] step: 507850, training_loss: 1.45427e+02
I1112 01:48:30.989122 140264174335808 run_lib.py:153] step: 507900, training_loss: 1.16191e+02
I1112 01:48:41.037230 140264174335808 run_lib.py:153] step: 507950, training_loss: 1.51118e+02
I1112 01:48:51.582529 140264174335808 run_lib.py:153] step: 508000, training_loss: 1.08987e+02
I1112 01:49:01.043067 140264174335808 run_lib.py:153] step: 508050, training_loss: 1.01813e+02
I1112 01:49:11.188122 140264174335808 run_lib.py:153] step: 508100, training_loss: 1.49245e+02
I1112 01:49:20.938027 140264174335808 run_lib.py:153] step: 508150, training_loss: 1.14856e+02
I1112 01:49:31.178540 140264174335808 run_lib.py:153] step: 508200, training_loss: 1.42692e+02
I1112 01:49:40.974571 140264174335808 run_lib.py:153] step: 508250, training_loss: 1.16415e+02
I1112 01:49:50.901542 140264174335808 run_lib.py:153] step: 508300, training_loss: 9.73183e+01
I1112 01:50:01.340266 140264174335808 run_lib.py:153] step: 508350, training_loss: 1.51749e+02
I1112 01:50:11.734682 140264174335808 run_lib.py:153] step: 508400, training_loss: 1.27974e+02
I1112 01:50:22.009485 140264174335808 run_lib.py:153] step: 508450, training_loss: 1.23995e+02
I1112 01:50:31.623337 140264174335808 run_lib.py:153] step: 508500, training_loss: 1.31789e+02
I1112 01:50:41.458916 140264174335808 run_lib.py:153] step: 508550, training_loss: 1.26958e+02
I1112 01:50:51.996135 140264174335808 run_lib.py:153] step: 508600, training_loss: 1.48497e+02
I1112 01:51:02.281352 140264174335808 run_lib.py:153] step: 508650, training_loss: 1.44329e+02
I1112 01:51:13.039887 140264174335808 run_lib.py:153] step: 508700, training_loss: 1.10176e+02
I1112 01:51:22.811033 140264174335808 run_lib.py:153] step: 508750, training_loss: 1.25650e+02
I1112 01:51:32.415179 140264174335808 run_lib.py:153] step: 508800, training_loss: 1.00947e+02
I1112 01:51:42.093683 140264174335808 run_lib.py:153] step: 508850, training_loss: 1.08322e+02
I1112 01:51:52.201158 140264174335808 run_lib.py:153] step: 508900, training_loss: 1.11378e+02
I1112 01:52:03.083733 140264174335808 run_lib.py:153] step: 508950, training_loss: 1.53198e+02
I1112 01:52:13.262637 140264174335808 run_lib.py:153] step: 509000, training_loss: 1.27539e+02
I1112 01:52:22.793380 140264174335808 run_lib.py:153] step: 509050, training_loss: 1.31340e+02
I1112 01:52:32.931457 140264174335808 run_lib.py:153] step: 509100, training_loss: 1.07562e+02
I1112 01:52:42.420657 140264174335808 run_lib.py:153] step: 509150, training_loss: 1.01933e+02
I1112 01:52:52.353464 140264174335808 run_lib.py:153] step: 509200, training_loss: 1.31733e+02
I1112 01:53:02.476795 140264174335808 run_lib.py:153] step: 509250, training_loss: 9.32523e+01
I1112 01:53:12.131721 140264174335808 run_lib.py:153] step: 509300, training_loss: 1.30016e+02
I1112 01:53:22.223707 140264174335808 run_lib.py:153] step: 509350, training_loss: 1.24430e+02
I1112 01:53:32.081032 140264174335808 run_lib.py:153] step: 509400, training_loss: 1.33221e+02
I1112 01:53:41.703421 140264174335808 run_lib.py:153] step: 509450, training_loss: 1.63601e+02
I1112 01:53:52.189874 140264174335808 run_lib.py:153] step: 509500, training_loss: 1.39903e+02
I1112 01:54:01.673020 140264174335808 run_lib.py:153] step: 509550, training_loss: 1.03667e+02
I1112 01:54:10.944651 140264174335808 run_lib.py:153] step: 509600, training_loss: 1.43693e+02
I1112 01:54:20.792186 140264174335808 run_lib.py:153] step: 509650, training_loss: 1.31246e+02
I1112 01:54:31.503522 140264174335808 run_lib.py:153] step: 509700, training_loss: 1.31634e+02
I1112 01:54:41.317536 140264174335808 run_lib.py:153] step: 509750, training_loss: 1.26986e+02
I1112 01:54:51.439081 140264174335808 run_lib.py:153] step: 509800, training_loss: 1.17962e+02
I1112 01:55:01.272711 140264174335808 run_lib.py:153] step: 509850, training_loss: 1.20050e+02
I1112 01:55:11.140028 140264174335808 run_lib.py:153] step: 509900, training_loss: 1.21718e+02
I1112 01:55:21.149492 140264174335808 run_lib.py:153] step: 509950, training_loss: 1.62991e+02
I1112 01:55:31.617511 140264174335808 run_lib.py:153] step: 510000, training_loss: 1.33587e+02
I1112 01:55:32.198572 140264174335808 run_lib.py:166] step: 510000, eval_loss: 1.09930e+02
I1112 01:55:41.851639 140264174335808 run_lib.py:153] step: 510050, training_loss: 1.49058e+02
I1112 01:55:52.796211 140264174335808 run_lib.py:153] step: 510100, training_loss: 1.04547e+02
I1112 01:56:02.438655 140264174335808 run_lib.py:153] step: 510150, training_loss: 1.13088e+02
I1112 01:56:12.199249 140264174335808 run_lib.py:153] step: 510200, training_loss: 1.12888e+02
I1112 01:56:22.022363 140264174335808 run_lib.py:153] step: 510250, training_loss: 1.31333e+02
I1112 01:56:32.380645 140264174335808 run_lib.py:153] step: 510300, training_loss: 1.21814e+02
I1112 01:56:42.855847 140264174335808 run_lib.py:153] step: 510350, training_loss: 1.24825e+02
I1112 01:56:53.092882 140264174335808 run_lib.py:153] step: 510400, training_loss: 1.41204e+02
I1112 01:57:03.324247 140264174335808 run_lib.py:153] step: 510450, training_loss: 1.35704e+02
I1112 01:57:13.208930 140264174335808 run_lib.py:153] step: 510500, training_loss: 1.45003e+02
I1112 01:57:23.290340 140264174335808 run_lib.py:153] step: 510550, training_loss: 1.40500e+02
I1112 01:57:33.503189 140264174335808 run_lib.py:153] step: 510600, training_loss: 1.26518e+02
I1112 01:57:44.211026 140264174335808 run_lib.py:153] step: 510650, training_loss: 1.11345e+02
I1112 01:57:54.668711 140264174335808 run_lib.py:153] step: 510700, training_loss: 1.23145e+02
I1112 01:58:05.172232 140264174335808 run_lib.py:153] step: 510750, training_loss: 1.11493e+02
I1112 01:58:15.280344 140264174335808 run_lib.py:153] step: 510800, training_loss: 1.09618e+02
I1112 01:58:26.027851 140264174335808 run_lib.py:153] step: 510850, training_loss: 1.45552e+02
I1112 01:58:36.541332 140264174335808 run_lib.py:153] step: 510900, training_loss: 1.33784e+02
I1112 01:58:46.554037 140264174335808 run_lib.py:153] step: 510950, training_loss: 1.44656e+02
I1112 01:58:56.452883 140264174335808 run_lib.py:153] step: 511000, training_loss: 1.25921e+02
I1112 01:59:06.929854 140264174335808 run_lib.py:153] step: 511050, training_loss: 1.46663e+02
I1112 01:59:16.503832 140264174335808 run_lib.py:153] step: 511100, training_loss: 1.25928e+02
I1112 01:59:26.503888 140264174335808 run_lib.py:153] step: 511150, training_loss: 1.51903e+02
I1112 01:59:36.213447 140264174335808 run_lib.py:153] step: 511200, training_loss: 1.03465e+02
I1112 01:59:46.209230 140264174335808 run_lib.py:153] step: 511250, training_loss: 1.27695e+02
I1112 01:59:56.398076 140264174335808 run_lib.py:153] step: 511300, training_loss: 1.23433e+02
I1112 02:00:06.927069 140264174335808 run_lib.py:153] step: 511350, training_loss: 1.32537e+02
I1112 02:00:16.659620 140264174335808 run_lib.py:153] step: 511400, training_loss: 1.24723e+02
I1112 02:00:26.449891 140264174335808 run_lib.py:153] step: 511450, training_loss: 1.56411e+02
I1112 02:00:36.434607 140264174335808 run_lib.py:153] step: 511500, training_loss: 1.11689e+02
I1112 02:00:46.125374 140264174335808 run_lib.py:153] step: 511550, training_loss: 1.25200e+02
I1112 02:00:55.778136 140264174335808 run_lib.py:153] step: 511600, training_loss: 1.13884e+02
I1112 02:01:05.207500 140264174335808 run_lib.py:153] step: 511650, training_loss: 1.03149e+02
I1112 02:01:15.552592 140264174335808 run_lib.py:153] step: 511700, training_loss: 1.03130e+02
I1112 02:01:25.256565 140264174335808 run_lib.py:153] step: 511750, training_loss: 1.33695e+02
I1112 02:01:34.894531 140264174335808 run_lib.py:153] step: 511800, training_loss: 1.19827e+02
I1112 02:01:44.787690 140264174335808 run_lib.py:153] step: 511850, training_loss: 1.74497e+02
I1112 02:01:54.259121 140264174335808 run_lib.py:153] step: 511900, training_loss: 1.47589e+02
I1112 02:02:03.579415 140264174335808 run_lib.py:153] step: 511950, training_loss: 1.17855e+02
I1112 02:02:12.933466 140264174335808 run_lib.py:153] step: 512000, training_loss: 1.06486e+02
I1112 02:02:22.794131 140264174335808 run_lib.py:153] step: 512050, training_loss: 1.33870e+02
I1112 02:02:32.389651 140264174335808 run_lib.py:153] step: 512100, training_loss: 1.07616e+02
I1112 02:02:41.934489 140264174335808 run_lib.py:153] step: 512150, training_loss: 1.05775e+02
I1112 02:02:51.608624 140264174335808 run_lib.py:153] step: 512200, training_loss: 1.29608e+02
I1112 02:03:01.459025 140264174335808 run_lib.py:153] step: 512250, training_loss: 1.01422e+02
I1112 02:03:11.263427 140264174335808 run_lib.py:153] step: 512300, training_loss: 1.28861e+02
I1112 02:03:21.042500 140264174335808 run_lib.py:153] step: 512350, training_loss: 1.11605e+02
I1112 02:03:30.593276 140264174335808 run_lib.py:153] step: 512400, training_loss: 1.45645e+02
I1112 02:03:40.489123 140264174335808 run_lib.py:153] step: 512450, training_loss: 1.11756e+02
I1112 02:03:49.779555 140264174335808 run_lib.py:153] step: 512500, training_loss: 1.38626e+02
I1112 02:03:59.088542 140264174335808 run_lib.py:153] step: 512550, training_loss: 1.12569e+02
I1112 02:04:09.324886 140264174335808 run_lib.py:153] step: 512600, training_loss: 1.20207e+02
I1112 02:04:19.495809 140264174335808 run_lib.py:153] step: 512650, training_loss: 1.29663e+02
I1112 02:04:29.113727 140264174335808 run_lib.py:153] step: 512700, training_loss: 1.28422e+02
I1112 02:04:38.782862 140264174335808 run_lib.py:153] step: 512750, training_loss: 1.24343e+02
I1112 02:04:48.932169 140264174335808 run_lib.py:153] step: 512800, training_loss: 1.45534e+02
I1112 02:04:58.517786 140264174335808 run_lib.py:153] step: 512850, training_loss: 1.16926e+02
I1112 02:05:08.281944 140264174335808 run_lib.py:153] step: 512900, training_loss: 1.25509e+02
I1112 02:05:18.044403 140264174335808 run_lib.py:153] step: 512950, training_loss: 1.26331e+02
I1112 02:05:27.787062 140264174335808 run_lib.py:153] step: 513000, training_loss: 1.27121e+02
I1112 02:05:37.791914 140264174335808 run_lib.py:153] step: 513050, training_loss: 1.32884e+02
I1112 02:05:47.913873 140264174335808 run_lib.py:153] step: 513100, training_loss: 1.27137e+02
I1112 02:05:57.410801 140264174335808 run_lib.py:153] step: 513150, training_loss: 1.86281e+02
I1112 02:06:07.053745 140264174335808 run_lib.py:153] step: 513200, training_loss: 1.33970e+02
I1112 02:06:16.947952 140264174335808 run_lib.py:153] step: 513250, training_loss: 1.31144e+02
I1112 02:06:26.770195 140264174335808 run_lib.py:153] step: 513300, training_loss: 1.15636e+02
I1112 02:06:36.726167 140264174335808 run_lib.py:153] step: 513350, training_loss: 1.40554e+02
I1112 02:06:46.914055 140264174335808 run_lib.py:153] step: 513400, training_loss: 1.13137e+02
I1112 02:06:57.104347 140264174335808 run_lib.py:153] step: 513450, training_loss: 1.21269e+02
I1112 02:07:07.360984 140264174335808 run_lib.py:153] step: 513500, training_loss: 1.09738e+02
I1112 02:07:17.231807 140264174335808 run_lib.py:153] step: 513550, training_loss: 1.66855e+02
I1112 02:07:27.511080 140264174335808 run_lib.py:153] step: 513600, training_loss: 1.11888e+02
I1112 02:07:37.383977 140264174335808 run_lib.py:153] step: 513650, training_loss: 1.47588e+02
I1112 02:07:47.281831 140264174335808 run_lib.py:153] step: 513700, training_loss: 1.31130e+02
I1112 02:07:57.045611 140264174335808 run_lib.py:153] step: 513750, training_loss: 1.34586e+02
I1112 02:08:07.860092 140264174335808 run_lib.py:153] step: 513800, training_loss: 1.39348e+02
I1112 02:08:17.605312 140264174335808 run_lib.py:153] step: 513850, training_loss: 1.29041e+02
I1112 02:08:28.214241 140264174335808 run_lib.py:153] step: 513900, training_loss: 1.10818e+02
I1112 02:08:38.536875 140264174335808 run_lib.py:153] step: 513950, training_loss: 1.41114e+02
I1112 02:08:49.288208 140264174335808 run_lib.py:153] step: 514000, training_loss: 1.03710e+02
I1112 02:08:58.927720 140264174335808 run_lib.py:153] step: 514050, training_loss: 1.45312e+02
I1112 02:09:09.346007 140264174335808 run_lib.py:153] step: 514100, training_loss: 1.32277e+02
I1112 02:09:18.820363 140264174335808 run_lib.py:153] step: 514150, training_loss: 1.46316e+02
I1112 02:09:28.168126 140264174335808 run_lib.py:153] step: 514200, training_loss: 1.32817e+02
I1112 02:09:37.968692 140264174335808 run_lib.py:153] step: 514250, training_loss: 1.27978e+02
I1112 02:09:47.659580 140264174335808 run_lib.py:153] step: 514300, training_loss: 1.36158e+02
I1112 02:09:57.498313 140264174335808 run_lib.py:153] step: 514350, training_loss: 1.26586e+02
I1112 02:10:07.683219 140264174335808 run_lib.py:153] step: 514400, training_loss: 1.18137e+02
I1112 02:10:17.336669 140264174335808 run_lib.py:153] step: 514450, training_loss: 1.50641e+02
I1112 02:10:27.364854 140264174335808 run_lib.py:153] step: 514500, training_loss: 1.39658e+02
I1112 02:10:37.916567 140264174335808 run_lib.py:153] step: 514550, training_loss: 1.22246e+02
I1112 02:10:47.617621 140264174335808 run_lib.py:153] step: 514600, training_loss: 1.62054e+02
I1112 02:10:57.389936 140264174335808 run_lib.py:153] step: 514650, training_loss: 1.17794e+02
I1112 02:11:07.724821 140264174335808 run_lib.py:153] step: 514700, training_loss: 1.65974e+02
I1112 02:11:17.883183 140264174335808 run_lib.py:153] step: 514750, training_loss: 1.23065e+02
I1112 02:11:27.621293 140264174335808 run_lib.py:153] step: 514800, training_loss: 1.16667e+02
I1112 02:11:37.898540 140264174335808 run_lib.py:153] step: 514850, training_loss: 1.43553e+02
I1112 02:11:48.886635 140264174335808 run_lib.py:153] step: 514900, training_loss: 1.27357e+02
I1112 02:11:59.356510 140264174335808 run_lib.py:153] step: 514950, training_loss: 8.57280e+01
I1112 02:12:09.105730 140264174335808 run_lib.py:153] step: 515000, training_loss: 1.17992e+02
I1112 02:12:09.207862 140264174335808 run_lib.py:166] step: 515000, eval_loss: 1.35740e+02
I1112 02:12:19.046619 140264174335808 run_lib.py:153] step: 515050, training_loss: 1.32310e+02
I1112 02:12:28.778819 140264174335808 run_lib.py:153] step: 515100, training_loss: 1.30803e+02
I1112 02:12:39.056056 140264174335808 run_lib.py:153] step: 515150, training_loss: 1.12722e+02
I1112 02:12:49.357418 140264174335808 run_lib.py:153] step: 515200, training_loss: 1.24984e+02
I1112 02:12:58.854675 140264174335808 run_lib.py:153] step: 515250, training_loss: 1.22439e+02
I1112 02:13:08.867764 140264174335808 run_lib.py:153] step: 515300, training_loss: 1.29659e+02
I1112 02:13:18.961412 140264174335808 run_lib.py:153] step: 515350, training_loss: 1.27238e+02
I1112 02:13:29.252715 140264174335808 run_lib.py:153] step: 515400, training_loss: 1.43000e+02
I1112 02:13:39.190003 140264174335808 run_lib.py:153] step: 515450, training_loss: 1.35095e+02
I1112 02:13:48.637506 140264174335808 run_lib.py:153] step: 515500, training_loss: 1.51540e+02
I1112 02:13:57.857241 140264174335808 run_lib.py:153] step: 515550, training_loss: 1.22055e+02
I1112 02:14:07.763678 140264174335808 run_lib.py:153] step: 515600, training_loss: 1.26440e+02
I1112 02:14:17.515060 140264174335808 run_lib.py:153] step: 515650, training_loss: 1.46328e+02
I1112 02:14:27.493899 140264174335808 run_lib.py:153] step: 515700, training_loss: 1.19160e+02
I1112 02:14:36.971467 140264174335808 run_lib.py:153] step: 515750, training_loss: 1.19320e+02
I1112 02:14:47.093983 140264174335808 run_lib.py:153] step: 515800, training_loss: 1.30226e+02
I1112 02:14:56.435585 140264174335808 run_lib.py:153] step: 515850, training_loss: 1.29787e+02
I1112 02:15:05.753445 140264174335808 run_lib.py:153] step: 515900, training_loss: 1.23197e+02
I1112 02:15:15.428833 140264174335808 run_lib.py:153] step: 515950, training_loss: 1.30802e+02
I1112 02:15:26.152294 140264174335808 run_lib.py:153] step: 516000, training_loss: 1.32208e+02
I1112 02:15:36.153327 140264174335808 run_lib.py:153] step: 516050, training_loss: 1.30564e+02
I1112 02:15:46.400371 140264174335808 run_lib.py:153] step: 516100, training_loss: 1.41365e+02
I1112 02:15:56.386951 140264174335808 run_lib.py:153] step: 516150, training_loss: 1.28603e+02
I1112 02:16:06.539299 140264174335808 run_lib.py:153] step: 516200, training_loss: 1.28950e+02
I1112 02:16:16.416004 140264174335808 run_lib.py:153] step: 516250, training_loss: 9.41838e+01
I1112 02:16:26.384375 140264174335808 run_lib.py:153] step: 516300, training_loss: 1.63085e+02
I1112 02:16:36.123485 140264174335808 run_lib.py:153] step: 516350, training_loss: 1.31549e+02
I1112 02:16:46.046651 140264174335808 run_lib.py:153] step: 516400, training_loss: 1.05220e+02
I1112 02:16:55.758890 140264174335808 run_lib.py:153] step: 516450, training_loss: 1.46430e+02
I1112 02:17:05.737375 140264174335808 run_lib.py:153] step: 516500, training_loss: 1.12549e+02
I1112 02:17:15.808058 140264174335808 run_lib.py:153] step: 516550, training_loss: 1.03999e+02
I1112 02:17:25.715690 140264174335808 run_lib.py:153] step: 516600, training_loss: 9.95521e+01
I1112 02:17:35.888798 140264174335808 run_lib.py:153] step: 516650, training_loss: 1.20417e+02
I1112 02:17:45.985669 140264174335808 run_lib.py:153] step: 516700, training_loss: 1.37878e+02
I1112 02:17:56.237521 140264174335808 run_lib.py:153] step: 516750, training_loss: 1.06610e+02
I1112 02:18:06.326413 140264174335808 run_lib.py:153] step: 516800, training_loss: 1.07239e+02
I1112 02:18:16.388508 140264174335808 run_lib.py:153] step: 516850, training_loss: 1.15201e+02
I1112 02:18:26.076187 140264174335808 run_lib.py:153] step: 516900, training_loss: 1.26310e+02
I1112 02:18:36.401733 140264174335808 run_lib.py:153] step: 516950, training_loss: 1.32486e+02
I1112 02:18:47.036235 140264174335808 run_lib.py:153] step: 517000, training_loss: 1.36147e+02
I1112 02:18:56.943246 140264174335808 run_lib.py:153] step: 517050, training_loss: 9.75499e+01
I1112 02:19:06.379266 140264174335808 run_lib.py:153] step: 517100, training_loss: 1.18413e+02
I1112 02:19:16.910678 140264174335808 run_lib.py:153] step: 517150, training_loss: 1.24120e+02
I1112 02:19:26.664705 140264174335808 run_lib.py:153] step: 517200, training_loss: 1.28400e+02
I1112 02:19:36.345959 140264174335808 run_lib.py:153] step: 517250, training_loss: 1.38320e+02
I1112 02:19:46.274775 140264174335808 run_lib.py:153] step: 517300, training_loss: 1.22104e+02
I1112 02:19:56.383216 140264174335808 run_lib.py:153] step: 517350, training_loss: 1.26739e+02
I1112 02:20:05.942148 140264174335808 run_lib.py:153] step: 517400, training_loss: 1.19568e+02
I1112 02:20:16.535208 140264174335808 run_lib.py:153] step: 517450, training_loss: 1.30307e+02
I1112 02:20:26.832200 140264174335808 run_lib.py:153] step: 517500, training_loss: 1.52949e+02
I1112 02:20:36.731050 140264174335808 run_lib.py:153] step: 517550, training_loss: 1.07931e+02
I1112 02:20:46.453588 140264174335808 run_lib.py:153] step: 517600, training_loss: 1.45068e+02
I1112 02:20:55.900728 140264174335808 run_lib.py:153] step: 517650, training_loss: 1.08129e+02
I1112 02:21:05.390048 140264174335808 run_lib.py:153] step: 517700, training_loss: 1.52677e+02
I1112 02:21:15.819215 140264174335808 run_lib.py:153] step: 517750, training_loss: 1.12050e+02
I1112 02:21:25.979341 140264174335808 run_lib.py:153] step: 517800, training_loss: 1.25175e+02
I1112 02:21:35.727832 140264174335808 run_lib.py:153] step: 517850, training_loss: 1.35017e+02
I1112 02:21:45.860672 140264174335808 run_lib.py:153] step: 517900, training_loss: 1.32881e+02
I1112 02:21:55.617115 140264174335808 run_lib.py:153] step: 517950, training_loss: 1.52905e+02
I1112 02:22:05.238197 140264174335808 run_lib.py:153] step: 518000, training_loss: 1.18215e+02
I1112 02:22:15.895509 140264174335808 run_lib.py:153] step: 518050, training_loss: 1.51490e+02
I1112 02:22:26.723551 140264174335808 run_lib.py:153] step: 518100, training_loss: 1.28461e+02
I1112 02:22:36.490327 140264174335808 run_lib.py:153] step: 518150, training_loss: 1.29866e+02
I1112 02:22:47.115088 140264174335808 run_lib.py:153] step: 518200, training_loss: 1.00019e+02
I1112 02:22:57.303572 140264174335808 run_lib.py:153] step: 518250, training_loss: 9.71155e+01
I1112 02:23:07.121637 140264174335808 run_lib.py:153] step: 518300, training_loss: 1.33047e+02
I1112 02:23:18.035731 140264174335808 run_lib.py:153] step: 518350, training_loss: 1.16028e+02
I1112 02:23:28.634474 140264174335808 run_lib.py:153] step: 518400, training_loss: 8.80008e+01
I1112 02:23:38.589258 140264174335808 run_lib.py:153] step: 518450, training_loss: 1.34351e+02
I1112 02:23:48.590886 140264174335808 run_lib.py:153] step: 518500, training_loss: 1.31074e+02
I1112 02:23:57.932467 140264174335808 run_lib.py:153] step: 518550, training_loss: 1.41579e+02
I1112 02:24:07.493716 140264174335808 run_lib.py:153] step: 518600, training_loss: 1.09672e+02
I1112 02:24:17.739161 140264174335808 run_lib.py:153] step: 518650, training_loss: 1.38264e+02
I1112 02:24:28.335154 140264174335808 run_lib.py:153] step: 518700, training_loss: 1.15873e+02
I1112 02:24:38.358033 140264174335808 run_lib.py:153] step: 518750, training_loss: 1.09942e+02
I1112 02:24:47.959223 140264174335808 run_lib.py:153] step: 518800, training_loss: 1.33072e+02
I1112 02:24:57.758458 140264174335808 run_lib.py:153] step: 518850, training_loss: 1.13786e+02
I1112 02:25:07.787763 140264174335808 run_lib.py:153] step: 518900, training_loss: 1.14544e+02
I1112 02:25:17.179486 140264174335808 run_lib.py:153] step: 518950, training_loss: 1.08567e+02
I1112 02:25:26.770260 140264174335808 run_lib.py:153] step: 519000, training_loss: 1.20153e+02
I1112 02:25:37.108455 140264174335808 run_lib.py:153] step: 519050, training_loss: 1.21093e+02
I1112 02:25:47.466561 140264174335808 run_lib.py:153] step: 519100, training_loss: 1.44608e+02
I1112 02:25:57.158136 140264174335808 run_lib.py:153] step: 519150, training_loss: 1.38367e+02
I1112 02:26:06.945300 140264174335808 run_lib.py:153] step: 519200, training_loss: 1.05386e+02
I1112 02:26:17.358996 140264174335808 run_lib.py:153] step: 519250, training_loss: 1.23138e+02
I1112 02:26:27.706037 140264174335808 run_lib.py:153] step: 519300, training_loss: 1.21399e+02
I1112 02:26:38.100071 140264174335808 run_lib.py:153] step: 519350, training_loss: 1.24492e+02
I1112 02:26:48.015809 140264174335808 run_lib.py:153] step: 519400, training_loss: 1.59516e+02
I1112 02:26:58.176464 140264174335808 run_lib.py:153] step: 519450, training_loss: 1.31354e+02
I1112 02:27:09.166598 140264174335808 run_lib.py:153] step: 519500, training_loss: 1.45067e+02
I1112 02:27:18.890315 140264174335808 run_lib.py:153] step: 519550, training_loss: 1.08240e+02
I1112 02:27:28.138763 140264174335808 run_lib.py:153] step: 519600, training_loss: 1.12937e+02
I1112 02:27:37.785709 140264174335808 run_lib.py:153] step: 519650, training_loss: 1.03229e+02
I1112 02:27:47.865470 140264174335808 run_lib.py:153] step: 519700, training_loss: 1.45227e+02
I1112 02:27:58.114140 140264174335808 run_lib.py:153] step: 519750, training_loss: 1.17451e+02
I1112 02:28:07.631050 140264174335808 run_lib.py:153] step: 519800, training_loss: 1.40594e+02
I1112 02:28:17.678230 140264174335808 run_lib.py:153] step: 519850, training_loss: 1.29943e+02
I1112 02:28:28.253729 140264174335808 run_lib.py:153] step: 519900, training_loss: 1.19328e+02
I1112 02:28:38.123610 140264174335808 run_lib.py:153] step: 519950, training_loss: 1.08850e+02
I1112 02:28:47.916288 140264174335808 run_lib.py:153] step: 520000, training_loss: 1.03043e+02
I1112 02:28:48.529128 140264174335808 run_lib.py:166] step: 520000, eval_loss: 1.25567e+02
I1112 02:28:58.854360 140264174335808 run_lib.py:153] step: 520050, training_loss: 1.46862e+02
I1112 02:29:09.621678 140264174335808 run_lib.py:153] step: 520100, training_loss: 1.35877e+02
I1112 02:29:20.066266 140264174335808 run_lib.py:153] step: 520150, training_loss: 1.20441e+02
I1112 02:29:30.394508 140264174335808 run_lib.py:153] step: 520200, training_loss: 1.42427e+02
I1112 02:29:40.183423 140264174335808 run_lib.py:153] step: 520250, training_loss: 1.52825e+02
I1112 02:29:50.839645 140264174335808 run_lib.py:153] step: 520300, training_loss: 1.28247e+02
I1112 02:30:01.030374 140264174335808 run_lib.py:153] step: 520350, training_loss: 1.12595e+02
I1112 02:30:11.220768 140264174335808 run_lib.py:153] step: 520400, training_loss: 1.35839e+02
I1112 02:30:21.434726 140264174335808 run_lib.py:153] step: 520450, training_loss: 1.65796e+02
I1112 02:30:32.125500 140264174335808 run_lib.py:153] step: 520500, training_loss: 1.35046e+02
I1112 02:30:42.069725 140264174335808 run_lib.py:153] step: 520550, training_loss: 1.12242e+02
I1112 02:30:52.563482 140264174335808 run_lib.py:153] step: 520600, training_loss: 1.20813e+02
I1112 02:31:03.607827 140264174335808 run_lib.py:153] step: 520650, training_loss: 1.27894e+02
I1112 02:31:13.848321 140264174335808 run_lib.py:153] step: 520700, training_loss: 1.08166e+02
I1112 02:31:23.985515 140264174335808 run_lib.py:153] step: 520750, training_loss: 1.12652e+02
I1112 02:31:33.840487 140264174335808 run_lib.py:153] step: 520800, training_loss: 1.26098e+02
I1112 02:31:44.035506 140264174335808 run_lib.py:153] step: 520850, training_loss: 1.24831e+02
I1112 02:31:54.581545 140264174335808 run_lib.py:153] step: 520900, training_loss: 1.36571e+02
I1112 02:32:04.010257 140264174335808 run_lib.py:153] step: 520950, training_loss: 1.65972e+02
I1112 02:32:14.526969 140264174335808 run_lib.py:153] step: 521000, training_loss: 1.25363e+02
I1112 02:32:24.186732 140264174335808 run_lib.py:153] step: 521050, training_loss: 1.35259e+02
I1112 02:32:34.095631 140264174335808 run_lib.py:153] step: 521100, training_loss: 1.33946e+02
I1112 02:32:44.065298 140264174335808 run_lib.py:153] step: 521150, training_loss: 1.18624e+02
I1112 02:32:53.858576 140264174335808 run_lib.py:153] step: 521200, training_loss: 1.25452e+02
I1112 02:33:04.165063 140264174335808 run_lib.py:153] step: 521250, training_loss: 1.03552e+02
I1112 02:33:14.728686 140264174335808 run_lib.py:153] step: 521300, training_loss: 1.00516e+02
I1112 02:33:24.619776 140264174335808 run_lib.py:153] step: 521350, training_loss: 1.28249e+02
I1112 02:33:34.453145 140264174335808 run_lib.py:153] step: 521400, training_loss: 1.06651e+02
I1112 02:33:44.412695 140264174335808 run_lib.py:153] step: 521450, training_loss: 9.54870e+01
I1112 02:33:54.457850 140264174335808 run_lib.py:153] step: 521500, training_loss: 1.37557e+02
I1112 02:34:04.903272 140264174335808 run_lib.py:153] step: 521550, training_loss: 1.22249e+02
I1112 02:34:14.647872 140264174335808 run_lib.py:153] step: 521600, training_loss: 1.25585e+02
I1112 02:34:24.400529 140264174335808 run_lib.py:153] step: 521650, training_loss: 9.58410e+01
I1112 02:34:34.481027 140264174335808 run_lib.py:153] step: 521700, training_loss: 1.14167e+02
I1112 02:34:44.352108 140264174335808 run_lib.py:153] step: 521750, training_loss: 1.18493e+02
I1112 02:34:54.675343 140264174335808 run_lib.py:153] step: 521800, training_loss: 9.56166e+01
I1112 02:35:04.976676 140264174335808 run_lib.py:153] step: 521850, training_loss: 1.54588e+02
I1112 02:35:14.466437 140264174335808 run_lib.py:153] step: 521900, training_loss: 1.26634e+02
I1112 02:35:25.047462 140264174335808 run_lib.py:153] step: 521950, training_loss: 1.16695e+02
I1112 02:35:35.181264 140264174335808 run_lib.py:153] step: 522000, training_loss: 1.22304e+02
I1112 02:35:44.811856 140264174335808 run_lib.py:153] step: 522050, training_loss: 9.97054e+01
I1112 02:35:54.612776 140264174335808 run_lib.py:153] step: 522100, training_loss: 1.18301e+02
I1112 02:36:04.676933 140264174335808 run_lib.py:153] step: 522150, training_loss: 1.22841e+02
I1112 02:36:14.462068 140264174335808 run_lib.py:153] step: 522200, training_loss: 1.38650e+02
I1112 02:36:24.551648 140264174335808 run_lib.py:153] step: 522250, training_loss: 1.28107e+02
I1112 02:36:34.474657 140264174335808 run_lib.py:153] step: 522300, training_loss: 1.36468e+02
I1112 02:36:44.515356 140264174335808 run_lib.py:153] step: 522350, training_loss: 1.37342e+02
I1112 02:36:54.190912 140264174335808 run_lib.py:153] step: 522400, training_loss: 1.41457e+02
I1112 02:37:03.869572 140264174335808 run_lib.py:153] step: 522450, training_loss: 1.30577e+02
I1112 02:37:13.838368 140264174335808 run_lib.py:153] step: 522500, training_loss: 1.14348e+02
I1112 02:37:23.798865 140264174335808 run_lib.py:153] step: 522550, training_loss: 1.20536e+02
I1112 02:37:33.820383 140264174335808 run_lib.py:153] step: 522600, training_loss: 1.40287e+02
I1112 02:37:43.441573 140264174335808 run_lib.py:153] step: 522650, training_loss: 1.66344e+02
I1112 02:37:53.618895 140264174335808 run_lib.py:153] step: 522700, training_loss: 1.15452e+02
I1112 02:38:03.269558 140264174335808 run_lib.py:153] step: 522750, training_loss: 1.38384e+02
I1112 02:38:12.931496 140264174335808 run_lib.py:153] step: 522800, training_loss: 1.14832e+02
I1112 02:38:22.831108 140264174335808 run_lib.py:153] step: 522850, training_loss: 9.99366e+01
I1112 02:38:32.505747 140264174335808 run_lib.py:153] step: 522900, training_loss: 1.46397e+02
I1112 02:38:42.018433 140264174335808 run_lib.py:153] step: 522950, training_loss: 1.20791e+02
I1112 02:38:52.417755 140264174335808 run_lib.py:153] step: 523000, training_loss: 1.11479e+02
I1112 02:39:02.550811 140264174335808 run_lib.py:153] step: 523050, training_loss: 1.49564e+02
I1112 02:39:11.836538 140264174335808 run_lib.py:153] step: 523100, training_loss: 9.45546e+01
I1112 02:39:22.297150 140264174335808 run_lib.py:153] step: 523150, training_loss: 1.38471e+02
I1112 02:39:32.360748 140264174335808 run_lib.py:153] step: 523200, training_loss: 1.23511e+02
I1112 02:39:41.876933 140264174335808 run_lib.py:153] step: 523250, training_loss: 1.30979e+02
I1112 02:39:52.494233 140264174335808 run_lib.py:153] step: 523300, training_loss: 1.20973e+02
I1112 02:40:02.195168 140264174335808 run_lib.py:153] step: 523350, training_loss: 1.25533e+02
I1112 02:40:11.422206 140264174335808 run_lib.py:153] step: 523400, training_loss: 1.34657e+02
I1112 02:40:21.026649 140264174335808 run_lib.py:153] step: 523450, training_loss: 1.35613e+02
I1112 02:40:31.199054 140264174335808 run_lib.py:153] step: 523500, training_loss: 1.16270e+02
I1112 02:40:41.122671 140264174335808 run_lib.py:153] step: 523550, training_loss: 1.45025e+02
I1112 02:40:50.838384 140264174335808 run_lib.py:153] step: 523600, training_loss: 1.41018e+02
I1112 02:41:00.885828 140264174335808 run_lib.py:153] step: 523650, training_loss: 1.28141e+02
I1112 02:41:10.929501 140264174335808 run_lib.py:153] step: 523700, training_loss: 1.31845e+02
I1112 02:41:20.693653 140264174335808 run_lib.py:153] step: 523750, training_loss: 1.46543e+02
I1112 02:41:30.915442 140264174335808 run_lib.py:153] step: 523800, training_loss: 1.46403e+02
I1112 02:41:41.010921 140264174335808 run_lib.py:153] step: 523850, training_loss: 1.59831e+02
I1112 02:41:51.700533 140264174335808 run_lib.py:153] step: 523900, training_loss: 1.32529e+02
I1112 02:42:02.058763 140264174335808 run_lib.py:153] step: 523950, training_loss: 1.37813e+02
I1112 02:42:12.722231 140264174335808 run_lib.py:153] step: 524000, training_loss: 1.02584e+02
I1112 02:42:23.125439 140264174335808 run_lib.py:153] step: 524050, training_loss: 1.33243e+02
I1112 02:42:33.406052 140264174335808 run_lib.py:153] step: 524100, training_loss: 1.39087e+02
I1112 02:42:44.456201 140264174335808 run_lib.py:153] step: 524150, training_loss: 1.42729e+02
I1112 02:42:54.510592 140264174335808 run_lib.py:153] step: 524200, training_loss: 1.24724e+02
I1112 02:43:04.196292 140264174335808 run_lib.py:153] step: 524250, training_loss: 1.13235e+02
I1112 02:43:14.384190 140264174335808 run_lib.py:153] step: 524300, training_loss: 1.07542e+02
I1112 02:43:24.239884 140264174335808 run_lib.py:153] step: 524350, training_loss: 1.30001e+02
I1112 02:43:34.157587 140264174335808 run_lib.py:153] step: 524400, training_loss: 1.19877e+02
I1112 02:43:43.741816 140264174335808 run_lib.py:153] step: 524450, training_loss: 1.06570e+02
I1112 02:43:53.284717 140264174335808 run_lib.py:153] step: 524500, training_loss: 1.17898e+02
I1112 02:44:02.784407 140264174335808 run_lib.py:153] step: 524550, training_loss: 9.48050e+01
I1112 02:44:13.561459 140264174335808 run_lib.py:153] step: 524600, training_loss: 1.22811e+02
I1112 02:44:23.318963 140264174335808 run_lib.py:153] step: 524650, training_loss: 9.05380e+01
I1112 02:44:33.356778 140264174335808 run_lib.py:153] step: 524700, training_loss: 9.47058e+01
I1112 02:44:43.361772 140264174335808 run_lib.py:153] step: 524750, training_loss: 1.11230e+02
I1112 02:44:53.786221 140264174335808 run_lib.py:153] step: 524800, training_loss: 1.24874e+02
I1112 02:45:04.779664 140264174335808 run_lib.py:153] step: 524850, training_loss: 1.28709e+02
I1112 02:45:15.019139 140264174335808 run_lib.py:153] step: 524900, training_loss: 1.11454e+02
I1112 02:45:25.152961 140264174335808 run_lib.py:153] step: 524950, training_loss: 1.53335e+02
I1112 02:45:35.397613 140264174335808 run_lib.py:153] step: 525000, training_loss: 1.29524e+02
I1112 02:45:35.500808 140264174335808 run_lib.py:166] step: 525000, eval_loss: 9.75332e+01
I1112 02:45:45.039337 140264174335808 run_lib.py:153] step: 525050, training_loss: 1.24546e+02
I1112 02:45:54.821774 140264174335808 run_lib.py:153] step: 525100, training_loss: 1.32519e+02
I1112 02:46:04.405062 140264174335808 run_lib.py:153] step: 525150, training_loss: 1.25437e+02
I1112 02:46:14.499388 140264174335808 run_lib.py:153] step: 525200, training_loss: 1.43095e+02
I1112 02:46:24.568924 140264174335808 run_lib.py:153] step: 525250, training_loss: 9.47471e+01
I1112 02:46:34.410505 140264174335808 run_lib.py:153] step: 525300, training_loss: 1.15421e+02
I1112 02:46:44.301041 140264174335808 run_lib.py:153] step: 525350, training_loss: 1.17647e+02
I1112 02:46:54.331652 140264174335808 run_lib.py:153] step: 525400, training_loss: 1.18224e+02
I1112 02:47:03.960630 140264174335808 run_lib.py:153] step: 525450, training_loss: 1.36292e+02
I1112 02:47:13.731740 140264174335808 run_lib.py:153] step: 525500, training_loss: 1.14327e+02
I1112 02:47:23.099472 140264174335808 run_lib.py:153] step: 525550, training_loss: 1.04585e+02
I1112 02:47:32.655812 140264174335808 run_lib.py:153] step: 525600, training_loss: 1.25480e+02
I1112 02:47:41.978150 140264174335808 run_lib.py:153] step: 525650, training_loss: 1.55855e+02
I1112 02:47:51.872651 140264174335808 run_lib.py:153] step: 525700, training_loss: 1.53830e+02
I1112 02:48:02.498662 140264174335808 run_lib.py:153] step: 525750, training_loss: 9.66915e+01
I1112 02:48:12.750407 140264174335808 run_lib.py:153] step: 525800, training_loss: 8.47633e+01
I1112 02:48:23.188949 140264174335808 run_lib.py:153] step: 525850, training_loss: 9.84535e+01
I1112 02:48:32.722730 140264174335808 run_lib.py:153] step: 525900, training_loss: 1.12024e+02
I1112 02:48:42.123139 140264174335808 run_lib.py:153] step: 525950, training_loss: 1.25083e+02
I1112 02:48:52.616510 140264174335808 run_lib.py:153] step: 526000, training_loss: 1.21579e+02
I1112 02:49:02.075586 140264174335808 run_lib.py:153] step: 526050, training_loss: 1.28327e+02
I1112 02:49:11.932829 140264174335808 run_lib.py:153] step: 526100, training_loss: 1.33976e+02
I1112 02:49:21.848771 140264174335808 run_lib.py:153] step: 526150, training_loss: 1.12567e+02
I1112 02:49:32.300401 140264174335808 run_lib.py:153] step: 526200, training_loss: 1.21626e+02
I1112 02:49:42.524042 140264174335808 run_lib.py:153] step: 526250, training_loss: 1.33182e+02
I1112 02:49:52.623605 140264174335808 run_lib.py:153] step: 526300, training_loss: 1.27590e+02
I1112 02:50:03.074131 140264174335808 run_lib.py:153] step: 526350, training_loss: 1.41752e+02
I1112 02:50:13.361005 140264174335808 run_lib.py:153] step: 526400, training_loss: 1.21894e+02
I1112 02:50:23.117188 140264174335808 run_lib.py:153] step: 526450, training_loss: 1.28217e+02
I1112 02:50:33.056453 140264174335808 run_lib.py:153] step: 526500, training_loss: 1.26499e+02
I1112 02:50:42.817149 140264174335808 run_lib.py:153] step: 526550, training_loss: 9.66774e+01
I1112 02:50:53.227888 140264174335808 run_lib.py:153] step: 526600, training_loss: 1.18773e+02
I1112 02:51:03.600820 140264174335808 run_lib.py:153] step: 526650, training_loss: 1.43821e+02
I1112 02:51:13.476200 140264174335808 run_lib.py:153] step: 526700, training_loss: 1.28866e+02
I1112 02:51:23.848823 140264174335808 run_lib.py:153] step: 526750, training_loss: 9.88165e+01
I1112 02:51:33.349320 140264174335808 run_lib.py:153] step: 526800, training_loss: 1.42709e+02
I1112 02:51:43.921491 140264174335808 run_lib.py:153] step: 526850, training_loss: 1.61741e+02
I1112 02:51:53.705321 140264174335808 run_lib.py:153] step: 526900, training_loss: 1.31737e+02
I1112 02:52:03.744420 140264174335808 run_lib.py:153] step: 526950, training_loss: 1.30267e+02
I1112 02:52:13.955147 140264174335808 run_lib.py:153] step: 527000, training_loss: 1.42342e+02
I1112 02:52:23.816114 140264174335808 run_lib.py:153] step: 527050, training_loss: 1.25033e+02
I1112 02:52:33.781854 140264174335808 run_lib.py:153] step: 527100, training_loss: 1.50072e+02
I1112 02:52:43.766776 140264174335808 run_lib.py:153] step: 527150, training_loss: 1.25032e+02
I1112 02:52:53.705381 140264174335808 run_lib.py:153] step: 527200, training_loss: 1.11662e+02
I1112 02:53:03.273137 140264174335808 run_lib.py:153] step: 527250, training_loss: 1.33884e+02
I1112 02:53:13.277569 140264174335808 run_lib.py:153] step: 527300, training_loss: 1.67410e+02
I1112 02:53:23.241281 140264174335808 run_lib.py:153] step: 527350, training_loss: 1.17008e+02
I1112 02:53:33.538923 140264174335808 run_lib.py:153] step: 527400, training_loss: 1.14349e+02
I1112 02:53:43.230935 140264174335808 run_lib.py:153] step: 527450, training_loss: 1.42275e+02
I1112 02:53:53.237788 140264174335808 run_lib.py:153] step: 527500, training_loss: 1.25565e+02
I1112 02:54:03.617893 140264174335808 run_lib.py:153] step: 527550, training_loss: 1.22890e+02
I1112 02:54:13.490881 140264174335808 run_lib.py:153] step: 527600, training_loss: 1.18038e+02
I1112 02:54:23.892517 140264174335808 run_lib.py:153] step: 527650, training_loss: 1.27294e+02
I1112 02:54:33.620058 140264174335808 run_lib.py:153] step: 527700, training_loss: 1.13897e+02
I1112 02:54:44.143815 140264174335808 run_lib.py:153] step: 527750, training_loss: 1.13924e+02
I1112 02:54:54.743783 140264174335808 run_lib.py:153] step: 527800, training_loss: 1.14486e+02
I1112 02:55:04.641119 140264174335808 run_lib.py:153] step: 527850, training_loss: 1.31507e+02
I1112 02:55:15.253925 140264174335808 run_lib.py:153] step: 527900, training_loss: 1.15346e+02
I1112 02:55:25.875308 140264174335808 run_lib.py:153] step: 527950, training_loss: 1.08984e+02
I1112 02:55:35.918766 140264174335808 run_lib.py:153] step: 528000, training_loss: 1.00369e+02
I1112 02:55:45.906527 140264174335808 run_lib.py:153] step: 528050, training_loss: 1.47384e+02
I1112 02:55:55.884794 140264174335808 run_lib.py:153] step: 528100, training_loss: 1.31279e+02
I1112 02:56:05.916319 140264174335808 run_lib.py:153] step: 528150, training_loss: 1.33535e+02
I1112 02:56:15.247781 140264174335808 run_lib.py:153] step: 528200, training_loss: 1.22434e+02
I1112 02:56:25.049150 140264174335808 run_lib.py:153] step: 528250, training_loss: 1.32710e+02
I1112 02:56:35.326521 140264174335808 run_lib.py:153] step: 528300, training_loss: 1.65498e+02
I1112 02:56:45.560169 140264174335808 run_lib.py:153] step: 528350, training_loss: 1.37200e+02
I1112 02:56:55.771781 140264174335808 run_lib.py:153] step: 528400, training_loss: 1.21718e+02
I1112 02:57:05.035609 140264174335808 run_lib.py:153] step: 528450, training_loss: 1.25128e+02
I1112 02:57:14.992348 140264174335808 run_lib.py:153] step: 528500, training_loss: 1.09320e+02
I1112 02:57:24.944480 140264174335808 run_lib.py:153] step: 528550, training_loss: 8.72011e+01
I1112 02:57:35.548893 140264174335808 run_lib.py:153] step: 528600, training_loss: 1.27907e+02
I1112 02:57:44.883677 140264174335808 run_lib.py:153] step: 528650, training_loss: 9.99265e+01
I1112 02:57:54.618067 140264174335808 run_lib.py:153] step: 528700, training_loss: 1.38815e+02
I1112 02:58:04.396424 140264174335808 run_lib.py:153] step: 528750, training_loss: 1.60910e+02
I1112 02:58:13.903426 140264174335808 run_lib.py:153] step: 528800, training_loss: 1.23257e+02
I1112 02:58:23.866817 140264174335808 run_lib.py:153] step: 528850, training_loss: 9.86666e+01
I1112 02:58:34.411774 140264174335808 run_lib.py:153] step: 528900, training_loss: 1.54223e+02
I1112 02:58:44.489808 140264174335808 run_lib.py:153] step: 528950, training_loss: 1.20471e+02
I1112 02:58:54.176658 140264174335808 run_lib.py:153] step: 529000, training_loss: 1.22339e+02
I1112 02:59:03.959581 140264174335808 run_lib.py:153] step: 529050, training_loss: 1.16413e+02
I1112 02:59:14.258269 140264174335808 run_lib.py:153] step: 529100, training_loss: 1.05880e+02
I1112 02:59:23.896397 140264174335808 run_lib.py:153] step: 529150, training_loss: 1.01997e+02
I1112 02:59:34.134963 140264174335808 run_lib.py:153] step: 529200, training_loss: 1.58067e+02
I1112 02:59:43.864436 140264174335808 run_lib.py:153] step: 529250, training_loss: 1.07007e+02
I1112 02:59:53.198886 140264174335808 run_lib.py:153] step: 529300, training_loss: 1.10417e+02
I1112 03:00:03.192796 140264174335808 run_lib.py:153] step: 529350, training_loss: 1.27232e+02
I1112 03:00:13.256579 140264174335808 run_lib.py:153] step: 529400, training_loss: 1.17264e+02
I1112 03:00:22.855981 140264174335808 run_lib.py:153] step: 529450, training_loss: 1.14887e+02
I1112 03:00:33.121637 140264174335808 run_lib.py:153] step: 529500, training_loss: 1.09470e+02
I1112 03:00:43.718099 140264174335808 run_lib.py:153] step: 529550, training_loss: 1.15874e+02
I1112 03:00:53.939838 140264174335808 run_lib.py:153] step: 529600, training_loss: 1.36957e+02
I1112 03:01:03.743961 140264174335808 run_lib.py:153] step: 529650, training_loss: 1.17022e+02
I1112 03:01:13.276965 140264174335808 run_lib.py:153] step: 529700, training_loss: 1.38534e+02
I1112 03:01:23.145051 140264174335808 run_lib.py:153] step: 529750, training_loss: 1.13892e+02
I1112 03:01:32.980321 140264174335808 run_lib.py:153] step: 529800, training_loss: 1.13781e+02
I1112 03:01:43.313661 140264174335808 run_lib.py:153] step: 529850, training_loss: 1.20716e+02
I1112 03:01:52.944350 140264174335808 run_lib.py:153] step: 529900, training_loss: 1.09478e+02
I1112 03:02:02.676451 140264174335808 run_lib.py:153] step: 529950, training_loss: 1.03031e+02
I1112 03:02:12.813737 140264174335808 run_lib.py:153] step: 530000, training_loss: 1.31883e+02
I1112 03:02:13.352675 140264174335808 run_lib.py:166] step: 530000, eval_loss: 1.17466e+02
I1112 03:02:23.593545 140264174335808 run_lib.py:153] step: 530050, training_loss: 1.44012e+02
I1112 03:02:34.013341 140264174335808 run_lib.py:153] step: 530100, training_loss: 1.04063e+02
I1112 03:02:43.929846 140264174335808 run_lib.py:153] step: 530150, training_loss: 1.30713e+02
I1112 03:02:53.634334 140264174335808 run_lib.py:153] step: 530200, training_loss: 1.38491e+02
I1112 03:03:03.121913 140264174335808 run_lib.py:153] step: 530250, training_loss: 1.32928e+02
I1112 03:03:13.229770 140264174335808 run_lib.py:153] step: 530300, training_loss: 1.33664e+02
I1112 03:03:22.932951 140264174335808 run_lib.py:153] step: 530350, training_loss: 1.27404e+02
I1112 03:03:33.445626 140264174335808 run_lib.py:153] step: 530400, training_loss: 1.24489e+02
I1112 03:03:43.564428 140264174335808 run_lib.py:153] step: 530450, training_loss: 1.11805e+02
I1112 03:03:53.439644 140264174335808 run_lib.py:153] step: 530500, training_loss: 9.04443e+01
I1112 03:04:03.526567 140264174335808 run_lib.py:153] step: 530550, training_loss: 1.27603e+02
I1112 03:04:13.585836 140264174335808 run_lib.py:153] step: 530600, training_loss: 1.38734e+02
I1112 03:04:24.083636 140264174335808 run_lib.py:153] step: 530650, training_loss: 1.09792e+02
I1112 03:04:34.387063 140264174335808 run_lib.py:153] step: 530700, training_loss: 1.20970e+02
I1112 03:04:44.289807 140264174335808 run_lib.py:153] step: 530750, training_loss: 1.31663e+02
I1112 03:04:53.977711 140264174335808 run_lib.py:153] step: 530800, training_loss: 1.19389e+02
I1112 03:05:04.046709 140264174335808 run_lib.py:153] step: 530850, training_loss: 1.24704e+02
I1112 03:05:14.148789 140264174335808 run_lib.py:153] step: 530900, training_loss: 1.21881e+02
I1112 03:05:24.617476 140264174335808 run_lib.py:153] step: 530950, training_loss: 1.26760e+02
I1112 03:05:34.673609 140264174335808 run_lib.py:153] step: 531000, training_loss: 1.29534e+02
I1112 03:05:44.523096 140264174335808 run_lib.py:153] step: 531050, training_loss: 1.30450e+02
I1112 03:05:55.034768 140264174335808 run_lib.py:153] step: 531100, training_loss: 1.35927e+02
I1112 03:06:04.959780 140264174335808 run_lib.py:153] step: 531150, training_loss: 1.36589e+02
I1112 03:06:14.985621 140264174335808 run_lib.py:153] step: 531200, training_loss: 1.44150e+02
I1112 03:06:24.715686 140264174335808 run_lib.py:153] step: 531250, training_loss: 1.30089e+02
I1112 03:06:34.640830 140264174335808 run_lib.py:153] step: 531300, training_loss: 1.46777e+02
I1112 03:06:44.245079 140264174335808 run_lib.py:153] step: 531350, training_loss: 1.28724e+02
I1112 03:06:54.195480 140264174335808 run_lib.py:153] step: 531400, training_loss: 1.21661e+02
I1112 03:07:05.255373 140264174335808 run_lib.py:153] step: 531450, training_loss: 1.33385e+02
I1112 03:07:15.371350 140264174335808 run_lib.py:153] step: 531500, training_loss: 1.48876e+02
I1112 03:07:25.542114 140264174335808 run_lib.py:153] step: 531550, training_loss: 1.35552e+02
I1112 03:07:35.925716 140264174335808 run_lib.py:153] step: 531600, training_loss: 1.39173e+02
I1112 03:07:45.970151 140264174335808 run_lib.py:153] step: 531650, training_loss: 1.30264e+02
I1112 03:07:55.823587 140264174335808 run_lib.py:153] step: 531700, training_loss: 1.42213e+02
I1112 03:08:05.891164 140264174335808 run_lib.py:153] step: 531750, training_loss: 1.27925e+02
I1112 03:08:15.533312 140264174335808 run_lib.py:153] step: 531800, training_loss: 1.28334e+02
I1112 03:08:25.603732 140264174335808 run_lib.py:153] step: 531850, training_loss: 1.11088e+02
I1112 03:08:35.760109 140264174335808 run_lib.py:153] step: 531900, training_loss: 1.16711e+02
I1112 03:08:45.501243 140264174335808 run_lib.py:153] step: 531950, training_loss: 1.19515e+02
I1112 03:08:55.685889 140264174335808 run_lib.py:153] step: 532000, training_loss: 1.24530e+02
I1112 03:09:06.631567 140264174335808 run_lib.py:153] step: 532050, training_loss: 1.36204e+02
I1112 03:09:17.374893 140264174335808 run_lib.py:153] step: 532100, training_loss: 1.15620e+02
I1112 03:09:27.904312 140264174335808 run_lib.py:153] step: 532150, training_loss: 1.42064e+02
I1112 03:09:38.317880 140264174335808 run_lib.py:153] step: 532200, training_loss: 1.21530e+02
I1112 03:09:48.640035 140264174335808 run_lib.py:153] step: 532250, training_loss: 1.41846e+02
I1112 03:09:58.568234 140264174335808 run_lib.py:153] step: 532300, training_loss: 1.43645e+02
I1112 03:10:08.848833 140264174335808 run_lib.py:153] step: 532350, training_loss: 1.31978e+02
I1112 03:10:19.012342 140264174335808 run_lib.py:153] step: 532400, training_loss: 1.35702e+02
I1112 03:10:28.566546 140264174335808 run_lib.py:153] step: 532450, training_loss: 1.24267e+02
I1112 03:10:38.681576 140264174335808 run_lib.py:153] step: 532500, training_loss: 1.44055e+02
I1112 03:10:48.457116 140264174335808 run_lib.py:153] step: 532550, training_loss: 1.10721e+02
I1112 03:10:59.318538 140264174335808 run_lib.py:153] step: 532600, training_loss: 1.45863e+02
I1112 03:11:09.869817 140264174335808 run_lib.py:153] step: 532650, training_loss: 8.50210e+01
I1112 03:11:20.091339 140264174335808 run_lib.py:153] step: 532700, training_loss: 1.07232e+02
I1112 03:11:30.378741 140264174335808 run_lib.py:153] step: 532750, training_loss: 1.23151e+02
I1112 03:11:40.406553 140264174335808 run_lib.py:153] step: 532800, training_loss: 1.11285e+02
I1112 03:11:50.620023 140264174335808 run_lib.py:153] step: 532850, training_loss: 1.21041e+02
I1112 03:12:01.357928 140264174335808 run_lib.py:153] step: 532900, training_loss: 1.19294e+02
I1112 03:12:11.708861 140264174335808 run_lib.py:153] step: 532950, training_loss: 1.28547e+02
I1112 03:12:22.103615 140264174335808 run_lib.py:153] step: 533000, training_loss: 1.31165e+02
I1112 03:12:32.531362 140264174335808 run_lib.py:153] step: 533050, training_loss: 1.26593e+02
I1112 03:12:42.192808 140264174335808 run_lib.py:153] step: 533100, training_loss: 1.05844e+02
I1112 03:12:51.990515 140264174335808 run_lib.py:153] step: 533150, training_loss: 1.25704e+02
I1112 03:13:02.072318 140264174335808 run_lib.py:153] step: 533200, training_loss: 1.37866e+02
I1112 03:13:12.353225 140264174335808 run_lib.py:153] step: 533250, training_loss: 1.37537e+02
I1112 03:13:22.554180 140264174335808 run_lib.py:153] step: 533300, training_loss: 1.28515e+02
I1112 03:13:33.653486 140264174335808 run_lib.py:153] step: 533350, training_loss: 1.00810e+02
I1112 03:13:44.360709 140264174335808 run_lib.py:153] step: 533400, training_loss: 1.00160e+02
I1112 03:13:54.242026 140264174335808 run_lib.py:153] step: 533450, training_loss: 1.27546e+02
I1112 03:14:04.475235 140264174335808 run_lib.py:153] step: 533500, training_loss: 1.34168e+02
I1112 03:14:14.883481 140264174335808 run_lib.py:153] step: 533550, training_loss: 1.24242e+02
I1112 03:14:25.656087 140264174335808 run_lib.py:153] step: 533600, training_loss: 1.19188e+02
I1112 03:14:35.927480 140264174335808 run_lib.py:153] step: 533650, training_loss: 1.18228e+02
I1112 03:14:45.665366 140264174335808 run_lib.py:153] step: 533700, training_loss: 1.45896e+02
I1112 03:14:55.880553 140264174335808 run_lib.py:153] step: 533750, training_loss: 1.28339e+02
I1112 03:15:06.085387 140264174335808 run_lib.py:153] step: 533800, training_loss: 1.38830e+02
I1112 03:15:16.230566 140264174335808 run_lib.py:153] step: 533850, training_loss: 1.27047e+02
I1112 03:15:26.852136 140264174335808 run_lib.py:153] step: 533900, training_loss: 1.27626e+02
I1112 03:15:37.292257 140264174335808 run_lib.py:153] step: 533950, training_loss: 1.04319e+02
I1112 03:15:47.897204 140264174335808 run_lib.py:153] step: 534000, training_loss: 1.21123e+02
I1112 03:15:58.007905 140264174335808 run_lib.py:153] step: 534050, training_loss: 1.51322e+02
I1112 03:16:08.425475 140264174335808 run_lib.py:153] step: 534100, training_loss: 1.55787e+02
I1112 03:16:18.277733 140264174335808 run_lib.py:153] step: 534150, training_loss: 1.51466e+02
I1112 03:16:27.884410 140264174335808 run_lib.py:153] step: 534200, training_loss: 1.20988e+02
I1112 03:16:37.991128 140264174335808 run_lib.py:153] step: 534250, training_loss: 1.35756e+02
I1112 03:16:48.032507 140264174335808 run_lib.py:153] step: 534300, training_loss: 1.30402e+02
I1112 03:16:58.250009 140264174335808 run_lib.py:153] step: 534350, training_loss: 1.05149e+02
I1112 03:17:08.419910 140264174335808 run_lib.py:153] step: 534400, training_loss: 1.38105e+02
I1112 03:17:18.635192 140264174335808 run_lib.py:153] step: 534450, training_loss: 1.32124e+02
I1112 03:17:28.969611 140264174335808 run_lib.py:153] step: 534500, training_loss: 1.63020e+02
I1112 03:17:40.349627 140264174335808 run_lib.py:153] step: 534550, training_loss: 1.42383e+02
I1112 03:17:50.865978 140264174335808 run_lib.py:153] step: 534600, training_loss: 1.39074e+02
I1112 03:18:00.791534 140264174335808 run_lib.py:153] step: 534650, training_loss: 1.28043e+02
I1112 03:18:10.803879 140264174335808 run_lib.py:153] step: 534700, training_loss: 1.26564e+02
I1112 03:18:20.551898 140264174335808 run_lib.py:153] step: 534750, training_loss: 1.17114e+02
I1112 03:18:30.153756 140264174335808 run_lib.py:153] step: 534800, training_loss: 1.08728e+02
I1112 03:18:39.781677 140264174335808 run_lib.py:153] step: 534850, training_loss: 1.16619e+02
I1112 03:18:49.703225 140264174335808 run_lib.py:153] step: 534900, training_loss: 1.27463e+02
I1112 03:18:59.814957 140264174335808 run_lib.py:153] step: 534950, training_loss: 1.40477e+02
I1112 03:19:09.535562 140264174335808 run_lib.py:153] step: 535000, training_loss: 1.13268e+02
I1112 03:19:09.638841 140264174335808 run_lib.py:166] step: 535000, eval_loss: 1.13340e+02
I1112 03:19:19.602595 140264174335808 run_lib.py:153] step: 535050, training_loss: 1.33475e+02
I1112 03:19:30.089927 140264174335808 run_lib.py:153] step: 535100, training_loss: 1.02347e+02
I1112 03:19:39.579619 140264174335808 run_lib.py:153] step: 535150, training_loss: 1.20917e+02
I1112 03:19:49.962841 140264174335808 run_lib.py:153] step: 535200, training_loss: 1.10944e+02
I1112 03:19:59.419225 140264174335808 run_lib.py:153] step: 535250, training_loss: 1.25303e+02
I1112 03:20:09.080596 140264174335808 run_lib.py:153] step: 535300, training_loss: 1.00506e+02
I1112 03:20:19.115883 140264174335808 run_lib.py:153] step: 535350, training_loss: 1.26307e+02
I1112 03:20:29.374822 140264174335808 run_lib.py:153] step: 535400, training_loss: 1.44263e+02
I1112 03:20:38.983387 140264174335808 run_lib.py:153] step: 535450, training_loss: 1.01780e+02
I1112 03:20:48.676832 140264174335808 run_lib.py:153] step: 535500, training_loss: 1.25171e+02
I1112 03:20:58.618199 140264174335808 run_lib.py:153] step: 535550, training_loss: 1.21982e+02
I1112 03:21:09.302172 140264174335808 run_lib.py:153] step: 535600, training_loss: 1.38016e+02
I1112 03:21:20.256073 140264174335808 run_lib.py:153] step: 535650, training_loss: 1.09803e+02
I1112 03:21:30.600773 140264174335808 run_lib.py:153] step: 535700, training_loss: 9.77571e+01
I1112 03:21:40.353340 140264174335808 run_lib.py:153] step: 535750, training_loss: 1.22545e+02
I1112 03:21:50.273662 140264174335808 run_lib.py:153] step: 535800, training_loss: 1.55533e+02
I1112 03:22:00.969323 140264174335808 run_lib.py:153] step: 535850, training_loss: 1.08233e+02
I1112 03:22:11.563738 140264174335808 run_lib.py:153] step: 535900, training_loss: 1.43885e+02
I1112 03:22:21.909874 140264174335808 run_lib.py:153] step: 535950, training_loss: 1.05631e+02
I1112 03:22:31.873793 140264174335808 run_lib.py:153] step: 536000, training_loss: 1.32315e+02
I1112 03:22:42.280137 140264174335808 run_lib.py:153] step: 536050, training_loss: 1.24192e+02
I1112 03:22:52.260221 140264174335808 run_lib.py:153] step: 536100, training_loss: 1.16729e+02
I1112 03:23:02.001440 140264174335808 run_lib.py:153] step: 536150, training_loss: 1.30857e+02
I1112 03:23:11.650225 140264174335808 run_lib.py:153] step: 536200, training_loss: 1.54392e+02
I1112 03:23:21.607308 140264174335808 run_lib.py:153] step: 536250, training_loss: 1.51506e+02
I1112 03:23:31.634918 140264174335808 run_lib.py:153] step: 536300, training_loss: 1.14525e+02
I1112 03:23:41.407526 140264174335808 run_lib.py:153] step: 536350, training_loss: 1.44442e+02
I1112 03:23:52.021958 140264174335808 run_lib.py:153] step: 536400, training_loss: 1.51128e+02
I1112 03:24:02.507592 140264174335808 run_lib.py:153] step: 536450, training_loss: 1.25840e+02
I1112 03:24:12.313475 140264174335808 run_lib.py:153] step: 536500, training_loss: 1.21566e+02
I1112 03:24:22.627764 140264174335808 run_lib.py:153] step: 536550, training_loss: 1.06451e+02
I1112 03:24:33.042201 140264174335808 run_lib.py:153] step: 536600, training_loss: 1.17087e+02
I1112 03:24:42.998786 140264174335808 run_lib.py:153] step: 536650, training_loss: 1.29222e+02
I1112 03:24:52.691138 140264174335808 run_lib.py:153] step: 536700, training_loss: 1.31394e+02
I1112 03:25:01.991605 140264174335808 run_lib.py:153] step: 536750, training_loss: 1.05933e+02
I1112 03:25:12.550182 140264174335808 run_lib.py:153] step: 536800, training_loss: 1.03624e+02
I1112 03:25:22.341404 140264174335808 run_lib.py:153] step: 536850, training_loss: 1.33205e+02
I1112 03:25:32.089718 140264174335808 run_lib.py:153] step: 536900, training_loss: 1.07753e+02
I1112 03:25:42.755025 140264174335808 run_lib.py:153] step: 536950, training_loss: 1.52419e+02
I1112 03:25:53.574816 140264174335808 run_lib.py:153] step: 537000, training_loss: 1.25486e+02
I1112 03:26:04.242878 140264174335808 run_lib.py:153] step: 537050, training_loss: 1.25329e+02
I1112 03:26:13.999942 140264174335808 run_lib.py:153] step: 537100, training_loss: 1.30257e+02
I1112 03:26:24.047286 140264174335808 run_lib.py:153] step: 537150, training_loss: 1.35399e+02
I1112 03:26:33.558043 140264174335808 run_lib.py:153] step: 537200, training_loss: 1.21163e+02
I1112 03:26:43.590734 140264174335808 run_lib.py:153] step: 537250, training_loss: 1.64753e+02
I1112 03:26:53.643407 140264174335808 run_lib.py:153] step: 537300, training_loss: 1.53311e+02
I1112 03:27:04.275867 140264174335808 run_lib.py:153] step: 537350, training_loss: 1.05590e+02
I1112 03:27:14.433720 140264174335808 run_lib.py:153] step: 537400, training_loss: 1.12338e+02
I1112 03:27:24.462478 140264174335808 run_lib.py:153] step: 537450, training_loss: 1.34876e+02
I1112 03:27:34.621250 140264174335808 run_lib.py:153] step: 537500, training_loss: 1.36804e+02
I1112 03:27:45.079483 140264174335808 run_lib.py:153] step: 537550, training_loss: 1.46514e+02
I1112 03:27:54.934787 140264174335808 run_lib.py:153] step: 537600, training_loss: 1.17175e+02
I1112 03:28:04.921808 140264174335808 run_lib.py:153] step: 537650, training_loss: 1.23798e+02
I1112 03:28:15.382111 140264174335808 run_lib.py:153] step: 537700, training_loss: 1.12726e+02
I1112 03:28:25.504709 140264174335808 run_lib.py:153] step: 537750, training_loss: 1.18374e+02
I1112 03:28:35.127369 140264174335808 run_lib.py:153] step: 537800, training_loss: 1.24352e+02
I1112 03:28:44.940415 140264174335808 run_lib.py:153] step: 537850, training_loss: 1.80502e+02
I1112 03:28:54.460637 140264174335808 run_lib.py:153] step: 537900, training_loss: 8.58702e+01
I1112 03:29:04.040544 140264174335808 run_lib.py:153] step: 537950, training_loss: 1.27860e+02
I1112 03:29:13.815966 140264174335808 run_lib.py:153] step: 538000, training_loss: 1.18428e+02
I1112 03:29:23.866748 140264174335808 run_lib.py:153] step: 538050, training_loss: 1.32464e+02
I1112 03:29:33.510550 140264174335808 run_lib.py:153] step: 538100, training_loss: 1.50950e+02
I1112 03:29:43.223054 140264174335808 run_lib.py:153] step: 538150, training_loss: 1.46186e+02
I1112 03:29:52.910308 140264174335808 run_lib.py:153] step: 538200, training_loss: 1.44239e+02
I1112 03:30:02.846763 140264174335808 run_lib.py:153] step: 538250, training_loss: 1.28872e+02
I1112 03:30:12.779621 140264174335808 run_lib.py:153] step: 538300, training_loss: 1.18613e+02
I1112 03:30:22.886190 140264174335808 run_lib.py:153] step: 538350, training_loss: 1.77331e+02
I1112 03:30:32.562827 140264174335808 run_lib.py:153] step: 538400, training_loss: 1.21809e+02
I1112 03:30:42.323966 140264174335808 run_lib.py:153] step: 538450, training_loss: 1.31422e+02
I1112 03:30:52.336111 140264174335808 run_lib.py:153] step: 538500, training_loss: 1.64057e+02
I1112 03:31:02.451916 140264174335808 run_lib.py:153] step: 538550, training_loss: 1.14070e+02
I1112 03:31:12.653445 140264174335808 run_lib.py:153] step: 538600, training_loss: 1.32850e+02
I1112 03:31:22.529594 140264174335808 run_lib.py:153] step: 538650, training_loss: 8.77278e+01
I1112 03:31:32.685646 140264174335808 run_lib.py:153] step: 538700, training_loss: 1.34144e+02
I1112 03:31:41.934495 140264174335808 run_lib.py:153] step: 538750, training_loss: 1.37330e+02
I1112 03:31:51.625715 140264174335808 run_lib.py:153] step: 538800, training_loss: 1.32997e+02
I1112 03:32:01.626579 140264174335808 run_lib.py:153] step: 538850, training_loss: 1.35654e+02
I1112 03:32:11.477079 140264174335808 run_lib.py:153] step: 538900, training_loss: 1.45943e+02
I1112 03:32:21.907158 140264174335808 run_lib.py:153] step: 538950, training_loss: 1.20978e+02
I1112 03:32:32.232440 140264174335808 run_lib.py:153] step: 539000, training_loss: 1.08104e+02
I1112 03:32:41.835074 140264174335808 run_lib.py:153] step: 539050, training_loss: 1.36004e+02
I1112 03:32:52.200560 140264174335808 run_lib.py:153] step: 539100, training_loss: 1.21887e+02
I1112 03:33:01.693578 140264174335808 run_lib.py:153] step: 539150, training_loss: 9.29555e+01
I1112 03:33:10.943648 140264174335808 run_lib.py:153] step: 539200, training_loss: 1.45902e+02
I1112 03:33:20.452121 140264174335808 run_lib.py:153] step: 539250, training_loss: 1.25210e+02
I1112 03:33:30.505585 140264174335808 run_lib.py:153] step: 539300, training_loss: 1.28579e+02
I1112 03:33:40.874299 140264174335808 run_lib.py:153] step: 539350, training_loss: 1.18481e+02
I1112 03:33:50.406018 140264174335808 run_lib.py:153] step: 539400, training_loss: 1.04872e+02
I1112 03:34:00.094999 140264174335808 run_lib.py:153] step: 539450, training_loss: 1.50049e+02
I1112 03:34:10.720224 140264174335808 run_lib.py:153] step: 539500, training_loss: 1.13412e+02
I1112 03:34:20.858254 140264174335808 run_lib.py:153] step: 539550, training_loss: 1.55578e+02
I1112 03:34:30.307761 140264174335808 run_lib.py:153] step: 539600, training_loss: 1.28900e+02
I1112 03:34:39.775382 140264174335808 run_lib.py:153] step: 539650, training_loss: 1.34902e+02
I1112 03:34:49.510250 140264174335808 run_lib.py:153] step: 539700, training_loss: 1.20527e+02
I1112 03:34:59.630309 140264174335808 run_lib.py:153] step: 539750, training_loss: 1.14730e+02
I1112 03:35:09.823755 140264174335808 run_lib.py:153] step: 539800, training_loss: 1.49172e+02
I1112 03:35:19.909101 140264174335808 run_lib.py:153] step: 539850, training_loss: 1.11088e+02
I1112 03:35:30.129908 140264174335808 run_lib.py:153] step: 539900, training_loss: 1.19669e+02
I1112 03:35:40.939480 140264174335808 run_lib.py:153] step: 539950, training_loss: 1.40483e+02
I1112 03:35:51.129613 140264174335808 run_lib.py:153] step: 540000, training_loss: 1.30856e+02
I1112 03:35:51.715722 140264174335808 run_lib.py:166] step: 540000, eval_loss: 1.34845e+02
I1112 03:36:02.251765 140264174335808 run_lib.py:153] step: 540050, training_loss: 1.30795e+02
I1112 03:36:12.680833 140264174335808 run_lib.py:153] step: 540100, training_loss: 1.30282e+02
I1112 03:36:22.281617 140264174335808 run_lib.py:153] step: 540150, training_loss: 1.42429e+02
I1112 03:36:32.284522 140264174335808 run_lib.py:153] step: 540200, training_loss: 1.45780e+02
I1112 03:36:42.731680 140264174335808 run_lib.py:153] step: 540250, training_loss: 1.71886e+02
I1112 03:36:52.989271 140264174335808 run_lib.py:153] step: 540300, training_loss: 1.04985e+02
I1112 03:37:02.351000 140264174335808 run_lib.py:153] step: 540350, training_loss: 1.35976e+02
I1112 03:37:12.065696 140264174335808 run_lib.py:153] step: 540400, training_loss: 1.07269e+02
I1112 03:37:21.746448 140264174335808 run_lib.py:153] step: 540450, training_loss: 1.27037e+02
I1112 03:37:31.067884 140264174335808 run_lib.py:153] step: 540500, training_loss: 1.08889e+02
I1112 03:37:40.758287 140264174335808 run_lib.py:153] step: 540550, training_loss: 1.42335e+02
I1112 03:37:50.920668 140264174335808 run_lib.py:153] step: 540600, training_loss: 9.57601e+01
I1112 03:38:00.826755 140264174335808 run_lib.py:153] step: 540650, training_loss: 1.34393e+02
I1112 03:38:10.936916 140264174335808 run_lib.py:153] step: 540700, training_loss: 1.13645e+02
I1112 03:38:20.801079 140264174335808 run_lib.py:153] step: 540750, training_loss: 1.63855e+02
I1112 03:38:30.789688 140264174335808 run_lib.py:153] step: 540800, training_loss: 1.10253e+02
I1112 03:38:40.598728 140264174335808 run_lib.py:153] step: 540850, training_loss: 1.21432e+02
I1112 03:38:50.144891 140264174335808 run_lib.py:153] step: 540900, training_loss: 1.08219e+02
I1112 03:39:00.082863 140264174335808 run_lib.py:153] step: 540950, training_loss: 1.30735e+02
I1112 03:39:09.603869 140264174335808 run_lib.py:153] step: 541000, training_loss: 1.26803e+02
I1112 03:39:19.431894 140264174335808 run_lib.py:153] step: 541050, training_loss: 1.05620e+02
I1112 03:39:29.860642 140264174335808 run_lib.py:153] step: 541100, training_loss: 1.27097e+02
I1112 03:39:39.575048 140264174335808 run_lib.py:153] step: 541150, training_loss: 1.10518e+02
I1112 03:39:49.722079 140264174335808 run_lib.py:153] step: 541200, training_loss: 1.21826e+02
I1112 03:39:59.641445 140264174335808 run_lib.py:153] step: 541250, training_loss: 1.53493e+02
I1112 03:40:09.983590 140264174335808 run_lib.py:153] step: 541300, training_loss: 1.21083e+02
I1112 03:40:19.939527 140264174335808 run_lib.py:153] step: 541350, training_loss: 1.32711e+02
I1112 03:40:30.101776 140264174335808 run_lib.py:153] step: 541400, training_loss: 1.48042e+02
I1112 03:40:40.292793 140264174335808 run_lib.py:153] step: 541450, training_loss: 1.35629e+02
I1112 03:40:49.979506 140264174335808 run_lib.py:153] step: 541500, training_loss: 9.73191e+01
I1112 03:40:59.391035 140264174335808 run_lib.py:153] step: 541550, training_loss: 1.43430e+02
I1112 03:41:09.090478 140264174335808 run_lib.py:153] step: 541600, training_loss: 1.53098e+02
I1112 03:41:18.632468 140264174335808 run_lib.py:153] step: 541650, training_loss: 1.28486e+02
I1112 03:41:29.125666 140264174335808 run_lib.py:153] step: 541700, training_loss: 1.42549e+02
I1112 03:41:39.335019 140264174335808 run_lib.py:153] step: 541750, training_loss: 1.13026e+02
I1112 03:41:49.275722 140264174335808 run_lib.py:153] step: 541800, training_loss: 1.43374e+02
I1112 03:42:00.219239 140264174335808 run_lib.py:153] step: 541850, training_loss: 9.92063e+01
I1112 03:42:10.332518 140264174335808 run_lib.py:153] step: 541900, training_loss: 1.17610e+02
I1112 03:42:20.270026 140264174335808 run_lib.py:153] step: 541950, training_loss: 1.04264e+02
I1112 03:42:30.610091 140264174335808 run_lib.py:153] step: 542000, training_loss: 1.41517e+02
I1112 03:42:40.262337 140264174335808 run_lib.py:153] step: 542050, training_loss: 1.24434e+02
I1112 03:42:50.864732 140264174335808 run_lib.py:153] step: 542100, training_loss: 1.32515e+02
I1112 03:43:01.246753 140264174335808 run_lib.py:153] step: 542150, training_loss: 9.35539e+01
I1112 03:43:11.924847 140264174335808 run_lib.py:153] step: 542200, training_loss: 1.27172e+02
I1112 03:43:22.091105 140264174335808 run_lib.py:153] step: 542250, training_loss: 1.12425e+02
I1112 03:43:31.518549 140264174335808 run_lib.py:153] step: 542300, training_loss: 1.46642e+02
I1112 03:43:41.177705 140264174335808 run_lib.py:153] step: 542350, training_loss: 1.23172e+02
I1112 03:43:50.738997 140264174335808 run_lib.py:153] step: 542400, training_loss: 1.16047e+02
I1112 03:44:00.300397 140264174335808 run_lib.py:153] step: 542450, training_loss: 1.23828e+02
I1112 03:44:10.292170 140264174335808 run_lib.py:153] step: 542500, training_loss: 1.33132e+02
I1112 03:44:20.225038 140264174335808 run_lib.py:153] step: 542550, training_loss: 1.31858e+02
I1112 03:44:30.145457 140264174335808 run_lib.py:153] step: 542600, training_loss: 1.36579e+02
I1112 03:44:40.343726 140264174335808 run_lib.py:153] step: 542650, training_loss: 1.16946e+02
I1112 03:44:50.441736 140264174335808 run_lib.py:153] step: 542700, training_loss: 1.19639e+02
I1112 03:45:00.418724 140264174335808 run_lib.py:153] step: 542750, training_loss: 1.18127e+02
I1112 03:45:10.625570 140264174335808 run_lib.py:153] step: 542800, training_loss: 1.38014e+02
I1112 03:45:20.931255 140264174335808 run_lib.py:153] step: 542850, training_loss: 1.09932e+02
I1112 03:45:31.183921 140264174335808 run_lib.py:153] step: 542900, training_loss: 1.36333e+02
I1112 03:45:41.106698 140264174335808 run_lib.py:153] step: 542950, training_loss: 1.10967e+02
I1112 03:45:50.555375 140264174335808 run_lib.py:153] step: 543000, training_loss: 1.12061e+02
I1112 03:46:00.569878 140264174335808 run_lib.py:153] step: 543050, training_loss: 1.26881e+02
I1112 03:46:10.931614 140264174335808 run_lib.py:153] step: 543100, training_loss: 1.20865e+02
I1112 03:46:20.848014 140264174335808 run_lib.py:153] step: 543150, training_loss: 1.36276e+02
I1112 03:46:30.735489 140264174335808 run_lib.py:153] step: 543200, training_loss: 1.44061e+02
I1112 03:46:40.119262 140264174335808 run_lib.py:153] step: 543250, training_loss: 1.33558e+02
I1112 03:46:50.028575 140264174335808 run_lib.py:153] step: 543300, training_loss: 1.19516e+02
I1112 03:46:59.827659 140264174335808 run_lib.py:153] step: 543350, training_loss: 1.34365e+02
I1112 03:47:09.317088 140264174335808 run_lib.py:153] step: 543400, training_loss: 1.22099e+02
I1112 03:47:18.685076 140264174335808 run_lib.py:153] step: 543450, training_loss: 1.55069e+02
I1112 03:47:28.648786 140264174335808 run_lib.py:153] step: 543500, training_loss: 1.14895e+02
I1112 03:47:38.018267 140264174335808 run_lib.py:153] step: 543550, training_loss: 1.05139e+02
I1112 03:47:47.465449 140264174335808 run_lib.py:153] step: 543600, training_loss: 1.33111e+02
I1112 03:47:57.507406 140264174335808 run_lib.py:153] step: 543650, training_loss: 1.31308e+02
I1112 03:48:07.105322 140264174335808 run_lib.py:153] step: 543700, training_loss: 1.66041e+02
I1112 03:48:16.772488 140264174335808 run_lib.py:153] step: 543750, training_loss: 9.83743e+01
I1112 03:48:26.639891 140264174335808 run_lib.py:153] step: 543800, training_loss: 9.98219e+01
I1112 03:48:36.733836 140264174335808 run_lib.py:153] step: 543850, training_loss: 1.39149e+02
I1112 03:48:46.946599 140264174335808 run_lib.py:153] step: 543900, training_loss: 1.31006e+02
I1112 03:48:56.839650 140264174335808 run_lib.py:153] step: 543950, training_loss: 1.05584e+02
I1112 03:49:07.613854 140264174335808 run_lib.py:153] step: 544000, training_loss: 1.11599e+02
I1112 03:49:17.631374 140264174335808 run_lib.py:153] step: 544050, training_loss: 1.64634e+02
I1112 03:49:27.736935 140264174335808 run_lib.py:153] step: 544100, training_loss: 1.24317e+02
I1112 03:49:37.581880 140264174335808 run_lib.py:153] step: 544150, training_loss: 1.12720e+02
I1112 03:49:47.008041 140264174335808 run_lib.py:153] step: 544200, training_loss: 1.35588e+02
I1112 03:49:56.681688 140264174335808 run_lib.py:153] step: 544250, training_loss: 1.37196e+02
I1112 03:50:06.743581 140264174335808 run_lib.py:153] step: 544300, training_loss: 1.25069e+02
I1112 03:50:16.185730 140264174335808 run_lib.py:153] step: 544350, training_loss: 1.13816e+02
I1112 03:50:26.017480 140264174335808 run_lib.py:153] step: 544400, training_loss: 1.21150e+02
I1112 03:50:35.719348 140264174335808 run_lib.py:153] step: 544450, training_loss: 1.01320e+02
I1112 03:50:46.375022 140264174335808 run_lib.py:153] step: 544500, training_loss: 1.06485e+02
I1112 03:50:56.756851 140264174335808 run_lib.py:153] step: 544550, training_loss: 1.32068e+02
I1112 03:51:06.375296 140264174335808 run_lib.py:153] step: 544600, training_loss: 1.20831e+02
I1112 03:51:15.839392 140264174335808 run_lib.py:153] step: 544650, training_loss: 8.95993e+01
I1112 03:51:25.331188 140264174335808 run_lib.py:153] step: 544700, training_loss: 1.34913e+02
I1112 03:51:35.323932 140264174335808 run_lib.py:153] step: 544750, training_loss: 1.53793e+02
I1112 03:51:45.435047 140264174335808 run_lib.py:153] step: 544800, training_loss: 1.57630e+02
I1112 03:51:55.107261 140264174335808 run_lib.py:153] step: 544850, training_loss: 1.31643e+02
I1112 03:52:04.987450 140264174335808 run_lib.py:153] step: 544900, training_loss: 1.76123e+02
I1112 03:52:14.410398 140264174335808 run_lib.py:153] step: 544950, training_loss: 1.15547e+02
I1112 03:52:24.054728 140264174335808 run_lib.py:153] step: 545000, training_loss: 1.40934e+02
I1112 03:52:24.154864 140264174335808 run_lib.py:166] step: 545000, eval_loss: 1.15297e+02
I1112 03:52:33.766151 140264174335808 run_lib.py:153] step: 545050, training_loss: 1.10154e+02
I1112 03:52:43.749049 140264174335808 run_lib.py:153] step: 545100, training_loss: 9.68579e+01
I1112 03:52:53.919282 140264174335808 run_lib.py:153] step: 545150, training_loss: 1.28580e+02
I1112 03:53:03.739359 140264174335808 run_lib.py:153] step: 545200, training_loss: 1.31570e+02
I1112 03:53:14.197588 140264174335808 run_lib.py:153] step: 545250, training_loss: 1.41871e+02
I1112 03:53:24.470472 140264174335808 run_lib.py:153] step: 545300, training_loss: 1.20372e+02
I1112 03:53:34.097216 140264174335808 run_lib.py:153] step: 545350, training_loss: 1.27811e+02
I1112 03:53:44.592837 140264174335808 run_lib.py:153] step: 545400, training_loss: 1.10694e+02
I1112 03:53:54.368262 140264174335808 run_lib.py:153] step: 545450, training_loss: 1.47649e+02
I1112 03:54:04.809600 140264174335808 run_lib.py:153] step: 545500, training_loss: 1.21881e+02
I1112 03:54:14.767770 140264174335808 run_lib.py:153] step: 545550, training_loss: 1.31577e+02
I1112 03:54:24.681526 140264174335808 run_lib.py:153] step: 545600, training_loss: 1.25384e+02
I1112 03:54:34.796285 140264174335808 run_lib.py:153] step: 545650, training_loss: 1.42475e+02
I1112 03:54:45.115577 140264174335808 run_lib.py:153] step: 545700, training_loss: 1.17356e+02
I1112 03:54:55.872456 140264174335808 run_lib.py:153] step: 545750, training_loss: 1.13650e+02
I1112 03:55:05.871659 140264174335808 run_lib.py:153] step: 545800, training_loss: 1.54463e+02
I1112 03:55:16.542998 140264174335808 run_lib.py:153] step: 545850, training_loss: 1.12711e+02
I1112 03:55:26.060938 140264174335808 run_lib.py:153] step: 545900, training_loss: 1.17846e+02
I1112 03:55:36.098016 140264174335808 run_lib.py:153] step: 545950, training_loss: 1.46927e+02
I1112 03:55:45.805870 140264174335808 run_lib.py:153] step: 546000, training_loss: 1.31191e+02
I1112 03:55:55.204880 140264174335808 run_lib.py:153] step: 546050, training_loss: 1.33922e+02
I1112 03:56:05.400611 140264174335808 run_lib.py:153] step: 546100, training_loss: 1.24132e+02
I1112 03:56:15.946273 140264174335808 run_lib.py:153] step: 546150, training_loss: 1.08079e+02
I1112 03:56:26.211639 140264174335808 run_lib.py:153] step: 546200, training_loss: 1.33557e+02
I1112 03:56:35.961641 140264174335808 run_lib.py:153] step: 546250, training_loss: 1.21955e+02
I1112 03:56:45.613250 140264174335808 run_lib.py:153] step: 546300, training_loss: 1.20388e+02
I1112 03:56:56.056208 140264174335808 run_lib.py:153] step: 546350, training_loss: 1.25652e+02
I1112 03:57:05.734884 140264174335808 run_lib.py:153] step: 546400, training_loss: 1.39252e+02
I1112 03:57:16.204841 140264174335808 run_lib.py:153] step: 546450, training_loss: 1.15431e+02
I1112 03:57:26.706121 140264174335808 run_lib.py:153] step: 546500, training_loss: 1.42255e+02
I1112 03:57:36.695649 140264174335808 run_lib.py:153] step: 546550, training_loss: 1.23117e+02
I1112 03:57:46.863460 140264174335808 run_lib.py:153] step: 546600, training_loss: 1.37835e+02
I1112 03:57:57.043781 140264174335808 run_lib.py:153] step: 546650, training_loss: 1.28431e+02
I1112 03:58:07.173512 140264174335808 run_lib.py:153] step: 546700, training_loss: 1.37503e+02
I1112 03:58:16.816209 140264174335808 run_lib.py:153] step: 546750, training_loss: 1.35255e+02
I1112 03:58:26.783094 140264174335808 run_lib.py:153] step: 546800, training_loss: 1.25301e+02
I1112 03:58:36.786585 140264174335808 run_lib.py:153] step: 546850, training_loss: 1.27726e+02
I1112 03:58:47.630108 140264174335808 run_lib.py:153] step: 546900, training_loss: 1.34760e+02
I1112 03:58:58.471008 140264174335808 run_lib.py:153] step: 546950, training_loss: 1.32764e+02
I1112 03:59:07.752771 140264174335808 run_lib.py:153] step: 547000, training_loss: 1.31229e+02
I1112 03:59:17.280509 140264174335808 run_lib.py:153] step: 547050, training_loss: 1.19155e+02
I1112 03:59:27.329523 140264174335808 run_lib.py:153] step: 547100, training_loss: 1.08210e+02
I1112 03:59:37.386894 140264174335808 run_lib.py:153] step: 547150, training_loss: 9.81037e+01
I1112 03:59:47.344926 140264174335808 run_lib.py:153] step: 547200, training_loss: 1.48514e+02
I1112 03:59:57.328337 140264174335808 run_lib.py:153] step: 547250, training_loss: 1.24989e+02
I1112 04:00:07.255340 140264174335808 run_lib.py:153] step: 547300, training_loss: 1.42209e+02
I1112 04:00:17.986676 140264174335808 run_lib.py:153] step: 547350, training_loss: 1.33051e+02
I1112 04:00:28.172101 140264174335808 run_lib.py:153] step: 547400, training_loss: 1.34538e+02
I1112 04:00:38.151243 140264174335808 run_lib.py:153] step: 547450, training_loss: 1.41335e+02
I1112 04:00:47.651617 140264174335808 run_lib.py:153] step: 547500, training_loss: 1.00258e+02
I1112 04:00:57.810580 140264174335808 run_lib.py:153] step: 547550, training_loss: 1.50654e+02
I1112 04:01:08.218189 140264174335808 run_lib.py:153] step: 547600, training_loss: 1.19636e+02
I1112 04:01:18.056334 140264174335808 run_lib.py:153] step: 547650, training_loss: 1.27962e+02
I1112 04:01:28.458764 140264174335808 run_lib.py:153] step: 547700, training_loss: 1.15989e+02
I1112 04:01:38.875307 140264174335808 run_lib.py:153] step: 547750, training_loss: 1.24092e+02
I1112 04:01:49.863800 140264174335808 run_lib.py:153] step: 547800, training_loss: 1.49029e+02
I1112 04:02:00.336629 140264174335808 run_lib.py:153] step: 547850, training_loss: 1.13873e+02
I1112 04:02:10.113879 140264174335808 run_lib.py:153] step: 547900, training_loss: 1.05759e+02
I1112 04:02:20.199702 140264174335808 run_lib.py:153] step: 547950, training_loss: 1.24442e+02
I1112 04:02:30.121397 140264174335808 run_lib.py:153] step: 548000, training_loss: 1.21789e+02
I1112 04:02:39.741084 140264174335808 run_lib.py:153] step: 548050, training_loss: 1.22261e+02
I1112 04:02:49.914242 140264174335808 run_lib.py:153] step: 548100, training_loss: 1.30641e+02
I1112 04:02:59.593172 140264174335808 run_lib.py:153] step: 548150, training_loss: 1.13670e+02
I1112 04:03:10.231957 140264174335808 run_lib.py:153] step: 548200, training_loss: 1.09794e+02
I1112 04:03:20.635184 140264174335808 run_lib.py:153] step: 548250, training_loss: 1.33633e+02
I1112 04:03:30.743003 140264174335808 run_lib.py:153] step: 548300, training_loss: 1.18895e+02
I1112 04:03:41.230451 140264174335808 run_lib.py:153] step: 548350, training_loss: 1.36831e+02
I1112 04:03:51.024988 140264174335808 run_lib.py:153] step: 548400, training_loss: 1.33959e+02
I1112 04:04:01.215120 140264174335808 run_lib.py:153] step: 548450, training_loss: 1.53430e+02
I1112 04:04:10.589623 140264174335808 run_lib.py:153] step: 548500, training_loss: 9.81385e+01
I1112 04:04:20.138547 140264174335808 run_lib.py:153] step: 548550, training_loss: 1.19417e+02
I1112 04:04:29.815837 140264174335808 run_lib.py:153] step: 548600, training_loss: 1.29752e+02
I1112 04:04:40.472940 140264174335808 run_lib.py:153] step: 548650, training_loss: 1.15649e+02
I1112 04:04:50.106976 140264174335808 run_lib.py:153] step: 548700, training_loss: 1.33932e+02
I1112 04:04:59.555474 140264174335808 run_lib.py:153] step: 548750, training_loss: 1.45428e+02
I1112 04:05:09.350164 140264174335808 run_lib.py:153] step: 548800, training_loss: 1.02182e+02
I1112 04:05:19.472640 140264174335808 run_lib.py:153] step: 548850, training_loss: 1.08811e+02
I1112 04:05:29.151658 140264174335808 run_lib.py:153] step: 548900, training_loss: 1.24268e+02
I1112 04:05:38.839679 140264174335808 run_lib.py:153] step: 548950, training_loss: 1.25411e+02
I1112 04:05:48.418578 140264174335808 run_lib.py:153] step: 549000, training_loss: 1.25828e+02
I1112 04:05:58.638956 140264174335808 run_lib.py:153] step: 549050, training_loss: 1.17011e+02
I1112 04:06:08.864967 140264174335808 run_lib.py:153] step: 549100, training_loss: 1.37460e+02
I1112 04:06:19.501355 140264174335808 run_lib.py:153] step: 549150, training_loss: 1.00624e+02
I1112 04:06:29.645588 140264174335808 run_lib.py:153] step: 549200, training_loss: 1.14460e+02
I1112 04:06:40.178823 140264174335808 run_lib.py:153] step: 549250, training_loss: 1.49388e+02
I1112 04:06:49.989770 140264174335808 run_lib.py:153] step: 549300, training_loss: 1.20582e+02
I1112 04:06:59.694561 140264174335808 run_lib.py:153] step: 549350, training_loss: 1.12906e+02
I1112 04:07:09.562598 140264174335808 run_lib.py:153] step: 549400, training_loss: 1.26892e+02
I1112 04:07:19.267162 140264174335808 run_lib.py:153] step: 549450, training_loss: 1.11208e+02
I1112 04:07:29.482514 140264174335808 run_lib.py:153] step: 549500, training_loss: 1.28975e+02
I1112 04:07:38.821615 140264174335808 run_lib.py:153] step: 549550, training_loss: 1.19314e+02
I1112 04:07:48.502396 140264174335808 run_lib.py:153] step: 549600, training_loss: 8.10031e+01
I1112 04:07:58.446897 140264174335808 run_lib.py:153] step: 549650, training_loss: 1.34762e+02
I1112 04:08:08.320522 140264174335808 run_lib.py:153] step: 549700, training_loss: 1.24327e+02
I1112 04:08:18.213182 140264174335808 run_lib.py:153] step: 549750, training_loss: 1.10065e+02
I1112 04:08:27.975173 140264174335808 run_lib.py:153] step: 549800, training_loss: 1.10416e+02
I1112 04:08:37.910001 140264174335808 run_lib.py:153] step: 549850, training_loss: 1.08891e+02
I1112 04:08:48.130899 140264174335808 run_lib.py:153] step: 549900, training_loss: 1.56321e+02
I1112 04:08:57.739472 140264174335808 run_lib.py:153] step: 549950, training_loss: 1.24119e+02
I1112 04:09:07.588517 140264174335808 run_lib.py:153] step: 550000, training_loss: 1.18083e+02
I1112 04:09:08.132122 140264174335808 run_lib.py:166] step: 550000, eval_loss: 1.16965e+02
I1112 04:09:18.384490 140264174335808 run_lib.py:153] step: 550050, training_loss: 1.34455e+02
I1112 04:09:28.693357 140264174335808 run_lib.py:153] step: 550100, training_loss: 9.62114e+01
I1112 04:09:38.359341 140264174335808 run_lib.py:153] step: 550150, training_loss: 1.25061e+02
I1112 04:09:48.179388 140264174335808 run_lib.py:153] step: 550200, training_loss: 1.36987e+02
I1112 04:09:57.628706 140264174335808 run_lib.py:153] step: 550250, training_loss: 1.14579e+02
I1112 04:10:07.440175 140264174335808 run_lib.py:153] step: 550300, training_loss: 1.25187e+02
I1112 04:10:16.788769 140264174335808 run_lib.py:153] step: 550350, training_loss: 1.24666e+02
I1112 04:10:26.442336 140264174335808 run_lib.py:153] step: 550400, training_loss: 1.11844e+02
I1112 04:10:36.557906 140264174335808 run_lib.py:153] step: 550450, training_loss: 1.16276e+02
I1112 04:10:46.032363 140264174335808 run_lib.py:153] step: 550500, training_loss: 1.14805e+02
I1112 04:10:55.982624 140264174335808 run_lib.py:153] step: 550550, training_loss: 1.34854e+02
I1112 04:11:06.057201 140264174335808 run_lib.py:153] step: 550600, training_loss: 1.23586e+02
I1112 04:11:16.248242 140264174335808 run_lib.py:153] step: 550650, training_loss: 1.13500e+02
I1112 04:11:25.752932 140264174335808 run_lib.py:153] step: 550700, training_loss: 1.31918e+02
I1112 04:11:36.280851 140264174335808 run_lib.py:153] step: 550750, training_loss: 1.21201e+02
I1112 04:11:46.941428 140264174335808 run_lib.py:153] step: 550800, training_loss: 1.37180e+02
I1112 04:11:56.597879 140264174335808 run_lib.py:153] step: 550850, training_loss: 1.41362e+02
I1112 04:12:06.076213 140264174335808 run_lib.py:153] step: 550900, training_loss: 1.21513e+02
I1112 04:12:15.791357 140264174335808 run_lib.py:153] step: 550950, training_loss: 1.20522e+02
I1112 04:12:26.174129 140264174335808 run_lib.py:153] step: 551000, training_loss: 1.41058e+02
I1112 04:12:36.278201 140264174335808 run_lib.py:153] step: 551050, training_loss: 1.08742e+02
I1112 04:12:46.609060 140264174335808 run_lib.py:153] step: 551100, training_loss: 1.17155e+02
I1112 04:12:56.545425 140264174335808 run_lib.py:153] step: 551150, training_loss: 1.13603e+02
I1112 04:13:06.649357 140264174335808 run_lib.py:153] step: 551200, training_loss: 1.22909e+02
I1112 04:13:16.384480 140264174335808 run_lib.py:153] step: 551250, training_loss: 1.16793e+02
I1112 04:13:26.984081 140264174335808 run_lib.py:153] step: 551300, training_loss: 9.90209e+01
I1112 04:13:37.071945 140264174335808 run_lib.py:153] step: 551350, training_loss: 1.30939e+02
I1112 04:13:46.893872 140264174335808 run_lib.py:153] step: 551400, training_loss: 1.57226e+02
I1112 04:13:57.182253 140264174335808 run_lib.py:153] step: 551450, training_loss: 1.45690e+02
I1112 04:14:07.310204 140264174335808 run_lib.py:153] step: 551500, training_loss: 1.35584e+02
I1112 04:14:17.693946 140264174335808 run_lib.py:153] step: 551550, training_loss: 9.93117e+01
I1112 04:14:27.106948 140264174335808 run_lib.py:153] step: 551600, training_loss: 1.10252e+02
I1112 04:14:36.752641 140264174335808 run_lib.py:153] step: 551650, training_loss: 1.18528e+02
I1112 04:14:47.224131 140264174335808 run_lib.py:153] step: 551700, training_loss: 1.59462e+02
I1112 04:14:57.591614 140264174335808 run_lib.py:153] step: 551750, training_loss: 1.52627e+02
I1112 04:15:07.719664 140264174335808 run_lib.py:153] step: 551800, training_loss: 1.04967e+02
I1112 04:15:17.675081 140264174335808 run_lib.py:153] step: 551850, training_loss: 1.10114e+02
I1112 04:15:27.434988 140264174335808 run_lib.py:153] step: 551900, training_loss: 1.37311e+02
I1112 04:15:37.635439 140264174335808 run_lib.py:153] step: 551950, training_loss: 1.37703e+02
I1112 04:15:47.542291 140264174335808 run_lib.py:153] step: 552000, training_loss: 1.08590e+02
I1112 04:15:57.775762 140264174335808 run_lib.py:153] step: 552050, training_loss: 1.52624e+02
I1112 04:16:07.357349 140264174335808 run_lib.py:153] step: 552100, training_loss: 1.30777e+02
I1112 04:16:17.577940 140264174335808 run_lib.py:153] step: 552150, training_loss: 1.05753e+02
I1112 04:16:27.591478 140264174335808 run_lib.py:153] step: 552200, training_loss: 1.33118e+02
I1112 04:16:37.150408 140264174335808 run_lib.py:153] step: 552250, training_loss: 1.09531e+02
I1112 04:16:46.475055 140264174335808 run_lib.py:153] step: 552300, training_loss: 1.28589e+02
I1112 04:16:55.952098 140264174335808 run_lib.py:153] step: 552350, training_loss: 1.22396e+02
I1112 04:17:06.356222 140264174335808 run_lib.py:153] step: 552400, training_loss: 1.23814e+02
I1112 04:17:16.788100 140264174335808 run_lib.py:153] step: 552450, training_loss: 1.18679e+02
I1112 04:17:26.643744 140264174335808 run_lib.py:153] step: 552500, training_loss: 9.46834e+01
I1112 04:17:36.490961 140264174335808 run_lib.py:153] step: 552550, training_loss: 1.24244e+02
I1112 04:17:47.608595 140264174335808 run_lib.py:153] step: 552600, training_loss: 9.76104e+01
I1112 04:17:58.230895 140264174335808 run_lib.py:153] step: 552650, training_loss: 1.33470e+02
I1112 04:18:08.085890 140264174335808 run_lib.py:153] step: 552700, training_loss: 1.32299e+02
I1112 04:18:17.567196 140264174335808 run_lib.py:153] step: 552750, training_loss: 1.30956e+02
I1112 04:18:27.507019 140264174335808 run_lib.py:153] step: 552800, training_loss: 1.34011e+02
I1112 04:18:37.513550 140264174335808 run_lib.py:153] step: 552850, training_loss: 1.29283e+02
I1112 04:18:47.434613 140264174335808 run_lib.py:153] step: 552900, training_loss: 1.29354e+02
I1112 04:18:57.803080 140264174335808 run_lib.py:153] step: 552950, training_loss: 1.28403e+02
I1112 04:19:07.285988 140264174335808 run_lib.py:153] step: 553000, training_loss: 1.50203e+02
I1112 04:19:16.815457 140264174335808 run_lib.py:153] step: 553050, training_loss: 1.42811e+02
I1112 04:19:26.180019 140264174335808 run_lib.py:153] step: 553100, training_loss: 1.22356e+02
I1112 04:19:35.920781 140264174335808 run_lib.py:153] step: 553150, training_loss: 1.23322e+02
I1112 04:19:46.360741 140264174335808 run_lib.py:153] step: 553200, training_loss: 1.12960e+02
I1112 04:19:56.643557 140264174335808 run_lib.py:153] step: 553250, training_loss: 1.28265e+02
I1112 04:20:06.691782 140264174335808 run_lib.py:153] step: 553300, training_loss: 1.15747e+02
I1112 04:20:16.989223 140264174335808 run_lib.py:153] step: 553350, training_loss: 1.08011e+02
I1112 04:20:27.321582 140264174335808 run_lib.py:153] step: 553400, training_loss: 1.00165e+02
I1112 04:20:37.946396 140264174335808 run_lib.py:153] step: 553450, training_loss: 1.08048e+02
I1112 04:20:47.316985 140264174335808 run_lib.py:153] step: 553500, training_loss: 1.05769e+02
I1112 04:20:57.576668 140264174335808 run_lib.py:153] step: 553550, training_loss: 1.22245e+02
I1112 04:21:07.885681 140264174335808 run_lib.py:153] step: 553600, training_loss: 1.40714e+02
I1112 04:21:17.928335 140264174335808 run_lib.py:153] step: 553650, training_loss: 1.18854e+02
I1112 04:21:27.740475 140264174335808 run_lib.py:153] step: 553700, training_loss: 1.33753e+02
I1112 04:21:37.580129 140264174335808 run_lib.py:153] step: 553750, training_loss: 1.27298e+02
I1112 04:21:48.082503 140264174335808 run_lib.py:153] step: 553800, training_loss: 1.38691e+02
I1112 04:21:57.562505 140264174335808 run_lib.py:153] step: 553850, training_loss: 1.34011e+02
I1112 04:22:07.679646 140264174335808 run_lib.py:153] step: 553900, training_loss: 1.44935e+02
I1112 04:22:17.678112 140264174335808 run_lib.py:153] step: 553950, training_loss: 1.30123e+02
I1112 04:22:27.927433 140264174335808 run_lib.py:153] step: 554000, training_loss: 1.18626e+02
I1112 04:22:37.471411 140264174335808 run_lib.py:153] step: 554050, training_loss: 1.52776e+02
I1112 04:22:47.558332 140264174335808 run_lib.py:153] step: 554100, training_loss: 1.03924e+02
I1112 04:22:57.428588 140264174335808 run_lib.py:153] step: 554150, training_loss: 1.60215e+02
I1112 04:23:07.586983 140264174335808 run_lib.py:153] step: 554200, training_loss: 1.46559e+02
I1112 04:23:16.858282 140264174335808 run_lib.py:153] step: 554250, training_loss: 1.20218e+02
I1112 04:23:26.168507 140264174335808 run_lib.py:153] step: 554300, training_loss: 1.29078e+02
I1112 04:23:35.703577 140264174335808 run_lib.py:153] step: 554350, training_loss: 1.51140e+02
I1112 04:23:45.828464 140264174335808 run_lib.py:153] step: 554400, training_loss: 1.09583e+02
I1112 04:23:56.186712 140264174335808 run_lib.py:153] step: 554450, training_loss: 1.38097e+02
I1112 04:24:06.143439 140264174335808 run_lib.py:153] step: 554500, training_loss: 1.48282e+02
I1112 04:24:15.967144 140264174335808 run_lib.py:153] step: 554550, training_loss: 1.47643e+02
I1112 04:24:26.803084 140264174335808 run_lib.py:153] step: 554600, training_loss: 1.65416e+02
I1112 04:24:36.734223 140264174335808 run_lib.py:153] step: 554650, training_loss: 1.16003e+02
I1112 04:24:46.878980 140264174335808 run_lib.py:153] step: 554700, training_loss: 1.14086e+02
I1112 04:24:57.207398 140264174335808 run_lib.py:153] step: 554750, training_loss: 1.25450e+02
I1112 04:25:06.835166 140264174335808 run_lib.py:153] step: 554800, training_loss: 1.38167e+02
I1112 04:25:16.633073 140264174335808 run_lib.py:153] step: 554850, training_loss: 1.14333e+02
I1112 04:25:26.790223 140264174335808 run_lib.py:153] step: 554900, training_loss: 1.53573e+02
I1112 04:25:37.374491 140264174335808 run_lib.py:153] step: 554950, training_loss: 1.43804e+02
I1112 04:25:47.534298 140264174335808 run_lib.py:153] step: 555000, training_loss: 1.22045e+02
I1112 04:25:47.636851 140264174335808 run_lib.py:166] step: 555000, eval_loss: 1.10650e+02
I1112 04:25:57.694934 140264174335808 run_lib.py:153] step: 555050, training_loss: 1.44169e+02
I1112 04:26:07.818999 140264174335808 run_lib.py:153] step: 555100, training_loss: 1.40118e+02
I1112 04:26:17.357518 140264174335808 run_lib.py:153] step: 555150, training_loss: 1.32787e+02
I1112 04:26:27.450763 140264174335808 run_lib.py:153] step: 555200, training_loss: 1.60760e+02
I1112 04:26:37.243785 140264174335808 run_lib.py:153] step: 555250, training_loss: 1.40181e+02
I1112 04:26:46.571851 140264174335808 run_lib.py:153] step: 555300, training_loss: 1.40314e+02
I1112 04:26:56.198012 140264174335808 run_lib.py:153] step: 555350, training_loss: 1.23533e+02
I1112 04:27:06.163197 140264174335808 run_lib.py:153] step: 555400, training_loss: 1.18936e+02
I1112 04:27:16.097723 140264174335808 run_lib.py:153] step: 555450, training_loss: 1.37472e+02
I1112 04:27:26.094322 140264174335808 run_lib.py:153] step: 555500, training_loss: 1.60273e+02
I1112 04:27:35.941370 140264174335808 run_lib.py:153] step: 555550, training_loss: 1.12931e+02
I1112 04:27:45.855655 140264174335808 run_lib.py:153] step: 555600, training_loss: 1.10911e+02
I1112 04:27:55.606255 140264174335808 run_lib.py:153] step: 555650, training_loss: 1.34600e+02
I1112 04:28:05.930706 140264174335808 run_lib.py:153] step: 555700, training_loss: 1.37858e+02
I1112 04:28:15.612083 140264174335808 run_lib.py:153] step: 555750, training_loss: 1.76385e+02
I1112 04:28:25.367161 140264174335808 run_lib.py:153] step: 555800, training_loss: 1.21150e+02
I1112 04:28:35.416444 140264174335808 run_lib.py:153] step: 555850, training_loss: 1.29459e+02
I1112 04:28:45.599862 140264174335808 run_lib.py:153] step: 555900, training_loss: 1.32250e+02
I1112 04:28:55.643357 140264174335808 run_lib.py:153] step: 555950, training_loss: 1.02486e+02
I1112 04:29:06.787578 140264174335808 run_lib.py:153] step: 556000, training_loss: 1.29351e+02
I1112 04:29:16.559900 140264174335808 run_lib.py:153] step: 556050, training_loss: 1.01328e+02
I1112 04:29:26.256869 140264174335808 run_lib.py:153] step: 556100, training_loss: 1.21910e+02
I1112 04:29:35.955515 140264174335808 run_lib.py:153] step: 556150, training_loss: 1.44244e+02
I1112 04:29:46.499659 140264174335808 run_lib.py:153] step: 556200, training_loss: 1.35455e+02
I1112 04:29:56.418060 140264174335808 run_lib.py:153] step: 556250, training_loss: 1.12560e+02
I1112 04:30:06.274906 140264174335808 run_lib.py:153] step: 556300, training_loss: 1.23636e+02
I1112 04:30:16.881347 140264174335808 run_lib.py:153] step: 556350, training_loss: 1.10713e+02
I1112 04:30:26.727369 140264174335808 run_lib.py:153] step: 556400, training_loss: 1.16177e+02
I1112 04:30:36.500620 140264174335808 run_lib.py:153] step: 556450, training_loss: 1.19974e+02
I1112 04:30:46.096124 140264174335808 run_lib.py:153] step: 556500, training_loss: 9.81985e+01
I1112 04:30:55.345646 140264174335808 run_lib.py:153] step: 556550, training_loss: 1.35074e+02
I1112 04:31:05.351773 140264174335808 run_lib.py:153] step: 556600, training_loss: 1.36553e+02
I1112 04:31:15.506665 140264174335808 run_lib.py:153] step: 556650, training_loss: 1.33911e+02
I1112 04:31:25.705296 140264174335808 run_lib.py:153] step: 556700, training_loss: 1.41895e+02
I1112 04:31:35.602615 140264174335808 run_lib.py:153] step: 556750, training_loss: 1.14441e+02
I1112 04:31:45.634334 140264174335808 run_lib.py:153] step: 556800, training_loss: 1.08811e+02
I1112 04:31:56.194092 140264174335808 run_lib.py:153] step: 556850, training_loss: 1.40465e+02
I1112 04:32:06.047462 140264174335808 run_lib.py:153] step: 556900, training_loss: 1.37042e+02
I1112 04:32:15.992295 140264174335808 run_lib.py:153] step: 556950, training_loss: 1.47451e+02
I1112 04:32:25.840770 140264174335808 run_lib.py:153] step: 557000, training_loss: 1.30057e+02
I1112 04:32:35.369805 140264174335808 run_lib.py:153] step: 557050, training_loss: 1.50833e+02
I1112 04:32:45.639057 140264174335808 run_lib.py:153] step: 557100, training_loss: 9.68217e+01
I1112 04:32:55.364897 140264174335808 run_lib.py:153] step: 557150, training_loss: 1.40832e+02
I1112 04:33:05.063255 140264174335808 run_lib.py:153] step: 557200, training_loss: 1.22060e+02
I1112 04:33:15.343428 140264174335808 run_lib.py:153] step: 557250, training_loss: 1.28389e+02
I1112 04:33:25.934853 140264174335808 run_lib.py:153] step: 557300, training_loss: 1.28973e+02
I1112 04:33:35.417499 140264174335808 run_lib.py:153] step: 557350, training_loss: 1.27183e+02
I1112 04:33:45.393676 140264174335808 run_lib.py:153] step: 557400, training_loss: 1.49955e+02
I1112 04:33:55.430970 140264174335808 run_lib.py:153] step: 557450, training_loss: 1.29365e+02
I1112 04:34:05.020928 140264174335808 run_lib.py:153] step: 557500, training_loss: 1.45822e+02
I1112 04:34:15.040024 140264174335808 run_lib.py:153] step: 557550, training_loss: 1.31897e+02
I1112 04:34:25.039609 140264174335808 run_lib.py:153] step: 557600, training_loss: 1.03627e+02
I1112 04:34:34.330031 140264174335808 run_lib.py:153] step: 557650, training_loss: 1.10645e+02
I1112 04:34:44.208801 140264174335808 run_lib.py:153] step: 557700, training_loss: 1.62691e+02
I1112 04:34:53.920871 140264174335808 run_lib.py:153] step: 557750, training_loss: 1.28885e+02
I1112 04:35:04.567639 140264174335808 run_lib.py:153] step: 557800, training_loss: 1.12228e+02
I1112 04:35:15.273974 140264174335808 run_lib.py:153] step: 557850, training_loss: 1.16957e+02
I1112 04:35:25.590914 140264174335808 run_lib.py:153] step: 557900, training_loss: 1.19349e+02
I1112 04:35:35.636950 140264174335808 run_lib.py:153] step: 557950, training_loss: 1.30572e+02
I1112 04:35:45.138277 140264174335808 run_lib.py:153] step: 558000, training_loss: 1.17513e+02
I1112 04:35:55.361532 140264174335808 run_lib.py:153] step: 558050, training_loss: 9.97346e+01
I1112 04:36:05.388257 140264174335808 run_lib.py:153] step: 558100, training_loss: 1.28768e+02
I1112 04:36:15.585250 140264174335808 run_lib.py:153] step: 558150, training_loss: 1.14570e+02
I1112 04:36:25.876498 140264174335808 run_lib.py:153] step: 558200, training_loss: 1.14248e+02
I1112 04:36:35.808069 140264174335808 run_lib.py:153] step: 558250, training_loss: 1.24787e+02
I1112 04:36:46.046536 140264174335808 run_lib.py:153] step: 558300, training_loss: 1.18719e+02
I1112 04:36:55.826299 140264174335808 run_lib.py:153] step: 558350, training_loss: 1.03135e+02
I1112 04:37:05.874307 140264174335808 run_lib.py:153] step: 558400, training_loss: 1.26645e+02
I1112 04:37:15.194938 140264174335808 run_lib.py:153] step: 558450, training_loss: 1.25884e+02
I1112 04:37:25.155885 140264174335808 run_lib.py:153] step: 558500, training_loss: 1.09314e+02
I1112 04:37:34.916072 140264174335808 run_lib.py:153] step: 558550, training_loss: 1.26084e+02
I1112 04:37:44.751331 140264174335808 run_lib.py:153] step: 558600, training_loss: 1.19934e+02
I1112 04:37:54.667888 140264174335808 run_lib.py:153] step: 558650, training_loss: 1.18194e+02
I1112 04:38:04.271480 140264174335808 run_lib.py:153] step: 558700, training_loss: 1.34421e+02
I1112 04:38:13.874668 140264174335808 run_lib.py:153] step: 558750, training_loss: 1.56564e+02
I1112 04:38:23.296200 140264174335808 run_lib.py:153] step: 558800, training_loss: 1.16116e+02
I1112 04:38:33.209580 140264174335808 run_lib.py:153] step: 558850, training_loss: 1.20525e+02
I1112 04:38:43.742844 140264174335808 run_lib.py:153] step: 558900, training_loss: 1.14331e+02
I1112 04:38:54.351791 140264174335808 run_lib.py:153] step: 558950, training_loss: 9.87887e+01
I1112 04:39:03.974647 140264174335808 run_lib.py:153] step: 559000, training_loss: 1.16069e+02
I1112 04:39:13.894913 140264174335808 run_lib.py:153] step: 559050, training_loss: 1.20106e+02
I1112 04:39:23.854352 140264174335808 run_lib.py:153] step: 559100, training_loss: 1.07730e+02
I1112 04:39:34.602417 140264174335808 run_lib.py:153] step: 559150, training_loss: 1.29132e+02
I1112 04:39:44.108553 140264174335808 run_lib.py:153] step: 559200, training_loss: 1.03648e+02
I1112 04:39:53.619675 140264174335808 run_lib.py:153] step: 559250, training_loss: 1.34621e+02
I1112 04:40:04.004516 140264174335808 run_lib.py:153] step: 559300, training_loss: 1.37418e+02
I1112 04:40:14.905266 140264174335808 run_lib.py:153] step: 559350, training_loss: 1.17828e+02
I1112 04:40:24.985200 140264174335808 run_lib.py:153] step: 559400, training_loss: 1.45075e+02
I1112 04:40:34.203585 140264174335808 run_lib.py:153] step: 559450, training_loss: 1.30395e+02
I1112 04:40:44.298952 140264174335808 run_lib.py:153] step: 559500, training_loss: 1.05773e+02
I1112 04:40:54.840133 140264174335808 run_lib.py:153] step: 559550, training_loss: 1.30590e+02
I1112 04:41:05.145786 140264174335808 run_lib.py:153] step: 559600, training_loss: 1.32479e+02
I1112 04:41:15.458029 140264174335808 run_lib.py:153] step: 559650, training_loss: 1.03552e+02
I1112 04:41:25.376235 140264174335808 run_lib.py:153] step: 559700, training_loss: 1.48627e+02
I1112 04:41:35.024424 140264174335808 run_lib.py:153] step: 559750, training_loss: 1.28754e+02
I1112 04:41:45.123554 140264174335808 run_lib.py:153] step: 559800, training_loss: 1.26761e+02
I1112 04:41:55.559966 140264174335808 run_lib.py:153] step: 559850, training_loss: 1.40668e+02
I1112 04:42:05.739363 140264174335808 run_lib.py:153] step: 559900, training_loss: 1.19949e+02
I1112 04:42:16.048467 140264174335808 run_lib.py:153] step: 559950, training_loss: 1.38260e+02
I1112 04:42:26.104038 140264174335808 run_lib.py:153] step: 560000, training_loss: 1.30314e+02
I1112 04:42:26.646620 140264174335808 run_lib.py:166] step: 560000, eval_loss: 1.24977e+02
I1112 04:42:36.206744 140264174335808 run_lib.py:153] step: 560050, training_loss: 1.33724e+02
I1112 04:42:46.627450 140264174335808 run_lib.py:153] step: 560100, training_loss: 1.21375e+02
I1112 04:42:56.316292 140264174335808 run_lib.py:153] step: 560150, training_loss: 1.04700e+02
I1112 04:43:05.833489 140264174335808 run_lib.py:153] step: 560200, training_loss: 1.48740e+02
I1112 04:43:15.806207 140264174335808 run_lib.py:153] step: 560250, training_loss: 1.10171e+02
I1112 04:43:26.123914 140264174335808 run_lib.py:153] step: 560300, training_loss: 1.14921e+02
I1112 04:43:36.181241 140264174335808 run_lib.py:153] step: 560350, training_loss: 1.44179e+02
I1112 04:43:46.237205 140264174335808 run_lib.py:153] step: 560400, training_loss: 1.15532e+02
I1112 04:43:55.837809 140264174335808 run_lib.py:153] step: 560450, training_loss: 9.10808e+01
I1112 04:44:06.359303 140264174335808 run_lib.py:153] step: 560500, training_loss: 1.05342e+02
I1112 04:44:15.992459 140264174335808 run_lib.py:153] step: 560550, training_loss: 1.28859e+02
I1112 04:44:25.972124 140264174335808 run_lib.py:153] step: 560600, training_loss: 1.63375e+02
I1112 04:44:36.011787 140264174335808 run_lib.py:153] step: 560650, training_loss: 1.12138e+02
I1112 04:44:45.694752 140264174335808 run_lib.py:153] step: 560700, training_loss: 1.30594e+02
I1112 04:44:55.556072 140264174335808 run_lib.py:153] step: 560750, training_loss: 1.04876e+02
I1112 04:45:05.550068 140264174335808 run_lib.py:153] step: 560800, training_loss: 1.02538e+02
I1112 04:45:16.000075 140264174335808 run_lib.py:153] step: 560850, training_loss: 1.04425e+02
I1112 04:45:26.216752 140264174335808 run_lib.py:153] step: 560900, training_loss: 1.13502e+02
I1112 04:45:36.259020 140264174335808 run_lib.py:153] step: 560950, training_loss: 1.32397e+02
I1112 04:45:46.486607 140264174335808 run_lib.py:153] step: 561000, training_loss: 8.40410e+01
I1112 04:45:56.651026 140264174335808 run_lib.py:153] step: 561050, training_loss: 1.47202e+02
I1112 04:46:06.542988 140264174335808 run_lib.py:153] step: 561100, training_loss: 9.58487e+01
I1112 04:46:16.501132 140264174335808 run_lib.py:153] step: 561150, training_loss: 1.21805e+02
I1112 04:46:26.527158 140264174335808 run_lib.py:153] step: 561200, training_loss: 1.55552e+02
I1112 04:46:36.575408 140264174335808 run_lib.py:153] step: 561250, training_loss: 1.26834e+02
I1112 04:46:46.775625 140264174335808 run_lib.py:153] step: 561300, training_loss: 1.20959e+02
I1112 04:46:56.560987 140264174335808 run_lib.py:153] step: 561350, training_loss: 1.21532e+02
I1112 04:47:07.252825 140264174335808 run_lib.py:153] step: 561400, training_loss: 1.13639e+02
I1112 04:47:17.614247 140264174335808 run_lib.py:153] step: 561450, training_loss: 1.04311e+02
I1112 04:47:27.439331 140264174335808 run_lib.py:153] step: 561500, training_loss: 1.06438e+02
I1112 04:47:37.382761 140264174335808 run_lib.py:153] step: 561550, training_loss: 1.05404e+02
I1112 04:47:47.670856 140264174335808 run_lib.py:153] step: 561600, training_loss: 1.29722e+02
I1112 04:47:58.030565 140264174335808 run_lib.py:153] step: 561650, training_loss: 1.04473e+02
I1112 04:48:08.071686 140264174335808 run_lib.py:153] step: 561700, training_loss: 1.17773e+02
I1112 04:48:18.193203 140264174335808 run_lib.py:153] step: 561750, training_loss: 1.26236e+02
I1112 04:48:28.095745 140264174335808 run_lib.py:153] step: 561800, training_loss: 1.17555e+02
I1112 04:48:38.889674 140264174335808 run_lib.py:153] step: 561850, training_loss: 1.82585e+02
I1112 04:48:49.005483 140264174335808 run_lib.py:153] step: 561900, training_loss: 1.21356e+02
I1112 04:48:59.423111 140264174335808 run_lib.py:153] step: 561950, training_loss: 1.38777e+02
I1112 04:49:09.585656 140264174335808 run_lib.py:153] step: 562000, training_loss: 1.45459e+02
I1112 04:49:19.087555 140264174335808 run_lib.py:153] step: 562050, training_loss: 1.02396e+02
I1112 04:49:30.162203 140264174335808 run_lib.py:153] step: 562100, training_loss: 1.18845e+02
I1112 04:49:39.771587 140264174335808 run_lib.py:153] step: 562150, training_loss: 1.38636e+02
I1112 04:49:50.183781 140264174335808 run_lib.py:153] step: 562200, training_loss: 1.57389e+02
I1112 04:49:59.651583 140264174335808 run_lib.py:153] step: 562250, training_loss: 1.17525e+02
I1112 04:50:09.907023 140264174335808 run_lib.py:153] step: 562300, training_loss: 1.10651e+02
I1112 04:50:19.949870 140264174335808 run_lib.py:153] step: 562350, training_loss: 1.43341e+02
I1112 04:50:30.459533 140264174335808 run_lib.py:153] step: 562400, training_loss: 1.32331e+02
I1112 04:50:41.106126 140264174335808 run_lib.py:153] step: 562450, training_loss: 1.23886e+02
I1112 04:50:51.283843 140264174335808 run_lib.py:153] step: 562500, training_loss: 1.38839e+02
I1112 04:51:01.827879 140264174335808 run_lib.py:153] step: 562550, training_loss: 1.43693e+02
I1112 04:51:12.314471 140264174335808 run_lib.py:153] step: 562600, training_loss: 1.23398e+02
I1112 04:51:22.067590 140264174335808 run_lib.py:153] step: 562650, training_loss: 1.29326e+02
I1112 04:51:32.943756 140264174335808 run_lib.py:153] step: 562700, training_loss: 1.14221e+02
I1112 04:51:43.284997 140264174335808 run_lib.py:153] step: 562750, training_loss: 9.90009e+01
I1112 04:51:53.043206 140264174335808 run_lib.py:153] step: 562800, training_loss: 1.16751e+02
I1112 04:52:03.642691 140264174335808 run_lib.py:153] step: 562850, training_loss: 1.25494e+02
I1112 04:52:13.706284 140264174335808 run_lib.py:153] step: 562900, training_loss: 1.18010e+02
I1112 04:52:23.977788 140264174335808 run_lib.py:153] step: 562950, training_loss: 1.15897e+02
I1112 04:52:33.994251 140264174335808 run_lib.py:153] step: 563000, training_loss: 1.28751e+02
I1112 04:52:43.913053 140264174335808 run_lib.py:153] step: 563050, training_loss: 1.12749e+02
I1112 04:52:54.210784 140264174335808 run_lib.py:153] step: 563100, training_loss: 1.31906e+02
I1112 04:53:03.946959 140264174335808 run_lib.py:153] step: 563150, training_loss: 1.29146e+02
I1112 04:53:13.980928 140264174335808 run_lib.py:153] step: 563200, training_loss: 1.01651e+02
I1112 04:53:24.116582 140264174335808 run_lib.py:153] step: 563250, training_loss: 1.34196e+02
I1112 04:53:34.523789 140264174335808 run_lib.py:153] step: 563300, training_loss: 1.25162e+02
I1112 04:53:44.771620 140264174335808 run_lib.py:153] step: 563350, training_loss: 1.28675e+02
I1112 04:53:54.487961 140264174335808 run_lib.py:153] step: 563400, training_loss: 1.32973e+02
I1112 04:54:04.873604 140264174335808 run_lib.py:153] step: 563450, training_loss: 1.27292e+02
I1112 04:54:15.120705 140264174335808 run_lib.py:153] step: 563500, training_loss: 1.17744e+02
I1112 04:54:25.256178 140264174335808 run_lib.py:153] step: 563550, training_loss: 1.28660e+02
I1112 04:54:36.398787 140264174335808 run_lib.py:153] step: 563600, training_loss: 1.27523e+02
I1112 04:54:46.341676 140264174335808 run_lib.py:153] step: 563650, training_loss: 1.16575e+02
I1112 04:54:56.300790 140264174335808 run_lib.py:153] step: 563700, training_loss: 1.40038e+02
I1112 04:55:06.351443 140264174335808 run_lib.py:153] step: 563750, training_loss: 1.33867e+02
I1112 04:55:15.858214 140264174335808 run_lib.py:153] step: 563800, training_loss: 1.47857e+02
I1112 04:55:25.216763 140264174335808 run_lib.py:153] step: 563850, training_loss: 1.30225e+02
I1112 04:55:35.045184 140264174335808 run_lib.py:153] step: 563900, training_loss: 9.87272e+01
I1112 04:55:44.443006 140264174335808 run_lib.py:153] step: 563950, training_loss: 1.50268e+02
I1112 04:55:54.699728 140264174335808 run_lib.py:153] step: 564000, training_loss: 1.14053e+02
I1112 04:56:04.427835 140264174335808 run_lib.py:153] step: 564050, training_loss: 1.39153e+02
I1112 04:56:15.272728 140264174335808 run_lib.py:153] step: 564100, training_loss: 1.58287e+02
I1112 04:56:25.799486 140264174335808 run_lib.py:153] step: 564150, training_loss: 1.27262e+02
I1112 04:56:36.366263 140264174335808 run_lib.py:153] step: 564200, training_loss: 9.80538e+01
I1112 04:56:46.788242 140264174335808 run_lib.py:153] step: 564250, training_loss: 1.38980e+02
I1112 04:56:56.811123 140264174335808 run_lib.py:153] step: 564300, training_loss: 1.26808e+02
I1112 04:57:07.028543 140264174335808 run_lib.py:153] step: 564350, training_loss: 1.30859e+02
I1112 04:57:17.346233 140264174335808 run_lib.py:153] step: 564400, training_loss: 1.31401e+02
I1112 04:57:27.796313 140264174335808 run_lib.py:153] step: 564450, training_loss: 1.38464e+02
I1112 04:57:38.025515 140264174335808 run_lib.py:153] step: 564500, training_loss: 1.30520e+02
I1112 04:57:47.666634 140264174335808 run_lib.py:153] step: 564550, training_loss: 1.37993e+02
I1112 04:57:58.452985 140264174335808 run_lib.py:153] step: 564600, training_loss: 1.22930e+02
I1112 04:58:09.005874 140264174335808 run_lib.py:153] step: 564650, training_loss: 1.27125e+02
I1112 04:58:18.986904 140264174335808 run_lib.py:153] step: 564700, training_loss: 1.11632e+02
I1112 04:58:29.494522 140264174335808 run_lib.py:153] step: 564750, training_loss: 1.11589e+02
I1112 04:58:39.666341 140264174335808 run_lib.py:153] step: 564800, training_loss: 1.34400e+02
I1112 04:58:49.530036 140264174335808 run_lib.py:153] step: 564850, training_loss: 1.22674e+02
I1112 04:58:59.260282 140264174335808 run_lib.py:153] step: 564900, training_loss: 1.34695e+02
I1112 04:59:09.084995 140264174335808 run_lib.py:153] step: 564950, training_loss: 1.28722e+02
I1112 04:59:18.892531 140264174335808 run_lib.py:153] step: 565000, training_loss: 1.27010e+02
I1112 04:59:18.995572 140264174335808 run_lib.py:166] step: 565000, eval_loss: 1.39021e+02
I1112 04:59:28.476079 140264174335808 run_lib.py:153] step: 565050, training_loss: 1.06779e+02
I1112 04:59:38.598010 140264174335808 run_lib.py:153] step: 565100, training_loss: 1.33329e+02
I1112 04:59:49.375862 140264174335808 run_lib.py:153] step: 565150, training_loss: 1.43825e+02
I1112 04:59:59.328153 140264174335808 run_lib.py:153] step: 565200, training_loss: 1.17992e+02
I1112 05:00:08.936797 140264174335808 run_lib.py:153] step: 565250, training_loss: 1.13902e+02
I1112 05:00:19.161280 140264174335808 run_lib.py:153] step: 565300, training_loss: 1.21756e+02
I1112 05:00:28.971450 140264174335808 run_lib.py:153] step: 565350, training_loss: 1.29325e+02
I1112 05:00:38.638088 140264174335808 run_lib.py:153] step: 565400, training_loss: 9.12943e+01
I1112 05:00:48.374049 140264174335808 run_lib.py:153] step: 565450, training_loss: 1.10560e+02
I1112 05:00:57.815936 140264174335808 run_lib.py:153] step: 565500, training_loss: 1.18059e+02
I1112 05:01:07.272893 140264174335808 run_lib.py:153] step: 565550, training_loss: 1.18667e+02
I1112 05:01:16.605256 140264174335808 run_lib.py:153] step: 565600, training_loss: 1.16665e+02
I1112 05:01:26.654783 140264174335808 run_lib.py:153] step: 565650, training_loss: 1.11602e+02
I1112 05:01:36.137574 140264174335808 run_lib.py:153] step: 565700, training_loss: 1.09394e+02
I1112 05:01:46.324587 140264174335808 run_lib.py:153] step: 565750, training_loss: 1.28638e+02
I1112 05:01:56.722413 140264174335808 run_lib.py:153] step: 565800, training_loss: 1.27636e+02
I1112 05:02:06.335696 140264174335808 run_lib.py:153] step: 565850, training_loss: 1.16358e+02
I1112 05:02:16.012855 140264174335808 run_lib.py:153] step: 565900, training_loss: 1.22429e+02
I1112 05:02:25.376406 140264174335808 run_lib.py:153] step: 565950, training_loss: 1.55413e+02
I1112 05:02:34.883277 140264174335808 run_lib.py:153] step: 566000, training_loss: 1.25636e+02
I1112 05:02:45.434509 140264174335808 run_lib.py:153] step: 566050, training_loss: 1.34519e+02
I1112 05:02:55.763623 140264174335808 run_lib.py:153] step: 566100, training_loss: 1.39907e+02
I1112 05:03:06.001795 140264174335808 run_lib.py:153] step: 566150, training_loss: 1.22447e+02
I1112 05:03:15.761969 140264174335808 run_lib.py:153] step: 566200, training_loss: 1.41242e+02
I1112 05:03:25.543789 140264174335808 run_lib.py:153] step: 566250, training_loss: 1.27239e+02
I1112 05:03:34.949707 140264174335808 run_lib.py:153] step: 566300, training_loss: 1.15237e+02
I1112 05:03:45.043329 140264174335808 run_lib.py:153] step: 566350, training_loss: 1.46585e+02
I1112 05:03:55.438251 140264174335808 run_lib.py:153] step: 566400, training_loss: 1.32517e+02
I1112 05:04:05.653271 140264174335808 run_lib.py:153] step: 566450, training_loss: 1.39807e+02
I1112 05:04:15.450931 140264174335808 run_lib.py:153] step: 566500, training_loss: 9.27449e+01
I1112 05:04:25.531196 140264174335808 run_lib.py:153] step: 566550, training_loss: 9.96533e+01
I1112 05:04:35.921422 140264174335808 run_lib.py:153] step: 566600, training_loss: 1.13843e+02
I1112 05:04:45.655407 140264174335808 run_lib.py:153] step: 566650, training_loss: 1.37503e+02
I1112 05:04:55.654032 140264174335808 run_lib.py:153] step: 566700, training_loss: 1.32299e+02
I1112 05:05:06.303098 140264174335808 run_lib.py:153] step: 566750, training_loss: 1.14755e+02
I1112 05:05:16.226114 140264174335808 run_lib.py:153] step: 566800, training_loss: 9.00577e+01
I1112 05:05:26.367916 140264174335808 run_lib.py:153] step: 566850, training_loss: 1.18673e+02
I1112 05:05:36.383098 140264174335808 run_lib.py:153] step: 566900, training_loss: 1.26092e+02
I1112 05:05:47.046861 140264174335808 run_lib.py:153] step: 566950, training_loss: 1.22521e+02
I1112 05:05:56.778119 140264174335808 run_lib.py:153] step: 567000, training_loss: 1.46689e+02
I1112 05:06:07.065240 140264174335808 run_lib.py:153] step: 567050, training_loss: 1.20136e+02
I1112 05:06:17.581099 140264174335808 run_lib.py:153] step: 567100, training_loss: 1.18064e+02
I1112 05:06:28.347584 140264174335808 run_lib.py:153] step: 567150, training_loss: 1.21494e+02
I1112 05:06:38.859622 140264174335808 run_lib.py:153] step: 567200, training_loss: 1.19451e+02
I1112 05:06:48.605019 140264174335808 run_lib.py:153] step: 567250, training_loss: 1.21067e+02
I1112 05:06:59.053076 140264174335808 run_lib.py:153] step: 567300, training_loss: 1.30684e+02
I1112 05:07:08.963749 140264174335808 run_lib.py:153] step: 567350, training_loss: 1.17071e+02
I1112 05:07:18.758851 140264174335808 run_lib.py:153] step: 567400, training_loss: 1.03899e+02
I1112 05:07:28.885148 140264174335808 run_lib.py:153] step: 567450, training_loss: 1.21729e+02
I1112 05:07:39.119155 140264174335808 run_lib.py:153] step: 567500, training_loss: 1.29297e+02
I1112 05:07:49.489294 140264174335808 run_lib.py:153] step: 567550, training_loss: 1.60267e+02
I1112 05:07:58.987923 140264174335808 run_lib.py:153] step: 567600, training_loss: 1.14018e+02
I1112 05:08:09.151987 140264174335808 run_lib.py:153] step: 567650, training_loss: 1.09578e+02
I1112 05:08:19.573997 140264174335808 run_lib.py:153] step: 567700, training_loss: 1.27888e+02
I1112 05:08:30.539501 140264174335808 run_lib.py:153] step: 567750, training_loss: 1.10881e+02
I1112 05:08:40.159941 140264174335808 run_lib.py:153] step: 567800, training_loss: 1.44990e+02
I1112 05:08:50.486775 140264174335808 run_lib.py:153] step: 567850, training_loss: 1.13667e+02
I1112 05:09:00.967405 140264174335808 run_lib.py:153] step: 567900, training_loss: 1.26795e+02
I1112 05:09:10.635208 140264174335808 run_lib.py:153] step: 567950, training_loss: 1.17805e+02
I1112 05:09:21.107106 140264174335808 run_lib.py:153] step: 568000, training_loss: 1.15118e+02
I1112 05:09:31.014437 140264174335808 run_lib.py:153] step: 568050, training_loss: 1.28961e+02
I1112 05:09:40.828899 140264174335808 run_lib.py:153] step: 568100, training_loss: 1.57616e+02
I1112 05:09:50.421650 140264174335808 run_lib.py:153] step: 568150, training_loss: 1.27474e+02
I1112 05:10:00.376171 140264174335808 run_lib.py:153] step: 568200, training_loss: 1.30483e+02
I1112 05:10:10.259987 140264174335808 run_lib.py:153] step: 568250, training_loss: 1.33655e+02
I1112 05:10:20.646650 140264174335808 run_lib.py:153] step: 568300, training_loss: 1.23041e+02
I1112 05:10:30.901892 140264174335808 run_lib.py:153] step: 568350, training_loss: 1.39849e+02
I1112 05:10:40.622404 140264174335808 run_lib.py:153] step: 568400, training_loss: 1.06708e+02
I1112 05:10:50.127180 140264174335808 run_lib.py:153] step: 568450, training_loss: 1.40393e+02
I1112 05:11:00.056165 140264174335808 run_lib.py:153] step: 568500, training_loss: 1.71339e+02
I1112 05:11:10.645457 140264174335808 run_lib.py:153] step: 568550, training_loss: 1.46554e+02
I1112 05:11:21.007144 140264174335808 run_lib.py:153] step: 568600, training_loss: 1.08270e+02
I1112 05:11:31.522065 140264174335808 run_lib.py:153] step: 568650, training_loss: 1.43566e+02
I1112 05:11:41.551613 140264174335808 run_lib.py:153] step: 568700, training_loss: 1.24646e+02
I1112 05:11:50.794034 140264174335808 run_lib.py:153] step: 568750, training_loss: 1.37430e+02
I1112 05:12:00.414546 140264174335808 run_lib.py:153] step: 568800, training_loss: 1.53394e+02
I1112 05:12:10.832183 140264174335808 run_lib.py:153] step: 568850, training_loss: 1.00729e+02
I1112 05:12:21.216942 140264174335808 run_lib.py:153] step: 568900, training_loss: 9.76584e+01
I1112 05:12:31.398969 140264174335808 run_lib.py:153] step: 568950, training_loss: 9.05591e+01
I1112 05:12:40.872251 140264174335808 run_lib.py:153] step: 569000, training_loss: 1.24726e+02
I1112 05:12:51.257130 140264174335808 run_lib.py:153] step: 569050, training_loss: 1.10651e+02
I1112 05:13:01.832218 140264174335808 run_lib.py:153] step: 569100, training_loss: 1.15900e+02
I1112 05:13:12.582525 140264174335808 run_lib.py:153] step: 569150, training_loss: 1.49831e+02
I1112 05:13:22.552901 140264174335808 run_lib.py:153] step: 569200, training_loss: 1.23008e+02
I1112 05:13:32.517211 140264174335808 run_lib.py:153] step: 569250, training_loss: 1.25574e+02
I1112 05:13:42.563519 140264174335808 run_lib.py:153] step: 569300, training_loss: 1.24757e+02
I1112 05:13:53.138615 140264174335808 run_lib.py:153] step: 569350, training_loss: 1.40916e+02
I1112 05:14:02.803955 140264174335808 run_lib.py:153] step: 569400, training_loss: 1.23101e+02
I1112 05:14:12.266573 140264174335808 run_lib.py:153] step: 569450, training_loss: 1.60531e+02
I1112 05:14:22.493856 140264174335808 run_lib.py:153] step: 569500, training_loss: 1.24624e+02
I1112 05:14:33.217450 140264174335808 run_lib.py:153] step: 569550, training_loss: 1.18296e+02
I1112 05:14:43.124621 140264174335808 run_lib.py:153] step: 569600, training_loss: 1.05797e+02
I1112 05:14:53.167648 140264174335808 run_lib.py:153] step: 569650, training_loss: 1.38843e+02
I1112 05:15:03.428860 140264174335808 run_lib.py:153] step: 569700, training_loss: 1.42243e+02
I1112 05:15:13.848284 140264174335808 run_lib.py:153] step: 569750, training_loss: 1.20499e+02
I1112 05:15:24.402648 140264174335808 run_lib.py:153] step: 569800, training_loss: 1.01920e+02
I1112 05:15:34.228008 140264174335808 run_lib.py:153] step: 569850, training_loss: 1.12159e+02
I1112 05:15:43.944750 140264174335808 run_lib.py:153] step: 569900, training_loss: 9.76260e+01
I1112 05:15:53.559999 140264174335808 run_lib.py:153] step: 569950, training_loss: 1.28568e+02
I1112 05:16:03.344952 140264174335808 run_lib.py:153] step: 570000, training_loss: 1.38235e+02
I1112 05:16:03.907239 140264174335808 run_lib.py:166] step: 570000, eval_loss: 1.44435e+02
I1112 05:16:13.595901 140264174335808 run_lib.py:153] step: 570050, training_loss: 1.30123e+02
I1112 05:16:23.905610 140264174335808 run_lib.py:153] step: 570100, training_loss: 1.21243e+02
I1112 05:16:33.620627 140264174335808 run_lib.py:153] step: 570150, training_loss: 1.16337e+02
I1112 05:16:44.104125 140264174335808 run_lib.py:153] step: 570200, training_loss: 1.16629e+02
I1112 05:16:54.291940 140264174335808 run_lib.py:153] step: 570250, training_loss: 1.44539e+02
I1112 05:17:04.844362 140264174335808 run_lib.py:153] step: 570300, training_loss: 1.22948e+02
I1112 05:17:14.706586 140264174335808 run_lib.py:153] step: 570350, training_loss: 1.22701e+02
I1112 05:17:25.533247 140264174335808 run_lib.py:153] step: 570400, training_loss: 9.70011e+01
I1112 05:17:35.689294 140264174335808 run_lib.py:153] step: 570450, training_loss: 1.12970e+02
I1112 05:17:45.476143 140264174335808 run_lib.py:153] step: 570500, training_loss: 1.31176e+02
I1112 05:17:55.788191 140264174335808 run_lib.py:153] step: 570550, training_loss: 1.43768e+02
I1112 05:18:05.937124 140264174335808 run_lib.py:153] step: 570600, training_loss: 1.31027e+02
I1112 05:18:16.032150 140264174335808 run_lib.py:153] step: 570650, training_loss: 1.34926e+02
I1112 05:18:25.990116 140264174335808 run_lib.py:153] step: 570700, training_loss: 1.18038e+02
I1112 05:18:35.970063 140264174335808 run_lib.py:153] step: 570750, training_loss: 1.31336e+02
I1112 05:18:45.294053 140264174335808 run_lib.py:153] step: 570800, training_loss: 1.22347e+02
I1112 05:18:55.128541 140264174335808 run_lib.py:153] step: 570850, training_loss: 1.45230e+02
I1112 05:19:05.805897 140264174335808 run_lib.py:153] step: 570900, training_loss: 8.75486e+01
I1112 05:19:15.804488 140264174335808 run_lib.py:153] step: 570950, training_loss: 1.59861e+02
I1112 05:19:25.249675 140264174335808 run_lib.py:153] step: 571000, training_loss: 1.24757e+02
I1112 05:19:34.681970 140264174335808 run_lib.py:153] step: 571050, training_loss: 1.43151e+02
I1112 05:19:44.876015 140264174335808 run_lib.py:153] step: 571100, training_loss: 1.46870e+02
I1112 05:19:54.525862 140264174335808 run_lib.py:153] step: 571150, training_loss: 1.38190e+02
I1112 05:20:04.560782 140264174335808 run_lib.py:153] step: 571200, training_loss: 1.25846e+02
I1112 05:20:15.220985 140264174335808 run_lib.py:153] step: 571250, training_loss: 1.01254e+02
I1112 05:20:25.644701 140264174335808 run_lib.py:153] step: 571300, training_loss: 1.11751e+02
I1112 05:20:35.173450 140264174335808 run_lib.py:153] step: 571350, training_loss: 1.36729e+02
I1112 05:20:45.724684 140264174335808 run_lib.py:153] step: 571400, training_loss: 1.42672e+02
I1112 05:20:55.826328 140264174335808 run_lib.py:153] step: 571450, training_loss: 1.27345e+02
I1112 05:21:05.921520 140264174335808 run_lib.py:153] step: 571500, training_loss: 1.49394e+02
I1112 05:21:15.424882 140264174335808 run_lib.py:153] step: 571550, training_loss: 1.44766e+02
I1112 05:21:25.546825 140264174335808 run_lib.py:153] step: 571600, training_loss: 1.14222e+02
I1112 05:21:35.818086 140264174335808 run_lib.py:153] step: 571650, training_loss: 1.39824e+02
I1112 05:21:45.800805 140264174335808 run_lib.py:153] step: 571700, training_loss: 1.33958e+02
I1112 05:21:56.394228 140264174335808 run_lib.py:153] step: 571750, training_loss: 1.28579e+02
I1112 05:22:06.351024 140264174335808 run_lib.py:153] step: 571800, training_loss: 1.63630e+02
I1112 05:22:16.564107 140264174335808 run_lib.py:153] step: 571850, training_loss: 1.35310e+02
I1112 05:22:26.966399 140264174335808 run_lib.py:153] step: 571900, training_loss: 1.19444e+02
I1112 05:22:37.275604 140264174335808 run_lib.py:153] step: 571950, training_loss: 1.15478e+02
I1112 05:22:47.544721 140264174335808 run_lib.py:153] step: 572000, training_loss: 1.21750e+02
I1112 05:22:57.740291 140264174335808 run_lib.py:153] step: 572050, training_loss: 1.50560e+02
I1112 05:23:07.428785 140264174335808 run_lib.py:153] step: 572100, training_loss: 1.36737e+02
I1112 05:23:17.359065 140264174335808 run_lib.py:153] step: 572150, training_loss: 1.62865e+02
I1112 05:23:28.391745 140264174335808 run_lib.py:153] step: 572200, training_loss: 1.52225e+02
I1112 05:23:38.548993 140264174335808 run_lib.py:153] step: 572250, training_loss: 1.45563e+02
I1112 05:23:47.965171 140264174335808 run_lib.py:153] step: 572300, training_loss: 1.28648e+02
I1112 05:23:57.737198 140264174335808 run_lib.py:153] step: 572350, training_loss: 9.59200e+01
I1112 05:24:08.088488 140264174335808 run_lib.py:153] step: 572400, training_loss: 1.59990e+02
I1112 05:24:18.188444 140264174335808 run_lib.py:153] step: 572450, training_loss: 1.32751e+02
I1112 05:24:28.540377 140264174335808 run_lib.py:153] step: 572500, training_loss: 1.34487e+02
I1112 05:24:38.099858 140264174335808 run_lib.py:153] step: 572550, training_loss: 1.20505e+02
I1112 05:24:47.885031 140264174335808 run_lib.py:153] step: 572600, training_loss: 1.33446e+02
I1112 05:24:58.269071 140264174335808 run_lib.py:153] step: 572650, training_loss: 1.30437e+02
I1112 05:25:08.119436 140264174335808 run_lib.py:153] step: 572700, training_loss: 1.22522e+02
I1112 05:25:17.858332 140264174335808 run_lib.py:153] step: 572750, training_loss: 1.12029e+02
I1112 05:25:27.687091 140264174335808 run_lib.py:153] step: 572800, training_loss: 1.28034e+02
I1112 05:25:37.371960 140264174335808 run_lib.py:153] step: 572850, training_loss: 1.51700e+02
I1112 05:25:46.613549 140264174335808 run_lib.py:153] step: 572900, training_loss: 1.00618e+02
I1112 05:25:57.698790 140264174335808 run_lib.py:153] step: 572950, training_loss: 1.35230e+02
I1112 05:26:08.077791 140264174335808 run_lib.py:153] step: 573000, training_loss: 1.13695e+02
I1112 05:26:17.833819 140264174335808 run_lib.py:153] step: 573050, training_loss: 1.14303e+02
I1112 05:26:27.855970 140264174335808 run_lib.py:153] step: 573100, training_loss: 1.16391e+02
I1112 05:26:37.154585 140264174335808 run_lib.py:153] step: 573150, training_loss: 1.31588e+02
I1112 05:26:47.419704 140264174335808 run_lib.py:153] step: 573200, training_loss: 1.16182e+02
I1112 05:26:58.254331 140264174335808 run_lib.py:153] step: 573250, training_loss: 1.23975e+02
I1112 05:27:08.483707 140264174335808 run_lib.py:153] step: 573300, training_loss: 1.43701e+02
I1112 05:27:18.147491 140264174335808 run_lib.py:153] step: 573350, training_loss: 1.35729e+02
I1112 05:27:28.285978 140264174335808 run_lib.py:153] step: 573400, training_loss: 1.13851e+02
I1112 05:27:37.903213 140264174335808 run_lib.py:153] step: 573450, training_loss: 1.59383e+02
I1112 05:27:47.532123 140264174335808 run_lib.py:153] step: 573500, training_loss: 1.23999e+02
I1112 05:27:57.876766 140264174335808 run_lib.py:153] step: 573550, training_loss: 1.18808e+02
I1112 05:28:08.120290 140264174335808 run_lib.py:153] step: 573600, training_loss: 1.19238e+02
I1112 05:28:18.946010 140264174335808 run_lib.py:153] step: 573650, training_loss: 1.06634e+02
I1112 05:28:29.157366 140264174335808 run_lib.py:153] step: 573700, training_loss: 1.10871e+02
I1112 05:28:38.775492 140264174335808 run_lib.py:153] step: 573750, training_loss: 1.24097e+02
I1112 05:28:48.755769 140264174335808 run_lib.py:153] step: 573800, training_loss: 1.37101e+02
I1112 05:28:59.026271 140264174335808 run_lib.py:153] step: 573850, training_loss: 1.46344e+02
I1112 05:29:08.745254 140264174335808 run_lib.py:153] step: 573900, training_loss: 1.10308e+02
I1112 05:29:18.176583 140264174335808 run_lib.py:153] step: 573950, training_loss: 1.36005e+02
I1112 05:29:27.909929 140264174335808 run_lib.py:153] step: 574000, training_loss: 1.43136e+02
I1112 05:29:38.216216 140264174335808 run_lib.py:153] step: 574050, training_loss: 1.37097e+02
I1112 05:29:47.677285 140264174335808 run_lib.py:153] step: 574100, training_loss: 1.47503e+02
I1112 05:29:57.530283 140264174335808 run_lib.py:153] step: 574150, training_loss: 1.16124e+02
I1112 05:30:07.578807 140264174335808 run_lib.py:153] step: 574200, training_loss: 1.22452e+02
I1112 05:30:16.825732 140264174335808 run_lib.py:153] step: 574250, training_loss: 1.32092e+02
I1112 05:30:26.868199 140264174335808 run_lib.py:153] step: 574300, training_loss: 1.39635e+02
I1112 05:30:36.418989 140264174335808 run_lib.py:153] step: 574350, training_loss: 9.90908e+01
I1112 05:30:46.330856 140264174335808 run_lib.py:153] step: 574400, training_loss: 1.26985e+02
I1112 05:30:55.555654 140264174335808 run_lib.py:153] step: 574450, training_loss: 1.07765e+02
I1112 05:31:05.156588 140264174335808 run_lib.py:153] step: 574500, training_loss: 1.25102e+02
I1112 05:31:14.888009 140264174335808 run_lib.py:153] step: 574550, training_loss: 1.26925e+02
I1112 05:31:24.546139 140264174335808 run_lib.py:153] step: 574600, training_loss: 1.33711e+02
I1112 05:31:34.539769 140264174335808 run_lib.py:153] step: 574650, training_loss: 1.15420e+02
I1112 05:31:44.209450 140264174335808 run_lib.py:153] step: 574700, training_loss: 1.16760e+02
I1112 05:31:53.759447 140264174335808 run_lib.py:153] step: 574750, training_loss: 1.44419e+02
I1112 05:32:03.446028 140264174335808 run_lib.py:153] step: 574800, training_loss: 1.54775e+02
I1112 05:32:13.993134 140264174335808 run_lib.py:153] step: 574850, training_loss: 1.31186e+02
I1112 05:32:23.720520 140264174335808 run_lib.py:153] step: 574900, training_loss: 1.25065e+02
I1112 05:32:34.011288 140264174335808 run_lib.py:153] step: 574950, training_loss: 1.29725e+02
I1112 05:32:44.754397 140264174335808 run_lib.py:153] step: 575000, training_loss: 1.11608e+02
I1112 05:32:44.894780 140264174335808 run_lib.py:166] step: 575000, eval_loss: 1.31299e+02
I1112 05:32:54.869718 140264174335808 run_lib.py:153] step: 575050, training_loss: 1.22539e+02
I1112 05:33:05.543798 140264174335808 run_lib.py:153] step: 575100, training_loss: 1.29688e+02
I1112 05:33:15.686805 140264174335808 run_lib.py:153] step: 575150, training_loss: 1.09058e+02
I1112 05:33:25.675964 140264174335808 run_lib.py:153] step: 575200, training_loss: 1.37616e+02
I1112 05:33:35.513236 140264174335808 run_lib.py:153] step: 575250, training_loss: 1.30609e+02
I1112 05:33:45.385819 140264174335808 run_lib.py:153] step: 575300, training_loss: 1.23518e+02
I1112 05:33:55.169266 140264174335808 run_lib.py:153] step: 575350, training_loss: 1.24394e+02
I1112 05:34:04.805876 140264174335808 run_lib.py:153] step: 575400, training_loss: 1.49946e+02
I1112 05:34:14.207470 140264174335808 run_lib.py:153] step: 575450, training_loss: 1.30432e+02
I1112 05:34:24.349521 140264174335808 run_lib.py:153] step: 575500, training_loss: 1.09830e+02
I1112 05:34:34.170731 140264174335808 run_lib.py:153] step: 575550, training_loss: 1.46401e+02
I1112 05:34:43.955727 140264174335808 run_lib.py:153] step: 575600, training_loss: 1.30116e+02
I1112 05:34:54.094073 140264174335808 run_lib.py:153] step: 575650, training_loss: 1.31261e+02
I1112 05:35:03.509957 140264174335808 run_lib.py:153] step: 575700, training_loss: 1.24762e+02
I1112 05:35:13.676171 140264174335808 run_lib.py:153] step: 575750, training_loss: 1.43164e+02
I1112 05:35:23.414209 140264174335808 run_lib.py:153] step: 575800, training_loss: 1.20070e+02
I1112 05:35:33.116912 140264174335808 run_lib.py:153] step: 575850, training_loss: 1.40631e+02
I1112 05:35:42.828706 140264174335808 run_lib.py:153] step: 575900, training_loss: 1.57104e+02
I1112 05:35:52.266026 140264174335808 run_lib.py:153] step: 575950, training_loss: 1.23924e+02
I1112 05:36:03.101258 140264174335808 run_lib.py:153] step: 576000, training_loss: 1.02043e+02
I1112 05:36:12.887824 140264174335808 run_lib.py:153] step: 576050, training_loss: 1.17392e+02
I1112 05:36:22.292062 140264174335808 run_lib.py:153] step: 576100, training_loss: 1.33365e+02
I1112 05:36:32.077261 140264174335808 run_lib.py:153] step: 576150, training_loss: 1.34739e+02
I1112 05:36:41.764983 140264174335808 run_lib.py:153] step: 576200, training_loss: 1.54528e+02
I1112 05:36:51.703922 140264174335808 run_lib.py:153] step: 576250, training_loss: 1.14961e+02
I1112 05:37:01.177216 140264174335808 run_lib.py:153] step: 576300, training_loss: 1.27805e+02
I1112 05:37:10.768854 140264174335808 run_lib.py:153] step: 576350, training_loss: 1.48364e+02
I1112 05:37:20.519156 140264174335808 run_lib.py:153] step: 576400, training_loss: 1.51088e+02
I1112 05:37:29.892765 140264174335808 run_lib.py:153] step: 576450, training_loss: 1.36042e+02
I1112 05:37:39.181996 140264174335808 run_lib.py:153] step: 576500, training_loss: 1.52514e+02
I1112 05:37:48.754997 140264174335808 run_lib.py:153] step: 576550, training_loss: 1.34051e+02
I1112 05:37:58.865243 140264174335808 run_lib.py:153] step: 576600, training_loss: 1.34190e+02
I1112 05:38:08.722737 140264174335808 run_lib.py:153] step: 576650, training_loss: 1.49313e+02
I1112 05:38:18.162644 140264174335808 run_lib.py:153] step: 576700, training_loss: 1.50176e+02
I1112 05:38:28.479250 140264174335808 run_lib.py:153] step: 576750, training_loss: 1.35376e+02
I1112 05:38:38.522995 140264174335808 run_lib.py:153] step: 576800, training_loss: 1.33426e+02
I1112 05:38:49.021932 140264174335808 run_lib.py:153] step: 576850, training_loss: 1.07486e+02
I1112 05:38:58.611344 140264174335808 run_lib.py:153] step: 576900, training_loss: 1.36890e+02
I1112 05:39:09.191954 140264174335808 run_lib.py:153] step: 576950, training_loss: 1.61783e+02
I1112 05:39:19.441352 140264174335808 run_lib.py:153] step: 577000, training_loss: 1.26873e+02
I1112 05:39:29.323788 140264174335808 run_lib.py:153] step: 577050, training_loss: 1.28061e+02
I1112 05:39:38.555825 140264174335808 run_lib.py:153] step: 577100, training_loss: 1.42216e+02
I1112 05:39:48.099383 140264174335808 run_lib.py:153] step: 577150, training_loss: 1.11546e+02
I1112 05:39:58.234699 140264174335808 run_lib.py:153] step: 577200, training_loss: 1.55034e+02
I1112 05:40:08.018584 140264174335808 run_lib.py:153] step: 577250, training_loss: 1.44990e+02
I1112 05:40:17.571482 140264174335808 run_lib.py:153] step: 577300, training_loss: 1.10005e+02
I1112 05:40:27.217798 140264174335808 run_lib.py:153] step: 577350, training_loss: 1.50462e+02
I1112 05:40:36.546986 140264174335808 run_lib.py:153] step: 577400, training_loss: 1.39160e+02
I1112 05:40:46.676323 140264174335808 run_lib.py:153] step: 577450, training_loss: 1.50665e+02
I1112 05:40:56.867383 140264174335808 run_lib.py:153] step: 577500, training_loss: 1.04258e+02
I1112 05:41:06.374006 140264174335808 run_lib.py:153] step: 577550, training_loss: 1.35937e+02
I1112 05:41:15.971828 140264174335808 run_lib.py:153] step: 577600, training_loss: 1.27869e+02
I1112 05:41:25.633308 140264174335808 run_lib.py:153] step: 577650, training_loss: 1.41654e+02
I1112 05:41:34.988210 140264174335808 run_lib.py:153] step: 577700, training_loss: 1.27387e+02
I1112 05:41:45.206940 140264174335808 run_lib.py:153] step: 577750, training_loss: 1.14361e+02
I1112 05:41:54.637703 140264174335808 run_lib.py:153] step: 577800, training_loss: 1.05219e+02
I1112 05:42:04.317635 140264174335808 run_lib.py:153] step: 577850, training_loss: 1.31735e+02
I1112 05:42:13.907027 140264174335808 run_lib.py:153] step: 577900, training_loss: 1.07381e+02
I1112 05:42:23.418329 140264174335808 run_lib.py:153] step: 577950, training_loss: 1.23238e+02
I1112 05:42:33.175991 140264174335808 run_lib.py:153] step: 578000, training_loss: 1.10360e+02
I1112 05:42:43.032039 140264174335808 run_lib.py:153] step: 578050, training_loss: 1.02925e+02
I1112 05:42:52.281832 140264174335808 run_lib.py:153] step: 578100, training_loss: 1.38400e+02
I1112 05:43:01.565893 140264174335808 run_lib.py:153] step: 578150, training_loss: 1.30380e+02
I1112 05:43:11.209398 140264174335808 run_lib.py:153] step: 578200, training_loss: 1.39677e+02
I1112 05:43:21.557728 140264174335808 run_lib.py:153] step: 578250, training_loss: 1.29785e+02
I1112 05:43:30.733432 140264174335808 run_lib.py:153] step: 578300, training_loss: 1.37141e+02
I1112 05:43:41.686715 140264174335808 run_lib.py:153] step: 578350, training_loss: 1.44588e+02
I1112 05:43:51.852649 140264174335808 run_lib.py:153] step: 578400, training_loss: 1.36893e+02
I1112 05:44:01.424125 140264174335808 run_lib.py:153] step: 578450, training_loss: 1.21063e+02
I1112 05:44:11.048730 140264174335808 run_lib.py:153] step: 578500, training_loss: 1.33555e+02
I1112 05:44:21.132874 140264174335808 run_lib.py:153] step: 578550, training_loss: 1.45114e+02
I1112 05:44:30.889755 140264174335808 run_lib.py:153] step: 578600, training_loss: 1.13754e+02
I1112 05:44:40.639871 140264174335808 run_lib.py:153] step: 578650, training_loss: 1.20650e+02
I1112 05:44:50.806422 140264174335808 run_lib.py:153] step: 578700, training_loss: 1.21233e+02
I1112 05:45:00.967585 140264174335808 run_lib.py:153] step: 578750, training_loss: 1.24209e+02
I1112 05:45:10.675035 140264174335808 run_lib.py:153] step: 578800, training_loss: 1.17465e+02
I1112 05:45:20.497115 140264174335808 run_lib.py:153] step: 578850, training_loss: 1.22888e+02
I1112 05:45:30.638011 140264174335808 run_lib.py:153] step: 578900, training_loss: 1.45839e+02
I1112 05:45:40.843097 140264174335808 run_lib.py:153] step: 578950, training_loss: 1.34912e+02
I1112 05:45:51.047385 140264174335808 run_lib.py:153] step: 579000, training_loss: 1.06108e+02
I1112 05:46:00.907996 140264174335808 run_lib.py:153] step: 579050, training_loss: 9.71334e+01
I1112 05:46:10.630548 140264174335808 run_lib.py:153] step: 579100, training_loss: 1.31790e+02
I1112 05:46:20.215008 140264174335808 run_lib.py:153] step: 579150, training_loss: 1.28303e+02
I1112 05:46:30.249780 140264174335808 run_lib.py:153] step: 579200, training_loss: 1.22478e+02
I1112 05:46:39.782131 140264174335808 run_lib.py:153] step: 579250, training_loss: 8.81674e+01
I1112 05:46:49.753927 140264174335808 run_lib.py:153] step: 579300, training_loss: 1.50127e+02
I1112 05:46:59.725672 140264174335808 run_lib.py:153] step: 579350, training_loss: 1.16111e+02
I1112 05:47:09.504511 140264174335808 run_lib.py:153] step: 579400, training_loss: 1.20402e+02
I1112 05:47:19.335069 140264174335808 run_lib.py:153] step: 579450, training_loss: 1.17300e+02
I1112 05:47:29.013927 140264174335808 run_lib.py:153] step: 579500, training_loss: 1.55773e+02
I1112 05:47:39.333051 140264174335808 run_lib.py:153] step: 579550, training_loss: 1.23101e+02
I1112 05:47:50.036269 140264174335808 run_lib.py:153] step: 579600, training_loss: 1.33578e+02
I1112 05:48:00.303299 140264174335808 run_lib.py:153] step: 579650, training_loss: 9.35340e+01
I1112 05:48:10.483751 140264174335808 run_lib.py:153] step: 579700, training_loss: 1.35038e+02
I1112 05:48:20.474544 140264174335808 run_lib.py:153] step: 579750, training_loss: 1.26558e+02
I1112 05:48:30.291037 140264174335808 run_lib.py:153] step: 579800, training_loss: 1.16306e+02
I1112 05:48:40.752380 140264174335808 run_lib.py:153] step: 579850, training_loss: 1.40020e+02
I1112 05:48:51.149375 140264174335808 run_lib.py:153] step: 579900, training_loss: 1.31056e+02
I1112 05:49:01.088354 140264174335808 run_lib.py:153] step: 579950, training_loss: 1.39005e+02
I1112 05:49:10.253932 140264174335808 run_lib.py:153] step: 580000, training_loss: 1.35193e+02
I1112 05:49:10.834807 140264174335808 run_lib.py:166] step: 580000, eval_loss: 1.24361e+02
I1112 05:49:20.736006 140264174335808 run_lib.py:153] step: 580050, training_loss: 1.46587e+02
I1112 05:49:30.449487 140264174335808 run_lib.py:153] step: 580100, training_loss: 1.02150e+02
I1112 05:49:40.894233 140264174335808 run_lib.py:153] step: 580150, training_loss: 1.29079e+02
I1112 05:49:51.037290 140264174335808 run_lib.py:153] step: 580200, training_loss: 1.09733e+02
I1112 05:50:01.081018 140264174335808 run_lib.py:153] step: 580250, training_loss: 1.18440e+02
I1112 05:50:11.752871 140264174335808 run_lib.py:153] step: 580300, training_loss: 1.49407e+02
I1112 05:50:21.943945 140264174335808 run_lib.py:153] step: 580350, training_loss: 1.10108e+02
I1112 05:50:32.185405 140264174335808 run_lib.py:153] step: 580400, training_loss: 1.06795e+02
I1112 05:50:42.211357 140264174335808 run_lib.py:153] step: 580450, training_loss: 9.66156e+01
I1112 05:50:51.840319 140264174335808 run_lib.py:153] step: 580500, training_loss: 1.31395e+02
I1112 05:51:01.928413 140264174335808 run_lib.py:153] step: 580550, training_loss: 1.04560e+02
I1112 05:51:11.121882 140264174335808 run_lib.py:153] step: 580600, training_loss: 1.64799e+02
I1112 05:51:21.616091 140264174335808 run_lib.py:153] step: 580650, training_loss: 1.41841e+02
I1112 05:51:31.617856 140264174335808 run_lib.py:153] step: 580700, training_loss: 1.13216e+02
I1112 05:51:41.788499 140264174335808 run_lib.py:153] step: 580750, training_loss: 1.32889e+02
I1112 05:51:52.286166 140264174335808 run_lib.py:153] step: 580800, training_loss: 1.39980e+02
I1112 05:52:01.767624 140264174335808 run_lib.py:153] step: 580850, training_loss: 1.22143e+02
I1112 05:52:12.151336 140264174335808 run_lib.py:153] step: 580900, training_loss: 1.29466e+02
I1112 05:52:22.001946 140264174335808 run_lib.py:153] step: 580950, training_loss: 1.28681e+02
I1112 05:52:32.361107 140264174335808 run_lib.py:153] step: 581000, training_loss: 1.34793e+02
I1112 05:52:41.865572 140264174335808 run_lib.py:153] step: 581050, training_loss: 1.37608e+02
I1112 05:52:51.790914 140264174335808 run_lib.py:153] step: 581100, training_loss: 1.10539e+02
I1112 05:53:01.957698 140264174335808 run_lib.py:153] step: 581150, training_loss: 1.23711e+02
I1112 05:53:11.594803 140264174335808 run_lib.py:153] step: 581200, training_loss: 1.43328e+02
I1112 05:53:21.821913 140264174335808 run_lib.py:153] step: 581250, training_loss: 1.38565e+02
I1112 05:53:32.296622 140264174335808 run_lib.py:153] step: 581300, training_loss: 1.30953e+02
I1112 05:53:42.903319 140264174335808 run_lib.py:153] step: 581350, training_loss: 1.44888e+02
I1112 05:53:53.461068 140264174335808 run_lib.py:153] step: 581400, training_loss: 1.00108e+02
I1112 05:54:04.592667 140264174335808 run_lib.py:153] step: 581450, training_loss: 1.39026e+02
I1112 05:54:14.878164 140264174335808 run_lib.py:153] step: 581500, training_loss: 1.43299e+02
I1112 05:54:25.156835 140264174335808 run_lib.py:153] step: 581550, training_loss: 1.37782e+02
I1112 05:54:35.382842 140264174335808 run_lib.py:153] step: 581600, training_loss: 1.18538e+02
I1112 05:54:45.128736 140264174335808 run_lib.py:153] step: 581650, training_loss: 1.20057e+02
I1112 05:54:55.947776 140264174335808 run_lib.py:153] step: 581700, training_loss: 1.24258e+02
I1112 05:55:06.433317 140264174335808 run_lib.py:153] step: 581750, training_loss: 1.29331e+02
I1112 05:55:16.522518 140264174335808 run_lib.py:153] step: 581800, training_loss: 1.59087e+02
I1112 05:55:27.095400 140264174335808 run_lib.py:153] step: 581850, training_loss: 1.01819e+02
I1112 05:55:36.984277 140264174335808 run_lib.py:153] step: 581900, training_loss: 1.36789e+02
I1112 05:55:46.823027 140264174335808 run_lib.py:153] step: 581950, training_loss: 1.49266e+02
I1112 05:55:57.057924 140264174335808 run_lib.py:153] step: 582000, training_loss: 1.03219e+02
I1112 05:56:06.640188 140264174335808 run_lib.py:153] step: 582050, training_loss: 1.29898e+02
I1112 05:56:17.584108 140264174335808 run_lib.py:153] step: 582100, training_loss: 1.09144e+02
I1112 05:56:28.695744 140264174335808 run_lib.py:153] step: 582150, training_loss: 1.39440e+02
I1112 05:56:39.580895 140264174335808 run_lib.py:153] step: 582200, training_loss: 9.46534e+01
I1112 05:56:49.954392 140264174335808 run_lib.py:153] step: 582250, training_loss: 9.69789e+01
I1112 05:56:59.930634 140264174335808 run_lib.py:153] step: 582300, training_loss: 1.43793e+02
I1112 05:57:09.420367 140264174335808 run_lib.py:153] step: 582350, training_loss: 1.35435e+02
I1112 05:57:20.161765 140264174335808 run_lib.py:153] step: 582400, training_loss: 1.59066e+02
I1112 05:57:30.659185 140264174335808 run_lib.py:153] step: 582450, training_loss: 1.30032e+02
I1112 05:57:40.639037 140264174335808 run_lib.py:153] step: 582500, training_loss: 1.31960e+02
I1112 05:57:50.814547 140264174335808 run_lib.py:153] step: 582550, training_loss: 8.92798e+01
I1112 05:58:00.849348 140264174335808 run_lib.py:153] step: 582600, training_loss: 1.07360e+02
I1112 05:58:11.678332 140264174335808 run_lib.py:153] step: 582650, training_loss: 1.32033e+02
I1112 05:58:21.639615 140264174335808 run_lib.py:153] step: 582700, training_loss: 1.03711e+02
I1112 05:58:31.218947 140264174335808 run_lib.py:153] step: 582750, training_loss: 1.72760e+02
I1112 05:58:41.101157 140264174335808 run_lib.py:153] step: 582800, training_loss: 1.25605e+02
I1112 05:58:51.260196 140264174335808 run_lib.py:153] step: 582850, training_loss: 1.19201e+02
I1112 05:59:01.792779 140264174335808 run_lib.py:153] step: 582900, training_loss: 1.25469e+02
I1112 05:59:12.173310 140264174335808 run_lib.py:153] step: 582950, training_loss: 1.23337e+02
I1112 05:59:22.240280 140264174335808 run_lib.py:153] step: 583000, training_loss: 1.19930e+02
I1112 05:59:31.604790 140264174335808 run_lib.py:153] step: 583050, training_loss: 1.18677e+02
I1112 05:59:41.659638 140264174335808 run_lib.py:153] step: 583100, training_loss: 1.29448e+02
I1112 05:59:52.451676 140264174335808 run_lib.py:153] step: 583150, training_loss: 1.56596e+02
I1112 06:00:02.718629 140264174335808 run_lib.py:153] step: 583200, training_loss: 1.25698e+02
I1112 06:00:12.637178 140264174335808 run_lib.py:153] step: 583250, training_loss: 9.58353e+01
I1112 06:00:23.036739 140264174335808 run_lib.py:153] step: 583300, training_loss: 1.22152e+02
I1112 06:00:33.324343 140264174335808 run_lib.py:153] step: 583350, training_loss: 1.43315e+02
I1112 06:00:43.477293 140264174335808 run_lib.py:153] step: 583400, training_loss: 1.18552e+02
I1112 06:00:53.704753 140264174335808 run_lib.py:153] step: 583450, training_loss: 1.11445e+02
I1112 06:01:03.736276 140264174335808 run_lib.py:153] step: 583500, training_loss: 1.41276e+02
I1112 06:01:14.636171 140264174335808 run_lib.py:153] step: 583550, training_loss: 1.48635e+02
I1112 06:01:25.610736 140264174335808 run_lib.py:153] step: 583600, training_loss: 1.39463e+02
I1112 06:01:36.341549 140264174335808 run_lib.py:153] step: 583650, training_loss: 1.63345e+02
I1112 06:01:46.246154 140264174335808 run_lib.py:153] step: 583700, training_loss: 1.35673e+02
I1112 06:01:56.112395 140264174335808 run_lib.py:153] step: 583750, training_loss: 1.29823e+02
I1112 06:02:06.764923 140264174335808 run_lib.py:153] step: 583800, training_loss: 1.40051e+02
I1112 06:02:16.914259 140264174335808 run_lib.py:153] step: 583850, training_loss: 1.34831e+02
I1112 06:02:26.365507 140264174335808 run_lib.py:153] step: 583900, training_loss: 1.02707e+02
I1112 06:02:36.415647 140264174335808 run_lib.py:153] step: 583950, training_loss: 1.14361e+02
I1112 06:02:46.344165 140264174335808 run_lib.py:153] step: 584000, training_loss: 1.00261e+02
I1112 06:02:55.717187 140264174335808 run_lib.py:153] step: 584050, training_loss: 1.06651e+02
I1112 06:03:05.611695 140264174335808 run_lib.py:153] step: 584100, training_loss: 1.28359e+02
I1112 06:03:16.047104 140264174335808 run_lib.py:153] step: 584150, training_loss: 1.14104e+02
I1112 06:03:25.384663 140264174335808 run_lib.py:153] step: 584200, training_loss: 1.18697e+02
I1112 06:03:35.504159 140264174335808 run_lib.py:153] step: 584250, training_loss: 1.16934e+02
I1112 06:03:45.364202 140264174335808 run_lib.py:153] step: 584300, training_loss: 1.18823e+02
I1112 06:03:55.413148 140264174335808 run_lib.py:153] step: 584350, training_loss: 1.28994e+02
I1112 06:04:04.804116 140264174335808 run_lib.py:153] step: 584400, training_loss: 1.13910e+02
I1112 06:04:15.228214 140264174335808 run_lib.py:153] step: 584450, training_loss: 1.30452e+02
I1112 06:04:25.283243 140264174335808 run_lib.py:153] step: 584500, training_loss: 1.22242e+02
I1112 06:04:35.332260 140264174335808 run_lib.py:153] step: 584550, training_loss: 1.44937e+02
I1112 06:04:45.606671 140264174335808 run_lib.py:153] step: 584600, training_loss: 1.10787e+02
I1112 06:04:55.189624 140264174335808 run_lib.py:153] step: 584650, training_loss: 1.10369e+02
I1112 06:05:05.466719 140264174335808 run_lib.py:153] step: 584700, training_loss: 1.39882e+02
I1112 06:05:15.511423 140264174335808 run_lib.py:153] step: 584750, training_loss: 1.34879e+02
I1112 06:05:25.231609 140264174335808 run_lib.py:153] step: 584800, training_loss: 1.20620e+02
I1112 06:05:35.203826 140264174335808 run_lib.py:153] step: 584850, training_loss: 1.12588e+02
I1112 06:05:44.854445 140264174335808 run_lib.py:153] step: 584900, training_loss: 1.41958e+02
I1112 06:05:54.649213 140264174335808 run_lib.py:153] step: 584950, training_loss: 1.11000e+02
I1112 06:06:05.303463 140264174335808 run_lib.py:153] step: 585000, training_loss: 1.26184e+02
I1112 06:06:05.442408 140264174335808 run_lib.py:166] step: 585000, eval_loss: 1.60630e+02
I1112 06:06:15.792396 140264174335808 run_lib.py:153] step: 585050, training_loss: 1.27223e+02
I1112 06:06:26.179510 140264174335808 run_lib.py:153] step: 585100, training_loss: 1.22427e+02
I1112 06:06:36.087829 140264174335808 run_lib.py:153] step: 585150, training_loss: 1.21314e+02
I1112 06:06:46.155504 140264174335808 run_lib.py:153] step: 585200, training_loss: 1.27647e+02
I1112 06:06:56.734102 140264174335808 run_lib.py:153] step: 585250, training_loss: 1.47414e+02
I1112 06:07:06.748316 140264174335808 run_lib.py:153] step: 585300, training_loss: 1.16205e+02
I1112 06:07:17.039575 140264174335808 run_lib.py:153] step: 585350, training_loss: 1.43222e+02
I1112 06:07:27.415528 140264174335808 run_lib.py:153] step: 585400, training_loss: 1.14170e+02
I1112 06:07:37.679090 140264174335808 run_lib.py:153] step: 585450, training_loss: 9.52253e+01
I1112 06:07:47.153517 140264174335808 run_lib.py:153] step: 585500, training_loss: 1.20015e+02
I1112 06:07:56.987151 140264174335808 run_lib.py:153] step: 585550, training_loss: 1.25512e+02
I1112 06:08:06.912926 140264174335808 run_lib.py:153] step: 585600, training_loss: 1.11587e+02
I1112 06:08:17.445357 140264174335808 run_lib.py:153] step: 585650, training_loss: 1.36973e+02
I1112 06:08:27.526741 140264174335808 run_lib.py:153] step: 585700, training_loss: 1.29338e+02
I1112 06:08:37.235841 140264174335808 run_lib.py:153] step: 585750, training_loss: 1.11930e+02
I1112 06:08:48.253344 140264174335808 run_lib.py:153] step: 585800, training_loss: 1.09952e+02
I1112 06:08:57.956621 140264174335808 run_lib.py:153] step: 585850, training_loss: 1.31815e+02
I1112 06:09:07.619848 140264174335808 run_lib.py:153] step: 585900, training_loss: 1.27537e+02
I1112 06:09:17.029175 140264174335808 run_lib.py:153] step: 585950, training_loss: 1.19826e+02
I1112 06:09:26.910058 140264174335808 run_lib.py:153] step: 586000, training_loss: 1.34607e+02
I1112 06:09:36.194957 140264174335808 run_lib.py:153] step: 586050, training_loss: 1.14634e+02
I1112 06:09:45.859141 140264174335808 run_lib.py:153] step: 586100, training_loss: 1.25925e+02
I1112 06:09:55.375107 140264174335808 run_lib.py:153] step: 586150, training_loss: 1.51158e+02
I1112 06:10:05.365905 140264174335808 run_lib.py:153] step: 586200, training_loss: 1.24482e+02
I1112 06:10:15.850699 140264174335808 run_lib.py:153] step: 586250, training_loss: 1.09678e+02
I1112 06:10:25.971969 140264174335808 run_lib.py:153] step: 586300, training_loss: 1.08717e+02
I1112 06:10:36.252897 140264174335808 run_lib.py:153] step: 586350, training_loss: 1.42314e+02
I1112 06:10:46.446197 140264174335808 run_lib.py:153] step: 586400, training_loss: 1.29937e+02
I1112 06:10:56.766086 140264174335808 run_lib.py:153] step: 586450, training_loss: 1.10970e+02
I1112 06:11:07.053537 140264174335808 run_lib.py:153] step: 586500, training_loss: 1.49086e+02
I1112 06:11:17.556734 140264174335808 run_lib.py:153] step: 586550, training_loss: 1.44415e+02
I1112 06:11:27.560934 140264174335808 run_lib.py:153] step: 586600, training_loss: 1.06989e+02
I1112 06:11:37.649201 140264174335808 run_lib.py:153] step: 586650, training_loss: 1.47557e+02
I1112 06:11:47.848889 140264174335808 run_lib.py:153] step: 586700, training_loss: 9.40965e+01
I1112 06:11:58.427280 140264174335808 run_lib.py:153] step: 586750, training_loss: 1.15037e+02
I1112 06:12:08.597758 140264174335808 run_lib.py:153] step: 586800, training_loss: 1.42383e+02
I1112 06:12:19.138594 140264174335808 run_lib.py:153] step: 586850, training_loss: 1.22483e+02
I1112 06:12:29.366990 140264174335808 run_lib.py:153] step: 586900, training_loss: 1.54102e+02
I1112 06:12:40.027483 140264174335808 run_lib.py:153] step: 586950, training_loss: 1.35192e+02
I1112 06:12:50.503885 140264174335808 run_lib.py:153] step: 587000, training_loss: 1.13270e+02
I1112 06:13:01.192415 140264174335808 run_lib.py:153] step: 587050, training_loss: 1.31765e+02
I1112 06:13:11.619876 140264174335808 run_lib.py:153] step: 587100, training_loss: 1.56184e+02
I1112 06:13:22.228509 140264174335808 run_lib.py:153] step: 587150, training_loss: 1.45530e+02
I1112 06:13:32.240719 140264174335808 run_lib.py:153] step: 587200, training_loss: 1.44118e+02
I1112 06:13:41.807927 140264174335808 run_lib.py:153] step: 587250, training_loss: 1.13453e+02
I1112 06:13:51.618147 140264174335808 run_lib.py:153] step: 587300, training_loss: 1.68100e+02
I1112 06:14:01.414481 140264174335808 run_lib.py:153] step: 587350, training_loss: 1.34495e+02
I1112 06:14:10.944250 140264174335808 run_lib.py:153] step: 587400, training_loss: 1.12045e+02
I1112 06:14:20.591661 140264174335808 run_lib.py:153] step: 587450, training_loss: 1.13235e+02
I1112 06:14:30.965265 140264174335808 run_lib.py:153] step: 587500, training_loss: 1.39769e+02
I1112 06:14:40.768855 140264174335808 run_lib.py:153] step: 587550, training_loss: 1.24611e+02
I1112 06:14:51.062868 140264174335808 run_lib.py:153] step: 587600, training_loss: 1.57378e+02
I1112 06:15:00.839516 140264174335808 run_lib.py:153] step: 587650, training_loss: 1.45863e+02
I1112 06:15:10.526347 140264174335808 run_lib.py:153] step: 587700, training_loss: 1.20637e+02
I1112 06:15:20.983693 140264174335808 run_lib.py:153] step: 587750, training_loss: 1.28532e+02
I1112 06:15:31.050186 140264174335808 run_lib.py:153] step: 587800, training_loss: 1.24767e+02
I1112 06:15:41.801813 140264174335808 run_lib.py:153] step: 587850, training_loss: 1.29549e+02
I1112 06:15:51.293501 140264174335808 run_lib.py:153] step: 587900, training_loss: 1.18360e+02
I1112 06:16:01.719128 140264174335808 run_lib.py:153] step: 587950, training_loss: 1.03366e+02
I1112 06:16:11.494651 140264174335808 run_lib.py:153] step: 588000, training_loss: 1.36639e+02
I1112 06:16:21.692742 140264174335808 run_lib.py:153] step: 588050, training_loss: 1.54034e+02
I1112 06:16:31.767051 140264174335808 run_lib.py:153] step: 588100, training_loss: 1.13962e+02
I1112 06:16:42.697987 140264174335808 run_lib.py:153] step: 588150, training_loss: 1.50510e+02
I1112 06:16:52.983420 140264174335808 run_lib.py:153] step: 588200, training_loss: 1.38553e+02
I1112 06:17:03.798134 140264174335808 run_lib.py:153] step: 588250, training_loss: 1.14818e+02
I1112 06:17:13.769388 140264174335808 run_lib.py:153] step: 588300, training_loss: 1.22870e+02
I1112 06:17:24.636347 140264174335808 run_lib.py:153] step: 588350, training_loss: 1.16260e+02
I1112 06:17:35.097367 140264174335808 run_lib.py:153] step: 588400, training_loss: 1.14690e+02
I1112 06:17:45.274989 140264174335808 run_lib.py:153] step: 588450, training_loss: 1.09237e+02
I1112 06:17:55.492370 140264174335808 run_lib.py:153] step: 588500, training_loss: 1.25379e+02
I1112 06:18:06.119260 140264174335808 run_lib.py:153] step: 588550, training_loss: 1.08378e+02
I1112 06:18:16.139029 140264174335808 run_lib.py:153] step: 588600, training_loss: 1.02323e+02
I1112 06:18:26.338023 140264174335808 run_lib.py:153] step: 588650, training_loss: 1.05396e+02
I1112 06:18:37.445923 140264174335808 run_lib.py:153] step: 588700, training_loss: 1.50630e+02
I1112 06:18:48.432176 140264174335808 run_lib.py:153] step: 588750, training_loss: 1.23513e+02
I1112 06:18:58.049058 140264174335808 run_lib.py:153] step: 588800, training_loss: 1.33620e+02
I1112 06:19:08.283940 140264174335808 run_lib.py:153] step: 588850, training_loss: 1.43322e+02
I1112 06:19:18.324554 140264174335808 run_lib.py:153] step: 588900, training_loss: 1.39206e+02
I1112 06:19:27.937345 140264174335808 run_lib.py:153] step: 588950, training_loss: 9.68636e+01
I1112 06:19:38.581651 140264174335808 run_lib.py:153] step: 589000, training_loss: 1.22810e+02
I1112 06:19:48.414852 140264174335808 run_lib.py:153] step: 589050, training_loss: 1.15639e+02
I1112 06:19:58.685914 140264174335808 run_lib.py:153] step: 589100, training_loss: 9.63129e+01
I1112 06:20:09.467562 140264174335808 run_lib.py:153] step: 589150, training_loss: 1.34554e+02
I1112 06:20:19.826219 140264174335808 run_lib.py:153] step: 589200, training_loss: 1.44054e+02
I1112 06:20:30.115186 140264174335808 run_lib.py:153] step: 589250, training_loss: 1.25276e+02
I1112 06:20:40.306213 140264174335808 run_lib.py:153] step: 589300, training_loss: 1.43620e+02
I1112 06:20:50.900749 140264174335808 run_lib.py:153] step: 589350, training_loss: 1.22112e+02
I1112 06:21:01.191011 140264174335808 run_lib.py:153] step: 589400, training_loss: 1.14110e+02
I1112 06:21:10.725349 140264174335808 run_lib.py:153] step: 589450, training_loss: 1.15216e+02
I1112 06:21:20.875281 140264174335808 run_lib.py:153] step: 589500, training_loss: 1.23622e+02
I1112 06:21:30.664477 140264174335808 run_lib.py:153] step: 589550, training_loss: 1.26431e+02
I1112 06:21:40.862825 140264174335808 run_lib.py:153] step: 589600, training_loss: 1.54138e+02
I1112 06:21:51.153939 140264174335808 run_lib.py:153] step: 589650, training_loss: 1.10888e+02
I1112 06:22:01.722027 140264174335808 run_lib.py:153] step: 589700, training_loss: 1.51519e+02
I1112 06:22:12.170250 140264174335808 run_lib.py:153] step: 589750, training_loss: 1.18538e+02
I1112 06:22:22.041540 140264174335808 run_lib.py:153] step: 589800, training_loss: 1.29069e+02
I1112 06:22:31.647038 140264174335808 run_lib.py:153] step: 589850, training_loss: 1.41633e+02
I1112 06:22:42.128919 140264174335808 run_lib.py:153] step: 589900, training_loss: 1.30490e+02
I1112 06:22:52.391328 140264174335808 run_lib.py:153] step: 589950, training_loss: 1.31088e+02
I1112 06:23:02.093573 140264174335808 run_lib.py:153] step: 590000, training_loss: 1.44577e+02
I1112 06:23:02.681046 140264174335808 run_lib.py:166] step: 590000, eval_loss: 1.03671e+02
I1112 06:23:12.629271 140264174335808 run_lib.py:153] step: 590050, training_loss: 1.39650e+02
I1112 06:23:22.473365 140264174335808 run_lib.py:153] step: 590100, training_loss: 1.25042e+02
I1112 06:23:31.832558 140264174335808 run_lib.py:153] step: 590150, training_loss: 1.43642e+02
I1112 06:23:41.371153 140264174335808 run_lib.py:153] step: 590200, training_loss: 9.30134e+01
I1112 06:23:51.050644 140264174335808 run_lib.py:153] step: 590250, training_loss: 1.15329e+02
I1112 06:24:00.939534 140264174335808 run_lib.py:153] step: 590300, training_loss: 1.15342e+02
I1112 06:24:10.519841 140264174335808 run_lib.py:153] step: 590350, training_loss: 1.32587e+02
I1112 06:24:20.985013 140264174335808 run_lib.py:153] step: 590400, training_loss: 1.06733e+02
I1112 06:24:30.980388 140264174335808 run_lib.py:153] step: 590450, training_loss: 1.54335e+02
I1112 06:24:40.746447 140264174335808 run_lib.py:153] step: 590500, training_loss: 1.16337e+02
I1112 06:24:50.248026 140264174335808 run_lib.py:153] step: 590550, training_loss: 1.19842e+02
I1112 06:24:59.771545 140264174335808 run_lib.py:153] step: 590600, training_loss: 1.08246e+02
I1112 06:25:09.814947 140264174335808 run_lib.py:153] step: 590650, training_loss: 1.08689e+02
I1112 06:25:19.710848 140264174335808 run_lib.py:153] step: 590700, training_loss: 1.22861e+02
I1112 06:25:29.367182 140264174335808 run_lib.py:153] step: 590750, training_loss: 1.59635e+02
I1112 06:25:38.683606 140264174335808 run_lib.py:153] step: 590800, training_loss: 1.35667e+02
I1112 06:25:48.171525 140264174335808 run_lib.py:153] step: 590850, training_loss: 1.08240e+02
I1112 06:25:57.912086 140264174335808 run_lib.py:153] step: 590900, training_loss: 1.42783e+02
I1112 06:26:07.847995 140264174335808 run_lib.py:153] step: 590950, training_loss: 1.22127e+02
I1112 06:26:17.995030 140264174335808 run_lib.py:153] step: 591000, training_loss: 1.04336e+02
I1112 06:26:27.973831 140264174335808 run_lib.py:153] step: 591050, training_loss: 1.34032e+02
I1112 06:26:37.724024 140264174335808 run_lib.py:153] step: 591100, training_loss: 1.47722e+02
I1112 06:26:48.049859 140264174335808 run_lib.py:153] step: 591150, training_loss: 1.39146e+02
I1112 06:26:58.498825 140264174335808 run_lib.py:153] step: 591200, training_loss: 1.42200e+02
I1112 06:27:08.889039 140264174335808 run_lib.py:153] step: 591250, training_loss: 1.49718e+02
I1112 06:27:19.080478 140264174335808 run_lib.py:153] step: 591300, training_loss: 1.61761e+02
I1112 06:27:28.719631 140264174335808 run_lib.py:153] step: 591350, training_loss: 1.18762e+02
I1112 06:27:38.967161 140264174335808 run_lib.py:153] step: 591400, training_loss: 1.26298e+02
I1112 06:27:49.338972 140264174335808 run_lib.py:153] step: 591450, training_loss: 1.19994e+02
I1112 06:27:59.292567 140264174335808 run_lib.py:153] step: 591500, training_loss: 1.36989e+02
I1112 06:28:08.941982 140264174335808 run_lib.py:153] step: 591550, training_loss: 9.44074e+01
I1112 06:28:18.632171 140264174335808 run_lib.py:153] step: 591600, training_loss: 1.31434e+02
I1112 06:28:28.279728 140264174335808 run_lib.py:153] step: 591650, training_loss: 1.47931e+02
I1112 06:28:38.295830 140264174335808 run_lib.py:153] step: 591700, training_loss: 1.45603e+02
I1112 06:28:48.805305 140264174335808 run_lib.py:153] step: 591750, training_loss: 1.08748e+02
I1112 06:28:59.182430 140264174335808 run_lib.py:153] step: 591800, training_loss: 1.09155e+02
I1112 06:29:10.072466 140264174335808 run_lib.py:153] step: 591850, training_loss: 1.26695e+02
I1112 06:29:21.042751 140264174335808 run_lib.py:153] step: 591900, training_loss: 1.35707e+02
I1112 06:29:31.480185 140264174335808 run_lib.py:153] step: 591950, training_loss: 1.38299e+02
I1112 06:29:41.837682 140264174335808 run_lib.py:153] step: 592000, training_loss: 1.08291e+02
I1112 06:29:52.267597 140264174335808 run_lib.py:153] step: 592050, training_loss: 1.31889e+02
I1112 06:30:02.445257 140264174335808 run_lib.py:153] step: 592100, training_loss: 1.38982e+02
I1112 06:30:12.456480 140264174335808 run_lib.py:153] step: 592150, training_loss: 1.25262e+02
I1112 06:30:22.526653 140264174335808 run_lib.py:153] step: 592200, training_loss: 1.29949e+02
I1112 06:30:33.007989 140264174335808 run_lib.py:153] step: 592250, training_loss: 1.50185e+02
I1112 06:30:42.635051 140264174335808 run_lib.py:153] step: 592300, training_loss: 1.25612e+02
I1112 06:30:52.599730 140264174335808 run_lib.py:153] step: 592350, training_loss: 1.54679e+02
I1112 06:31:02.638735 140264174335808 run_lib.py:153] step: 592400, training_loss: 1.07278e+02
I1112 06:31:12.631989 140264174335808 run_lib.py:153] step: 592450, training_loss: 1.53146e+02
I1112 06:31:22.722820 140264174335808 run_lib.py:153] step: 592500, training_loss: 1.20442e+02
I1112 06:31:32.662290 140264174335808 run_lib.py:153] step: 592550, training_loss: 1.19527e+02
I1112 06:31:42.226747 140264174335808 run_lib.py:153] step: 592600, training_loss: 1.17405e+02
I1112 06:31:52.717873 140264174335808 run_lib.py:153] step: 592650, training_loss: 1.20462e+02
I1112 06:32:02.219747 140264174335808 run_lib.py:153] step: 592700, training_loss: 1.58307e+02
I1112 06:32:12.111373 140264174335808 run_lib.py:153] step: 592750, training_loss: 1.20569e+02
I1112 06:32:22.030652 140264174335808 run_lib.py:153] step: 592800, training_loss: 1.49798e+02
I1112 06:32:31.886681 140264174335808 run_lib.py:153] step: 592850, training_loss: 1.32091e+02
I1112 06:32:42.374497 140264174335808 run_lib.py:153] step: 592900, training_loss: 1.34675e+02
I1112 06:32:52.352743 140264174335808 run_lib.py:153] step: 592950, training_loss: 1.16300e+02
I1112 06:33:02.029584 140264174335808 run_lib.py:153] step: 593000, training_loss: 1.11448e+02
I1112 06:33:11.647854 140264174335808 run_lib.py:153] step: 593050, training_loss: 1.10873e+02
I1112 06:33:21.359119 140264174335808 run_lib.py:153] step: 593100, training_loss: 1.00205e+02
I1112 06:33:31.202064 140264174335808 run_lib.py:153] step: 593150, training_loss: 1.26568e+02
I1112 06:33:40.655553 140264174335808 run_lib.py:153] step: 593200, training_loss: 1.19823e+02
I1112 06:33:50.211099 140264174335808 run_lib.py:153] step: 593250, training_loss: 1.41007e+02
I1112 06:34:00.628468 140264174335808 run_lib.py:153] step: 593300, training_loss: 9.64403e+01
I1112 06:34:10.390836 140264174335808 run_lib.py:153] step: 593350, training_loss: 1.11941e+02
I1112 06:34:20.418528 140264174335808 run_lib.py:153] step: 593400, training_loss: 1.20855e+02
I1112 06:34:30.492828 140264174335808 run_lib.py:153] step: 593450, training_loss: 1.23653e+02
I1112 06:34:40.561305 140264174335808 run_lib.py:153] step: 593500, training_loss: 1.25022e+02
I1112 06:34:50.336668 140264174335808 run_lib.py:153] step: 593550, training_loss: 1.36236e+02
I1112 06:35:01.425400 140264174335808 run_lib.py:153] step: 593600, training_loss: 1.27266e+02
I1112 06:35:10.998694 140264174335808 run_lib.py:153] step: 593650, training_loss: 1.12416e+02
I1112 06:35:20.648579 140264174335808 run_lib.py:153] step: 593700, training_loss: 9.28804e+01
I1112 06:35:30.499426 140264174335808 run_lib.py:153] step: 593750, training_loss: 1.08799e+02
I1112 06:35:40.134438 140264174335808 run_lib.py:153] step: 593800, training_loss: 1.25111e+02
I1112 06:35:50.294629 140264174335808 run_lib.py:153] step: 593850, training_loss: 1.29609e+02
I1112 06:36:00.490925 140264174335808 run_lib.py:153] step: 593900, training_loss: 1.29518e+02
I1112 06:36:10.133988 140264174335808 run_lib.py:153] step: 593950, training_loss: 9.25449e+01
I1112 06:36:19.667704 140264174335808 run_lib.py:153] step: 594000, training_loss: 1.37920e+02
I1112 06:36:29.606261 140264174335808 run_lib.py:153] step: 594050, training_loss: 1.56258e+02
I1112 06:36:39.826058 140264174335808 run_lib.py:153] step: 594100, training_loss: 1.21139e+02
I1112 06:36:49.100935 140264174335808 run_lib.py:153] step: 594150, training_loss: 1.63553e+02
I1112 06:36:58.544177 140264174335808 run_lib.py:153] step: 594200, training_loss: 1.19244e+02
I1112 06:37:08.308775 140264174335808 run_lib.py:153] step: 594250, training_loss: 1.39549e+02
I1112 06:37:17.974370 140264174335808 run_lib.py:153] step: 594300, training_loss: 1.06775e+02
I1112 06:37:28.146714 140264174335808 run_lib.py:153] step: 594350, training_loss: 1.33954e+02
I1112 06:37:37.696064 140264174335808 run_lib.py:153] step: 594400, training_loss: 1.13670e+02
I1112 06:37:47.941650 140264174335808 run_lib.py:153] step: 594450, training_loss: 1.26592e+02
I1112 06:37:58.784300 140264174335808 run_lib.py:153] step: 594500, training_loss: 1.20222e+02
I1112 06:38:08.843060 140264174335808 run_lib.py:153] step: 594550, training_loss: 1.25885e+02
I1112 06:38:19.036157 140264174335808 run_lib.py:153] step: 594600, training_loss: 1.50516e+02
I1112 06:38:29.351939 140264174335808 run_lib.py:153] step: 594650, training_loss: 1.46234e+02
I1112 06:38:39.470698 140264174335808 run_lib.py:153] step: 594700, training_loss: 1.55983e+02
I1112 06:38:49.197078 140264174335808 run_lib.py:153] step: 594750, training_loss: 1.50226e+02
I1112 06:38:58.768078 140264174335808 run_lib.py:153] step: 594800, training_loss: 1.51565e+02
I1112 06:39:08.343745 140264174335808 run_lib.py:153] step: 594850, training_loss: 1.27548e+02
I1112 06:39:18.000129 140264174335808 run_lib.py:153] step: 594900, training_loss: 1.52410e+02
I1112 06:39:27.522671 140264174335808 run_lib.py:153] step: 594950, training_loss: 9.52148e+01
I1112 06:39:37.387875 140264174335808 run_lib.py:153] step: 595000, training_loss: 1.09358e+02
I1112 06:39:37.490885 140264174335808 run_lib.py:166] step: 595000, eval_loss: 1.29428e+02
I1112 06:39:47.386652 140264174335808 run_lib.py:153] step: 595050, training_loss: 1.27267e+02
I1112 06:39:57.337894 140264174335808 run_lib.py:153] step: 595100, training_loss: 1.13724e+02
I1112 06:40:06.932190 140264174335808 run_lib.py:153] step: 595150, training_loss: 1.02222e+02
I1112 06:40:16.686022 140264174335808 run_lib.py:153] step: 595200, training_loss: 1.40173e+02
I1112 06:40:26.879348 140264174335808 run_lib.py:153] step: 595250, training_loss: 1.30196e+02
I1112 06:40:36.895406 140264174335808 run_lib.py:153] step: 595300, training_loss: 1.12551e+02
I1112 06:40:46.678724 140264174335808 run_lib.py:153] step: 595350, training_loss: 9.90173e+01
I1112 06:40:57.023115 140264174335808 run_lib.py:153] step: 595400, training_loss: 1.21488e+02
I1112 06:41:07.271696 140264174335808 run_lib.py:153] step: 595450, training_loss: 1.08292e+02
I1112 06:41:16.653235 140264174335808 run_lib.py:153] step: 595500, training_loss: 1.16622e+02
I1112 06:41:26.440208 140264174335808 run_lib.py:153] step: 595550, training_loss: 1.19416e+02
I1112 06:41:36.358728 140264174335808 run_lib.py:153] step: 595600, training_loss: 1.33151e+02
I1112 06:41:46.305792 140264174335808 run_lib.py:153] step: 595650, training_loss: 1.25315e+02
I1112 06:41:56.557203 140264174335808 run_lib.py:153] step: 595700, training_loss: 1.25672e+02
I1112 06:42:05.883040 140264174335808 run_lib.py:153] step: 595750, training_loss: 1.25636e+02
I1112 06:42:15.643498 140264174335808 run_lib.py:153] step: 595800, training_loss: 1.45176e+02
I1112 06:42:25.622309 140264174335808 run_lib.py:153] step: 595850, training_loss: 1.30579e+02
I1112 06:42:35.784141 140264174335808 run_lib.py:153] step: 595900, training_loss: 1.44647e+02
I1112 06:42:46.088770 140264174335808 run_lib.py:153] step: 595950, training_loss: 1.43259e+02
I1112 06:42:56.350166 140264174335808 run_lib.py:153] step: 596000, training_loss: 1.23312e+02
I1112 06:43:05.928450 140264174335808 run_lib.py:153] step: 596050, training_loss: 1.35988e+02
I1112 06:43:15.358540 140264174335808 run_lib.py:153] step: 596100, training_loss: 1.43553e+02
I1112 06:43:25.049395 140264174335808 run_lib.py:153] step: 596150, training_loss: 1.24806e+02
I1112 06:43:35.734142 140264174335808 run_lib.py:153] step: 596200, training_loss: 1.29696e+02
I1112 06:43:45.871519 140264174335808 run_lib.py:153] step: 596250, training_loss: 1.18536e+02
I1112 06:43:55.355476 140264174335808 run_lib.py:153] step: 596300, training_loss: 8.66823e+01
I1112 06:44:04.997344 140264174335808 run_lib.py:153] step: 596350, training_loss: 1.25652e+02
I1112 06:44:14.380506 140264174335808 run_lib.py:153] step: 596400, training_loss: 1.20863e+02
I1112 06:44:24.571521 140264174335808 run_lib.py:153] step: 596450, training_loss: 1.45765e+02
I1112 06:44:34.430634 140264174335808 run_lib.py:153] step: 596500, training_loss: 1.15819e+02
I1112 06:44:44.748183 140264174335808 run_lib.py:153] step: 596550, training_loss: 1.28248e+02
I1112 06:44:54.531475 140264174335808 run_lib.py:153] step: 596600, training_loss: 1.17853e+02
I1112 06:45:04.213272 140264174335808 run_lib.py:153] step: 596650, training_loss: 1.31998e+02
I1112 06:45:13.552601 140264174335808 run_lib.py:153] step: 596700, training_loss: 1.33136e+02
I1112 06:45:24.146236 140264174335808 run_lib.py:153] step: 596750, training_loss: 9.26692e+01
I1112 06:45:33.527938 140264174335808 run_lib.py:153] step: 596800, training_loss: 1.24837e+02
I1112 06:45:43.649544 140264174335808 run_lib.py:153] step: 596850, training_loss: 1.34045e+02
I1112 06:45:53.747921 140264174335808 run_lib.py:153] step: 596900, training_loss: 1.02624e+02
I1112 06:46:03.521050 140264174335808 run_lib.py:153] step: 596950, training_loss: 1.12246e+02
I1112 06:46:14.032502 140264174335808 run_lib.py:153] step: 597000, training_loss: 1.40120e+02
I1112 06:46:24.480764 140264174335808 run_lib.py:153] step: 597050, training_loss: 1.35342e+02
I1112 06:46:35.051569 140264174335808 run_lib.py:153] step: 597100, training_loss: 1.23495e+02
I1112 06:46:45.243128 140264174335808 run_lib.py:153] step: 597150, training_loss: 1.22439e+02
I1112 06:46:55.136614 140264174335808 run_lib.py:153] step: 597200, training_loss: 8.60549e+01
I1112 06:47:04.525817 140264174335808 run_lib.py:153] step: 597250, training_loss: 1.50922e+02
I1112 06:47:14.090376 140264174335808 run_lib.py:153] step: 597300, training_loss: 1.50053e+02
I1112 06:47:23.683253 140264174335808 run_lib.py:153] step: 597350, training_loss: 1.00697e+02
I1112 06:47:33.260102 140264174335808 run_lib.py:153] step: 597400, training_loss: 1.36250e+02
I1112 06:47:43.101697 140264174335808 run_lib.py:153] step: 597450, training_loss: 1.17122e+02
I1112 06:47:52.570616 140264174335808 run_lib.py:153] step: 597500, training_loss: 1.45803e+02
I1112 06:48:02.061272 140264174335808 run_lib.py:153] step: 597550, training_loss: 1.34538e+02
I1112 06:48:11.842113 140264174335808 run_lib.py:153] step: 597600, training_loss: 1.44891e+02
I1112 06:48:21.438095 140264174335808 run_lib.py:153] step: 597650, training_loss: 1.29790e+02
I1112 06:48:31.320432 140264174335808 run_lib.py:153] step: 597700, training_loss: 1.40099e+02
I1112 06:48:41.022733 140264174335808 run_lib.py:153] step: 597750, training_loss: 1.04360e+02
I1112 06:48:51.122265 140264174335808 run_lib.py:153] step: 597800, training_loss: 1.09121e+02
I1112 06:49:00.813438 140264174335808 run_lib.py:153] step: 597850, training_loss: 1.32987e+02
I1112 06:49:10.072463 140264174335808 run_lib.py:153] step: 597900, training_loss: 1.11218e+02
I1112 06:49:20.393356 140264174335808 run_lib.py:153] step: 597950, training_loss: 1.42995e+02
I1112 06:49:30.762629 140264174335808 run_lib.py:153] step: 598000, training_loss: 9.19840e+01
I1112 06:49:41.156768 140264174335808 run_lib.py:153] step: 598050, training_loss: 9.59821e+01
I1112 06:49:50.523316 140264174335808 run_lib.py:153] step: 598100, training_loss: 1.12604e+02
I1112 06:50:00.468592 140264174335808 run_lib.py:153] step: 598150, training_loss: 1.29475e+02
I1112 06:50:10.109189 140264174335808 run_lib.py:153] step: 598200, training_loss: 1.43571e+02
I1112 06:50:19.778767 140264174335808 run_lib.py:153] step: 598250, training_loss: 1.38834e+02
I1112 06:50:30.163696 140264174335808 run_lib.py:153] step: 598300, training_loss: 1.25644e+02
I1112 06:50:40.458105 140264174335808 run_lib.py:153] step: 598350, training_loss: 1.21580e+02
I1112 06:50:50.083882 140264174335808 run_lib.py:153] step: 598400, training_loss: 1.14923e+02
I1112 06:50:59.783887 140264174335808 run_lib.py:153] step: 598450, training_loss: 1.09905e+02
I1112 06:51:09.149440 140264174335808 run_lib.py:153] step: 598500, training_loss: 1.21090e+02
I1112 06:51:19.254128 140264174335808 run_lib.py:153] step: 598550, training_loss: 1.07999e+02
I1112 06:51:29.076948 140264174335808 run_lib.py:153] step: 598600, training_loss: 1.29662e+02
I1112 06:51:39.515896 140264174335808 run_lib.py:153] step: 598650, training_loss: 1.26785e+02
I1112 06:51:49.422076 140264174335808 run_lib.py:153] step: 598700, training_loss: 1.08995e+02
I1112 06:51:59.370204 140264174335808 run_lib.py:153] step: 598750, training_loss: 1.41674e+02
I1112 06:52:09.786299 140264174335808 run_lib.py:153] step: 598800, training_loss: 1.16181e+02
I1112 06:52:19.957564 140264174335808 run_lib.py:153] step: 598850, training_loss: 1.45394e+02
I1112 06:52:30.329719 140264174335808 run_lib.py:153] step: 598900, training_loss: 9.83791e+01
I1112 06:52:40.017253 140264174335808 run_lib.py:153] step: 598950, training_loss: 1.21065e+02
I1112 06:52:49.736581 140264174335808 run_lib.py:153] step: 599000, training_loss: 1.02016e+02
I1112 06:52:59.478220 140264174335808 run_lib.py:153] step: 599050, training_loss: 1.28571e+02
I1112 06:53:10.403695 140264174335808 run_lib.py:153] step: 599100, training_loss: 1.41744e+02
I1112 06:53:20.408704 140264174335808 run_lib.py:153] step: 599150, training_loss: 1.66648e+02
I1112 06:53:30.368013 140264174335808 run_lib.py:153] step: 599200, training_loss: 1.43542e+02
I1112 06:53:40.170086 140264174335808 run_lib.py:153] step: 599250, training_loss: 1.24381e+02
I1112 06:53:50.008460 140264174335808 run_lib.py:153] step: 599300, training_loss: 1.45419e+02
I1112 06:54:00.262660 140264174335808 run_lib.py:153] step: 599350, training_loss: 1.31998e+02
I1112 06:54:10.813921 140264174335808 run_lib.py:153] step: 599400, training_loss: 1.08606e+02
I1112 06:54:20.363746 140264174335808 run_lib.py:153] step: 599450, training_loss: 1.09287e+02
I1112 06:54:30.425974 140264174335808 run_lib.py:153] step: 599500, training_loss: 1.13516e+02
I1112 06:54:40.923778 140264174335808 run_lib.py:153] step: 599550, training_loss: 1.30275e+02
I1112 06:54:51.412723 140264174335808 run_lib.py:153] step: 599600, training_loss: 8.83607e+01
I1112 06:55:01.977758 140264174335808 run_lib.py:153] step: 599650, training_loss: 1.18505e+02
I1112 06:55:11.541363 140264174335808 run_lib.py:153] step: 599700, training_loss: 1.19840e+02
I1112 06:55:20.847862 140264174335808 run_lib.py:153] step: 599750, training_loss: 1.05975e+02
I1112 06:55:30.468312 140264174335808 run_lib.py:153] step: 599800, training_loss: 1.17065e+02
I1112 06:55:40.661886 140264174335808 run_lib.py:153] step: 599850, training_loss: 1.24874e+02
I1112 06:55:51.231533 140264174335808 run_lib.py:153] step: 599900, training_loss: 1.43701e+02
I1112 06:56:01.574684 140264174335808 run_lib.py:153] step: 599950, training_loss: 1.21263e+02
I1112 06:56:11.622670 140264174335808 run_lib.py:153] step: 600000, training_loss: 1.35952e+02
I1112 06:56:12.177231 140264174335808 run_lib.py:166] step: 600000, eval_loss: 1.34384e+02
I1112 06:56:22.443345 140264174335808 run_lib.py:153] step: 600050, training_loss: 1.31328e+02
I1112 06:56:33.113519 140264174335808 run_lib.py:153] step: 600100, training_loss: 1.14981e+02
I1112 06:56:43.435158 140264174335808 run_lib.py:153] step: 600150, training_loss: 1.32185e+02
I1112 06:56:54.089972 140264174335808 run_lib.py:153] step: 600200, training_loss: 1.33038e+02
I1112 06:57:03.911892 140264174335808 run_lib.py:153] step: 600250, training_loss: 1.31011e+02
I1112 06:57:13.964010 140264174335808 run_lib.py:153] step: 600300, training_loss: 1.49355e+02
I1112 06:57:23.788606 140264174335808 run_lib.py:153] step: 600350, training_loss: 1.33462e+02
I1112 06:57:34.692164 140264174335808 run_lib.py:153] step: 600400, training_loss: 1.51040e+02
I1112 06:57:44.540953 140264174335808 run_lib.py:153] step: 600450, training_loss: 1.42022e+02
I1112 06:57:54.584015 140264174335808 run_lib.py:153] step: 600500, training_loss: 1.25586e+02
I1112 06:58:04.965218 140264174335808 run_lib.py:153] step: 600550, training_loss: 1.29901e+02
I1112 06:58:15.100255 140264174335808 run_lib.py:153] step: 600600, training_loss: 1.14988e+02
I1112 06:58:25.042325 140264174335808 run_lib.py:153] step: 600650, training_loss: 1.07094e+02
I1112 06:58:35.494269 140264174335808 run_lib.py:153] step: 600700, training_loss: 1.42209e+02
I1112 06:58:45.689286 140264174335808 run_lib.py:153] step: 600750, training_loss: 1.35272e+02
I1112 06:58:55.970381 140264174335808 run_lib.py:153] step: 600800, training_loss: 1.18930e+02
I1112 06:59:06.185618 140264174335808 run_lib.py:153] step: 600850, training_loss: 1.42477e+02
I1112 06:59:15.667504 140264174335808 run_lib.py:153] step: 600900, training_loss: 1.33874e+02
I1112 06:59:25.723845 140264174335808 run_lib.py:153] step: 600950, training_loss: 1.34497e+02
I1112 06:59:35.670032 140264174335808 run_lib.py:153] step: 601000, training_loss: 1.20908e+02
I1112 06:59:45.562456 140264174335808 run_lib.py:153] step: 601050, training_loss: 1.17489e+02
I1112 06:59:55.683549 140264174335808 run_lib.py:153] step: 601100, training_loss: 1.22482e+02
I1112 07:00:06.275084 140264174335808 run_lib.py:153] step: 601150, training_loss: 1.22893e+02
I1112 07:00:16.577318 140264174335808 run_lib.py:153] step: 601200, training_loss: 1.10903e+02
I1112 07:00:26.548673 140264174335808 run_lib.py:153] step: 601250, training_loss: 1.37626e+02
I1112 07:00:36.251470 140264174335808 run_lib.py:153] step: 601300, training_loss: 1.19396e+02
I1112 07:00:46.154077 140264174335808 run_lib.py:153] step: 601350, training_loss: 1.04839e+02
I1112 07:00:56.588027 140264174335808 run_lib.py:153] step: 601400, training_loss: 1.13746e+02
I1112 07:01:06.772351 140264174335808 run_lib.py:153] step: 601450, training_loss: 8.11395e+01
I1112 07:01:17.353671 140264174335808 run_lib.py:153] step: 601500, training_loss: 1.16525e+02
I1112 07:01:28.241559 140264174335808 run_lib.py:153] step: 601550, training_loss: 1.30219e+02
I1112 07:01:38.853718 140264174335808 run_lib.py:153] step: 601600, training_loss: 1.80291e+02
I1112 07:01:49.186743 140264174335808 run_lib.py:153] step: 601650, training_loss: 1.18365e+02
I1112 07:01:58.956140 140264174335808 run_lib.py:153] step: 601700, training_loss: 1.46980e+02
I1112 07:02:09.241014 140264174335808 run_lib.py:153] step: 601750, training_loss: 1.41614e+02
I1112 07:02:19.500458 140264174335808 run_lib.py:153] step: 601800, training_loss: 1.34899e+02
I1112 07:02:29.323450 140264174335808 run_lib.py:153] step: 601850, training_loss: 1.04647e+02
I1112 07:02:39.045288 140264174335808 run_lib.py:153] step: 601900, training_loss: 1.47250e+02
I1112 07:02:48.880396 140264174335808 run_lib.py:153] step: 601950, training_loss: 1.47300e+02
I1112 07:02:59.455324 140264174335808 run_lib.py:153] step: 602000, training_loss: 1.29542e+02
I1112 07:03:09.165759 140264174335808 run_lib.py:153] step: 602050, training_loss: 1.48262e+02
I1112 07:03:18.997789 140264174335808 run_lib.py:153] step: 602100, training_loss: 1.38490e+02
I1112 07:03:30.355832 140264174335808 run_lib.py:153] step: 602150, training_loss: 1.26958e+02
I1112 07:03:39.913492 140264174335808 run_lib.py:153] step: 602200, training_loss: 1.19254e+02
I1112 07:03:49.934675 140264174335808 run_lib.py:153] step: 602250, training_loss: 1.25539e+02
I1112 07:03:59.302012 140264174335808 run_lib.py:153] step: 602300, training_loss: 1.42956e+02
I1112 07:04:08.579545 140264174335808 run_lib.py:153] step: 602350, training_loss: 1.56612e+02
I1112 07:04:19.544546 140264174335808 run_lib.py:153] step: 602400, training_loss: 1.12251e+02
I1112 07:04:29.501930 140264174335808 run_lib.py:153] step: 602450, training_loss: 1.16508e+02
I1112 07:04:39.969719 140264174335808 run_lib.py:153] step: 602500, training_loss: 1.11867e+02
I1112 07:04:50.168189 140264174335808 run_lib.py:153] step: 602550, training_loss: 1.05894e+02
I1112 07:05:00.478060 140264174335808 run_lib.py:153] step: 602600, training_loss: 1.17192e+02
I1112 07:05:10.922317 140264174335808 run_lib.py:153] step: 602650, training_loss: 1.28136e+02
I1112 07:05:21.593045 140264174335808 run_lib.py:153] step: 602700, training_loss: 1.52261e+02
I1112 07:05:32.067290 140264174335808 run_lib.py:153] step: 602750, training_loss: 1.17921e+02
I1112 07:05:41.626720 140264174335808 run_lib.py:153] step: 602800, training_loss: 1.31901e+02
I1112 07:05:51.222362 140264174335808 run_lib.py:153] step: 602850, training_loss: 1.11303e+02
I1112 07:06:00.848579 140264174335808 run_lib.py:153] step: 602900, training_loss: 1.25424e+02
I1112 07:06:11.235720 140264174335808 run_lib.py:153] step: 602950, training_loss: 1.25700e+02
I1112 07:06:20.926773 140264174335808 run_lib.py:153] step: 603000, training_loss: 1.44471e+02
I1112 07:06:31.521318 140264174335808 run_lib.py:153] step: 603050, training_loss: 1.53281e+02
I1112 07:06:41.991762 140264174335808 run_lib.py:153] step: 603100, training_loss: 1.24871e+02
I1112 07:06:52.630264 140264174335808 run_lib.py:153] step: 603150, training_loss: 9.79857e+01
I1112 07:07:02.871833 140264174335808 run_lib.py:153] step: 603200, training_loss: 1.44641e+02
I1112 07:07:13.258018 140264174335808 run_lib.py:153] step: 603250, training_loss: 9.98435e+01
I1112 07:07:23.437165 140264174335808 run_lib.py:153] step: 603300, training_loss: 1.21161e+02
I1112 07:07:33.386191 140264174335808 run_lib.py:153] step: 603350, training_loss: 1.01087e+02
I1112 07:07:43.800088 140264174335808 run_lib.py:153] step: 603400, training_loss: 9.77353e+01
I1112 07:07:54.445390 140264174335808 run_lib.py:153] step: 603450, training_loss: 9.82720e+01
I1112 07:08:04.953164 140264174335808 run_lib.py:153] step: 603500, training_loss: 1.20925e+02
I1112 07:08:15.379368 140264174335808 run_lib.py:153] step: 603550, training_loss: 1.39654e+02
I1112 07:08:25.172600 140264174335808 run_lib.py:153] step: 603600, training_loss: 1.16201e+02
I1112 07:08:34.595941 140264174335808 run_lib.py:153] step: 603650, training_loss: 1.25792e+02
I1112 07:08:44.752265 140264174335808 run_lib.py:153] step: 603700, training_loss: 1.15807e+02
I1112 07:08:54.350178 140264174335808 run_lib.py:153] step: 603750, training_loss: 1.47524e+02
I1112 07:09:04.733780 140264174335808 run_lib.py:153] step: 603800, training_loss: 1.35196e+02
I1112 07:09:15.666554 140264174335808 run_lib.py:153] step: 603850, training_loss: 1.29765e+02
I1112 07:09:25.567493 140264174335808 run_lib.py:153] step: 603900, training_loss: 1.57747e+02
I1112 07:09:35.630728 140264174335808 run_lib.py:153] step: 603950, training_loss: 1.26404e+02
I1112 07:09:45.491660 140264174335808 run_lib.py:153] step: 604000, training_loss: 1.49085e+02
I1112 07:09:54.994199 140264174335808 run_lib.py:153] step: 604050, training_loss: 1.12813e+02
I1112 07:10:04.540673 140264174335808 run_lib.py:153] step: 604100, training_loss: 1.15683e+02
I1112 07:10:14.943831 140264174335808 run_lib.py:153] step: 604150, training_loss: 1.12131e+02
I1112 07:10:24.571827 140264174335808 run_lib.py:153] step: 604200, training_loss: 1.42983e+02
I1112 07:10:34.483556 140264174335808 run_lib.py:153] step: 604250, training_loss: 1.15867e+02
I1112 07:10:44.490428 140264174335808 run_lib.py:153] step: 604300, training_loss: 1.23117e+02
I1112 07:10:54.110166 140264174335808 run_lib.py:153] step: 604350, training_loss: 1.50630e+02
I1112 07:11:04.541971 140264174335808 run_lib.py:153] step: 604400, training_loss: 1.15121e+02
I1112 07:11:14.207344 140264174335808 run_lib.py:153] step: 604450, training_loss: 1.32993e+02
I1112 07:11:24.207232 140264174335808 run_lib.py:153] step: 604500, training_loss: 1.40761e+02
I1112 07:11:34.140292 140264174335808 run_lib.py:153] step: 604550, training_loss: 1.38418e+02
I1112 07:11:44.322921 140264174335808 run_lib.py:153] step: 604600, training_loss: 1.23052e+02
I1112 07:11:54.941072 140264174335808 run_lib.py:153] step: 604650, training_loss: 1.51256e+02
I1112 07:12:04.540526 140264174335808 run_lib.py:153] step: 604700, training_loss: 1.21053e+02
I1112 07:12:15.180475 140264174335808 run_lib.py:153] step: 604750, training_loss: 1.25270e+02
I1112 07:12:24.975931 140264174335808 run_lib.py:153] step: 604800, training_loss: 1.25061e+02
I1112 07:12:35.057988 140264174335808 run_lib.py:153] step: 604850, training_loss: 1.32438e+02
I1112 07:12:45.266173 140264174335808 run_lib.py:153] step: 604900, training_loss: 1.58069e+02
I1112 07:12:55.654968 140264174335808 run_lib.py:153] step: 604950, training_loss: 1.46659e+02
I1112 07:13:06.386364 140264174335808 run_lib.py:153] step: 605000, training_loss: 1.47932e+02
I1112 07:13:06.489041 140264174335808 run_lib.py:166] step: 605000, eval_loss: 1.13757e+02
I1112 07:13:16.455291 140264174335808 run_lib.py:153] step: 605050, training_loss: 1.21180e+02
I1112 07:13:25.959883 140264174335808 run_lib.py:153] step: 605100, training_loss: 1.26067e+02
I1112 07:13:35.920506 140264174335808 run_lib.py:153] step: 605150, training_loss: 1.19723e+02
I1112 07:13:46.336682 140264174335808 run_lib.py:153] step: 605200, training_loss: 1.34652e+02
I1112 07:13:56.714879 140264174335808 run_lib.py:153] step: 605250, training_loss: 1.32889e+02
I1112 07:14:07.846796 140264174335808 run_lib.py:153] step: 605300, training_loss: 1.37576e+02
I1112 07:14:18.064301 140264174335808 run_lib.py:153] step: 605350, training_loss: 1.18098e+02
I1112 07:14:28.325347 140264174335808 run_lib.py:153] step: 605400, training_loss: 1.48413e+02
I1112 07:14:38.582036 140264174335808 run_lib.py:153] step: 605450, training_loss: 1.23182e+02
I1112 07:14:48.895158 140264174335808 run_lib.py:153] step: 605500, training_loss: 1.68714e+02
I1112 07:14:59.161407 140264174335808 run_lib.py:153] step: 605550, training_loss: 1.00890e+02
I1112 07:15:09.217370 140264174335808 run_lib.py:153] step: 605600, training_loss: 1.54605e+02
I1112 07:15:19.354732 140264174335808 run_lib.py:153] step: 605650, training_loss: 1.16259e+02
I1112 07:15:28.827968 140264174335808 run_lib.py:153] step: 605700, training_loss: 1.26127e+02
I1112 07:15:39.951685 140264174335808 run_lib.py:153] step: 605750, training_loss: 1.31682e+02
I1112 07:15:49.240343 140264174335808 run_lib.py:153] step: 605800, training_loss: 1.07227e+02
I1112 07:15:59.414443 140264174335808 run_lib.py:153] step: 605850, training_loss: 1.30968e+02
I1112 07:16:09.350474 140264174335808 run_lib.py:153] step: 605900, training_loss: 9.89693e+01
I1112 07:16:19.356699 140264174335808 run_lib.py:153] step: 605950, training_loss: 1.08884e+02
I1112 07:16:29.989999 140264174335808 run_lib.py:153] step: 606000, training_loss: 1.13958e+02
I1112 07:16:39.261691 140264174335808 run_lib.py:153] step: 606050, training_loss: 1.14662e+02
I1112 07:16:48.826701 140264174335808 run_lib.py:153] step: 606100, training_loss: 1.43398e+02
I1112 07:16:58.885365 140264174335808 run_lib.py:153] step: 606150, training_loss: 1.37395e+02
I1112 07:17:09.043662 140264174335808 run_lib.py:153] step: 606200, training_loss: 1.29783e+02
I1112 07:17:19.063099 140264174335808 run_lib.py:153] step: 606250, training_loss: 1.19766e+02
I1112 07:17:29.177596 140264174335808 run_lib.py:153] step: 606300, training_loss: 1.12314e+02
I1112 07:17:39.293327 140264174335808 run_lib.py:153] step: 606350, training_loss: 1.27265e+02
I1112 07:17:49.371274 140264174335808 run_lib.py:153] step: 606400, training_loss: 1.04440e+02
I1112 07:17:59.165708 140264174335808 run_lib.py:153] step: 606450, training_loss: 1.39245e+02
I1112 07:18:10.260200 140264174335808 run_lib.py:153] step: 606500, training_loss: 1.26649e+02
I1112 07:18:20.469218 140264174335808 run_lib.py:153] step: 606550, training_loss: 1.20445e+02
I1112 07:18:30.893056 140264174335808 run_lib.py:153] step: 606600, training_loss: 1.04120e+02
I1112 07:18:40.901053 140264174335808 run_lib.py:153] step: 606650, training_loss: 1.12051e+02
I1112 07:18:51.019086 140264174335808 run_lib.py:153] step: 606700, training_loss: 1.22477e+02
I1112 07:19:00.643070 140264174335808 run_lib.py:153] step: 606750, training_loss: 1.18892e+02
I1112 07:19:10.036513 140264174335808 run_lib.py:153] step: 606800, training_loss: 1.27368e+02
I1112 07:19:19.731261 140264174335808 run_lib.py:153] step: 606850, training_loss: 1.32261e+02
I1112 07:19:30.222490 140264174335808 run_lib.py:153] step: 606900, training_loss: 1.21293e+02
I1112 07:19:40.477495 140264174335808 run_lib.py:153] step: 606950, training_loss: 1.49074e+02
I1112 07:19:50.203326 140264174335808 run_lib.py:153] step: 607000, training_loss: 1.44745e+02
I1112 07:20:00.341676 140264174335808 run_lib.py:153] step: 607050, training_loss: 1.00286e+02
I1112 07:20:10.272140 140264174335808 run_lib.py:153] step: 607100, training_loss: 1.28708e+02
I1112 07:20:20.821082 140264174335808 run_lib.py:153] step: 607150, training_loss: 1.15344e+02
I1112 07:20:30.820996 140264174335808 run_lib.py:153] step: 607200, training_loss: 1.11540e+02
I1112 07:20:40.893783 140264174335808 run_lib.py:153] step: 607250, training_loss: 1.50624e+02
I1112 07:20:51.001341 140264174335808 run_lib.py:153] step: 607300, training_loss: 1.27520e+02
I1112 07:21:00.795486 140264174335808 run_lib.py:153] step: 607350, training_loss: 1.07693e+02
I1112 07:21:11.496075 140264174335808 run_lib.py:153] step: 607400, training_loss: 1.45137e+02
I1112 07:21:22.179126 140264174335808 run_lib.py:153] step: 607450, training_loss: 1.19004e+02
I1112 07:21:32.336220 140264174335808 run_lib.py:153] step: 607500, training_loss: 1.21154e+02
I1112 07:21:42.441988 140264174335808 run_lib.py:153] step: 607550, training_loss: 1.15899e+02
I1112 07:21:52.131992 140264174335808 run_lib.py:153] step: 607600, training_loss: 1.19449e+02
I1112 07:22:02.168478 140264174335808 run_lib.py:153] step: 607650, training_loss: 1.16387e+02
I1112 07:22:12.373963 140264174335808 run_lib.py:153] step: 607700, training_loss: 1.09663e+02
I1112 07:22:22.922368 140264174335808 run_lib.py:153] step: 607750, training_loss: 1.45380e+02
I1112 07:22:33.231494 140264174335808 run_lib.py:153] step: 607800, training_loss: 1.74569e+02
I1112 07:22:43.153247 140264174335808 run_lib.py:153] step: 607850, training_loss: 1.35692e+02
I1112 07:22:53.021259 140264174335808 run_lib.py:153] step: 607900, training_loss: 1.11655e+02
I1112 07:23:03.168246 140264174335808 run_lib.py:153] step: 607950, training_loss: 8.98891e+01
I1112 07:23:13.075385 140264174335808 run_lib.py:153] step: 608000, training_loss: 1.25418e+02
I1112 07:23:23.030869 140264174335808 run_lib.py:153] step: 608050, training_loss: 1.15564e+02
I1112 07:23:33.579287 140264174335808 run_lib.py:153] step: 608100, training_loss: 1.30107e+02
I1112 07:23:44.226917 140264174335808 run_lib.py:153] step: 608150, training_loss: 1.27403e+02
I1112 07:23:54.196851 140264174335808 run_lib.py:153] step: 608200, training_loss: 1.10316e+02
I1112 07:24:04.417615 140264174335808 run_lib.py:153] step: 608250, training_loss: 1.45371e+02
I1112 07:24:14.885571 140264174335808 run_lib.py:153] step: 608300, training_loss: 1.32748e+02
I1112 07:24:24.988343 140264174335808 run_lib.py:153] step: 608350, training_loss: 1.15347e+02
I1112 07:24:35.386476 140264174335808 run_lib.py:153] step: 608400, training_loss: 1.21016e+02
I1112 07:24:45.910771 140264174335808 run_lib.py:153] step: 608450, training_loss: 1.32492e+02
I1112 07:24:55.157603 140264174335808 run_lib.py:153] step: 608500, training_loss: 1.30979e+02
I1112 07:25:05.272715 140264174335808 run_lib.py:153] step: 608550, training_loss: 1.46661e+02
I1112 07:25:15.124932 140264174335808 run_lib.py:153] step: 608600, training_loss: 1.29455e+02
I1112 07:25:24.727169 140264174335808 run_lib.py:153] step: 608650, training_loss: 1.42006e+02
I1112 07:25:35.004472 140264174335808 run_lib.py:153] step: 608700, training_loss: 1.11271e+02
I1112 07:25:44.969873 140264174335808 run_lib.py:153] step: 608750, training_loss: 1.16214e+02
I1112 07:25:54.488782 140264174335808 run_lib.py:153] step: 608800, training_loss: 1.29070e+02
I1112 07:26:04.430906 140264174335808 run_lib.py:153] step: 608850, training_loss: 1.20648e+02
I1112 07:26:14.803926 140264174335808 run_lib.py:153] step: 608900, training_loss: 1.49275e+02
I1112 07:26:25.513030 140264174335808 run_lib.py:153] step: 608950, training_loss: 1.16514e+02
I1112 07:26:35.774258 140264174335808 run_lib.py:153] step: 609000, training_loss: 1.85303e+02
I1112 07:26:46.437933 140264174335808 run_lib.py:153] step: 609050, training_loss: 1.15902e+02
I1112 07:26:56.024981 140264174335808 run_lib.py:153] step: 609100, training_loss: 1.37824e+02
I1112 07:27:05.501062 140264174335808 run_lib.py:153] step: 609150, training_loss: 1.09621e+02
I1112 07:27:16.229911 140264174335808 run_lib.py:153] step: 609200, training_loss: 1.27364e+02
I1112 07:27:26.572419 140264174335808 run_lib.py:153] step: 609250, training_loss: 1.22358e+02
I1112 07:27:36.014894 140264174335808 run_lib.py:153] step: 609300, training_loss: 1.18013e+02
I1112 07:27:46.454169 140264174335808 run_lib.py:153] step: 609350, training_loss: 1.18619e+02
I1112 07:27:56.871414 140264174335808 run_lib.py:153] step: 609400, training_loss: 1.12184e+02
I1112 07:28:06.994781 140264174335808 run_lib.py:153] step: 609450, training_loss: 1.15013e+02
I1112 07:28:17.564859 140264174335808 run_lib.py:153] step: 609500, training_loss: 1.02472e+02
I1112 07:28:27.781519 140264174335808 run_lib.py:153] step: 609550, training_loss: 1.10844e+02
I1112 07:28:37.465536 140264174335808 run_lib.py:153] step: 609600, training_loss: 1.19487e+02
I1112 07:28:47.692654 140264174335808 run_lib.py:153] step: 609650, training_loss: 1.05693e+02
I1112 07:28:58.195012 140264174335808 run_lib.py:153] step: 609700, training_loss: 1.26917e+02
I1112 07:29:08.464138 140264174335808 run_lib.py:153] step: 609750, training_loss: 1.19415e+02
I1112 07:29:18.493161 140264174335808 run_lib.py:153] step: 609800, training_loss: 1.07869e+02
I1112 07:29:28.621237 140264174335808 run_lib.py:153] step: 609850, training_loss: 1.02517e+02
I1112 07:29:38.305073 140264174335808 run_lib.py:153] step: 609900, training_loss: 1.09141e+02
I1112 07:29:48.398745 140264174335808 run_lib.py:153] step: 609950, training_loss: 1.39709e+02
I1112 07:29:58.382127 140264174335808 run_lib.py:153] step: 610000, training_loss: 1.20812e+02
I1112 07:29:58.953109 140264174335808 run_lib.py:166] step: 610000, eval_loss: 1.38885e+02
I1112 07:30:08.708209 140264174335808 run_lib.py:153] step: 610050, training_loss: 1.02002e+02
I1112 07:30:19.233301 140264174335808 run_lib.py:153] step: 610100, training_loss: 1.18278e+02
I1112 07:30:29.200173 140264174335808 run_lib.py:153] step: 610150, training_loss: 1.08436e+02
I1112 07:30:39.255162 140264174335808 run_lib.py:153] step: 610200, training_loss: 1.52358e+02
I1112 07:30:49.429735 140264174335808 run_lib.py:153] step: 610250, training_loss: 1.27269e+02
I1112 07:30:59.762817 140264174335808 run_lib.py:153] step: 610300, training_loss: 1.19085e+02
I1112 07:31:10.634430 140264174335808 run_lib.py:153] step: 610350, training_loss: 1.16751e+02
I1112 07:31:20.805581 140264174335808 run_lib.py:153] step: 610400, training_loss: 1.22666e+02
I1112 07:31:30.204091 140264174335808 run_lib.py:153] step: 610450, training_loss: 1.17044e+02
I1112 07:31:40.404095 140264174335808 run_lib.py:153] step: 610500, training_loss: 1.16125e+02
I1112 07:31:50.565609 140264174335808 run_lib.py:153] step: 610550, training_loss: 1.34323e+02
I1112 07:32:00.410709 140264174335808 run_lib.py:153] step: 610600, training_loss: 1.57126e+02
I1112 07:32:09.703746 140264174335808 run_lib.py:153] step: 610650, training_loss: 1.57959e+02
I1112 07:32:19.939414 140264174335808 run_lib.py:153] step: 610700, training_loss: 1.44225e+02
I1112 07:32:29.795698 140264174335808 run_lib.py:153] step: 610750, training_loss: 1.44127e+02
I1112 07:32:40.068032 140264174335808 run_lib.py:153] step: 610800, training_loss: 1.23884e+02
I1112 07:32:50.436935 140264174335808 run_lib.py:153] step: 610850, training_loss: 1.44141e+02
I1112 07:33:00.922632 140264174335808 run_lib.py:153] step: 610900, training_loss: 1.01715e+02
I1112 07:33:11.140867 140264174335808 run_lib.py:153] step: 610950, training_loss: 1.02140e+02
I1112 07:33:21.319529 140264174335808 run_lib.py:153] step: 611000, training_loss: 1.32424e+02
I1112 07:33:31.404776 140264174335808 run_lib.py:153] step: 611050, training_loss: 1.26988e+02
I1112 07:33:41.551154 140264174335808 run_lib.py:153] step: 611100, training_loss: 1.26054e+02
I1112 07:33:50.971651 140264174335808 run_lib.py:153] step: 611150, training_loss: 1.11422e+02
I1112 07:34:01.896804 140264174335808 run_lib.py:153] step: 611200, training_loss: 1.43188e+02
I1112 07:34:11.702684 140264174335808 run_lib.py:153] step: 611250, training_loss: 1.21248e+02
I1112 07:34:21.979453 140264174335808 run_lib.py:153] step: 611300, training_loss: 1.30839e+02
I1112 07:34:32.284888 140264174335808 run_lib.py:153] step: 611350, training_loss: 1.30644e+02
I1112 07:34:42.379018 140264174335808 run_lib.py:153] step: 611400, training_loss: 1.27346e+02
I1112 07:34:52.443670 140264174335808 run_lib.py:153] step: 611450, training_loss: 9.74450e+01
I1112 07:35:02.447583 140264174335808 run_lib.py:153] step: 611500, training_loss: 1.44144e+02
I1112 07:35:12.839501 140264174335808 run_lib.py:153] step: 611550, training_loss: 1.26095e+02
I1112 07:35:23.643712 140264174335808 run_lib.py:153] step: 611600, training_loss: 1.36262e+02
I1112 07:35:33.368316 140264174335808 run_lib.py:153] step: 611650, training_loss: 9.00098e+01
I1112 07:35:43.606951 140264174335808 run_lib.py:153] step: 611700, training_loss: 1.37902e+02
I1112 07:35:53.187387 140264174335808 run_lib.py:153] step: 611750, training_loss: 1.25255e+02
I1112 07:36:03.084836 140264174335808 run_lib.py:153] step: 611800, training_loss: 1.18325e+02
I1112 07:36:13.147813 140264174335808 run_lib.py:153] step: 611850, training_loss: 1.25193e+02
I1112 07:36:23.074467 140264174335808 run_lib.py:153] step: 611900, training_loss: 1.56875e+02
I1112 07:36:33.010313 140264174335808 run_lib.py:153] step: 611950, training_loss: 1.25027e+02
I1112 07:36:42.620856 140264174335808 run_lib.py:153] step: 612000, training_loss: 1.12751e+02
I1112 07:36:53.241039 140264174335808 run_lib.py:153] step: 612050, training_loss: 1.44503e+02
I1112 07:37:02.727752 140264174335808 run_lib.py:153] step: 612100, training_loss: 1.49476e+02
I1112 07:37:12.877439 140264174335808 run_lib.py:153] step: 612150, training_loss: 1.38810e+02
I1112 07:37:22.528032 140264174335808 run_lib.py:153] step: 612200, training_loss: 1.21037e+02
I1112 07:37:32.525069 140264174335808 run_lib.py:153] step: 612250, training_loss: 1.41115e+02
I1112 07:37:42.334308 140264174335808 run_lib.py:153] step: 612300, training_loss: 1.34261e+02
I1112 07:37:52.024040 140264174335808 run_lib.py:153] step: 612350, training_loss: 1.15400e+02
I1112 07:38:01.669415 140264174335808 run_lib.py:153] step: 612400, training_loss: 1.44361e+02
I1112 07:38:11.136382 140264174335808 run_lib.py:153] step: 612450, training_loss: 1.06305e+02
I1112 07:38:21.116744 140264174335808 run_lib.py:153] step: 612500, training_loss: 1.11574e+02
I1112 07:38:31.026951 140264174335808 run_lib.py:153] step: 612550, training_loss: 1.30798e+02
I1112 07:38:41.103594 140264174335808 run_lib.py:153] step: 612600, training_loss: 1.58423e+02
I1112 07:38:51.139963 140264174335808 run_lib.py:153] step: 612650, training_loss: 1.04726e+02
I1112 07:39:01.526554 140264174335808 run_lib.py:153] step: 612700, training_loss: 1.32177e+02
I1112 07:39:11.268492 140264174335808 run_lib.py:153] step: 612750, training_loss: 1.30158e+02
I1112 07:39:20.946588 140264174335808 run_lib.py:153] step: 612800, training_loss: 1.26712e+02
I1112 07:39:31.216999 140264174335808 run_lib.py:153] step: 612850, training_loss: 1.38069e+02
I1112 07:39:41.935033 140264174335808 run_lib.py:153] step: 612900, training_loss: 1.27555e+02
I1112 07:39:52.295036 140264174335808 run_lib.py:153] step: 612950, training_loss: 1.13026e+02
I1112 07:40:02.250685 140264174335808 run_lib.py:153] step: 613000, training_loss: 1.30556e+02
I1112 07:40:12.302467 140264174335808 run_lib.py:153] step: 613050, training_loss: 9.82928e+01
I1112 07:40:22.068784 140264174335808 run_lib.py:153] step: 613100, training_loss: 1.15375e+02
I1112 07:40:32.176503 140264174335808 run_lib.py:153] step: 613150, training_loss: 1.10361e+02
I1112 07:40:42.685079 140264174335808 run_lib.py:153] step: 613200, training_loss: 1.25302e+02
I1112 07:40:52.998964 140264174335808 run_lib.py:153] step: 613250, training_loss: 1.17469e+02
I1112 07:41:02.600661 140264174335808 run_lib.py:153] step: 613300, training_loss: 1.20571e+02
I1112 07:41:12.384737 140264174335808 run_lib.py:153] step: 613350, training_loss: 1.14429e+02
I1112 07:41:21.865032 140264174335808 run_lib.py:153] step: 613400, training_loss: 1.35219e+02
I1112 07:41:32.299601 140264174335808 run_lib.py:153] step: 613450, training_loss: 1.47035e+02
I1112 07:41:41.845215 140264174335808 run_lib.py:153] step: 613500, training_loss: 1.41132e+02
I1112 07:41:52.323752 140264174335808 run_lib.py:153] step: 613550, training_loss: 1.04927e+02
I1112 07:42:02.735405 140264174335808 run_lib.py:153] step: 613600, training_loss: 1.35251e+02
I1112 07:42:13.066889 140264174335808 run_lib.py:153] step: 613650, training_loss: 1.32084e+02
I1112 07:42:23.616211 140264174335808 run_lib.py:153] step: 613700, training_loss: 1.38236e+02
I1112 07:42:33.748286 140264174335808 run_lib.py:153] step: 613750, training_loss: 1.54486e+02
I1112 07:42:43.547681 140264174335808 run_lib.py:153] step: 613800, training_loss: 1.58947e+02
I1112 07:42:53.440841 140264174335808 run_lib.py:153] step: 613850, training_loss: 1.05596e+02
I1112 07:43:04.175540 140264174335808 run_lib.py:153] step: 613900, training_loss: 1.45280e+02
I1112 07:43:13.729126 140264174335808 run_lib.py:153] step: 613950, training_loss: 1.29132e+02
I1112 07:43:23.773968 140264174335808 run_lib.py:153] step: 614000, training_loss: 1.52392e+02
I1112 07:43:33.305370 140264174335808 run_lib.py:153] step: 614050, training_loss: 1.22970e+02
I1112 07:43:43.311112 140264174335808 run_lib.py:153] step: 614100, training_loss: 1.13711e+02
I1112 07:43:53.062002 140264174335808 run_lib.py:153] step: 614150, training_loss: 1.48969e+02
I1112 07:44:03.093778 140264174335808 run_lib.py:153] step: 614200, training_loss: 1.32042e+02
I1112 07:44:12.993674 140264174335808 run_lib.py:153] step: 614250, training_loss: 1.38758e+02
I1112 07:44:22.621151 140264174335808 run_lib.py:153] step: 614300, training_loss: 1.16225e+02
I1112 07:44:32.851923 140264174335808 run_lib.py:153] step: 614350, training_loss: 1.20830e+02
I1112 07:44:43.226227 140264174335808 run_lib.py:153] step: 614400, training_loss: 1.45147e+02
I1112 07:44:52.668189 140264174335808 run_lib.py:153] step: 614450, training_loss: 1.37521e+02
I1112 07:45:03.200210 140264174335808 run_lib.py:153] step: 614500, training_loss: 1.52296e+02
I1112 07:45:13.936969 140264174335808 run_lib.py:153] step: 614550, training_loss: 1.30419e+02
I1112 07:45:23.777336 140264174335808 run_lib.py:153] step: 614600, training_loss: 1.19670e+02
I1112 07:45:34.299834 140264174335808 run_lib.py:153] step: 614650, training_loss: 1.29699e+02
I1112 07:45:44.689911 140264174335808 run_lib.py:153] step: 614700, training_loss: 1.27462e+02
I1112 07:45:54.761969 140264174335808 run_lib.py:153] step: 614750, training_loss: 1.12505e+02
I1112 07:46:04.260456 140264174335808 run_lib.py:153] step: 614800, training_loss: 1.28309e+02
I1112 07:46:14.691612 140264174335808 run_lib.py:153] step: 614850, training_loss: 1.30441e+02
I1112 07:46:24.947936 140264174335808 run_lib.py:153] step: 614900, training_loss: 1.10509e+02
I1112 07:46:34.769864 140264174335808 run_lib.py:153] step: 614950, training_loss: 1.27654e+02
I1112 07:46:44.953677 140264174335808 run_lib.py:153] step: 615000, training_loss: 1.42251e+02
I1112 07:46:45.091698 140264174335808 run_lib.py:166] step: 615000, eval_loss: 1.32186e+02
I1112 07:46:55.099705 140264174335808 run_lib.py:153] step: 615050, training_loss: 1.22850e+02
I1112 07:47:05.182190 140264174335808 run_lib.py:153] step: 615100, training_loss: 1.04240e+02
I1112 07:47:14.913732 140264174335808 run_lib.py:153] step: 615150, training_loss: 1.20698e+02
I1112 07:47:25.472517 140264174335808 run_lib.py:153] step: 615200, training_loss: 1.27818e+02
I1112 07:47:35.718439 140264174335808 run_lib.py:153] step: 615250, training_loss: 1.08097e+02
I1112 07:47:45.971047 140264174335808 run_lib.py:153] step: 615300, training_loss: 1.18339e+02
I1112 07:47:55.911584 140264174335808 run_lib.py:153] step: 615350, training_loss: 1.14889e+02
I1112 07:48:05.788183 140264174335808 run_lib.py:153] step: 615400, training_loss: 1.19893e+02
I1112 07:48:15.826891 140264174335808 run_lib.py:153] step: 615450, training_loss: 1.32687e+02
I1112 07:48:25.780350 140264174335808 run_lib.py:153] step: 615500, training_loss: 1.03368e+02
I1112 07:48:35.673492 140264174335808 run_lib.py:153] step: 615550, training_loss: 1.29929e+02
I1112 07:48:45.573335 140264174335808 run_lib.py:153] step: 615600, training_loss: 1.30456e+02
I1112 07:48:55.071158 140264174335808 run_lib.py:153] step: 615650, training_loss: 1.19377e+02
I1112 07:49:05.215616 140264174335808 run_lib.py:153] step: 615700, training_loss: 8.99330e+01
I1112 07:49:14.928166 140264174335808 run_lib.py:153] step: 615750, training_loss: 1.21730e+02
I1112 07:49:24.456474 140264174335808 run_lib.py:153] step: 615800, training_loss: 1.25463e+02
I1112 07:49:34.426551 140264174335808 run_lib.py:153] step: 615850, training_loss: 1.31772e+02
I1112 07:49:44.718292 140264174335808 run_lib.py:153] step: 615900, training_loss: 1.24619e+02
I1112 07:49:55.033285 140264174335808 run_lib.py:153] step: 615950, training_loss: 1.44821e+02
I1112 07:50:05.259911 140264174335808 run_lib.py:153] step: 616000, training_loss: 1.15597e+02
I1112 07:50:14.859169 140264174335808 run_lib.py:153] step: 616050, training_loss: 1.50028e+02
I1112 07:50:25.197874 140264174335808 run_lib.py:153] step: 616100, training_loss: 1.23239e+02
I1112 07:50:35.103455 140264174335808 run_lib.py:153] step: 616150, training_loss: 1.22671e+02
I1112 07:50:44.981968 140264174335808 run_lib.py:153] step: 616200, training_loss: 1.35848e+02
I1112 07:50:55.175619 140264174335808 run_lib.py:153] step: 616250, training_loss: 1.30316e+02
I1112 07:51:06.079434 140264174335808 run_lib.py:153] step: 616300, training_loss: 1.34961e+02
I1112 07:51:16.066973 140264174335808 run_lib.py:153] step: 616350, training_loss: 1.08594e+02
I1112 07:51:25.773900 140264174335808 run_lib.py:153] step: 616400, training_loss: 1.08402e+02
I1112 07:51:36.398907 140264174335808 run_lib.py:153] step: 616450, training_loss: 1.32691e+02
I1112 07:51:46.499092 140264174335808 run_lib.py:153] step: 616500, training_loss: 1.01222e+02
I1112 07:51:56.391073 140264174335808 run_lib.py:153] step: 616550, training_loss: 1.38295e+02
I1112 07:52:07.132632 140264174335808 run_lib.py:153] step: 616600, training_loss: 1.33109e+02
I1112 07:52:16.722354 140264174335808 run_lib.py:153] step: 616650, training_loss: 1.40951e+02
I1112 07:52:26.224275 140264174335808 run_lib.py:153] step: 616700, training_loss: 1.14851e+02
I1112 07:52:36.590785 140264174335808 run_lib.py:153] step: 616750, training_loss: 1.38635e+02
I1112 07:52:46.970643 140264174335808 run_lib.py:153] step: 616800, training_loss: 1.11980e+02
I1112 07:52:57.016088 140264174335808 run_lib.py:153] step: 616850, training_loss: 1.24370e+02
I1112 07:53:06.812869 140264174335808 run_lib.py:153] step: 616900, training_loss: 1.51171e+02
I1112 07:53:17.713145 140264174335808 run_lib.py:153] step: 616950, training_loss: 1.06562e+02
I1112 07:53:27.804237 140264174335808 run_lib.py:153] step: 617000, training_loss: 1.26392e+02
I1112 07:53:37.830753 140264174335808 run_lib.py:153] step: 617050, training_loss: 1.40431e+02
I1112 07:53:47.530753 140264174335808 run_lib.py:153] step: 617100, training_loss: 1.50940e+02
I1112 07:53:57.273153 140264174335808 run_lib.py:153] step: 617150, training_loss: 1.26999e+02
I1112 07:54:07.012143 140264174335808 run_lib.py:153] step: 617200, training_loss: 1.33823e+02
I1112 07:54:16.863875 140264174335808 run_lib.py:153] step: 617250, training_loss: 1.50862e+02
I1112 07:54:26.676182 140264174335808 run_lib.py:153] step: 617300, training_loss: 1.33068e+02
I1112 07:54:36.528948 140264174335808 run_lib.py:153] step: 617350, training_loss: 1.27803e+02
I1112 07:54:45.940469 140264174335808 run_lib.py:153] step: 617400, training_loss: 1.25791e+02
I1112 07:54:55.650350 140264174335808 run_lib.py:153] step: 617450, training_loss: 1.19625e+02
I1112 07:55:05.729010 140264174335808 run_lib.py:153] step: 617500, training_loss: 1.22142e+02
I1112 07:55:15.684966 140264174335808 run_lib.py:153] step: 617550, training_loss: 1.04745e+02
I1112 07:55:25.700779 140264174335808 run_lib.py:153] step: 617600, training_loss: 1.17108e+02
I1112 07:55:35.722081 140264174335808 run_lib.py:153] step: 617650, training_loss: 1.18273e+02
I1112 07:55:45.603432 140264174335808 run_lib.py:153] step: 617700, training_loss: 1.47407e+02
I1112 07:55:55.579596 140264174335808 run_lib.py:153] step: 617750, training_loss: 1.08093e+02
I1112 07:56:05.834106 140264174335808 run_lib.py:153] step: 617800, training_loss: 1.51270e+02
I1112 07:56:16.378632 140264174335808 run_lib.py:153] step: 617850, training_loss: 1.35689e+02
I1112 07:56:27.231045 140264174335808 run_lib.py:153] step: 617900, training_loss: 1.46734e+02
I1112 07:56:37.630833 140264174335808 run_lib.py:153] step: 617950, training_loss: 1.28096e+02
I1112 07:56:47.733096 140264174335808 run_lib.py:153] step: 618000, training_loss: 1.17738e+02
I1112 07:56:58.076030 140264174335808 run_lib.py:153] step: 618050, training_loss: 1.20780e+02
I1112 07:57:08.161155 140264174335808 run_lib.py:153] step: 618100, training_loss: 1.40969e+02
I1112 07:57:18.640377 140264174335808 run_lib.py:153] step: 618150, training_loss: 1.23213e+02
I1112 07:57:29.375196 140264174335808 run_lib.py:153] step: 618200, training_loss: 9.79733e+01
I1112 07:57:39.891519 140264174335808 run_lib.py:153] step: 618250, training_loss: 1.14579e+02
I1112 07:57:50.709501 140264174335808 run_lib.py:153] step: 618300, training_loss: 1.21078e+02
I1112 07:58:00.620221 140264174335808 run_lib.py:153] step: 618350, training_loss: 1.40221e+02
I1112 07:58:10.971714 140264174335808 run_lib.py:153] step: 618400, training_loss: 1.22527e+02
I1112 07:58:20.834410 140264174335808 run_lib.py:153] step: 618450, training_loss: 1.45672e+02
I1112 07:58:31.016638 140264174335808 run_lib.py:153] step: 618500, training_loss: 1.36301e+02
I1112 07:58:41.166843 140264174335808 run_lib.py:153] step: 618550, training_loss: 1.20018e+02
I1112 07:58:51.266060 140264174335808 run_lib.py:153] step: 618600, training_loss: 1.06914e+02
I1112 07:59:01.498569 140264174335808 run_lib.py:153] step: 618650, training_loss: 8.56765e+01
I1112 07:59:11.126188 140264174335808 run_lib.py:153] step: 618700, training_loss: 1.19623e+02
I1112 07:59:20.510679 140264174335808 run_lib.py:153] step: 618750, training_loss: 1.34966e+02
I1112 07:59:30.271378 140264174335808 run_lib.py:153] step: 618800, training_loss: 1.12394e+02
I1112 07:59:40.183614 140264174335808 run_lib.py:153] step: 618850, training_loss: 1.38023e+02
I1112 07:59:51.183388 140264174335808 run_lib.py:153] step: 618900, training_loss: 1.21014e+02
I1112 08:00:00.678435 140264174335808 run_lib.py:153] step: 618950, training_loss: 1.35969e+02
I1112 08:00:10.728592 140264174335808 run_lib.py:153] step: 619000, training_loss: 1.29788e+02
I1112 08:00:20.465909 140264174335808 run_lib.py:153] step: 619050, training_loss: 1.34977e+02
I1112 08:00:29.883402 140264174335808 run_lib.py:153] step: 619100, training_loss: 1.22459e+02
I1112 08:00:40.715185 140264174335808 run_lib.py:153] step: 619150, training_loss: 1.36369e+02
I1112 08:00:50.093358 140264174335808 run_lib.py:153] step: 619200, training_loss: 1.25862e+02
I1112 08:00:59.743020 140264174335808 run_lib.py:153] step: 619250, training_loss: 1.26426e+02
I1112 08:01:10.312840 140264174335808 run_lib.py:153] step: 619300, training_loss: 1.02228e+02
I1112 08:01:20.450209 140264174335808 run_lib.py:153] step: 619350, training_loss: 1.51288e+02
I1112 08:01:30.476478 140264174335808 run_lib.py:153] step: 619400, training_loss: 1.17889e+02
I1112 08:01:40.164616 140264174335808 run_lib.py:153] step: 619450, training_loss: 1.34251e+02
I1112 08:01:50.334892 140264174335808 run_lib.py:153] step: 619500, training_loss: 1.34451e+02
I1112 08:02:00.475668 140264174335808 run_lib.py:153] step: 619550, training_loss: 1.07593e+02
I1112 08:02:09.778964 140264174335808 run_lib.py:153] step: 619600, training_loss: 1.10681e+02
I1112 08:02:19.314162 140264174335808 run_lib.py:153] step: 619650, training_loss: 1.03016e+02
I1112 08:02:29.355941 140264174335808 run_lib.py:153] step: 619700, training_loss: 1.04657e+02
I1112 08:02:39.332867 140264174335808 run_lib.py:153] step: 619750, training_loss: 1.09869e+02
I1112 08:02:49.383010 140264174335808 run_lib.py:153] step: 619800, training_loss: 1.29062e+02
I1112 08:02:59.450062 140264174335808 run_lib.py:153] step: 619850, training_loss: 1.14773e+02
I1112 08:03:08.960438 140264174335808 run_lib.py:153] step: 619900, training_loss: 1.38202e+02
I1112 08:03:18.638688 140264174335808 run_lib.py:153] step: 619950, training_loss: 1.35271e+02
I1112 08:03:29.083712 140264174335808 run_lib.py:153] step: 620000, training_loss: 1.46092e+02
I1112 08:03:29.653951 140264174335808 run_lib.py:166] step: 620000, eval_loss: 1.19579e+02
I1112 08:03:39.689885 140264174335808 run_lib.py:153] step: 620050, training_loss: 1.67761e+02
I1112 08:03:49.295320 140264174335808 run_lib.py:153] step: 620100, training_loss: 1.29226e+02
I1112 08:03:58.743291 140264174335808 run_lib.py:153] step: 620150, training_loss: 1.24850e+02
I1112 08:04:08.934409 140264174335808 run_lib.py:153] step: 620200, training_loss: 1.57821e+02
I1112 08:04:19.135461 140264174335808 run_lib.py:153] step: 620250, training_loss: 1.14496e+02
I1112 08:04:29.767280 140264174335808 run_lib.py:153] step: 620300, training_loss: 1.47602e+02
I1112 08:04:39.035624 140264174335808 run_lib.py:153] step: 620350, training_loss: 1.32688e+02
I1112 08:04:49.256757 140264174335808 run_lib.py:153] step: 620400, training_loss: 1.05026e+02
I1112 08:04:58.993359 140264174335808 run_lib.py:153] step: 620450, training_loss: 1.46295e+02
I1112 08:05:09.075180 140264174335808 run_lib.py:153] step: 620500, training_loss: 1.25920e+02
I1112 08:05:18.354511 140264174335808 run_lib.py:153] step: 620550, training_loss: 1.32866e+02
I1112 08:05:28.216364 140264174335808 run_lib.py:153] step: 620600, training_loss: 1.14957e+02
I1112 08:05:38.314184 140264174335808 run_lib.py:153] step: 620650, training_loss: 1.08375e+02
I1112 08:05:48.669936 140264174335808 run_lib.py:153] step: 620700, training_loss: 1.44588e+02
I1112 08:05:59.312015 140264174335808 run_lib.py:153] step: 620750, training_loss: 1.59195e+02
I1112 08:06:09.167963 140264174335808 run_lib.py:153] step: 620800, training_loss: 1.26101e+02
I1112 08:06:19.222506 140264174335808 run_lib.py:153] step: 620850, training_loss: 1.15592e+02
I1112 08:06:29.338270 140264174335808 run_lib.py:153] step: 620900, training_loss: 1.37435e+02
I1112 08:06:39.630954 140264174335808 run_lib.py:153] step: 620950, training_loss: 1.37212e+02
I1112 08:06:49.593350 140264174335808 run_lib.py:153] step: 621000, training_loss: 9.33612e+01
I1112 08:06:59.537705 140264174335808 run_lib.py:153] step: 621050, training_loss: 1.26920e+02
I1112 08:07:09.614718 140264174335808 run_lib.py:153] step: 621100, training_loss: 1.09118e+02
I1112 08:07:19.473095 140264174335808 run_lib.py:153] step: 621150, training_loss: 1.55636e+02
I1112 08:07:29.766161 140264174335808 run_lib.py:153] step: 621200, training_loss: 1.44461e+02
I1112 08:07:40.139175 140264174335808 run_lib.py:153] step: 621250, training_loss: 1.24378e+02
I1112 08:07:49.647506 140264174335808 run_lib.py:153] step: 621300, training_loss: 1.16229e+02
I1112 08:07:59.063411 140264174335808 run_lib.py:153] step: 621350, training_loss: 1.29047e+02
I1112 08:08:09.686378 140264174335808 run_lib.py:153] step: 621400, training_loss: 1.25416e+02
I1112 08:08:19.536576 140264174335808 run_lib.py:153] step: 621450, training_loss: 1.41899e+02
I1112 08:08:29.321679 140264174335808 run_lib.py:153] step: 621500, training_loss: 1.31197e+02
I1112 08:08:38.894530 140264174335808 run_lib.py:153] step: 621550, training_loss: 8.76695e+01
I1112 08:08:48.845566 140264174335808 run_lib.py:153] step: 621600, training_loss: 1.21040e+02
I1112 08:08:58.200190 140264174335808 run_lib.py:153] step: 621650, training_loss: 1.26252e+02
I1112 08:09:08.066624 140264174335808 run_lib.py:153] step: 621700, training_loss: 1.13921e+02
I1112 08:09:18.559741 140264174335808 run_lib.py:153] step: 621750, training_loss: 1.10876e+02
I1112 08:09:28.708401 140264174335808 run_lib.py:153] step: 621800, training_loss: 1.04492e+02
I1112 08:09:39.108681 140264174335808 run_lib.py:153] step: 621850, training_loss: 1.82427e+02
I1112 08:09:49.230117 140264174335808 run_lib.py:153] step: 621900, training_loss: 8.77663e+01
I1112 08:09:59.413252 140264174335808 run_lib.py:153] step: 621950, training_loss: 1.28066e+02
I1112 08:10:10.099509 140264174335808 run_lib.py:153] step: 622000, training_loss: 8.35878e+01
I1112 08:10:19.986717 140264174335808 run_lib.py:153] step: 622050, training_loss: 1.12561e+02
I1112 08:10:29.661981 140264174335808 run_lib.py:153] step: 622100, training_loss: 1.04646e+02
I1112 08:10:40.022769 140264174335808 run_lib.py:153] step: 622150, training_loss: 1.12306e+02
I1112 08:10:50.146265 140264174335808 run_lib.py:153] step: 622200, training_loss: 1.55239e+02
I1112 08:10:59.887525 140264174335808 run_lib.py:153] step: 622250, training_loss: 1.21129e+02
I1112 08:11:09.212818 140264174335808 run_lib.py:153] step: 622300, training_loss: 1.22650e+02
I1112 08:11:19.791687 140264174335808 run_lib.py:153] step: 622350, training_loss: 1.61828e+02
I1112 08:11:29.909934 140264174335808 run_lib.py:153] step: 622400, training_loss: 1.36590e+02
I1112 08:11:40.343141 140264174335808 run_lib.py:153] step: 622450, training_loss: 1.33157e+02
I1112 08:11:50.903815 140264174335808 run_lib.py:153] step: 622500, training_loss: 1.55526e+02
I1112 08:12:00.787320 140264174335808 run_lib.py:153] step: 622550, training_loss: 1.48659e+02
I1112 08:12:10.778801 140264174335808 run_lib.py:153] step: 622600, training_loss: 1.38922e+02
I1112 08:12:21.000889 140264174335808 run_lib.py:153] step: 622650, training_loss: 1.42625e+02
I1112 08:12:30.859532 140264174335808 run_lib.py:153] step: 622700, training_loss: 1.26698e+02
I1112 08:12:40.919808 140264174335808 run_lib.py:153] step: 622750, training_loss: 1.52847e+02
I1112 08:12:50.766953 140264174335808 run_lib.py:153] step: 622800, training_loss: 9.45964e+01
I1112 08:13:01.212494 140264174335808 run_lib.py:153] step: 622850, training_loss: 1.63913e+02
I1112 08:13:11.534748 140264174335808 run_lib.py:153] step: 622900, training_loss: 1.32620e+02
I1112 08:13:21.805830 140264174335808 run_lib.py:153] step: 622950, training_loss: 1.66794e+02
I1112 08:13:32.016304 140264174335808 run_lib.py:153] step: 623000, training_loss: 1.15221e+02
I1112 08:13:41.599725 140264174335808 run_lib.py:153] step: 623050, training_loss: 1.61970e+02
I1112 08:13:51.942386 140264174335808 run_lib.py:153] step: 623100, training_loss: 1.34371e+02
I1112 08:14:01.657372 140264174335808 run_lib.py:153] step: 623150, training_loss: 1.01803e+02
I1112 08:14:11.914228 140264174335808 run_lib.py:153] step: 623200, training_loss: 1.10829e+02
I1112 08:14:21.564230 140264174335808 run_lib.py:153] step: 623250, training_loss: 1.09355e+02
I1112 08:14:31.591790 140264174335808 run_lib.py:153] step: 623300, training_loss: 1.23500e+02
I1112 08:14:41.545983 140264174335808 run_lib.py:153] step: 623350, training_loss: 1.26989e+02
I1112 08:14:51.909328 140264174335808 run_lib.py:153] step: 623400, training_loss: 1.19167e+02
I1112 08:15:02.089227 140264174335808 run_lib.py:153] step: 623450, training_loss: 1.38207e+02
I1112 08:15:11.712490 140264174335808 run_lib.py:153] step: 623500, training_loss: 1.32283e+02
I1112 08:15:21.886382 140264174335808 run_lib.py:153] step: 623550, training_loss: 1.19341e+02
I1112 08:15:32.011321 140264174335808 run_lib.py:153] step: 623600, training_loss: 1.27173e+02
I1112 08:15:41.910172 140264174335808 run_lib.py:153] step: 623650, training_loss: 1.50921e+02
I1112 08:15:51.922116 140264174335808 run_lib.py:153] step: 623700, training_loss: 1.15320e+02
I1112 08:16:02.269621 140264174335808 run_lib.py:153] step: 623750, training_loss: 1.14489e+02
I1112 08:16:12.214566 140264174335808 run_lib.py:153] step: 623800, training_loss: 1.26575e+02
I1112 08:16:22.570362 140264174335808 run_lib.py:153] step: 623850, training_loss: 9.98729e+01
I1112 08:16:33.156781 140264174335808 run_lib.py:153] step: 623900, training_loss: 1.18628e+02
I1112 08:16:43.929531 140264174335808 run_lib.py:153] step: 623950, training_loss: 1.09320e+02
I1112 08:16:54.121164 140264174335808 run_lib.py:153] step: 624000, training_loss: 1.34769e+02
I1112 08:17:03.940748 140264174335808 run_lib.py:153] step: 624050, training_loss: 1.37736e+02
I1112 08:17:13.178622 140264174335808 run_lib.py:153] step: 624100, training_loss: 1.43341e+02
I1112 08:17:23.179771 140264174335808 run_lib.py:153] step: 624150, training_loss: 1.16266e+02
I1112 08:17:32.727331 140264174335808 run_lib.py:153] step: 624200, training_loss: 1.42376e+02
I1112 08:17:42.668546 140264174335808 run_lib.py:153] step: 624250, training_loss: 1.53835e+02
I1112 08:17:52.889551 140264174335808 run_lib.py:153] step: 624300, training_loss: 1.28228e+02
I1112 08:18:03.189515 140264174335808 run_lib.py:153] step: 624350, training_loss: 1.08666e+02
I1112 08:18:13.034714 140264174335808 run_lib.py:153] step: 624400, training_loss: 1.59321e+02
I1112 08:18:22.518553 140264174335808 run_lib.py:153] step: 624450, training_loss: 1.28928e+02
I1112 08:18:32.375086 140264174335808 run_lib.py:153] step: 624500, training_loss: 1.11704e+02
I1112 08:18:42.282357 140264174335808 run_lib.py:153] step: 624550, training_loss: 1.50330e+02
I1112 08:18:53.017153 140264174335808 run_lib.py:153] step: 624600, training_loss: 1.16443e+02
I1112 08:19:03.192168 140264174335808 run_lib.py:153] step: 624650, training_loss: 1.30055e+02
I1112 08:19:13.188899 140264174335808 run_lib.py:153] step: 624700, training_loss: 1.01960e+02
I1112 08:19:23.516928 140264174335808 run_lib.py:153] step: 624750, training_loss: 1.53807e+02
I1112 08:19:33.717944 140264174335808 run_lib.py:153] step: 624800, training_loss: 1.25231e+02
I1112 08:19:43.718186 140264174335808 run_lib.py:153] step: 624850, training_loss: 9.70471e+01
I1112 08:19:53.119796 140264174335808 run_lib.py:153] step: 624900, training_loss: 1.53052e+02
I1112 08:20:03.891253 140264174335808 run_lib.py:153] step: 624950, training_loss: 1.55109e+02
I1112 08:20:13.906808 140264174335808 run_lib.py:153] step: 625000, training_loss: 1.04537e+02
I1112 08:20:14.011593 140264174335808 run_lib.py:166] step: 625000, eval_loss: 1.28381e+02
I1112 08:20:23.922503 140264174335808 run_lib.py:153] step: 625050, training_loss: 1.05488e+02
I1112 08:20:33.908717 140264174335808 run_lib.py:153] step: 625100, training_loss: 1.23119e+02
I1112 08:20:44.261334 140264174335808 run_lib.py:153] step: 625150, training_loss: 1.14838e+02
I1112 08:20:54.659498 140264174335808 run_lib.py:153] step: 625200, training_loss: 1.17276e+02
I1112 08:21:04.202563 140264174335808 run_lib.py:153] step: 625250, training_loss: 1.35660e+02
I1112 08:21:14.535011 140264174335808 run_lib.py:153] step: 625300, training_loss: 1.07075e+02
I1112 08:21:24.712554 140264174335808 run_lib.py:153] step: 625350, training_loss: 9.83108e+01
I1112 08:21:35.037879 140264174335808 run_lib.py:153] step: 625400, training_loss: 1.02634e+02
I1112 08:21:45.143505 140264174335808 run_lib.py:153] step: 625450, training_loss: 1.27481e+02
I1112 08:21:55.690107 140264174335808 run_lib.py:153] step: 625500, training_loss: 1.39721e+02
I1112 08:22:05.342117 140264174335808 run_lib.py:153] step: 625550, training_loss: 1.21172e+02
I1112 08:22:15.214425 140264174335808 run_lib.py:153] step: 625600, training_loss: 1.10827e+02
I1112 08:22:24.885283 140264174335808 run_lib.py:153] step: 625650, training_loss: 1.28827e+02
I1112 08:22:35.387133 140264174335808 run_lib.py:153] step: 625700, training_loss: 1.50597e+02
I1112 08:22:45.001402 140264174335808 run_lib.py:153] step: 625750, training_loss: 1.65810e+02
I1112 08:22:55.108902 140264174335808 run_lib.py:153] step: 625800, training_loss: 1.22657e+02
I1112 08:23:04.539589 140264174335808 run_lib.py:153] step: 625850, training_loss: 1.22767e+02
I1112 08:23:14.626203 140264174335808 run_lib.py:153] step: 625900, training_loss: 1.22268e+02
I1112 08:23:24.627882 140264174335808 run_lib.py:153] step: 625950, training_loss: 1.22054e+02
I1112 08:23:35.205612 140264174335808 run_lib.py:153] step: 626000, training_loss: 1.16555e+02
I1112 08:23:45.444920 140264174335808 run_lib.py:153] step: 626050, training_loss: 1.20398e+02
I1112 08:23:54.831906 140264174335808 run_lib.py:153] step: 626100, training_loss: 1.16131e+02
I1112 08:24:05.116600 140264174335808 run_lib.py:153] step: 626150, training_loss: 1.32618e+02
I1112 08:24:15.114856 140264174335808 run_lib.py:153] step: 626200, training_loss: 1.39582e+02
I1112 08:24:25.657984 140264174335808 run_lib.py:153] step: 626250, training_loss: 1.20357e+02
I1112 08:24:35.146613 140264174335808 run_lib.py:153] step: 626300, training_loss: 1.18357e+02
I1112 08:24:45.449183 140264174335808 run_lib.py:153] step: 626350, training_loss: 1.16566e+02
I1112 08:24:55.409050 140264174335808 run_lib.py:153] step: 626400, training_loss: 1.24319e+02
I1112 08:25:05.812190 140264174335808 run_lib.py:153] step: 626450, training_loss: 1.33131e+02
I1112 08:25:15.531350 140264174335808 run_lib.py:153] step: 626500, training_loss: 1.13787e+02
I1112 08:25:25.797403 140264174335808 run_lib.py:153] step: 626550, training_loss: 1.21051e+02
I1112 08:25:35.428051 140264174335808 run_lib.py:153] step: 626600, training_loss: 1.56467e+02
I1112 08:25:45.022080 140264174335808 run_lib.py:153] step: 626650, training_loss: 1.22894e+02
I1112 08:25:54.620234 140264174335808 run_lib.py:153] step: 626700, training_loss: 1.19572e+02
I1112 08:26:04.425821 140264174335808 run_lib.py:153] step: 626750, training_loss: 1.05521e+02
I1112 08:26:14.357284 140264174335808 run_lib.py:153] step: 626800, training_loss: 1.35919e+02
I1112 08:26:24.224623 140264174335808 run_lib.py:153] step: 626850, training_loss: 1.17432e+02
I1112 08:26:33.580684 140264174335808 run_lib.py:153] step: 626900, training_loss: 1.29742e+02
I1112 08:26:43.603609 140264174335808 run_lib.py:153] step: 626950, training_loss: 1.54325e+02
I1112 08:26:52.900692 140264174335808 run_lib.py:153] step: 627000, training_loss: 1.10178e+02
I1112 08:27:02.921519 140264174335808 run_lib.py:153] step: 627050, training_loss: 1.53447e+02
I1112 08:27:12.952680 140264174335808 run_lib.py:153] step: 627100, training_loss: 1.27482e+02
I1112 08:27:23.271358 140264174335808 run_lib.py:153] step: 627150, training_loss: 1.54867e+02
I1112 08:27:32.763562 140264174335808 run_lib.py:153] step: 627200, training_loss: 1.46675e+02
I1112 08:27:42.131181 140264174335808 run_lib.py:153] step: 627250, training_loss: 1.11441e+02
I1112 08:27:52.244837 140264174335808 run_lib.py:153] step: 627300, training_loss: 1.46095e+02
I1112 08:28:01.536607 140264174335808 run_lib.py:153] step: 627350, training_loss: 1.25305e+02
I1112 08:28:11.558728 140264174335808 run_lib.py:153] step: 627400, training_loss: 1.29821e+02
I1112 08:28:20.932409 140264174335808 run_lib.py:153] step: 627450, training_loss: 1.11567e+02
I1112 08:28:30.944235 140264174335808 run_lib.py:153] step: 627500, training_loss: 1.33523e+02
I1112 08:28:40.448430 140264174335808 run_lib.py:153] step: 627550, training_loss: 1.23072e+02
I1112 08:28:50.518690 140264174335808 run_lib.py:153] step: 627600, training_loss: 1.24512e+02
I1112 08:29:00.807346 140264174335808 run_lib.py:153] step: 627650, training_loss: 1.36304e+02
I1112 08:29:10.828119 140264174335808 run_lib.py:153] step: 627700, training_loss: 1.34397e+02
I1112 08:29:21.590581 140264174335808 run_lib.py:153] step: 627750, training_loss: 1.55846e+02
I1112 08:29:31.803446 140264174335808 run_lib.py:153] step: 627800, training_loss: 1.29737e+02
I1112 08:29:41.174256 140264174335808 run_lib.py:153] step: 627850, training_loss: 9.75562e+01
I1112 08:29:50.678890 140264174335808 run_lib.py:153] step: 627900, training_loss: 1.14117e+02
I1112 08:30:00.634697 140264174335808 run_lib.py:153] step: 627950, training_loss: 1.22475e+02
I1112 08:30:11.206863 140264174335808 run_lib.py:153] step: 628000, training_loss: 1.65222e+02
I1112 08:30:21.745434 140264174335808 run_lib.py:153] step: 628050, training_loss: 1.21485e+02
I1112 08:30:31.305457 140264174335808 run_lib.py:153] step: 628100, training_loss: 1.20940e+02
I1112 08:30:41.383495 140264174335808 run_lib.py:153] step: 628150, training_loss: 1.20301e+02
I1112 08:30:51.289365 140264174335808 run_lib.py:153] step: 628200, training_loss: 1.33869e+02
I1112 08:31:01.433704 140264174335808 run_lib.py:153] step: 628250, training_loss: 1.48351e+02
I1112 08:31:12.042805 140264174335808 run_lib.py:153] step: 628300, training_loss: 1.24070e+02
I1112 08:31:22.857196 140264174335808 run_lib.py:153] step: 628350, training_loss: 1.48932e+02
I1112 08:31:34.020624 140264174335808 run_lib.py:153] step: 628400, training_loss: 1.27851e+02
I1112 08:31:43.796198 140264174335808 run_lib.py:153] step: 628450, training_loss: 1.15615e+02
I1112 08:31:53.918282 140264174335808 run_lib.py:153] step: 628500, training_loss: 1.00725e+02
I1112 08:32:03.661749 140264174335808 run_lib.py:153] step: 628550, training_loss: 1.19667e+02
I1112 08:32:13.120868 140264174335808 run_lib.py:153] step: 628600, training_loss: 1.01560e+02
I1112 08:32:23.423237 140264174335808 run_lib.py:153] step: 628650, training_loss: 1.23082e+02
I1112 08:32:33.399042 140264174335808 run_lib.py:153] step: 628700, training_loss: 1.48406e+02
I1112 08:32:43.035137 140264174335808 run_lib.py:153] step: 628750, training_loss: 1.39107e+02
I1112 08:32:53.199066 140264174335808 run_lib.py:153] step: 628800, training_loss: 1.53042e+02
I1112 08:33:03.380853 140264174335808 run_lib.py:153] step: 628850, training_loss: 1.24389e+02
I1112 08:33:13.450593 140264174335808 run_lib.py:153] step: 628900, training_loss: 1.15075e+02
I1112 08:33:24.082254 140264174335808 run_lib.py:153] step: 628950, training_loss: 1.28172e+02
I1112 08:33:34.380648 140264174335808 run_lib.py:153] step: 629000, training_loss: 9.72482e+01
I1112 08:33:44.386982 140264174335808 run_lib.py:153] step: 629050, training_loss: 1.33848e+02
I1112 08:33:54.775000 140264174335808 run_lib.py:153] step: 629100, training_loss: 1.24191e+02
I1112 08:34:04.710249 140264174335808 run_lib.py:153] step: 629150, training_loss: 1.28746e+02
I1112 08:34:15.674924 140264174335808 run_lib.py:153] step: 629200, training_loss: 1.09188e+02
I1112 08:34:26.064703 140264174335808 run_lib.py:153] step: 629250, training_loss: 1.17911e+02
I1112 08:34:36.004799 140264174335808 run_lib.py:153] step: 629300, training_loss: 1.08145e+02
I1112 08:34:46.763810 140264174335808 run_lib.py:153] step: 629350, training_loss: 1.31232e+02
I1112 08:34:56.747312 140264174335808 run_lib.py:153] step: 629400, training_loss: 1.25314e+02
I1112 08:35:06.933748 140264174335808 run_lib.py:153] step: 629450, training_loss: 1.26894e+02
I1112 08:35:17.292274 140264174335808 run_lib.py:153] step: 629500, training_loss: 1.06508e+02
I1112 08:35:27.911834 140264174335808 run_lib.py:153] step: 629550, training_loss: 1.22109e+02
I1112 08:35:38.251051 140264174335808 run_lib.py:153] step: 629600, training_loss: 1.29171e+02
I1112 08:35:49.232955 140264174335808 run_lib.py:153] step: 629650, training_loss: 1.06335e+02
I1112 08:35:59.405334 140264174335808 run_lib.py:153] step: 629700, training_loss: 1.14086e+02
I1112 08:36:08.687425 140264174335808 run_lib.py:153] step: 629750, training_loss: 1.35722e+02
I1112 08:36:18.174216 140264174335808 run_lib.py:153] step: 629800, training_loss: 1.37178e+02
I1112 08:36:28.384879 140264174335808 run_lib.py:153] step: 629850, training_loss: 1.23446e+02
I1112 08:36:38.873909 140264174335808 run_lib.py:153] step: 629900, training_loss: 1.27706e+02
I1112 08:36:48.440771 140264174335808 run_lib.py:153] step: 629950, training_loss: 1.32802e+02
I1112 08:36:57.974668 140264174335808 run_lib.py:153] step: 630000, training_loss: 1.18494e+02
I1112 08:36:58.520469 140264174335808 run_lib.py:166] step: 630000, eval_loss: 1.42866e+02
I1112 08:37:08.207329 140264174335808 run_lib.py:153] step: 630050, training_loss: 1.34435e+02
I1112 08:37:18.442195 140264174335808 run_lib.py:153] step: 630100, training_loss: 1.41899e+02
I1112 08:37:28.421467 140264174335808 run_lib.py:153] step: 630150, training_loss: 1.44295e+02
I1112 08:37:38.731757 140264174335808 run_lib.py:153] step: 630200, training_loss: 1.15037e+02
I1112 08:37:48.917919 140264174335808 run_lib.py:153] step: 630250, training_loss: 1.47561e+02
I1112 08:37:58.209132 140264174335808 run_lib.py:153] step: 630300, training_loss: 1.10813e+02
I1112 08:38:08.562215 140264174335808 run_lib.py:153] step: 630350, training_loss: 1.62008e+02
I1112 08:38:18.635612 140264174335808 run_lib.py:153] step: 630400, training_loss: 1.62725e+02
I1112 08:38:28.324994 140264174335808 run_lib.py:153] step: 630450, training_loss: 1.06994e+02
I1112 08:38:38.251839 140264174335808 run_lib.py:153] step: 630500, training_loss: 1.09358e+02
I1112 08:38:48.351807 140264174335808 run_lib.py:153] step: 630550, training_loss: 1.52374e+02
I1112 08:38:57.986624 140264174335808 run_lib.py:153] step: 630600, training_loss: 1.24430e+02
I1112 08:39:08.186078 140264174335808 run_lib.py:153] step: 630650, training_loss: 1.44598e+02
I1112 08:39:17.990952 140264174335808 run_lib.py:153] step: 630700, training_loss: 1.18344e+02
I1112 08:39:28.204216 140264174335808 run_lib.py:153] step: 630750, training_loss: 1.19513e+02
I1112 08:39:39.075348 140264174335808 run_lib.py:153] step: 630800, training_loss: 1.35482e+02
I1112 08:39:48.879951 140264174335808 run_lib.py:153] step: 630850, training_loss: 1.43190e+02
I1112 08:39:58.544137 140264174335808 run_lib.py:153] step: 630900, training_loss: 1.50691e+02
I1112 08:40:07.914333 140264174335808 run_lib.py:153] step: 630950, training_loss: 1.23585e+02
I1112 08:40:17.567341 140264174335808 run_lib.py:153] step: 631000, training_loss: 9.31597e+01
I1112 08:40:27.157232 140264174335808 run_lib.py:153] step: 631050, training_loss: 1.28777e+02
I1112 08:40:36.786277 140264174335808 run_lib.py:153] step: 631100, training_loss: 1.33617e+02
I1112 08:40:47.472850 140264174335808 run_lib.py:153] step: 631150, training_loss: 1.32350e+02
I1112 08:40:57.768773 140264174335808 run_lib.py:153] step: 631200, training_loss: 1.18181e+02
I1112 08:41:07.607023 140264174335808 run_lib.py:153] step: 631250, training_loss: 1.20084e+02
I1112 08:41:18.048165 140264174335808 run_lib.py:153] step: 631300, training_loss: 1.36919e+02
I1112 08:41:28.229172 140264174335808 run_lib.py:153] step: 631350, training_loss: 1.19908e+02
I1112 08:41:38.698534 140264174335808 run_lib.py:153] step: 631400, training_loss: 1.10338e+02
I1112 08:41:49.203938 140264174335808 run_lib.py:153] step: 631450, training_loss: 1.26615e+02
I1112 08:41:58.769579 140264174335808 run_lib.py:153] step: 631500, training_loss: 1.37226e+02
I1112 08:42:08.874174 140264174335808 run_lib.py:153] step: 631550, training_loss: 1.31363e+02
I1112 08:42:19.429551 140264174335808 run_lib.py:153] step: 631600, training_loss: 1.05893e+02
I1112 08:42:28.911372 140264174335808 run_lib.py:153] step: 631650, training_loss: 1.23257e+02
I1112 08:42:39.433212 140264174335808 run_lib.py:153] step: 631700, training_loss: 1.36100e+02
I1112 08:42:49.317003 140264174335808 run_lib.py:153] step: 631750, training_loss: 1.00427e+02
I1112 08:42:59.693488 140264174335808 run_lib.py:153] step: 631800, training_loss: 1.27248e+02
I1112 08:43:09.387932 140264174335808 run_lib.py:153] step: 631850, training_loss: 1.50746e+02
I1112 08:43:19.351351 140264174335808 run_lib.py:153] step: 631900, training_loss: 1.07934e+02
I1112 08:43:29.475903 140264174335808 run_lib.py:153] step: 631950, training_loss: 1.27420e+02
I1112 08:43:39.680456 140264174335808 run_lib.py:153] step: 632000, training_loss: 1.16557e+02
I1112 08:43:49.358803 140264174335808 run_lib.py:153] step: 632050, training_loss: 1.13268e+02
I1112 08:43:59.080477 140264174335808 run_lib.py:153] step: 632100, training_loss: 1.05795e+02
I1112 08:44:09.198512 140264174335808 run_lib.py:153] step: 632150, training_loss: 1.18574e+02
I1112 08:44:18.923443 140264174335808 run_lib.py:153] step: 632200, training_loss: 1.39087e+02
I1112 08:44:29.121435 140264174335808 run_lib.py:153] step: 632250, training_loss: 1.16961e+02
I1112 08:44:38.775459 140264174335808 run_lib.py:153] step: 632300, training_loss: 1.10048e+02
I1112 08:44:48.828589 140264174335808 run_lib.py:153] step: 632350, training_loss: 1.14694e+02
I1112 08:44:59.368130 140264174335808 run_lib.py:153] step: 632400, training_loss: 1.11387e+02
I1112 08:45:09.696721 140264174335808 run_lib.py:153] step: 632450, training_loss: 1.40386e+02
I1112 08:45:20.128570 140264174335808 run_lib.py:153] step: 632500, training_loss: 1.34609e+02
I1112 08:45:30.208734 140264174335808 run_lib.py:153] step: 632550, training_loss: 1.10163e+02
I1112 08:45:40.345835 140264174335808 run_lib.py:153] step: 632600, training_loss: 1.03969e+02
I1112 08:45:50.591170 140264174335808 run_lib.py:153] step: 632650, training_loss: 1.18079e+02
I1112 08:46:01.229946 140264174335808 run_lib.py:153] step: 632700, training_loss: 1.10525e+02
I1112 08:46:11.337239 140264174335808 run_lib.py:153] step: 632750, training_loss: 1.25527e+02
I1112 08:46:21.125335 140264174335808 run_lib.py:153] step: 632800, training_loss: 1.37382e+02
I1112 08:46:30.938602 140264174335808 run_lib.py:153] step: 632850, training_loss: 1.11123e+02
I1112 08:46:41.117958 140264174335808 run_lib.py:153] step: 632900, training_loss: 1.10571e+02
I1112 08:46:51.321044 140264174335808 run_lib.py:153] step: 632950, training_loss: 1.27903e+02
I1112 08:47:01.331655 140264174335808 run_lib.py:153] step: 633000, training_loss: 1.22961e+02
I1112 08:47:11.657282 140264174335808 run_lib.py:153] step: 633050, training_loss: 1.18905e+02
I1112 08:47:21.775437 140264174335808 run_lib.py:153] step: 633100, training_loss: 1.30543e+02
I1112 08:47:31.444912 140264174335808 run_lib.py:153] step: 633150, training_loss: 1.12582e+02
I1112 08:47:41.451840 140264174335808 run_lib.py:153] step: 633200, training_loss: 1.25972e+02
I1112 08:47:51.332901 140264174335808 run_lib.py:153] step: 633250, training_loss: 1.42601e+02
I1112 08:48:01.040103 140264174335808 run_lib.py:153] step: 633300, training_loss: 1.47314e+02
I1112 08:48:11.376498 140264174335808 run_lib.py:153] step: 633350, training_loss: 9.28923e+01
I1112 08:48:21.385174 140264174335808 run_lib.py:153] step: 633400, training_loss: 1.47532e+02
I1112 08:48:30.954299 140264174335808 run_lib.py:153] step: 633450, training_loss: 1.34721e+02
I1112 08:48:41.500520 140264174335808 run_lib.py:153] step: 633500, training_loss: 1.32337e+02
I1112 08:48:52.131280 140264174335808 run_lib.py:153] step: 633550, training_loss: 1.32626e+02
I1112 08:49:02.636030 140264174335808 run_lib.py:153] step: 633600, training_loss: 1.04501e+02
I1112 08:49:13.187432 140264174335808 run_lib.py:153] step: 633650, training_loss: 1.20630e+02
I1112 08:49:23.615644 140264174335808 run_lib.py:153] step: 633700, training_loss: 1.26805e+02
I1112 08:49:34.285927 140264174335808 run_lib.py:153] step: 633750, training_loss: 1.23238e+02
I1112 08:49:43.948129 140264174335808 run_lib.py:153] step: 633800, training_loss: 1.16209e+02
I1112 08:49:53.826995 140264174335808 run_lib.py:153] step: 633850, training_loss: 1.15501e+02
I1112 08:50:04.116117 140264174335808 run_lib.py:153] step: 633900, training_loss: 1.30036e+02
I1112 08:50:14.203327 140264174335808 run_lib.py:153] step: 633950, training_loss: 1.15436e+02
I1112 08:50:24.258672 140264174335808 run_lib.py:153] step: 634000, training_loss: 1.36682e+02
I1112 08:50:34.287535 140264174335808 run_lib.py:153] step: 634050, training_loss: 1.40091e+02
I1112 08:50:44.533734 140264174335808 run_lib.py:153] step: 634100, training_loss: 1.27281e+02
I1112 08:50:54.919349 140264174335808 run_lib.py:153] step: 634150, training_loss: 1.17887e+02
I1112 08:51:04.781319 140264174335808 run_lib.py:153] step: 634200, training_loss: 1.25279e+02
I1112 08:51:14.755540 140264174335808 run_lib.py:153] step: 634250, training_loss: 1.31438e+02
I1112 08:51:25.097093 140264174335808 run_lib.py:153] step: 634300, training_loss: 1.34175e+02
I1112 08:51:35.229047 140264174335808 run_lib.py:153] step: 634350, training_loss: 1.47392e+02
I1112 08:51:45.182380 140264174335808 run_lib.py:153] step: 634400, training_loss: 1.44096e+02
I1112 08:51:55.427611 140264174335808 run_lib.py:153] step: 634450, training_loss: 1.15206e+02
I1112 08:52:05.046723 140264174335808 run_lib.py:153] step: 634500, training_loss: 1.36030e+02
I1112 08:52:15.025329 140264174335808 run_lib.py:153] step: 634550, training_loss: 1.18061e+02
I1112 08:52:25.670785 140264174335808 run_lib.py:153] step: 634600, training_loss: 1.17516e+02
I1112 08:52:36.106645 140264174335808 run_lib.py:153] step: 634650, training_loss: 1.34886e+02
I1112 08:52:46.204381 140264174335808 run_lib.py:153] step: 634700, training_loss: 1.18767e+02
I1112 08:52:56.422648 140264174335808 run_lib.py:153] step: 634750, training_loss: 9.78144e+01
I1112 08:53:06.149244 140264174335808 run_lib.py:153] step: 634800, training_loss: 1.40133e+02
I1112 08:53:15.738998 140264174335808 run_lib.py:153] step: 634850, training_loss: 1.26198e+02
I1112 08:53:25.516625 140264174335808 run_lib.py:153] step: 634900, training_loss: 1.26971e+02
I1112 08:53:35.322928 140264174335808 run_lib.py:153] step: 634950, training_loss: 1.25683e+02
I1112 08:53:45.585280 140264174335808 run_lib.py:153] step: 635000, training_loss: 1.37119e+02
I1112 08:53:45.689650 140264174335808 run_lib.py:166] step: 635000, eval_loss: 1.19329e+02
I1112 08:53:55.863832 140264174335808 run_lib.py:153] step: 635050, training_loss: 1.29857e+02
I1112 08:54:05.533906 140264174335808 run_lib.py:153] step: 635100, training_loss: 1.28332e+02
I1112 08:54:15.728178 140264174335808 run_lib.py:153] step: 635150, training_loss: 1.26568e+02
I1112 08:54:25.625988 140264174335808 run_lib.py:153] step: 635200, training_loss: 1.10602e+02
I1112 08:54:35.321244 140264174335808 run_lib.py:153] step: 635250, training_loss: 1.28218e+02
I1112 08:54:45.916960 140264174335808 run_lib.py:153] step: 635300, training_loss: 1.29091e+02
I1112 08:54:56.552905 140264174335808 run_lib.py:153] step: 635350, training_loss: 1.56060e+02
I1112 08:55:07.317993 140264174335808 run_lib.py:153] step: 635400, training_loss: 1.24060e+02
I1112 08:55:17.410212 140264174335808 run_lib.py:153] step: 635450, training_loss: 1.32138e+02
I1112 08:55:27.346361 140264174335808 run_lib.py:153] step: 635500, training_loss: 1.01479e+02
I1112 08:55:37.351013 140264174335808 run_lib.py:153] step: 635550, training_loss: 1.31400e+02
I1112 08:55:47.060295 140264174335808 run_lib.py:153] step: 635600, training_loss: 1.39020e+02
I1112 08:55:56.396718 140264174335808 run_lib.py:153] step: 635650, training_loss: 1.12238e+02
I1112 08:56:06.259198 140264174335808 run_lib.py:153] step: 635700, training_loss: 1.59078e+02
I1112 08:56:16.286138 140264174335808 run_lib.py:153] step: 635750, training_loss: 1.31003e+02
I1112 08:56:26.308735 140264174335808 run_lib.py:153] step: 635800, training_loss: 9.36975e+01
I1112 08:56:36.995589 140264174335808 run_lib.py:153] step: 635850, training_loss: 1.09950e+02
I1112 08:56:47.477206 140264174335808 run_lib.py:153] step: 635900, training_loss: 1.27934e+02
I1112 08:56:57.501458 140264174335808 run_lib.py:153] step: 635950, training_loss: 1.08370e+02
I1112 08:57:07.710601 140264174335808 run_lib.py:153] step: 636000, training_loss: 1.23951e+02
I1112 08:57:17.364059 140264174335808 run_lib.py:153] step: 636050, training_loss: 1.08344e+02
I1112 08:57:27.172271 140264174335808 run_lib.py:153] step: 636100, training_loss: 1.28424e+02
I1112 08:57:37.828022 140264174335808 run_lib.py:153] step: 636150, training_loss: 1.20235e+02
I1112 08:57:47.918375 140264174335808 run_lib.py:153] step: 636200, training_loss: 1.05108e+02
I1112 08:57:58.116489 140264174335808 run_lib.py:153] step: 636250, training_loss: 1.19436e+02
I1112 08:58:08.654880 140264174335808 run_lib.py:153] step: 636300, training_loss: 1.05711e+02
I1112 08:58:18.529202 140264174335808 run_lib.py:153] step: 636350, training_loss: 1.21018e+02
I1112 08:58:28.397728 140264174335808 run_lib.py:153] step: 636400, training_loss: 1.43956e+02
I1112 08:58:38.701954 140264174335808 run_lib.py:153] step: 636450, training_loss: 1.95061e+02
I1112 08:58:48.466249 140264174335808 run_lib.py:153] step: 636500, training_loss: 1.37157e+02
I1112 08:58:57.898636 140264174335808 run_lib.py:153] step: 636550, training_loss: 9.68420e+01
I1112 08:59:07.792555 140264174335808 run_lib.py:153] step: 636600, training_loss: 1.87187e+02
I1112 08:59:18.403083 140264174335808 run_lib.py:153] step: 636650, training_loss: 1.35170e+02
I1112 08:59:28.473659 140264174335808 run_lib.py:153] step: 636700, training_loss: 1.36076e+02
I1112 08:59:38.068392 140264174335808 run_lib.py:153] step: 636750, training_loss: 1.10606e+02
I1112 08:59:47.993553 140264174335808 run_lib.py:153] step: 636800, training_loss: 1.65394e+02
I1112 08:59:58.374868 140264174335808 run_lib.py:153] step: 636850, training_loss: 1.22226e+02
I1112 09:00:08.679574 140264174335808 run_lib.py:153] step: 636900, training_loss: 1.07587e+02
I1112 09:00:18.653593 140264174335808 run_lib.py:153] step: 636950, training_loss: 1.44247e+02
I1112 09:00:28.795463 140264174335808 run_lib.py:153] step: 637000, training_loss: 1.40745e+02
I1112 09:00:38.945454 140264174335808 run_lib.py:153] step: 637050, training_loss: 1.18274e+02
I1112 09:00:49.230889 140264174335808 run_lib.py:153] step: 637100, training_loss: 1.37188e+02
I1112 09:00:59.360189 140264174335808 run_lib.py:153] step: 637150, training_loss: 1.20931e+02
I1112 09:01:09.851263 140264174335808 run_lib.py:153] step: 637200, training_loss: 1.17428e+02
I1112 09:01:19.964999 140264174335808 run_lib.py:153] step: 637250, training_loss: 1.15072e+02
I1112 09:01:30.397947 140264174335808 run_lib.py:153] step: 637300, training_loss: 1.22604e+02
I1112 09:01:40.292906 140264174335808 run_lib.py:153] step: 637350, training_loss: 1.30015e+02
I1112 09:01:49.863481 140264174335808 run_lib.py:153] step: 637400, training_loss: 1.20394e+02
I1112 09:01:59.817972 140264174335808 run_lib.py:153] step: 637450, training_loss: 1.18133e+02
I1112 09:02:10.201539 140264174335808 run_lib.py:153] step: 637500, training_loss: 1.23789e+02
I1112 09:02:20.664517 140264174335808 run_lib.py:153] step: 637550, training_loss: 1.70443e+02
I1112 09:02:30.602236 140264174335808 run_lib.py:153] step: 637600, training_loss: 1.07919e+02
I1112 09:02:39.889306 140264174335808 run_lib.py:153] step: 637650, training_loss: 1.14295e+02
I1112 09:02:49.555544 140264174335808 run_lib.py:153] step: 637700, training_loss: 1.10003e+02
I1112 09:02:59.564181 140264174335808 run_lib.py:153] step: 637750, training_loss: 1.42026e+02
I1112 09:03:10.018998 140264174335808 run_lib.py:153] step: 637800, training_loss: 1.14543e+02
I1112 09:03:19.897233 140264174335808 run_lib.py:153] step: 637850, training_loss: 1.25120e+02
I1112 09:03:30.124459 140264174335808 run_lib.py:153] step: 637900, training_loss: 1.23429e+02
I1112 09:03:40.681912 140264174335808 run_lib.py:153] step: 637950, training_loss: 1.10003e+02
I1112 09:03:51.433297 140264174335808 run_lib.py:153] step: 638000, training_loss: 1.23916e+02
I1112 09:04:01.919450 140264174335808 run_lib.py:153] step: 638050, training_loss: 1.52691e+02
I1112 09:04:12.271502 140264174335808 run_lib.py:153] step: 638100, training_loss: 1.55937e+02
I1112 09:04:22.093916 140264174335808 run_lib.py:153] step: 638150, training_loss: 1.04100e+02
I1112 09:04:32.203806 140264174335808 run_lib.py:153] step: 638200, training_loss: 1.53130e+02
I1112 09:04:42.031505 140264174335808 run_lib.py:153] step: 638250, training_loss: 1.07187e+02
I1112 09:04:52.022649 140264174335808 run_lib.py:153] step: 638300, training_loss: 1.32282e+02
I1112 09:05:02.691412 140264174335808 run_lib.py:153] step: 638350, training_loss: 1.19795e+02
I1112 09:05:12.361002 140264174335808 run_lib.py:153] step: 638400, training_loss: 1.53315e+02
I1112 09:05:22.398986 140264174335808 run_lib.py:153] step: 638450, training_loss: 1.34436e+02
I1112 09:05:32.389517 140264174335808 run_lib.py:153] step: 638500, training_loss: 1.14774e+02
I1112 09:05:42.203063 140264174335808 run_lib.py:153] step: 638550, training_loss: 1.49732e+02
I1112 09:05:52.121891 140264174335808 run_lib.py:153] step: 638600, training_loss: 1.38291e+02
I1112 09:06:02.205711 140264174335808 run_lib.py:153] step: 638650, training_loss: 1.36336e+02
I1112 09:06:12.093005 140264174335808 run_lib.py:153] step: 638700, training_loss: 1.47175e+02
I1112 09:06:22.736315 140264174335808 run_lib.py:153] step: 638750, training_loss: 1.40679e+02
I1112 09:06:33.061269 140264174335808 run_lib.py:153] step: 638800, training_loss: 1.12182e+02
I1112 09:06:42.804357 140264174335808 run_lib.py:153] step: 638850, training_loss: 1.33349e+02
I1112 09:06:53.067331 140264174335808 run_lib.py:153] step: 638900, training_loss: 1.89942e+02
I1112 09:07:02.933699 140264174335808 run_lib.py:153] step: 638950, training_loss: 1.19245e+02
I1112 09:07:12.971113 140264174335808 run_lib.py:153] step: 639000, training_loss: 1.16553e+02
I1112 09:07:22.621971 140264174335808 run_lib.py:153] step: 639050, training_loss: 1.24135e+02
I1112 09:07:32.246283 140264174335808 run_lib.py:153] step: 639100, training_loss: 1.50220e+02
I1112 09:07:41.546152 140264174335808 run_lib.py:153] step: 639150, training_loss: 1.45573e+02
I1112 09:07:51.086266 140264174335808 run_lib.py:153] step: 639200, training_loss: 1.13385e+02
I1112 09:08:00.888907 140264174335808 run_lib.py:153] step: 639250, training_loss: 1.25355e+02
I1112 09:08:10.274042 140264174335808 run_lib.py:153] step: 639300, training_loss: 1.69957e+02
I1112 09:08:20.049316 140264174335808 run_lib.py:153] step: 639350, training_loss: 1.16953e+02
I1112 09:08:29.583626 140264174335808 run_lib.py:153] step: 639400, training_loss: 1.10210e+02
I1112 09:08:39.946217 140264174335808 run_lib.py:153] step: 639450, training_loss: 1.41144e+02
I1112 09:08:49.994483 140264174335808 run_lib.py:153] step: 639500, training_loss: 1.38588e+02
I1112 09:08:59.274648 140264174335808 run_lib.py:153] step: 639550, training_loss: 1.53836e+02
I1112 09:09:08.834810 140264174335808 run_lib.py:153] step: 639600, training_loss: 1.23221e+02
I1112 09:09:18.319652 140264174335808 run_lib.py:153] step: 639650, training_loss: 1.23097e+02
I1112 09:09:27.898003 140264174335808 run_lib.py:153] step: 639700, training_loss: 1.03239e+02
I1112 09:09:37.285074 140264174335808 run_lib.py:153] step: 639750, training_loss: 1.32003e+02
I1112 09:09:46.906909 140264174335808 run_lib.py:153] step: 639800, training_loss: 1.54948e+02
I1112 09:09:56.402703 140264174335808 run_lib.py:153] step: 639850, training_loss: 1.54028e+02
I1112 09:10:05.742926 140264174335808 run_lib.py:153] step: 639900, training_loss: 1.49440e+02
I1112 09:10:15.522479 140264174335808 run_lib.py:153] step: 639950, training_loss: 1.21556e+02
I1112 09:10:25.678779 140264174335808 run_lib.py:153] step: 640000, training_loss: 1.36765e+02
I1112 09:10:26.255739 140264174335808 run_lib.py:166] step: 640000, eval_loss: 1.15054e+02
I1112 09:10:37.021382 140264174335808 run_lib.py:153] step: 640050, training_loss: 1.05404e+02
I1112 09:10:46.972908 140264174335808 run_lib.py:153] step: 640100, training_loss: 1.32448e+02
I1112 09:10:57.186541 140264174335808 run_lib.py:153] step: 640150, training_loss: 1.23160e+02
I1112 09:11:07.045159 140264174335808 run_lib.py:153] step: 640200, training_loss: 1.16355e+02
I1112 09:11:17.529759 140264174335808 run_lib.py:153] step: 640250, training_loss: 1.19684e+02
I1112 09:11:27.769659 140264174335808 run_lib.py:153] step: 640300, training_loss: 1.61287e+02
I1112 09:11:37.130249 140264174335808 run_lib.py:153] step: 640350, training_loss: 1.04365e+02
I1112 09:11:47.041247 140264174335808 run_lib.py:153] step: 640400, training_loss: 1.23633e+02
I1112 09:11:56.918363 140264174335808 run_lib.py:153] step: 640450, training_loss: 1.07775e+02
I1112 09:12:07.015509 140264174335808 run_lib.py:153] step: 640500, training_loss: 9.02197e+01
I1112 09:12:16.583915 140264174335808 run_lib.py:153] step: 640550, training_loss: 9.18193e+01
I1112 09:12:26.714828 140264174335808 run_lib.py:153] step: 640600, training_loss: 1.43323e+02
I1112 09:12:36.685452 140264174335808 run_lib.py:153] step: 640650, training_loss: 1.48697e+02
I1112 09:12:46.931680 140264174335808 run_lib.py:153] step: 640700, training_loss: 1.31260e+02
I1112 09:12:56.621380 140264174335808 run_lib.py:153] step: 640750, training_loss: 1.16466e+02
I1112 09:13:06.107716 140264174335808 run_lib.py:153] step: 640800, training_loss: 1.33213e+02
I1112 09:13:15.923633 140264174335808 run_lib.py:153] step: 640850, training_loss: 1.27116e+02
I1112 09:13:25.468598 140264174335808 run_lib.py:153] step: 640900, training_loss: 1.05285e+02
I1112 09:13:35.259268 140264174335808 run_lib.py:153] step: 640950, training_loss: 1.06080e+02
I1112 09:13:45.554164 140264174335808 run_lib.py:153] step: 641000, training_loss: 1.41780e+02
I1112 09:13:55.578981 140264174335808 run_lib.py:153] step: 641050, training_loss: 1.20400e+02
I1112 09:14:04.830359 140264174335808 run_lib.py:153] step: 641100, training_loss: 1.10395e+02
I1112 09:14:14.413463 140264174335808 run_lib.py:153] step: 641150, training_loss: 1.34913e+02
I1112 09:14:24.416664 140264174335808 run_lib.py:153] step: 641200, training_loss: 1.46969e+02
I1112 09:14:34.161038 140264174335808 run_lib.py:153] step: 641250, training_loss: 1.04696e+02
I1112 09:14:43.879159 140264174335808 run_lib.py:153] step: 641300, training_loss: 1.10590e+02
I1112 09:14:54.063661 140264174335808 run_lib.py:153] step: 641350, training_loss: 1.25825e+02
I1112 09:15:04.645936 140264174335808 run_lib.py:153] step: 641400, training_loss: 1.42801e+02
I1112 09:15:14.704007 140264174335808 run_lib.py:153] step: 641450, training_loss: 1.10597e+02
I1112 09:15:24.761515 140264174335808 run_lib.py:153] step: 641500, training_loss: 1.52217e+02
I1112 09:15:34.419602 140264174335808 run_lib.py:153] step: 641550, training_loss: 1.02595e+02
I1112 09:15:44.038748 140264174335808 run_lib.py:153] step: 641600, training_loss: 1.33493e+02
I1112 09:15:53.934875 140264174335808 run_lib.py:153] step: 641650, training_loss: 1.08680e+02
I1112 09:16:03.943790 140264174335808 run_lib.py:153] step: 641700, training_loss: 1.32798e+02
I1112 09:16:14.589357 140264174335808 run_lib.py:153] step: 641750, training_loss: 1.30823e+02
I1112 09:16:24.426730 140264174335808 run_lib.py:153] step: 641800, training_loss: 1.03682e+02
I1112 09:16:35.679299 140264174335808 run_lib.py:153] step: 641850, training_loss: 1.26970e+02
I1112 09:16:45.724083 140264174335808 run_lib.py:153] step: 641900, training_loss: 1.28661e+02
I1112 09:16:56.154432 140264174335808 run_lib.py:153] step: 641950, training_loss: 1.32922e+02
I1112 09:17:05.909087 140264174335808 run_lib.py:153] step: 642000, training_loss: 1.25971e+02
I1112 09:17:16.190890 140264174335808 run_lib.py:153] step: 642050, training_loss: 1.16936e+02
I1112 09:17:25.966853 140264174335808 run_lib.py:153] step: 642100, training_loss: 9.51106e+01
I1112 09:17:36.952231 140264174335808 run_lib.py:153] step: 642150, training_loss: 1.26123e+02
I1112 09:17:47.469886 140264174335808 run_lib.py:153] step: 642200, training_loss: 1.44722e+02
I1112 09:17:57.972756 140264174335808 run_lib.py:153] step: 642250, training_loss: 1.43498e+02
I1112 09:18:08.322786 140264174335808 run_lib.py:153] step: 642300, training_loss: 1.23855e+02
I1112 09:18:18.614511 140264174335808 run_lib.py:153] step: 642350, training_loss: 1.42202e+02
I1112 09:18:28.623841 140264174335808 run_lib.py:153] step: 642400, training_loss: 1.08346e+02
I1112 09:18:39.732316 140264174335808 run_lib.py:153] step: 642450, training_loss: 1.22613e+02
I1112 09:18:50.380591 140264174335808 run_lib.py:153] step: 642500, training_loss: 1.28604e+02
I1112 09:19:00.008327 140264174335808 run_lib.py:153] step: 642550, training_loss: 1.40890e+02
I1112 09:19:10.156859 140264174335808 run_lib.py:153] step: 642600, training_loss: 1.47286e+02
I1112 09:19:20.336964 140264174335808 run_lib.py:153] step: 642650, training_loss: 1.40302e+02
I1112 09:19:30.209353 140264174335808 run_lib.py:153] step: 642700, training_loss: 1.21183e+02
I1112 09:19:39.768282 140264174335808 run_lib.py:153] step: 642750, training_loss: 1.16314e+02
I1112 09:19:50.032695 140264174335808 run_lib.py:153] step: 642800, training_loss: 1.36245e+02
I1112 09:20:00.323955 140264174335808 run_lib.py:153] step: 642850, training_loss: 1.04033e+02
I1112 09:20:09.655042 140264174335808 run_lib.py:153] step: 642900, training_loss: 1.02937e+02
I1112 09:20:19.815490 140264174335808 run_lib.py:153] step: 642950, training_loss: 1.07224e+02
I1112 09:20:30.317419 140264174335808 run_lib.py:153] step: 643000, training_loss: 1.29547e+02
I1112 09:20:40.337583 140264174335808 run_lib.py:153] step: 643050, training_loss: 1.36475e+02
I1112 09:20:50.874544 140264174335808 run_lib.py:153] step: 643100, training_loss: 1.18191e+02
I1112 09:21:00.967723 140264174335808 run_lib.py:153] step: 643150, training_loss: 1.29440e+02
I1112 09:21:11.011394 140264174335808 run_lib.py:153] step: 643200, training_loss: 1.19129e+02
I1112 09:21:21.434248 140264174335808 run_lib.py:153] step: 643250, training_loss: 1.07889e+02
I1112 09:21:31.767332 140264174335808 run_lib.py:153] step: 643300, training_loss: 1.40376e+02
I1112 09:21:42.092941 140264174335808 run_lib.py:153] step: 643350, training_loss: 1.24480e+02
I1112 09:21:52.586384 140264174335808 run_lib.py:153] step: 643400, training_loss: 1.24674e+02
I1112 09:22:02.263384 140264174335808 run_lib.py:153] step: 643450, training_loss: 1.40553e+02
I1112 09:22:12.431714 140264174335808 run_lib.py:153] step: 643500, training_loss: 1.07542e+02
I1112 09:22:22.229130 140264174335808 run_lib.py:153] step: 643550, training_loss: 1.35763e+02
I1112 09:22:32.228882 140264174335808 run_lib.py:153] step: 643600, training_loss: 1.51884e+02
I1112 09:22:41.930942 140264174335808 run_lib.py:153] step: 643650, training_loss: 1.06820e+02
I1112 09:22:51.810180 140264174335808 run_lib.py:153] step: 643700, training_loss: 1.27343e+02
I1112 09:23:01.601881 140264174335808 run_lib.py:153] step: 643750, training_loss: 1.29254e+02
I1112 09:23:11.671376 140264174335808 run_lib.py:153] step: 643800, training_loss: 1.10307e+02
I1112 09:23:22.054376 140264174335808 run_lib.py:153] step: 643850, training_loss: 1.26198e+02
I1112 09:23:32.091093 140264174335808 run_lib.py:153] step: 643900, training_loss: 1.40683e+02
I1112 09:23:42.164421 140264174335808 run_lib.py:153] step: 643950, training_loss: 1.01830e+02
I1112 09:23:52.615216 140264174335808 run_lib.py:153] step: 644000, training_loss: 1.38996e+02
I1112 09:24:02.739606 140264174335808 run_lib.py:153] step: 644050, training_loss: 1.18561e+02
I1112 09:24:12.594017 140264174335808 run_lib.py:153] step: 644100, training_loss: 1.12044e+02
I1112 09:24:22.595307 140264174335808 run_lib.py:153] step: 644150, training_loss: 1.27462e+02
I1112 09:24:32.513722 140264174335808 run_lib.py:153] step: 644200, training_loss: 1.41242e+02
I1112 09:24:43.484710 140264174335808 run_lib.py:153] step: 644250, training_loss: 1.37216e+02
I1112 09:24:53.577927 140264174335808 run_lib.py:153] step: 644300, training_loss: 1.28169e+02
I1112 09:25:03.667983 140264174335808 run_lib.py:153] step: 644350, training_loss: 1.34114e+02
I1112 09:25:13.593618 140264174335808 run_lib.py:153] step: 644400, training_loss: 1.32319e+02
I1112 09:25:23.635882 140264174335808 run_lib.py:153] step: 644450, training_loss: 1.41559e+02
I1112 09:25:33.298928 140264174335808 run_lib.py:153] step: 644500, training_loss: 1.25960e+02
I1112 09:25:43.880654 140264174335808 run_lib.py:153] step: 644550, training_loss: 1.57100e+02
I1112 09:25:53.988203 140264174335808 run_lib.py:153] step: 644600, training_loss: 1.36166e+02
I1112 09:26:03.725307 140264174335808 run_lib.py:153] step: 644650, training_loss: 1.19294e+02
I1112 09:26:14.155611 140264174335808 run_lib.py:153] step: 644700, training_loss: 1.15842e+02
I1112 09:26:24.309108 140264174335808 run_lib.py:153] step: 644750, training_loss: 1.23512e+02
I1112 09:26:34.784136 140264174335808 run_lib.py:153] step: 644800, training_loss: 9.22707e+01
I1112 09:26:44.562973 140264174335808 run_lib.py:153] step: 644850, training_loss: 1.21201e+02
I1112 09:26:55.068286 140264174335808 run_lib.py:153] step: 644900, training_loss: 1.22133e+02
I1112 09:27:04.964129 140264174335808 run_lib.py:153] step: 644950, training_loss: 1.04834e+02
I1112 09:27:14.478151 140264174335808 run_lib.py:153] step: 645000, training_loss: 1.28034e+02
I1112 09:27:14.582290 140264174335808 run_lib.py:166] step: 645000, eval_loss: 1.41076e+02
I1112 09:27:23.957918 140264174335808 run_lib.py:153] step: 645050, training_loss: 1.55665e+02
I1112 09:27:34.085308 140264174335808 run_lib.py:153] step: 645100, training_loss: 1.18031e+02
I1112 09:27:43.578951 140264174335808 run_lib.py:153] step: 645150, training_loss: 1.40448e+02
I1112 09:27:53.821129 140264174335808 run_lib.py:153] step: 645200, training_loss: 1.19540e+02
I1112 09:28:04.019633 140264174335808 run_lib.py:153] step: 645250, training_loss: 1.48927e+02
I1112 09:28:13.578872 140264174335808 run_lib.py:153] step: 645300, training_loss: 1.32166e+02
I1112 09:28:24.289116 140264174335808 run_lib.py:153] step: 645350, training_loss: 1.36219e+02
I1112 09:28:34.348504 140264174335808 run_lib.py:153] step: 645400, training_loss: 1.33940e+02
I1112 09:28:44.311462 140264174335808 run_lib.py:153] step: 645450, training_loss: 1.08890e+02
I1112 09:28:53.753057 140264174335808 run_lib.py:153] step: 645500, training_loss: 1.07135e+02
I1112 09:29:04.122758 140264174335808 run_lib.py:153] step: 645550, training_loss: 1.40429e+02
I1112 09:29:14.327812 140264174335808 run_lib.py:153] step: 645600, training_loss: 1.19925e+02
I1112 09:29:24.857628 140264174335808 run_lib.py:153] step: 645650, training_loss: 1.09050e+02
I1112 09:29:35.473221 140264174335808 run_lib.py:153] step: 645700, training_loss: 1.30378e+02
I1112 09:29:45.320840 140264174335808 run_lib.py:153] step: 645750, training_loss: 1.21664e+02
I1112 09:29:55.299617 140264174335808 run_lib.py:153] step: 645800, training_loss: 1.18854e+02
I1112 09:30:05.024380 140264174335808 run_lib.py:153] step: 645850, training_loss: 1.11531e+02
I1112 09:30:15.354091 140264174335808 run_lib.py:153] step: 645900, training_loss: 1.01791e+02
I1112 09:30:25.294166 140264174335808 run_lib.py:153] step: 645950, training_loss: 1.23577e+02
I1112 09:30:35.355643 140264174335808 run_lib.py:153] step: 646000, training_loss: 1.01980e+02
I1112 09:30:45.407618 140264174335808 run_lib.py:153] step: 646050, training_loss: 1.51329e+02
I1112 09:30:55.303737 140264174335808 run_lib.py:153] step: 646100, training_loss: 1.39744e+02
I1112 09:31:04.664318 140264174335808 run_lib.py:153] step: 646150, training_loss: 1.20840e+02
I1112 09:31:14.893068 140264174335808 run_lib.py:153] step: 646200, training_loss: 1.41410e+02
I1112 09:31:24.542302 140264174335808 run_lib.py:153] step: 646250, training_loss: 1.21444e+02
I1112 09:31:34.725944 140264174335808 run_lib.py:153] step: 646300, training_loss: 1.49002e+02
I1112 09:31:44.816659 140264174335808 run_lib.py:153] step: 646350, training_loss: 1.28351e+02
I1112 09:31:55.100356 140264174335808 run_lib.py:153] step: 646400, training_loss: 1.15192e+02
I1112 09:32:04.954054 140264174335808 run_lib.py:153] step: 646450, training_loss: 1.20769e+02
I1112 09:32:14.627113 140264174335808 run_lib.py:153] step: 646500, training_loss: 1.07262e+02
I1112 09:32:24.356442 140264174335808 run_lib.py:153] step: 646550, training_loss: 1.54417e+02
I1112 09:32:35.056699 140264174335808 run_lib.py:153] step: 646600, training_loss: 1.30028e+02
I1112 09:32:45.148310 140264174335808 run_lib.py:153] step: 646650, training_loss: 1.17369e+02
I1112 09:32:55.075628 140264174335808 run_lib.py:153] step: 646700, training_loss: 1.51514e+02
I1112 09:33:05.297336 140264174335808 run_lib.py:153] step: 646750, training_loss: 1.08569e+02
I1112 09:33:15.591205 140264174335808 run_lib.py:153] step: 646800, training_loss: 1.46211e+02
I1112 09:33:25.806476 140264174335808 run_lib.py:153] step: 646850, training_loss: 1.35655e+02
I1112 09:33:35.842605 140264174335808 run_lib.py:153] step: 646900, training_loss: 1.25560e+02
I1112 09:33:45.985796 140264174335808 run_lib.py:153] step: 646950, training_loss: 1.35106e+02
I1112 09:33:55.872033 140264174335808 run_lib.py:153] step: 647000, training_loss: 1.22951e+02
I1112 09:34:05.582723 140264174335808 run_lib.py:153] step: 647050, training_loss: 1.37235e+02
I1112 09:34:15.307625 140264174335808 run_lib.py:153] step: 647100, training_loss: 1.27056e+02
I1112 09:34:25.370818 140264174335808 run_lib.py:153] step: 647150, training_loss: 1.42486e+02
I1112 09:34:35.011013 140264174335808 run_lib.py:153] step: 647200, training_loss: 1.15027e+02
I1112 09:34:44.747752 140264174335808 run_lib.py:153] step: 647250, training_loss: 1.29751e+02
I1112 09:34:55.016972 140264174335808 run_lib.py:153] step: 647300, training_loss: 1.22282e+02
I1112 09:35:04.940003 140264174335808 run_lib.py:153] step: 647350, training_loss: 1.20384e+02
I1112 09:35:14.917148 140264174335808 run_lib.py:153] step: 647400, training_loss: 1.39448e+02
I1112 09:35:25.165524 140264174335808 run_lib.py:153] step: 647450, training_loss: 1.31363e+02
I1112 09:35:35.364444 140264174335808 run_lib.py:153] step: 647500, training_loss: 1.54313e+02
I1112 09:35:45.339951 140264174335808 run_lib.py:153] step: 647550, training_loss: 1.20920e+02
I1112 09:35:55.796753 140264174335808 run_lib.py:153] step: 647600, training_loss: 1.07091e+02
I1112 09:36:06.312618 140264174335808 run_lib.py:153] step: 647650, training_loss: 1.23215e+02
I1112 09:36:15.904546 140264174335808 run_lib.py:153] step: 647700, training_loss: 1.51498e+02
I1112 09:36:26.268641 140264174335808 run_lib.py:153] step: 647750, training_loss: 1.13096e+02
I1112 09:36:36.279622 140264174335808 run_lib.py:153] step: 647800, training_loss: 1.20101e+02
I1112 09:36:46.839939 140264174335808 run_lib.py:153] step: 647850, training_loss: 1.29026e+02
I1112 09:36:56.439160 140264174335808 run_lib.py:153] step: 647900, training_loss: 1.19848e+02
I1112 09:37:07.031580 140264174335808 run_lib.py:153] step: 647950, training_loss: 1.14539e+02
I1112 09:37:16.475119 140264174335808 run_lib.py:153] step: 648000, training_loss: 1.05704e+02
I1112 09:37:26.144206 140264174335808 run_lib.py:153] step: 648050, training_loss: 1.18127e+02
I1112 09:37:36.296769 140264174335808 run_lib.py:153] step: 648100, training_loss: 1.47054e+02
I1112 09:37:46.044929 140264174335808 run_lib.py:153] step: 648150, training_loss: 1.60809e+02
I1112 09:37:56.266322 140264174335808 run_lib.py:153] step: 648200, training_loss: 1.31861e+02
I1112 09:38:06.498446 140264174335808 run_lib.py:153] step: 648250, training_loss: 1.39814e+02
I1112 09:38:16.876380 140264174335808 run_lib.py:153] step: 648300, training_loss: 1.26236e+02
I1112 09:38:26.885792 140264174335808 run_lib.py:153] step: 648350, training_loss: 1.30805e+02
I1112 09:38:36.811603 140264174335808 run_lib.py:153] step: 648400, training_loss: 1.80020e+02
I1112 09:38:47.121924 140264174335808 run_lib.py:153] step: 648450, training_loss: 1.36099e+02
I1112 09:38:57.039907 140264174335808 run_lib.py:153] step: 648500, training_loss: 1.04541e+02
I1112 09:39:06.754214 140264174335808 run_lib.py:153] step: 648550, training_loss: 1.24526e+02
I1112 09:39:16.845221 140264174335808 run_lib.py:153] step: 648600, training_loss: 1.43108e+02
I1112 09:39:26.698988 140264174335808 run_lib.py:153] step: 648650, training_loss: 1.19418e+02
I1112 09:39:37.180528 140264174335808 run_lib.py:153] step: 648700, training_loss: 1.10128e+02
I1112 09:39:47.124859 140264174335808 run_lib.py:153] step: 648750, training_loss: 1.26537e+02
I1112 09:39:57.352397 140264174335808 run_lib.py:153] step: 648800, training_loss: 1.29135e+02
I1112 09:40:07.699737 140264174335808 run_lib.py:153] step: 648850, training_loss: 1.25133e+02
I1112 09:40:17.962898 140264174335808 run_lib.py:153] step: 648900, training_loss: 1.26963e+02
I1112 09:40:28.120426 140264174335808 run_lib.py:153] step: 648950, training_loss: 1.39597e+02
I1112 09:40:38.441036 140264174335808 run_lib.py:153] step: 649000, training_loss: 1.24108e+02
I1112 09:40:48.519434 140264174335808 run_lib.py:153] step: 649050, training_loss: 1.41808e+02
I1112 09:40:57.827385 140264174335808 run_lib.py:153] step: 649100, training_loss: 1.45332e+02
I1112 09:41:07.748568 140264174335808 run_lib.py:153] step: 649150, training_loss: 1.17715e+02
I1112 09:41:18.517816 140264174335808 run_lib.py:153] step: 649200, training_loss: 1.04459e+02
I1112 09:41:28.140695 140264174335808 run_lib.py:153] step: 649250, training_loss: 1.36348e+02
I1112 09:41:37.842364 140264174335808 run_lib.py:153] step: 649300, training_loss: 1.27998e+02
I1112 09:41:47.921510 140264174335808 run_lib.py:153] step: 649350, training_loss: 1.49577e+02
I1112 09:41:58.220924 140264174335808 run_lib.py:153] step: 649400, training_loss: 1.33783e+02
I1112 09:42:08.377147 140264174335808 run_lib.py:153] step: 649450, training_loss: 1.22229e+02
I1112 09:42:18.931732 140264174335808 run_lib.py:153] step: 649500, training_loss: 1.35628e+02
I1112 09:42:29.063926 140264174335808 run_lib.py:153] step: 649550, training_loss: 1.39370e+02
I1112 09:42:38.859405 140264174335808 run_lib.py:153] step: 649600, training_loss: 1.07342e+02
I1112 09:42:48.723165 140264174335808 run_lib.py:153] step: 649650, training_loss: 1.31159e+02
I1112 09:42:58.743924 140264174335808 run_lib.py:153] step: 649700, training_loss: 1.24109e+02
I1112 09:43:09.010701 140264174335808 run_lib.py:153] step: 649750, training_loss: 1.15114e+02
I1112 09:43:19.116174 140264174335808 run_lib.py:153] step: 649800, training_loss: 1.19558e+02
I1112 09:43:28.798751 140264174335808 run_lib.py:153] step: 649850, training_loss: 1.31877e+02
I1112 09:43:39.637890 140264174335808 run_lib.py:153] step: 649900, training_loss: 1.41893e+02
I1112 09:43:49.871114 140264174335808 run_lib.py:153] step: 649950, training_loss: 1.15230e+02
I1112 09:43:59.881746 140264174335808 run_lib.py:153] step: 650000, training_loss: 1.11034e+02
I1112 09:44:00.453881 140264174335808 run_lib.py:166] step: 650000, eval_loss: 1.31471e+02
I1112 09:44:10.567018 140264174335808 run_lib.py:153] step: 650050, training_loss: 1.04952e+02
I1112 09:44:21.253694 140264174335808 run_lib.py:153] step: 650100, training_loss: 1.45734e+02
I1112 09:44:30.990882 140264174335808 run_lib.py:153] step: 650150, training_loss: 1.10219e+02
I1112 09:44:40.650127 140264174335808 run_lib.py:153] step: 650200, training_loss: 1.34562e+02
I1112 09:44:51.130186 140264174335808 run_lib.py:153] step: 650250, training_loss: 1.21627e+02
I1112 09:45:00.674551 140264174335808 run_lib.py:153] step: 650300, training_loss: 1.16473e+02
I1112 09:45:10.400450 140264174335808 run_lib.py:153] step: 650350, training_loss: 1.24544e+02
I1112 09:45:20.327866 140264174335808 run_lib.py:153] step: 650400, training_loss: 1.16905e+02
I1112 09:45:30.737812 140264174335808 run_lib.py:153] step: 650450, training_loss: 1.23921e+02
I1112 09:45:40.536225 140264174335808 run_lib.py:153] step: 650500, training_loss: 1.19565e+02
I1112 09:45:50.635922 140264174335808 run_lib.py:153] step: 650550, training_loss: 1.14904e+02
I1112 09:46:00.667650 140264174335808 run_lib.py:153] step: 650600, training_loss: 1.11460e+02
I1112 09:46:10.712059 140264174335808 run_lib.py:153] step: 650650, training_loss: 1.07746e+02
I1112 09:46:20.147598 140264174335808 run_lib.py:153] step: 650700, training_loss: 1.25251e+02
I1112 09:46:29.949933 140264174335808 run_lib.py:153] step: 650750, training_loss: 1.17871e+02
I1112 09:46:39.377649 140264174335808 run_lib.py:153] step: 650800, training_loss: 1.27071e+02
I1112 09:46:49.940773 140264174335808 run_lib.py:153] step: 650850, training_loss: 8.90673e+01
I1112 09:46:59.697671 140264174335808 run_lib.py:153] step: 650900, training_loss: 9.05835e+01
I1112 09:47:09.773294 140264174335808 run_lib.py:153] step: 650950, training_loss: 1.24131e+02
I1112 09:47:20.224999 140264174335808 run_lib.py:153] step: 651000, training_loss: 1.29477e+02
I1112 09:47:30.143371 140264174335808 run_lib.py:153] step: 651050, training_loss: 1.11885e+02
I1112 09:47:39.964762 140264174335808 run_lib.py:153] step: 651100, training_loss: 1.32988e+02
I1112 09:47:50.137012 140264174335808 run_lib.py:153] step: 651150, training_loss: 1.19900e+02
I1112 09:48:00.074296 140264174335808 run_lib.py:153] step: 651200, training_loss: 1.49344e+02
I1112 09:48:10.346618 140264174335808 run_lib.py:153] step: 651250, training_loss: 1.28391e+02
I1112 09:48:21.046626 140264174335808 run_lib.py:153] step: 651300, training_loss: 1.41037e+02
I1112 09:48:30.678004 140264174335808 run_lib.py:153] step: 651350, training_loss: 1.74519e+02
I1112 09:48:40.086571 140264174335808 run_lib.py:153] step: 651400, training_loss: 1.25494e+02
I1112 09:48:50.574640 140264174335808 run_lib.py:153] step: 651450, training_loss: 1.28113e+02
I1112 09:49:00.429371 140264174335808 run_lib.py:153] step: 651500, training_loss: 1.15118e+02
I1112 09:49:10.672788 140264174335808 run_lib.py:153] step: 651550, training_loss: 8.70548e+01
I1112 09:49:21.174700 140264174335808 run_lib.py:153] step: 651600, training_loss: 1.19158e+02
I1112 09:49:30.755926 140264174335808 run_lib.py:153] step: 651650, training_loss: 1.40899e+02
I1112 09:49:40.430766 140264174335808 run_lib.py:153] step: 651700, training_loss: 1.18946e+02
I1112 09:49:50.410921 140264174335808 run_lib.py:153] step: 651750, training_loss: 1.14734e+02
I1112 09:50:00.246028 140264174335808 run_lib.py:153] step: 651800, training_loss: 1.21391e+02
I1112 09:50:10.531138 140264174335808 run_lib.py:153] step: 651850, training_loss: 1.23525e+02
I1112 09:50:20.830392 140264174335808 run_lib.py:153] step: 651900, training_loss: 1.15367e+02
I1112 09:50:31.727037 140264174335808 run_lib.py:153] step: 651950, training_loss: 1.29979e+02
I1112 09:50:42.607852 140264174335808 run_lib.py:153] step: 652000, training_loss: 1.01866e+02
I1112 09:50:53.017702 140264174335808 run_lib.py:153] step: 652050, training_loss: 1.26078e+02
I1112 09:51:03.798191 140264174335808 run_lib.py:153] step: 652100, training_loss: 1.14850e+02
I1112 09:51:13.840639 140264174335808 run_lib.py:153] step: 652150, training_loss: 1.31244e+02
I1112 09:51:23.628364 140264174335808 run_lib.py:153] step: 652200, training_loss: 1.42063e+02
I1112 09:51:33.381674 140264174335808 run_lib.py:153] step: 652250, training_loss: 1.36509e+02
I1112 09:51:43.749799 140264174335808 run_lib.py:153] step: 652300, training_loss: 1.08007e+02
I1112 09:51:54.601540 140264174335808 run_lib.py:153] step: 652350, training_loss: 1.13865e+02
I1112 09:52:05.494900 140264174335808 run_lib.py:153] step: 652400, training_loss: 1.52387e+02
I1112 09:52:16.479071 140264174335808 run_lib.py:153] step: 652450, training_loss: 1.33591e+02
I1112 09:52:26.362949 140264174335808 run_lib.py:153] step: 652500, training_loss: 9.44314e+01
I1112 09:52:36.172053 140264174335808 run_lib.py:153] step: 652550, training_loss: 1.41404e+02
I1112 09:52:46.136498 140264174335808 run_lib.py:153] step: 652600, training_loss: 1.18142e+02
I1112 09:52:56.361655 140264174335808 run_lib.py:153] step: 652650, training_loss: 1.42461e+02
I1112 09:53:06.319006 140264174335808 run_lib.py:153] step: 652700, training_loss: 1.30639e+02
I1112 09:53:16.197255 140264174335808 run_lib.py:153] step: 652750, training_loss: 1.38477e+02
I1112 09:53:25.989442 140264174335808 run_lib.py:153] step: 652800, training_loss: 1.40650e+02
I1112 09:53:35.573460 140264174335808 run_lib.py:153] step: 652850, training_loss: 1.20354e+02
I1112 09:53:45.159386 140264174335808 run_lib.py:153] step: 652900, training_loss: 1.22392e+02
I1112 09:53:55.243698 140264174335808 run_lib.py:153] step: 652950, training_loss: 1.42899e+02
I1112 09:54:05.187842 140264174335808 run_lib.py:153] step: 653000, training_loss: 1.27864e+02
I1112 09:54:15.676991 140264174335808 run_lib.py:153] step: 653050, training_loss: 1.08832e+02
I1112 09:54:26.050872 140264174335808 run_lib.py:153] step: 653100, training_loss: 1.25048e+02
I1112 09:54:35.828559 140264174335808 run_lib.py:153] step: 653150, training_loss: 1.23782e+02
I1112 09:54:46.374218 140264174335808 run_lib.py:153] step: 653200, training_loss: 1.38332e+02
I1112 09:54:56.057321 140264174335808 run_lib.py:153] step: 653250, training_loss: 1.25739e+02
I1112 09:55:06.226386 140264174335808 run_lib.py:153] step: 653300, training_loss: 1.31934e+02
I1112 09:55:16.438890 140264174335808 run_lib.py:153] step: 653350, training_loss: 1.37789e+02
I1112 09:55:26.397659 140264174335808 run_lib.py:153] step: 653400, training_loss: 1.17610e+02
I1112 09:55:37.065019 140264174335808 run_lib.py:153] step: 653450, training_loss: 1.28875e+02
I1112 09:55:46.861363 140264174335808 run_lib.py:153] step: 653500, training_loss: 1.49660e+02
I1112 09:55:56.839443 140264174335808 run_lib.py:153] step: 653550, training_loss: 1.52768e+02
I1112 09:56:06.859750 140264174335808 run_lib.py:153] step: 653600, training_loss: 1.20376e+02
I1112 09:56:16.941893 140264174335808 run_lib.py:153] step: 653650, training_loss: 1.34983e+02
I1112 09:56:26.530256 140264174335808 run_lib.py:153] step: 653700, training_loss: 1.24563e+02
I1112 09:56:35.970585 140264174335808 run_lib.py:153] step: 653750, training_loss: 1.03098e+02
I1112 09:56:45.718588 140264174335808 run_lib.py:153] step: 653800, training_loss: 1.00843e+02
I1112 09:56:55.839109 140264174335808 run_lib.py:153] step: 653850, training_loss: 1.16127e+02
I1112 09:57:05.934688 140264174335808 run_lib.py:153] step: 653900, training_loss: 1.22339e+02
I1112 09:57:15.825335 140264174335808 run_lib.py:153] step: 653950, training_loss: 1.13527e+02
I1112 09:57:25.656484 140264174335808 run_lib.py:153] step: 654000, training_loss: 1.39541e+02
I1112 09:57:35.896397 140264174335808 run_lib.py:153] step: 654050, training_loss: 1.18912e+02
I1112 09:57:45.526944 140264174335808 run_lib.py:153] step: 654100, training_loss: 1.50254e+02
I1112 09:57:55.498402 140264174335808 run_lib.py:153] step: 654150, training_loss: 1.14562e+02
I1112 09:58:05.295486 140264174335808 run_lib.py:153] step: 654200, training_loss: 1.11330e+02
I1112 09:58:15.490196 140264174335808 run_lib.py:153] step: 654250, training_loss: 1.37006e+02
I1112 09:58:26.204917 140264174335808 run_lib.py:153] step: 654300, training_loss: 1.13929e+02
I1112 09:58:36.215773 140264174335808 run_lib.py:153] step: 654350, training_loss: 1.10322e+02
I1112 09:58:46.814082 140264174335808 run_lib.py:153] step: 654400, training_loss: 1.07632e+02
I1112 09:58:56.970581 140264174335808 run_lib.py:153] step: 654450, training_loss: 1.38882e+02
I1112 09:59:06.907267 140264174335808 run_lib.py:153] step: 654500, training_loss: 1.46405e+02
I1112 09:59:17.291011 140264174335808 run_lib.py:153] step: 654550, training_loss: 1.28414e+02
I1112 09:59:27.250732 140264174335808 run_lib.py:153] step: 654600, training_loss: 1.32370e+02
I1112 09:59:37.117027 140264174335808 run_lib.py:153] step: 654650, training_loss: 1.17063e+02
I1112 09:59:47.393366 140264174335808 run_lib.py:153] step: 654700, training_loss: 1.43828e+02
I1112 09:59:57.413580 140264174335808 run_lib.py:153] step: 654750, training_loss: 1.07919e+02
I1112 10:00:07.014995 140264174335808 run_lib.py:153] step: 654800, training_loss: 1.23793e+02
I1112 10:00:16.894164 140264174335808 run_lib.py:153] step: 654850, training_loss: 1.41929e+02
I1112 10:00:26.588564 140264174335808 run_lib.py:153] step: 654900, training_loss: 1.43112e+02
I1112 10:00:36.641970 140264174335808 run_lib.py:153] step: 654950, training_loss: 1.08545e+02
I1112 10:00:46.485157 140264174335808 run_lib.py:153] step: 655000, training_loss: 1.05505e+02
I1112 10:00:46.587989 140264174335808 run_lib.py:166] step: 655000, eval_loss: 1.25206e+02
I1112 10:00:56.616983 140264174335808 run_lib.py:153] step: 655050, training_loss: 1.53190e+02
I1112 10:01:06.682414 140264174335808 run_lib.py:153] step: 655100, training_loss: 1.21218e+02
I1112 10:01:16.971216 140264174335808 run_lib.py:153] step: 655150, training_loss: 1.18178e+02
I1112 10:01:27.907147 140264174335808 run_lib.py:153] step: 655200, training_loss: 1.19625e+02
I1112 10:01:38.296016 140264174335808 run_lib.py:153] step: 655250, training_loss: 1.12545e+02
I1112 10:01:49.208845 140264174335808 run_lib.py:153] step: 655300, training_loss: 9.84309e+01
I1112 10:01:59.896400 140264174335808 run_lib.py:153] step: 655350, training_loss: 8.25190e+01
I1112 10:02:10.975052 140264174335808 run_lib.py:153] step: 655400, training_loss: 1.25041e+02
I1112 10:02:20.964915 140264174335808 run_lib.py:153] step: 655450, training_loss: 1.21021e+02
I1112 10:02:31.626737 140264174335808 run_lib.py:153] step: 655500, training_loss: 1.62591e+02
I1112 10:02:41.788953 140264174335808 run_lib.py:153] step: 655550, training_loss: 1.15084e+02
I1112 10:02:52.031488 140264174335808 run_lib.py:153] step: 655600, training_loss: 1.32633e+02
I1112 10:03:01.926264 140264174335808 run_lib.py:153] step: 655650, training_loss: 1.35211e+02
I1112 10:03:11.992732 140264174335808 run_lib.py:153] step: 655700, training_loss: 9.50663e+01
I1112 10:03:23.137845 140264174335808 run_lib.py:153] step: 655750, training_loss: 1.34963e+02
I1112 10:03:33.761375 140264174335808 run_lib.py:153] step: 655800, training_loss: 1.40054e+02
I1112 10:03:43.999952 140264174335808 run_lib.py:153] step: 655850, training_loss: 1.48374e+02
I1112 10:03:54.310566 140264174335808 run_lib.py:153] step: 655900, training_loss: 1.33326e+02
I1112 10:04:04.478734 140264174335808 run_lib.py:153] step: 655950, training_loss: 1.18917e+02
I1112 10:04:15.599822 140264174335808 run_lib.py:153] step: 656000, training_loss: 1.15492e+02
I1112 10:04:25.987977 140264174335808 run_lib.py:153] step: 656050, training_loss: 1.22923e+02
I1112 10:04:36.570941 140264174335808 run_lib.py:153] step: 656100, training_loss: 1.35006e+02
I1112 10:04:46.808583 140264174335808 run_lib.py:153] step: 656150, training_loss: 1.31667e+02
I1112 10:04:56.914934 140264174335808 run_lib.py:153] step: 656200, training_loss: 1.27987e+02
I1112 10:05:06.690148 140264174335808 run_lib.py:153] step: 656250, training_loss: 1.22811e+02
I1112 10:05:17.095913 140264174335808 run_lib.py:153] step: 656300, training_loss: 1.16281e+02
I1112 10:05:26.941844 140264174335808 run_lib.py:153] step: 656350, training_loss: 1.14382e+02
I1112 10:05:36.702205 140264174335808 run_lib.py:153] step: 656400, training_loss: 1.44680e+02
I1112 10:05:47.175996 140264174335808 run_lib.py:153] step: 656450, training_loss: 1.15632e+02
I1112 10:05:57.910432 140264174335808 run_lib.py:153] step: 656500, training_loss: 1.24245e+02
I1112 10:06:07.904230 140264174335808 run_lib.py:153] step: 656550, training_loss: 1.32602e+02
I1112 10:06:18.020092 140264174335808 run_lib.py:153] step: 656600, training_loss: 1.02253e+02
I1112 10:06:28.117110 140264174335808 run_lib.py:153] step: 656650, training_loss: 1.19698e+02
I1112 10:06:37.950893 140264174335808 run_lib.py:153] step: 656700, training_loss: 1.22551e+02
I1112 10:06:48.453252 140264174335808 run_lib.py:153] step: 656750, training_loss: 1.29524e+02
I1112 10:06:58.350501 140264174335808 run_lib.py:153] step: 656800, training_loss: 1.30037e+02
I1112 10:07:08.411241 140264174335808 run_lib.py:153] step: 656850, training_loss: 1.25885e+02
I1112 10:07:18.564236 140264174335808 run_lib.py:153] step: 656900, training_loss: 1.26419e+02
I1112 10:07:29.042695 140264174335808 run_lib.py:153] step: 656950, training_loss: 1.59613e+02
I1112 10:07:38.814030 140264174335808 run_lib.py:153] step: 657000, training_loss: 1.41109e+02
I1112 10:07:48.984901 140264174335808 run_lib.py:153] step: 657050, training_loss: 1.31910e+02
I1112 10:07:59.234783 140264174335808 run_lib.py:153] step: 657100, training_loss: 1.22424e+02
I1112 10:08:10.486355 140264174335808 run_lib.py:153] step: 657150, training_loss: 1.23736e+02
I1112 10:08:20.917853 140264174335808 run_lib.py:153] step: 657200, training_loss: 1.30623e+02
I1112 10:08:31.564489 140264174335808 run_lib.py:153] step: 657250, training_loss: 9.93498e+01
I1112 10:08:41.404411 140264174335808 run_lib.py:153] step: 657300, training_loss: 1.40426e+02
I1112 10:08:51.932439 140264174335808 run_lib.py:153] step: 657350, training_loss: 1.49192e+02
I1112 10:09:01.668740 140264174335808 run_lib.py:153] step: 657400, training_loss: 1.48300e+02
I1112 10:09:11.728706 140264174335808 run_lib.py:153] step: 657450, training_loss: 1.38086e+02
I1112 10:09:21.543182 140264174335808 run_lib.py:153] step: 657500, training_loss: 9.54598e+01
I1112 10:09:31.021211 140264174335808 run_lib.py:153] step: 657550, training_loss: 1.51674e+02
I1112 10:09:41.523143 140264174335808 run_lib.py:153] step: 657600, training_loss: 1.37303e+02
I1112 10:09:51.460351 140264174335808 run_lib.py:153] step: 657650, training_loss: 1.34048e+02
I1112 10:10:01.456672 140264174335808 run_lib.py:153] step: 657700, training_loss: 1.12950e+02
I1112 10:10:11.994025 140264174335808 run_lib.py:153] step: 657750, training_loss: 1.31887e+02
I1112 10:10:22.300172 140264174335808 run_lib.py:153] step: 657800, training_loss: 9.63274e+01
I1112 10:10:32.098566 140264174335808 run_lib.py:153] step: 657850, training_loss: 1.19178e+02
I1112 10:10:42.657334 140264174335808 run_lib.py:153] step: 657900, training_loss: 1.36127e+02
I1112 10:10:52.959057 140264174335808 run_lib.py:153] step: 657950, training_loss: 1.14417e+02
I1112 10:11:02.860322 140264174335808 run_lib.py:153] step: 658000, training_loss: 1.54214e+02
I1112 10:11:12.252280 140264174335808 run_lib.py:153] step: 658050, training_loss: 1.22887e+02
I1112 10:11:21.513155 140264174335808 run_lib.py:153] step: 658100, training_loss: 1.22911e+02
I1112 10:11:31.142240 140264174335808 run_lib.py:153] step: 658150, training_loss: 1.09869e+02
I1112 10:11:41.252190 140264174335808 run_lib.py:153] step: 658200, training_loss: 1.05920e+02
I1112 10:11:51.512662 140264174335808 run_lib.py:153] step: 658250, training_loss: 1.25609e+02
I1112 10:12:02.015146 140264174335808 run_lib.py:153] step: 658300, training_loss: 1.43816e+02
I1112 10:12:12.163164 140264174335808 run_lib.py:153] step: 658350, training_loss: 9.80342e+01
I1112 10:12:22.189892 140264174335808 run_lib.py:153] step: 658400, training_loss: 1.16427e+02
I1112 10:12:32.256298 140264174335808 run_lib.py:153] step: 658450, training_loss: 1.09514e+02
I1112 10:12:41.629180 140264174335808 run_lib.py:153] step: 658500, training_loss: 1.28885e+02
I1112 10:12:51.608965 140264174335808 run_lib.py:153] step: 658550, training_loss: 1.05077e+02
I1112 10:13:01.196977 140264174335808 run_lib.py:153] step: 658600, training_loss: 1.21038e+02
I1112 10:13:10.910566 140264174335808 run_lib.py:153] step: 658650, training_loss: 1.16407e+02
I1112 10:13:21.703274 140264174335808 run_lib.py:153] step: 658700, training_loss: 1.08337e+02
I1112 10:13:31.812665 140264174335808 run_lib.py:153] step: 658750, training_loss: 1.22281e+02
I1112 10:13:42.268190 140264174335808 run_lib.py:153] step: 658800, training_loss: 1.17642e+02
I1112 10:13:52.658007 140264174335808 run_lib.py:153] step: 658850, training_loss: 1.48779e+02
I1112 10:14:03.420132 140264174335808 run_lib.py:153] step: 658900, training_loss: 1.23064e+02
I1112 10:14:14.038088 140264174335808 run_lib.py:153] step: 658950, training_loss: 1.27153e+02
I1112 10:14:24.493575 140264174335808 run_lib.py:153] step: 659000, training_loss: 1.31064e+02
I1112 10:14:34.315620 140264174335808 run_lib.py:153] step: 659050, training_loss: 1.52934e+02
I1112 10:14:44.650407 140264174335808 run_lib.py:153] step: 659100, training_loss: 1.23667e+02
I1112 10:14:54.537986 140264174335808 run_lib.py:153] step: 659150, training_loss: 1.35203e+02
I1112 10:15:05.318528 140264174335808 run_lib.py:153] step: 659200, training_loss: 8.67210e+01
I1112 10:15:15.911038 140264174335808 run_lib.py:153] step: 659250, training_loss: 1.11161e+02
I1112 10:15:26.310622 140264174335808 run_lib.py:153] step: 659300, training_loss: 1.05510e+02
I1112 10:15:37.584536 140264174335808 run_lib.py:153] step: 659350, training_loss: 9.66428e+01
I1112 10:15:48.011198 140264174335808 run_lib.py:153] step: 659400, training_loss: 1.43732e+02
I1112 10:15:58.465307 140264174335808 run_lib.py:153] step: 659450, training_loss: 1.17406e+02
I1112 10:16:09.562355 140264174335808 run_lib.py:153] step: 659500, training_loss: 1.32950e+02
I1112 10:16:20.050291 140264174335808 run_lib.py:153] step: 659550, training_loss: 1.07291e+02
I1112 10:16:30.875347 140264174335808 run_lib.py:153] step: 659600, training_loss: 1.18183e+02
I1112 10:16:41.583661 140264174335808 run_lib.py:153] step: 659650, training_loss: 9.87248e+01
I1112 10:16:51.378697 140264174335808 run_lib.py:153] step: 659700, training_loss: 1.33642e+02
I1112 10:17:01.655813 140264174335808 run_lib.py:153] step: 659750, training_loss: 1.12691e+02
I1112 10:17:11.138369 140264174335808 run_lib.py:153] step: 659800, training_loss: 1.36488e+02
I1112 10:17:22.451538 140264174335808 run_lib.py:153] step: 659850, training_loss: 1.33476e+02
I1112 10:17:32.961839 140264174335808 run_lib.py:153] step: 659900, training_loss: 1.27538e+02
I1112 10:17:42.615652 140264174335808 run_lib.py:153] step: 659950, training_loss: 1.33691e+02
I1112 10:17:52.883865 140264174335808 run_lib.py:153] step: 660000, training_loss: 1.37188e+02
I1112 10:17:53.556388 140264174335808 run_lib.py:166] step: 660000, eval_loss: 1.23395e+02
I1112 10:18:03.915106 140264174335808 run_lib.py:153] step: 660050, training_loss: 1.12904e+02
I1112 10:18:14.094041 140264174335808 run_lib.py:153] step: 660100, training_loss: 1.35574e+02
I1112 10:18:23.499382 140264174335808 run_lib.py:153] step: 660150, training_loss: 1.49671e+02
I1112 10:18:33.060855 140264174335808 run_lib.py:153] step: 660200, training_loss: 1.17336e+02
I1112 10:18:42.501385 140264174335808 run_lib.py:153] step: 660250, training_loss: 1.25399e+02
I1112 10:18:51.981693 140264174335808 run_lib.py:153] step: 660300, training_loss: 1.17160e+02
I1112 10:19:01.674676 140264174335808 run_lib.py:153] step: 660350, training_loss: 1.52743e+02
I1112 10:19:11.551337 140264174335808 run_lib.py:153] step: 660400, training_loss: 1.52007e+02
I1112 10:19:21.723695 140264174335808 run_lib.py:153] step: 660450, training_loss: 1.39440e+02
I1112 10:19:32.033581 140264174335808 run_lib.py:153] step: 660500, training_loss: 1.41783e+02
I1112 10:19:42.553576 140264174335808 run_lib.py:153] step: 660550, training_loss: 1.29135e+02
I1112 10:19:52.154806 140264174335808 run_lib.py:153] step: 660600, training_loss: 1.46145e+02
I1112 10:20:01.621243 140264174335808 run_lib.py:153] step: 660650, training_loss: 1.05910e+02
I1112 10:20:11.556407 140264174335808 run_lib.py:153] step: 660700, training_loss: 1.45381e+02
I1112 10:20:22.234762 140264174335808 run_lib.py:153] step: 660750, training_loss: 1.58867e+02
I1112 10:20:32.238232 140264174335808 run_lib.py:153] step: 660800, training_loss: 1.41142e+02
I1112 10:20:42.538050 140264174335808 run_lib.py:153] step: 660850, training_loss: 1.28068e+02
I1112 10:20:52.724769 140264174335808 run_lib.py:153] step: 660900, training_loss: 1.39348e+02
I1112 10:21:03.055747 140264174335808 run_lib.py:153] step: 660950, training_loss: 1.16641e+02
I1112 10:21:13.129454 140264174335808 run_lib.py:153] step: 661000, training_loss: 1.29393e+02
I1112 10:21:24.052392 140264174335808 run_lib.py:153] step: 661050, training_loss: 1.02701e+02
I1112 10:21:34.206695 140264174335808 run_lib.py:153] step: 661100, training_loss: 1.34284e+02
I1112 10:21:44.868320 140264174335808 run_lib.py:153] step: 661150, training_loss: 1.54170e+02
I1112 10:21:55.469247 140264174335808 run_lib.py:153] step: 661200, training_loss: 1.27230e+02
I1112 10:22:05.395914 140264174335808 run_lib.py:153] step: 661250, training_loss: 1.39282e+02
I1112 10:22:15.479868 140264174335808 run_lib.py:153] step: 661300, training_loss: 1.02770e+02
I1112 10:22:25.529884 140264174335808 run_lib.py:153] step: 661350, training_loss: 1.55268e+02
I1112 10:22:35.230472 140264174335808 run_lib.py:153] step: 661400, training_loss: 1.24528e+02
I1112 10:22:44.607161 140264174335808 run_lib.py:153] step: 661450, training_loss: 1.46785e+02
I1112 10:22:54.164824 140264174335808 run_lib.py:153] step: 661500, training_loss: 1.18700e+02
I1112 10:23:03.996753 140264174335808 run_lib.py:153] step: 661550, training_loss: 1.37525e+02
I1112 10:23:13.287051 140264174335808 run_lib.py:153] step: 661600, training_loss: 1.24848e+02
I1112 10:23:23.457831 140264174335808 run_lib.py:153] step: 661650, training_loss: 1.30026e+02
I1112 10:23:33.302889 140264174335808 run_lib.py:153] step: 661700, training_loss: 1.49136e+02
I1112 10:23:43.155096 140264174335808 run_lib.py:153] step: 661750, training_loss: 1.79055e+02
I1112 10:23:52.707366 140264174335808 run_lib.py:153] step: 661800, training_loss: 1.48500e+02
I1112 10:24:03.766921 140264174335808 run_lib.py:153] step: 661850, training_loss: 1.04610e+02
I1112 10:24:13.600079 140264174335808 run_lib.py:153] step: 661900, training_loss: 1.28871e+02
I1112 10:24:23.889146 140264174335808 run_lib.py:153] step: 661950, training_loss: 1.20918e+02
I1112 10:24:33.847518 140264174335808 run_lib.py:153] step: 662000, training_loss: 1.38279e+02
I1112 10:24:44.112246 140264174335808 run_lib.py:153] step: 662050, training_loss: 1.50803e+02
I1112 10:24:53.733436 140264174335808 run_lib.py:153] step: 662100, training_loss: 1.10377e+02
I1112 10:25:03.953098 140264174335808 run_lib.py:153] step: 662150, training_loss: 1.26005e+02
I1112 10:25:13.740975 140264174335808 run_lib.py:153] step: 662200, training_loss: 1.28934e+02
I1112 10:25:23.873910 140264174335808 run_lib.py:153] step: 662250, training_loss: 1.35904e+02
I1112 10:25:33.872431 140264174335808 run_lib.py:153] step: 662300, training_loss: 1.26423e+02
I1112 10:25:43.980956 140264174335808 run_lib.py:153] step: 662350, training_loss: 1.22995e+02
I1112 10:25:53.750609 140264174335808 run_lib.py:153] step: 662400, training_loss: 8.92365e+01
I1112 10:26:03.704264 140264174335808 run_lib.py:153] step: 662450, training_loss: 1.10535e+02
I1112 10:26:13.295622 140264174335808 run_lib.py:153] step: 662500, training_loss: 1.14897e+02
I1112 10:26:22.584261 140264174335808 run_lib.py:153] step: 662550, training_loss: 1.51657e+02
I1112 10:26:32.286295 140264174335808 run_lib.py:153] step: 662600, training_loss: 1.53654e+02
I1112 10:26:41.964644 140264174335808 run_lib.py:153] step: 662650, training_loss: 1.38922e+02
I1112 10:26:51.835409 140264174335808 run_lib.py:153] step: 662700, training_loss: 1.23387e+02
I1112 10:27:02.213079 140264174335808 run_lib.py:153] step: 662750, training_loss: 1.16340e+02
I1112 10:27:12.010390 140264174335808 run_lib.py:153] step: 662800, training_loss: 1.46633e+02
I1112 10:27:22.263111 140264174335808 run_lib.py:153] step: 662850, training_loss: 1.26415e+02
I1112 10:27:32.220040 140264174335808 run_lib.py:153] step: 662900, training_loss: 1.21901e+02
I1112 10:27:42.437301 140264174335808 run_lib.py:153] step: 662950, training_loss: 1.46160e+02
I1112 10:27:52.164175 140264174335808 run_lib.py:153] step: 663000, training_loss: 1.06252e+02
I1112 10:28:02.126815 140264174335808 run_lib.py:153] step: 663050, training_loss: 1.56134e+02
I1112 10:28:12.109070 140264174335808 run_lib.py:153] step: 663100, training_loss: 1.59706e+02
I1112 10:28:22.209436 140264174335808 run_lib.py:153] step: 663150, training_loss: 1.29688e+02
I1112 10:28:32.471150 140264174335808 run_lib.py:153] step: 663200, training_loss: 1.56347e+02
I1112 10:28:42.262884 140264174335808 run_lib.py:153] step: 663250, training_loss: 1.49276e+02
I1112 10:28:52.270837 140264174335808 run_lib.py:153] step: 663300, training_loss: 1.36051e+02
I1112 10:29:01.514471 140264174335808 run_lib.py:153] step: 663350, training_loss: 1.21000e+02
I1112 10:29:10.850570 140264174335808 run_lib.py:153] step: 663400, training_loss: 1.44345e+02
I1112 10:29:20.467376 140264174335808 run_lib.py:153] step: 663450, training_loss: 1.05150e+02
I1112 10:29:30.016861 140264174335808 run_lib.py:153] step: 663500, training_loss: 1.14345e+02
I1112 10:29:39.295807 140264174335808 run_lib.py:153] step: 663550, training_loss: 1.25225e+02
I1112 10:29:49.954488 140264174335808 run_lib.py:153] step: 663600, training_loss: 1.22021e+02
I1112 10:29:59.807826 140264174335808 run_lib.py:153] step: 663650, training_loss: 1.15681e+02
I1112 10:30:09.773014 140264174335808 run_lib.py:153] step: 663700, training_loss: 1.37085e+02
I1112 10:30:19.001996 140264174335808 run_lib.py:153] step: 663750, training_loss: 1.08725e+02
I1112 10:30:28.630573 140264174335808 run_lib.py:153] step: 663800, training_loss: 1.22142e+02
I1112 10:30:39.357742 140264174335808 run_lib.py:153] step: 663850, training_loss: 1.15743e+02
I1112 10:30:49.113344 140264174335808 run_lib.py:153] step: 663900, training_loss: 1.36237e+02
I1112 10:30:59.111676 140264174335808 run_lib.py:153] step: 663950, training_loss: 1.40404e+02
I1112 10:31:08.813948 140264174335808 run_lib.py:153] step: 664000, training_loss: 1.29003e+02
I1112 10:31:18.221878 140264174335808 run_lib.py:153] step: 664050, training_loss: 1.23450e+02
I1112 10:31:28.173312 140264174335808 run_lib.py:153] step: 664100, training_loss: 1.59962e+02
I1112 10:31:38.253016 140264174335808 run_lib.py:153] step: 664150, training_loss: 1.13804e+02
I1112 10:31:48.315084 140264174335808 run_lib.py:153] step: 664200, training_loss: 1.21424e+02
I1112 10:31:57.673151 140264174335808 run_lib.py:153] step: 664250, training_loss: 1.35171e+02
I1112 10:32:07.715218 140264174335808 run_lib.py:153] step: 664300, training_loss: 1.25393e+02
I1112 10:32:17.410265 140264174335808 run_lib.py:153] step: 664350, training_loss: 1.08454e+02
I1112 10:32:26.798026 140264174335808 run_lib.py:153] step: 664400, training_loss: 1.41405e+02
I1112 10:32:36.467356 140264174335808 run_lib.py:153] step: 664450, training_loss: 1.51203e+02
I1112 10:32:45.884120 140264174335808 run_lib.py:153] step: 664500, training_loss: 1.45363e+02
I1112 10:32:55.583666 140264174335808 run_lib.py:153] step: 664550, training_loss: 1.42608e+02
I1112 10:33:05.076875 140264174335808 run_lib.py:153] step: 664600, training_loss: 1.47869e+02
I1112 10:33:14.297838 140264174335808 run_lib.py:153] step: 664650, training_loss: 1.35562e+02
I1112 10:33:23.880651 140264174335808 run_lib.py:153] step: 664700, training_loss: 1.04129e+02
I1112 10:33:33.163876 140264174335808 run_lib.py:153] step: 664750, training_loss: 1.15408e+02
I1112 10:33:42.979628 140264174335808 run_lib.py:153] step: 664800, training_loss: 1.16317e+02
I1112 10:33:53.362233 140264174335808 run_lib.py:153] step: 664850, training_loss: 1.36132e+02
I1112 10:34:02.940149 140264174335808 run_lib.py:153] step: 664900, training_loss: 1.09947e+02
I1112 10:34:12.703502 140264174335808 run_lib.py:153] step: 664950, training_loss: 1.23606e+02
I1112 10:34:22.955288 140264174335808 run_lib.py:153] step: 665000, training_loss: 1.20817e+02
I1112 10:34:23.055615 140264174335808 run_lib.py:166] step: 665000, eval_loss: 1.11593e+02
I1112 10:34:32.544562 140264174335808 run_lib.py:153] step: 665050, training_loss: 1.42106e+02
I1112 10:34:42.453094 140264174335808 run_lib.py:153] step: 665100, training_loss: 1.28618e+02
I1112 10:34:51.946425 140264174335808 run_lib.py:153] step: 665150, training_loss: 9.45465e+01
I1112 10:35:01.814024 140264174335808 run_lib.py:153] step: 665200, training_loss: 1.01868e+02
I1112 10:35:11.836499 140264174335808 run_lib.py:153] step: 665250, training_loss: 1.28837e+02
I1112 10:35:21.168446 140264174335808 run_lib.py:153] step: 665300, training_loss: 1.10982e+02
I1112 10:35:31.385237 140264174335808 run_lib.py:153] step: 665350, training_loss: 1.01801e+02
I1112 10:35:41.396127 140264174335808 run_lib.py:153] step: 665400, training_loss: 1.39888e+02
I1112 10:35:51.635410 140264174335808 run_lib.py:153] step: 665450, training_loss: 1.17314e+02
I1112 10:36:01.156033 140264174335808 run_lib.py:153] step: 665500, training_loss: 1.28658e+02
I1112 10:36:11.419627 140264174335808 run_lib.py:153] step: 665550, training_loss: 1.42712e+02
I1112 10:36:21.193961 140264174335808 run_lib.py:153] step: 665600, training_loss: 1.45103e+02
I1112 10:36:30.429444 140264174335808 run_lib.py:153] step: 665650, training_loss: 1.18412e+02
I1112 10:36:39.710647 140264174335808 run_lib.py:153] step: 665700, training_loss: 1.30615e+02
I1112 10:36:49.465666 140264174335808 run_lib.py:153] step: 665750, training_loss: 9.81908e+01
I1112 10:36:59.040576 140264174335808 run_lib.py:153] step: 665800, training_loss: 1.47566e+02
I1112 10:37:08.936264 140264174335808 run_lib.py:153] step: 665850, training_loss: 1.33623e+02
I1112 10:37:18.861606 140264174335808 run_lib.py:153] step: 665900, training_loss: 1.31726e+02
I1112 10:37:28.898782 140264174335808 run_lib.py:153] step: 665950, training_loss: 1.33189e+02
I1112 10:37:38.558377 140264174335808 run_lib.py:153] step: 666000, training_loss: 1.09663e+02
I1112 10:37:48.343461 140264174335808 run_lib.py:153] step: 666050, training_loss: 1.22695e+02
I1112 10:37:58.646399 140264174335808 run_lib.py:153] step: 666100, training_loss: 1.44989e+02
I1112 10:38:08.789896 140264174335808 run_lib.py:153] step: 666150, training_loss: 1.24361e+02
I1112 10:38:18.222387 140264174335808 run_lib.py:153] step: 666200, training_loss: 1.42478e+02
I1112 10:38:27.950742 140264174335808 run_lib.py:153] step: 666250, training_loss: 1.03124e+02
I1112 10:38:37.706832 140264174335808 run_lib.py:153] step: 666300, training_loss: 1.20618e+02
I1112 10:38:47.356450 140264174335808 run_lib.py:153] step: 666350, training_loss: 1.21210e+02
I1112 10:38:57.463273 140264174335808 run_lib.py:153] step: 666400, training_loss: 1.25539e+02
I1112 10:39:06.957084 140264174335808 run_lib.py:153] step: 666450, training_loss: 1.46897e+02
I1112 10:39:16.439162 140264174335808 run_lib.py:153] step: 666500, training_loss: 1.07042e+02
I1112 10:39:25.975667 140264174335808 run_lib.py:153] step: 666550, training_loss: 1.21455e+02
I1112 10:39:36.226086 140264174335808 run_lib.py:153] step: 666600, training_loss: 1.32928e+02
I1112 10:39:45.707160 140264174335808 run_lib.py:153] step: 666650, training_loss: 1.39078e+02
I1112 10:39:55.554374 140264174335808 run_lib.py:153] step: 666700, training_loss: 1.59109e+02
I1112 10:40:05.529961 140264174335808 run_lib.py:153] step: 666750, training_loss: 1.29132e+02
I1112 10:40:15.393332 140264174335808 run_lib.py:153] step: 666800, training_loss: 1.13873e+02
I1112 10:40:24.685513 140264174335808 run_lib.py:153] step: 666850, training_loss: 1.63448e+02
I1112 10:40:34.631535 140264174335808 run_lib.py:153] step: 666900, training_loss: 1.26811e+02
I1112 10:40:45.234558 140264174335808 run_lib.py:153] step: 666950, training_loss: 1.27332e+02
I1112 10:40:54.727356 140264174335808 run_lib.py:153] step: 667000, training_loss: 1.24905e+02
I1112 10:41:04.790171 140264174335808 run_lib.py:153] step: 667050, training_loss: 1.14547e+02
I1112 10:41:14.529125 140264174335808 run_lib.py:153] step: 667100, training_loss: 1.14983e+02
I1112 10:41:24.668487 140264174335808 run_lib.py:153] step: 667150, training_loss: 1.22049e+02
I1112 10:41:34.024315 140264174335808 run_lib.py:153] step: 667200, training_loss: 1.36999e+02
I1112 10:41:43.259358 140264174335808 run_lib.py:153] step: 667250, training_loss: 1.28969e+02
I1112 10:41:53.735580 140264174335808 run_lib.py:153] step: 667300, training_loss: 1.52781e+02
I1112 10:42:04.084371 140264174335808 run_lib.py:153] step: 667350, training_loss: 1.54185e+02
I1112 10:42:13.902541 140264174335808 run_lib.py:153] step: 667400, training_loss: 1.17557e+02
I1112 10:42:23.899073 140264174335808 run_lib.py:153] step: 667450, training_loss: 1.33244e+02
I1112 10:42:33.693726 140264174335808 run_lib.py:153] step: 667500, training_loss: 1.31229e+02
I1112 10:42:43.624806 140264174335808 run_lib.py:153] step: 667550, training_loss: 1.29112e+02
I1112 10:42:53.423239 140264174335808 run_lib.py:153] step: 667600, training_loss: 1.60400e+02
I1112 10:43:02.985860 140264174335808 run_lib.py:153] step: 667650, training_loss: 1.20058e+02
I1112 10:43:13.010601 140264174335808 run_lib.py:153] step: 667700, training_loss: 1.12678e+02
I1112 10:43:23.556455 140264174335808 run_lib.py:153] step: 667750, training_loss: 1.10598e+02
I1112 10:43:33.591317 140264174335808 run_lib.py:153] step: 667800, training_loss: 1.28742e+02
I1112 10:43:43.097617 140264174335808 run_lib.py:153] step: 667850, training_loss: 1.40667e+02
I1112 10:43:53.205875 140264174335808 run_lib.py:153] step: 667900, training_loss: 1.47981e+02
I1112 10:44:03.277395 140264174335808 run_lib.py:153] step: 667950, training_loss: 1.07233e+02
I1112 10:44:13.194492 140264174335808 run_lib.py:153] step: 668000, training_loss: 1.45124e+02
I1112 10:44:22.762333 140264174335808 run_lib.py:153] step: 668050, training_loss: 1.24559e+02
I1112 10:44:32.528405 140264174335808 run_lib.py:153] step: 668100, training_loss: 1.22195e+02
I1112 10:44:42.503588 140264174335808 run_lib.py:153] step: 668150, training_loss: 1.10325e+02
I1112 10:44:52.188404 140264174335808 run_lib.py:153] step: 668200, training_loss: 1.15377e+02
I1112 10:45:02.343594 140264174335808 run_lib.py:153] step: 668250, training_loss: 1.17846e+02
I1112 10:45:12.087296 140264174335808 run_lib.py:153] step: 668300, training_loss: 1.30151e+02
I1112 10:45:22.733133 140264174335808 run_lib.py:153] step: 668350, training_loss: 1.58142e+02
I1112 10:45:32.369261 140264174335808 run_lib.py:153] step: 668400, training_loss: 1.21141e+02
I1112 10:45:41.902907 140264174335808 run_lib.py:153] step: 668450, training_loss: 1.44810e+02
I1112 10:45:51.641349 140264174335808 run_lib.py:153] step: 668500, training_loss: 1.00155e+02
I1112 10:46:01.460605 140264174335808 run_lib.py:153] step: 668550, training_loss: 1.08114e+02
I1112 10:46:11.301600 140264174335808 run_lib.py:153] step: 668600, training_loss: 1.06663e+02
I1112 10:46:20.762402 140264174335808 run_lib.py:153] step: 668650, training_loss: 1.17070e+02
I1112 10:46:30.271772 140264174335808 run_lib.py:153] step: 668700, training_loss: 1.12312e+02
I1112 10:46:39.938286 140264174335808 run_lib.py:153] step: 668750, training_loss: 1.34659e+02
I1112 10:46:49.628801 140264174335808 run_lib.py:153] step: 668800, training_loss: 1.48351e+02
I1112 10:47:00.131711 140264174335808 run_lib.py:153] step: 668850, training_loss: 1.28453e+02
I1112 10:47:10.863554 140264174335808 run_lib.py:153] step: 668900, training_loss: 1.33481e+02
I1112 10:47:20.375887 140264174335808 run_lib.py:153] step: 668950, training_loss: 1.25906e+02
I1112 10:47:30.409660 140264174335808 run_lib.py:153] step: 669000, training_loss: 1.30404e+02
I1112 10:47:40.261417 140264174335808 run_lib.py:153] step: 669050, training_loss: 1.40813e+02
I1112 10:47:50.821145 140264174335808 run_lib.py:153] step: 669100, training_loss: 1.21872e+02
I1112 10:48:00.095841 140264174335808 run_lib.py:153] step: 669150, training_loss: 1.01335e+02
I1112 10:48:09.634571 140264174335808 run_lib.py:153] step: 669200, training_loss: 1.19179e+02
I1112 10:48:19.190230 140264174335808 run_lib.py:153] step: 669250, training_loss: 1.51564e+02
I1112 10:48:28.615757 140264174335808 run_lib.py:153] step: 669300, training_loss: 1.23821e+02
I1112 10:48:39.310146 140264174335808 run_lib.py:153] step: 669350, training_loss: 1.34827e+02
I1112 10:48:49.402380 140264174335808 run_lib.py:153] step: 669400, training_loss: 1.53699e+02
I1112 10:48:59.459281 140264174335808 run_lib.py:153] step: 669450, training_loss: 1.24124e+02
I1112 10:49:09.542113 140264174335808 run_lib.py:153] step: 669500, training_loss: 9.88919e+01
I1112 10:49:19.483959 140264174335808 run_lib.py:153] step: 669550, training_loss: 1.07106e+02
I1112 10:49:29.932998 140264174335808 run_lib.py:153] step: 669600, training_loss: 1.16041e+02
I1112 10:49:39.281397 140264174335808 run_lib.py:153] step: 669650, training_loss: 1.13659e+02
I1112 10:49:49.136079 140264174335808 run_lib.py:153] step: 669700, training_loss: 1.35078e+02
I1112 10:49:59.007622 140264174335808 run_lib.py:153] step: 669750, training_loss: 1.27153e+02
I1112 10:50:08.937299 140264174335808 run_lib.py:153] step: 669800, training_loss: 1.00591e+02
I1112 10:50:18.360934 140264174335808 run_lib.py:153] step: 669850, training_loss: 1.62740e+02
I1112 10:50:29.020036 140264174335808 run_lib.py:153] step: 669900, training_loss: 1.53024e+02
I1112 10:50:39.199050 140264174335808 run_lib.py:153] step: 669950, training_loss: 1.27213e+02
I1112 10:50:49.270281 140264174335808 run_lib.py:153] step: 670000, training_loss: 1.24454e+02
I1112 10:50:49.813052 140264174335808 run_lib.py:166] step: 670000, eval_loss: 1.14195e+02
I1112 10:50:59.369995 140264174335808 run_lib.py:153] step: 670050, training_loss: 1.27068e+02
I1112 10:51:09.766558 140264174335808 run_lib.py:153] step: 670100, training_loss: 1.44132e+02
I1112 10:51:19.786152 140264174335808 run_lib.py:153] step: 670150, training_loss: 1.18386e+02
I1112 10:51:29.026109 140264174335808 run_lib.py:153] step: 670200, training_loss: 1.41973e+02
I1112 10:51:38.907065 140264174335808 run_lib.py:153] step: 670250, training_loss: 1.19051e+02
I1112 10:51:49.434715 140264174335808 run_lib.py:153] step: 670300, training_loss: 1.21053e+02
I1112 10:51:58.878020 140264174335808 run_lib.py:153] step: 670350, training_loss: 1.38728e+02
I1112 10:52:08.574990 140264174335808 run_lib.py:153] step: 670400, training_loss: 1.61191e+02
I1112 10:52:18.049947 140264174335808 run_lib.py:153] step: 670450, training_loss: 1.35702e+02
I1112 10:52:27.965750 140264174335808 run_lib.py:153] step: 670500, training_loss: 1.35693e+02
I1112 10:52:38.048050 140264174335808 run_lib.py:153] step: 670550, training_loss: 1.39200e+02
I1112 10:52:48.600471 140264174335808 run_lib.py:153] step: 670600, training_loss: 9.47109e+01
I1112 10:52:59.229764 140264174335808 run_lib.py:153] step: 670650, training_loss: 1.10537e+02
I1112 10:53:09.318305 140264174335808 run_lib.py:153] step: 670700, training_loss: 1.21941e+02
I1112 10:53:19.779930 140264174335808 run_lib.py:153] step: 670750, training_loss: 1.24615e+02
I1112 10:53:30.403126 140264174335808 run_lib.py:153] step: 670800, training_loss: 1.20589e+02
I1112 10:53:40.617959 140264174335808 run_lib.py:153] step: 670850, training_loss: 1.38503e+02
I1112 10:53:51.035864 140264174335808 run_lib.py:153] step: 670900, training_loss: 1.44302e+02
I1112 10:54:01.274299 140264174335808 run_lib.py:153] step: 670950, training_loss: 1.38278e+02
I1112 10:54:12.052714 140264174335808 run_lib.py:153] step: 671000, training_loss: 1.18348e+02
I1112 10:54:22.897791 140264174335808 run_lib.py:153] step: 671050, training_loss: 1.29776e+02
I1112 10:54:33.146600 140264174335808 run_lib.py:153] step: 671100, training_loss: 1.33057e+02
I1112 10:54:42.766137 140264174335808 run_lib.py:153] step: 671150, training_loss: 1.18041e+02
I1112 10:54:52.037434 140264174335808 run_lib.py:153] step: 671200, training_loss: 1.26212e+02
I1112 10:55:02.038809 140264174335808 run_lib.py:153] step: 671250, training_loss: 1.09737e+02
I1112 10:55:12.075531 140264174335808 run_lib.py:153] step: 671300, training_loss: 9.09592e+01
I1112 10:55:22.226664 140264174335808 run_lib.py:153] step: 671350, training_loss: 1.08292e+02
I1112 10:55:32.279302 140264174335808 run_lib.py:153] step: 671400, training_loss: 1.22444e+02
I1112 10:55:41.675729 140264174335808 run_lib.py:153] step: 671450, training_loss: 1.25286e+02
I1112 10:55:51.186661 140264174335808 run_lib.py:153] step: 671500, training_loss: 1.23217e+02
I1112 10:56:00.824713 140264174335808 run_lib.py:153] step: 671550, training_loss: 1.33666e+02
I1112 10:56:11.395302 140264174335808 run_lib.py:153] step: 671600, training_loss: 1.23650e+02
I1112 10:56:21.444961 140264174335808 run_lib.py:153] step: 671650, training_loss: 1.29074e+02
I1112 10:56:31.100236 140264174335808 run_lib.py:153] step: 671700, training_loss: 1.46027e+02
I1112 10:56:40.947430 140264174335808 run_lib.py:153] step: 671750, training_loss: 1.38662e+02
I1112 10:56:50.985125 140264174335808 run_lib.py:153] step: 671800, training_loss: 1.18999e+02
I1112 10:57:01.755031 140264174335808 run_lib.py:153] step: 671850, training_loss: 1.33773e+02
I1112 10:57:12.065117 140264174335808 run_lib.py:153] step: 671900, training_loss: 1.44376e+02
I1112 10:57:21.773897 140264174335808 run_lib.py:153] step: 671950, training_loss: 1.22380e+02
I1112 10:57:31.859735 140264174335808 run_lib.py:153] step: 672000, training_loss: 1.33306e+02
I1112 10:57:42.195224 140264174335808 run_lib.py:153] step: 672050, training_loss: 1.34151e+02
I1112 10:57:51.560858 140264174335808 run_lib.py:153] step: 672100, training_loss: 1.38030e+02
I1112 10:58:01.645463 140264174335808 run_lib.py:153] step: 672150, training_loss: 1.31674e+02
I1112 10:58:12.175039 140264174335808 run_lib.py:153] step: 672200, training_loss: 1.31084e+02
I1112 10:58:22.733518 140264174335808 run_lib.py:153] step: 672250, training_loss: 1.15461e+02
I1112 10:58:32.288651 140264174335808 run_lib.py:153] step: 672300, training_loss: 1.28136e+02
I1112 10:58:42.193393 140264174335808 run_lib.py:153] step: 672350, training_loss: 1.47065e+02
I1112 10:58:52.494129 140264174335808 run_lib.py:153] step: 672400, training_loss: 9.39258e+01
I1112 10:59:02.375049 140264174335808 run_lib.py:153] step: 672450, training_loss: 1.42339e+02
I1112 10:59:12.218922 140264174335808 run_lib.py:153] step: 672500, training_loss: 1.61623e+02
I1112 10:59:22.297146 140264174335808 run_lib.py:153] step: 672550, training_loss: 1.06685e+02
I1112 10:59:33.472367 140264174335808 run_lib.py:153] step: 672600, training_loss: 1.32161e+02
I1112 10:59:44.285426 140264174335808 run_lib.py:153] step: 672650, training_loss: 1.42691e+02
I1112 10:59:54.157415 140264174335808 run_lib.py:153] step: 672700, training_loss: 1.09497e+02
I1112 11:00:04.202020 140264174335808 run_lib.py:153] step: 672750, training_loss: 1.52388e+02
I1112 11:00:14.259398 140264174335808 run_lib.py:153] step: 672800, training_loss: 1.28494e+02
I1112 11:00:23.496596 140264174335808 run_lib.py:153] step: 672850, training_loss: 1.45820e+02
I1112 11:00:33.087961 140264174335808 run_lib.py:153] step: 672900, training_loss: 1.81780e+02
I1112 11:00:42.929580 140264174335808 run_lib.py:153] step: 672950, training_loss: 1.17118e+02
I1112 11:00:53.277436 140264174335808 run_lib.py:153] step: 673000, training_loss: 1.22201e+02
I1112 11:01:03.628871 140264174335808 run_lib.py:153] step: 673050, training_loss: 9.60902e+01
I1112 11:01:13.437567 140264174335808 run_lib.py:153] step: 673100, training_loss: 1.22143e+02
I1112 11:01:23.351897 140264174335808 run_lib.py:153] step: 673150, training_loss: 1.13543e+02
I1112 11:01:33.970291 140264174335808 run_lib.py:153] step: 673200, training_loss: 1.26073e+02
I1112 11:01:44.210000 140264174335808 run_lib.py:153] step: 673250, training_loss: 1.27739e+02
I1112 11:01:54.738616 140264174335808 run_lib.py:153] step: 673300, training_loss: 1.25303e+02
I1112 11:02:04.991248 140264174335808 run_lib.py:153] step: 673350, training_loss: 1.37843e+02
I1112 11:02:14.949568 140264174335808 run_lib.py:153] step: 673400, training_loss: 1.25783e+02
I1112 11:02:25.256518 140264174335808 run_lib.py:153] step: 673450, training_loss: 1.21742e+02
I1112 11:02:35.463143 140264174335808 run_lib.py:153] step: 673500, training_loss: 1.15123e+02
I1112 11:02:45.229830 140264174335808 run_lib.py:153] step: 673550, training_loss: 1.38600e+02
I1112 11:02:55.179226 140264174335808 run_lib.py:153] step: 673600, training_loss: 1.44769e+02
I1112 11:03:05.311970 140264174335808 run_lib.py:153] step: 673650, training_loss: 1.14882e+02
I1112 11:03:15.596508 140264174335808 run_lib.py:153] step: 673700, training_loss: 1.41082e+02
I1112 11:03:25.090989 140264174335808 run_lib.py:153] step: 673750, training_loss: 1.20290e+02
I1112 11:03:35.805175 140264174335808 run_lib.py:153] step: 673800, training_loss: 9.88075e+01
I1112 11:03:46.111062 140264174335808 run_lib.py:153] step: 673850, training_loss: 1.34157e+02
I1112 11:03:57.141842 140264174335808 run_lib.py:153] step: 673900, training_loss: 1.11825e+02
I1112 11:04:07.128710 140264174335808 run_lib.py:153] step: 673950, training_loss: 1.29475e+02
I1112 11:04:17.270578 140264174335808 run_lib.py:153] step: 674000, training_loss: 1.15413e+02
I1112 11:04:27.201329 140264174335808 run_lib.py:153] step: 674050, training_loss: 1.27548e+02
I1112 11:04:36.482617 140264174335808 run_lib.py:153] step: 674100, training_loss: 1.40439e+02
I1112 11:04:47.369251 140264174335808 run_lib.py:153] step: 674150, training_loss: 9.35580e+01
I1112 11:04:57.219786 140264174335808 run_lib.py:153] step: 674200, training_loss: 1.59760e+02
I1112 11:05:07.783179 140264174335808 run_lib.py:153] step: 674250, training_loss: 1.52109e+02
I1112 11:05:17.689545 140264174335808 run_lib.py:153] step: 674300, training_loss: 9.80213e+01
I1112 11:05:27.474113 140264174335808 run_lib.py:153] step: 674350, training_loss: 9.96325e+01
I1112 11:05:37.511138 140264174335808 run_lib.py:153] step: 674400, training_loss: 1.67873e+02
I1112 11:05:46.902739 140264174335808 run_lib.py:153] step: 674450, training_loss: 1.22255e+02
I1112 11:05:57.544082 140264174335808 run_lib.py:153] step: 674500, training_loss: 1.29585e+02
I1112 11:06:07.638480 140264174335808 run_lib.py:153] step: 674550, training_loss: 1.22474e+02
I1112 11:06:18.178382 140264174335808 run_lib.py:153] step: 674600, training_loss: 1.33877e+02
I1112 11:06:28.142453 140264174335808 run_lib.py:153] step: 674650, training_loss: 1.70882e+02
I1112 11:06:38.505456 140264174335808 run_lib.py:153] step: 674700, training_loss: 1.23172e+02
I1112 11:06:48.912159 140264174335808 run_lib.py:153] step: 674750, training_loss: 1.17705e+02
I1112 11:06:59.672211 140264174335808 run_lib.py:153] step: 674800, training_loss: 1.40707e+02
I1112 11:07:09.862169 140264174335808 run_lib.py:153] step: 674850, training_loss: 1.32135e+02
I1112 11:07:19.520906 140264174335808 run_lib.py:153] step: 674900, training_loss: 1.04976e+02
I1112 11:07:29.363705 140264174335808 run_lib.py:153] step: 674950, training_loss: 1.25010e+02
I1112 11:07:39.049673 140264174335808 run_lib.py:153] step: 675000, training_loss: 1.15562e+02
I1112 11:07:39.152819 140264174335808 run_lib.py:166] step: 675000, eval_loss: 1.08263e+02
I1112 11:07:48.806831 140264174335808 run_lib.py:153] step: 675050, training_loss: 1.56159e+02
I1112 11:07:58.913007 140264174335808 run_lib.py:153] step: 675100, training_loss: 1.20786e+02
I1112 11:08:08.831004 140264174335808 run_lib.py:153] step: 675150, training_loss: 1.20396e+02
I1112 11:08:19.374773 140264174335808 run_lib.py:153] step: 675200, training_loss: 1.15783e+02
I1112 11:08:29.934274 140264174335808 run_lib.py:153] step: 675250, training_loss: 1.21117e+02
I1112 11:08:40.548198 140264174335808 run_lib.py:153] step: 675300, training_loss: 1.51024e+02
I1112 11:08:50.914720 140264174335808 run_lib.py:153] step: 675350, training_loss: 9.83027e+01
I1112 11:09:01.839657 140264174335808 run_lib.py:153] step: 675400, training_loss: 1.12968e+02
I1112 11:09:11.345414 140264174335808 run_lib.py:153] step: 675450, training_loss: 1.09038e+02
I1112 11:09:21.549088 140264174335808 run_lib.py:153] step: 675500, training_loss: 1.26954e+02
I1112 11:09:31.408045 140264174335808 run_lib.py:153] step: 675550, training_loss: 1.16576e+02
I1112 11:09:41.265683 140264174335808 run_lib.py:153] step: 675600, training_loss: 1.11713e+02
I1112 11:09:51.396703 140264174335808 run_lib.py:153] step: 675650, training_loss: 1.14697e+02
I1112 11:10:01.532783 140264174335808 run_lib.py:153] step: 675700, training_loss: 1.07505e+02
I1112 11:10:12.068722 140264174335808 run_lib.py:153] step: 675750, training_loss: 1.49020e+02
I1112 11:10:22.572682 140264174335808 run_lib.py:153] step: 675800, training_loss: 1.20500e+02
I1112 11:10:32.237702 140264174335808 run_lib.py:153] step: 675850, training_loss: 1.15433e+02
I1112 11:10:42.201296 140264174335808 run_lib.py:153] step: 675900, training_loss: 1.26719e+02
I1112 11:10:52.284921 140264174335808 run_lib.py:153] step: 675950, training_loss: 1.22037e+02
I1112 11:11:02.457348 140264174335808 run_lib.py:153] step: 676000, training_loss: 1.15405e+02
I1112 11:11:12.050499 140264174335808 run_lib.py:153] step: 676050, training_loss: 1.51018e+02
I1112 11:11:22.291505 140264174335808 run_lib.py:153] step: 676100, training_loss: 1.59565e+02
I1112 11:11:32.614557 140264174335808 run_lib.py:153] step: 676150, training_loss: 1.00591e+02
I1112 11:11:43.003506 140264174335808 run_lib.py:153] step: 676200, training_loss: 1.41885e+02
I1112 11:11:53.051985 140264174335808 run_lib.py:153] step: 676250, training_loss: 1.27732e+02
I1112 11:12:03.162116 140264174335808 run_lib.py:153] step: 676300, training_loss: 1.42820e+02
I1112 11:12:13.257774 140264174335808 run_lib.py:153] step: 676350, training_loss: 1.12623e+02
I1112 11:12:22.774831 140264174335808 run_lib.py:153] step: 676400, training_loss: 1.40847e+02
I1112 11:12:32.551247 140264174335808 run_lib.py:153] step: 676450, training_loss: 9.73178e+01
I1112 11:12:42.066422 140264174335808 run_lib.py:153] step: 676500, training_loss: 1.23749e+02
I1112 11:12:52.394627 140264174335808 run_lib.py:153] step: 676550, training_loss: 1.16647e+02
I1112 11:13:02.432739 140264174335808 run_lib.py:153] step: 676600, training_loss: 1.55434e+02
I1112 11:13:12.807653 140264174335808 run_lib.py:153] step: 676650, training_loss: 9.78016e+01
I1112 11:13:22.820946 140264174335808 run_lib.py:153] step: 676700, training_loss: 1.38221e+02
I1112 11:13:32.795432 140264174335808 run_lib.py:153] step: 676750, training_loss: 1.30075e+02
I1112 11:13:43.502881 140264174335808 run_lib.py:153] step: 676800, training_loss: 1.44465e+02
I1112 11:13:53.932510 140264174335808 run_lib.py:153] step: 676850, training_loss: 1.02153e+02
I1112 11:14:03.584787 140264174335808 run_lib.py:153] step: 676900, training_loss: 1.35917e+02
I1112 11:14:13.628460 140264174335808 run_lib.py:153] step: 676950, training_loss: 1.18378e+02
I1112 11:14:23.687693 140264174335808 run_lib.py:153] step: 677000, training_loss: 1.38206e+02
I1112 11:14:33.950440 140264174335808 run_lib.py:153] step: 677050, training_loss: 1.32354e+02
I1112 11:14:43.813457 140264174335808 run_lib.py:153] step: 677100, training_loss: 1.18129e+02
I1112 11:14:53.527361 140264174335808 run_lib.py:153] step: 677150, training_loss: 1.24719e+02
I1112 11:15:04.025134 140264174335808 run_lib.py:153] step: 677200, training_loss: 1.24469e+02
I1112 11:15:13.722788 140264174335808 run_lib.py:153] step: 677250, training_loss: 1.22495e+02
I1112 11:15:23.725832 140264174335808 run_lib.py:153] step: 677300, training_loss: 1.33959e+02
I1112 11:15:33.343509 140264174335808 run_lib.py:153] step: 677350, training_loss: 1.07336e+02
I1112 11:15:42.736161 140264174335808 run_lib.py:153] step: 677400, training_loss: 1.36518e+02
I1112 11:15:52.788689 140264174335808 run_lib.py:153] step: 677450, training_loss: 1.39606e+02
I1112 11:16:02.685317 140264174335808 run_lib.py:153] step: 677500, training_loss: 1.35721e+02
I1112 11:16:12.323657 140264174335808 run_lib.py:153] step: 677550, training_loss: 1.30894e+02
I1112 11:16:21.660962 140264174335808 run_lib.py:153] step: 677600, training_loss: 1.21903e+02
I1112 11:16:31.706430 140264174335808 run_lib.py:153] step: 677650, training_loss: 1.26057e+02
I1112 11:16:42.462535 140264174335808 run_lib.py:153] step: 677700, training_loss: 1.34020e+02
I1112 11:16:53.371315 140264174335808 run_lib.py:153] step: 677750, training_loss: 1.15576e+02
I1112 11:17:02.833116 140264174335808 run_lib.py:153] step: 677800, training_loss: 1.27448e+02
I1112 11:17:13.124126 140264174335808 run_lib.py:153] step: 677850, training_loss: 1.18624e+02
I1112 11:17:23.243853 140264174335808 run_lib.py:153] step: 677900, training_loss: 1.33711e+02
I1112 11:17:33.264974 140264174335808 run_lib.py:153] step: 677950, training_loss: 1.62413e+02
I1112 11:17:43.422191 140264174335808 run_lib.py:153] step: 678000, training_loss: 1.35735e+02
I1112 11:17:53.139248 140264174335808 run_lib.py:153] step: 678050, training_loss: 1.06627e+02
I1112 11:18:03.392940 140264174335808 run_lib.py:153] step: 678100, training_loss: 1.60334e+02
I1112 11:18:14.069736 140264174335808 run_lib.py:153] step: 678150, training_loss: 1.69838e+02
I1112 11:18:24.110293 140264174335808 run_lib.py:153] step: 678200, training_loss: 1.34883e+02
I1112 11:18:34.780147 140264174335808 run_lib.py:153] step: 678250, training_loss: 1.30700e+02
I1112 11:18:44.914803 140264174335808 run_lib.py:153] step: 678300, training_loss: 1.03465e+02
I1112 11:18:55.225571 140264174335808 run_lib.py:153] step: 678350, training_loss: 1.38476e+02
I1112 11:19:05.267143 140264174335808 run_lib.py:153] step: 678400, training_loss: 1.33550e+02
I1112 11:19:14.935407 140264174335808 run_lib.py:153] step: 678450, training_loss: 1.20899e+02
I1112 11:19:25.603225 140264174335808 run_lib.py:153] step: 678500, training_loss: 1.19357e+02
I1112 11:19:35.891995 140264174335808 run_lib.py:153] step: 678550, training_loss: 1.33023e+02
I1112 11:19:45.501752 140264174335808 run_lib.py:153] step: 678600, training_loss: 1.05906e+02
I1112 11:19:55.449860 140264174335808 run_lib.py:153] step: 678650, training_loss: 1.13460e+02
I1112 11:20:05.611699 140264174335808 run_lib.py:153] step: 678700, training_loss: 1.15239e+02
I1112 11:20:15.258942 140264174335808 run_lib.py:153] step: 678750, training_loss: 1.03737e+02
I1112 11:20:25.834123 140264174335808 run_lib.py:153] step: 678800, training_loss: 1.61565e+02
I1112 11:20:36.533678 140264174335808 run_lib.py:153] step: 678850, training_loss: 1.32806e+02
I1112 11:20:46.709393 140264174335808 run_lib.py:153] step: 678900, training_loss: 1.53666e+02
I1112 11:20:57.373659 140264174335808 run_lib.py:153] step: 678950, training_loss: 1.07351e+02
I1112 11:21:07.366876 140264174335808 run_lib.py:153] step: 679000, training_loss: 1.28859e+02
I1112 11:21:17.518328 140264174335808 run_lib.py:153] step: 679050, training_loss: 1.73466e+02
I1112 11:21:27.490206 140264174335808 run_lib.py:153] step: 679100, training_loss: 1.32490e+02
I1112 11:21:37.452599 140264174335808 run_lib.py:153] step: 679150, training_loss: 1.37872e+02
I1112 11:21:47.190339 140264174335808 run_lib.py:153] step: 679200, training_loss: 1.83033e+02
I1112 11:21:57.276662 140264174335808 run_lib.py:153] step: 679250, training_loss: 1.32772e+02
I1112 11:22:07.297718 140264174335808 run_lib.py:153] step: 679300, training_loss: 1.29120e+02
I1112 11:22:17.649864 140264174335808 run_lib.py:153] step: 679350, training_loss: 1.25765e+02
I1112 11:22:27.319384 140264174335808 run_lib.py:153] step: 679400, training_loss: 1.26210e+02
I1112 11:22:37.603590 140264174335808 run_lib.py:153] step: 679450, training_loss: 1.29608e+02
I1112 11:22:48.531248 140264174335808 run_lib.py:153] step: 679500, training_loss: 1.10092e+02
I1112 11:22:58.987702 140264174335808 run_lib.py:153] step: 679550, training_loss: 1.48043e+02
I1112 11:23:09.049656 140264174335808 run_lib.py:153] step: 679600, training_loss: 1.13549e+02
I1112 11:23:18.881406 140264174335808 run_lib.py:153] step: 679650, training_loss: 1.27643e+02
I1112 11:23:29.309678 140264174335808 run_lib.py:153] step: 679700, training_loss: 1.42336e+02
I1112 11:23:39.762608 140264174335808 run_lib.py:153] step: 679750, training_loss: 9.17288e+01
I1112 11:23:49.882525 140264174335808 run_lib.py:153] step: 679800, training_loss: 1.11163e+02
I1112 11:24:00.610287 140264174335808 run_lib.py:153] step: 679850, training_loss: 1.22783e+02
I1112 11:24:10.355198 140264174335808 run_lib.py:153] step: 679900, training_loss: 1.23855e+02
I1112 11:24:20.308240 140264174335808 run_lib.py:153] step: 679950, training_loss: 1.63393e+02
I1112 11:24:30.812066 140264174335808 run_lib.py:153] step: 680000, training_loss: 1.24651e+02
I1112 11:24:31.353530 140264174335808 run_lib.py:166] step: 680000, eval_loss: 1.33382e+02
I1112 11:24:40.600164 140264174335808 run_lib.py:153] step: 680050, training_loss: 1.30838e+02
I1112 11:24:50.740051 140264174335808 run_lib.py:153] step: 680100, training_loss: 1.41831e+02
I1112 11:25:01.470609 140264174335808 run_lib.py:153] step: 680150, training_loss: 1.29875e+02
I1112 11:25:11.622593 140264174335808 run_lib.py:153] step: 680200, training_loss: 9.90767e+01
I1112 11:25:21.208527 140264174335808 run_lib.py:153] step: 680250, training_loss: 1.33309e+02
I1112 11:25:30.694053 140264174335808 run_lib.py:153] step: 680300, training_loss: 1.38547e+02
I1112 11:25:41.080220 140264174335808 run_lib.py:153] step: 680350, training_loss: 1.25986e+02
I1112 11:25:51.212533 140264174335808 run_lib.py:153] step: 680400, training_loss: 1.07506e+02
I1112 11:26:01.583354 140264174335808 run_lib.py:153] step: 680450, training_loss: 1.28870e+02
I1112 11:26:12.646421 140264174335808 run_lib.py:153] step: 680500, training_loss: 1.25804e+02
I1112 11:26:22.261837 140264174335808 run_lib.py:153] step: 680550, training_loss: 1.31173e+02
I1112 11:26:32.821085 140264174335808 run_lib.py:153] step: 680600, training_loss: 1.02099e+02
I1112 11:26:43.192651 140264174335808 run_lib.py:153] step: 680650, training_loss: 1.25098e+02
I1112 11:26:53.167078 140264174335808 run_lib.py:153] step: 680700, training_loss: 1.27078e+02
I1112 11:27:03.126801 140264174335808 run_lib.py:153] step: 680750, training_loss: 1.18501e+02
I1112 11:27:12.723961 140264174335808 run_lib.py:153] step: 680800, training_loss: 1.02458e+02
I1112 11:27:22.627465 140264174335808 run_lib.py:153] step: 680850, training_loss: 1.39733e+02
I1112 11:27:32.884546 140264174335808 run_lib.py:153] step: 680900, training_loss: 1.41199e+02
I1112 11:27:42.324576 140264174335808 run_lib.py:153] step: 680950, training_loss: 1.51321e+02
I1112 11:27:51.924513 140264174335808 run_lib.py:153] step: 681000, training_loss: 1.44762e+02
I1112 11:28:01.855263 140264174335808 run_lib.py:153] step: 681050, training_loss: 1.53178e+02
I1112 11:28:11.269959 140264174335808 run_lib.py:153] step: 681100, training_loss: 1.47794e+02
I1112 11:28:21.958125 140264174335808 run_lib.py:153] step: 681150, training_loss: 1.24858e+02
I1112 11:28:32.876487 140264174335808 run_lib.py:153] step: 681200, training_loss: 1.21032e+02
I1112 11:28:42.869401 140264174335808 run_lib.py:153] step: 681250, training_loss: 1.38018e+02
I1112 11:28:52.990043 140264174335808 run_lib.py:153] step: 681300, training_loss: 1.07185e+02
I1112 11:29:02.999918 140264174335808 run_lib.py:153] step: 681350, training_loss: 1.10448e+02
I1112 11:29:12.751218 140264174335808 run_lib.py:153] step: 681400, training_loss: 1.28721e+02
I1112 11:29:23.797041 140264174335808 run_lib.py:153] step: 681450, training_loss: 1.46109e+02
I1112 11:29:34.335652 140264174335808 run_lib.py:153] step: 681500, training_loss: 1.39729e+02
I1112 11:29:43.626981 140264174335808 run_lib.py:153] step: 681550, training_loss: 1.65948e+02
I1112 11:29:53.199525 140264174335808 run_lib.py:153] step: 681600, training_loss: 1.05071e+02
I1112 11:30:03.850348 140264174335808 run_lib.py:153] step: 681650, training_loss: 1.46173e+02
I1112 11:30:13.631243 140264174335808 run_lib.py:153] step: 681700, training_loss: 1.29337e+02
I1112 11:30:23.240268 140264174335808 run_lib.py:153] step: 681750, training_loss: 1.22533e+02
I1112 11:30:33.324208 140264174335808 run_lib.py:153] step: 681800, training_loss: 1.35872e+02
I1112 11:30:43.675673 140264174335808 run_lib.py:153] step: 681850, training_loss: 1.13647e+02
I1112 11:30:53.460032 140264174335808 run_lib.py:153] step: 681900, training_loss: 1.32833e+02
I1112 11:31:04.075857 140264174335808 run_lib.py:153] step: 681950, training_loss: 1.16352e+02
I1112 11:31:14.953422 140264174335808 run_lib.py:153] step: 682000, training_loss: 1.12541e+02
I1112 11:31:25.056113 140264174335808 run_lib.py:153] step: 682050, training_loss: 1.30790e+02
I1112 11:31:34.756853 140264174335808 run_lib.py:153] step: 682100, training_loss: 1.20057e+02
I1112 11:31:45.097190 140264174335808 run_lib.py:153] step: 682150, training_loss: 1.15755e+02
I1112 11:31:55.657881 140264174335808 run_lib.py:153] step: 682200, training_loss: 1.41138e+02
I1112 11:32:05.319868 140264174335808 run_lib.py:153] step: 682250, training_loss: 1.17487e+02
I1112 11:32:15.720633 140264174335808 run_lib.py:153] step: 682300, training_loss: 1.34944e+02
I1112 11:32:25.963857 140264174335808 run_lib.py:153] step: 682350, training_loss: 1.12926e+02
I1112 11:32:36.263150 140264174335808 run_lib.py:153] step: 682400, training_loss: 1.13356e+02
I1112 11:32:47.331286 140264174335808 run_lib.py:153] step: 682450, training_loss: 1.15895e+02
I1112 11:32:57.438122 140264174335808 run_lib.py:153] step: 682500, training_loss: 1.09241e+02
I1112 11:33:07.082026 140264174335808 run_lib.py:153] step: 682550, training_loss: 1.42294e+02
I1112 11:33:17.439709 140264174335808 run_lib.py:153] step: 682600, training_loss: 1.36936e+02
I1112 11:33:27.179293 140264174335808 run_lib.py:153] step: 682650, training_loss: 1.35686e+02
I1112 11:33:37.605369 140264174335808 run_lib.py:153] step: 682700, training_loss: 1.33649e+02
I1112 11:33:47.495884 140264174335808 run_lib.py:153] step: 682750, training_loss: 1.20572e+02
I1112 11:33:57.409419 140264174335808 run_lib.py:153] step: 682800, training_loss: 1.60618e+02
I1112 11:34:07.373286 140264174335808 run_lib.py:153] step: 682850, training_loss: 1.31743e+02
I1112 11:34:17.147712 140264174335808 run_lib.py:153] step: 682900, training_loss: 1.26084e+02
I1112 11:34:26.758092 140264174335808 run_lib.py:153] step: 682950, training_loss: 1.30371e+02
I1112 11:34:36.930376 140264174335808 run_lib.py:153] step: 683000, training_loss: 1.39783e+02
I1112 11:34:46.295553 140264174335808 run_lib.py:153] step: 683050, training_loss: 1.19360e+02
I1112 11:34:55.878898 140264174335808 run_lib.py:153] step: 683100, training_loss: 1.26383e+02
I1112 11:35:05.425419 140264174335808 run_lib.py:153] step: 683150, training_loss: 1.12982e+02
I1112 11:35:15.094876 140264174335808 run_lib.py:153] step: 683200, training_loss: 1.03083e+02
I1112 11:35:25.065387 140264174335808 run_lib.py:153] step: 683250, training_loss: 1.16295e+02
I1112 11:35:35.405503 140264174335808 run_lib.py:153] step: 683300, training_loss: 1.12044e+02
I1112 11:35:45.675415 140264174335808 run_lib.py:153] step: 683350, training_loss: 1.19134e+02
I1112 11:35:55.900172 140264174335808 run_lib.py:153] step: 683400, training_loss: 1.58106e+02
I1112 11:36:05.944019 140264174335808 run_lib.py:153] step: 683450, training_loss: 1.37206e+02
I1112 11:36:15.433703 140264174335808 run_lib.py:153] step: 683500, training_loss: 9.22662e+01
I1112 11:36:25.750490 140264174335808 run_lib.py:153] step: 683550, training_loss: 1.31359e+02
I1112 11:36:36.135940 140264174335808 run_lib.py:153] step: 683600, training_loss: 1.41994e+02
I1112 11:36:46.169184 140264174335808 run_lib.py:153] step: 683650, training_loss: 9.81656e+01
I1112 11:36:56.198179 140264174335808 run_lib.py:153] step: 683700, training_loss: 1.25806e+02
I1112 11:37:06.315956 140264174335808 run_lib.py:153] step: 683750, training_loss: 1.41728e+02
I1112 11:37:15.932907 140264174335808 run_lib.py:153] step: 683800, training_loss: 1.39074e+02
I1112 11:37:25.407836 140264174335808 run_lib.py:153] step: 683850, training_loss: 1.09183e+02
I1112 11:37:35.110387 140264174335808 run_lib.py:153] step: 683900, training_loss: 1.12192e+02
I1112 11:37:45.943539 140264174335808 run_lib.py:153] step: 683950, training_loss: 1.17253e+02
I1112 11:37:56.444071 140264174335808 run_lib.py:153] step: 684000, training_loss: 1.22917e+02
I1112 11:38:06.145905 140264174335808 run_lib.py:153] step: 684050, training_loss: 1.15099e+02
I1112 11:38:16.423110 140264174335808 run_lib.py:153] step: 684100, training_loss: 1.04337e+02
I1112 11:38:26.414251 140264174335808 run_lib.py:153] step: 684150, training_loss: 1.19811e+02
I1112 11:38:36.846518 140264174335808 run_lib.py:153] step: 684200, training_loss: 1.32167e+02
I1112 11:38:47.161262 140264174335808 run_lib.py:153] step: 684250, training_loss: 1.60023e+02
I1112 11:38:57.870629 140264174335808 run_lib.py:153] step: 684300, training_loss: 1.23195e+02
I1112 11:39:08.153242 140264174335808 run_lib.py:153] step: 684350, training_loss: 1.24327e+02
I1112 11:39:18.266018 140264174335808 run_lib.py:153] step: 684400, training_loss: 1.15829e+02
I1112 11:39:28.746916 140264174335808 run_lib.py:153] step: 684450, training_loss: 1.22216e+02
I1112 11:39:39.128513 140264174335808 run_lib.py:153] step: 684500, training_loss: 1.12163e+02
I1112 11:39:49.268017 140264174335808 run_lib.py:153] step: 684550, training_loss: 1.15340e+02
I1112 11:39:58.946868 140264174335808 run_lib.py:153] step: 684600, training_loss: 1.36476e+02
I1112 11:40:09.251731 140264174335808 run_lib.py:153] step: 684650, training_loss: 1.54092e+02
I1112 11:40:20.035273 140264174335808 run_lib.py:153] step: 684700, training_loss: 1.64729e+02
I1112 11:40:29.972397 140264174335808 run_lib.py:153] step: 684750, training_loss: 1.36087e+02
I1112 11:40:39.552350 140264174335808 run_lib.py:153] step: 684800, training_loss: 1.17628e+02
I1112 11:40:49.442072 140264174335808 run_lib.py:153] step: 684850, training_loss: 1.11820e+02
I1112 11:40:59.813804 140264174335808 run_lib.py:153] step: 684900, training_loss: 1.22584e+02
I1112 11:41:09.482167 140264174335808 run_lib.py:153] step: 684950, training_loss: 1.24514e+02
I1112 11:41:19.322066 140264174335808 run_lib.py:153] step: 685000, training_loss: 1.22262e+02
I1112 11:41:19.423428 140264174335808 run_lib.py:166] step: 685000, eval_loss: 1.48260e+02
I1112 11:41:29.697551 140264174335808 run_lib.py:153] step: 685050, training_loss: 1.48431e+02
I1112 11:41:39.347981 140264174335808 run_lib.py:153] step: 685100, training_loss: 1.06341e+02
I1112 11:41:49.404292 140264174335808 run_lib.py:153] step: 685150, training_loss: 9.57010e+01
I1112 11:42:00.300385 140264174335808 run_lib.py:153] step: 685200, training_loss: 1.22890e+02
I1112 11:42:10.515667 140264174335808 run_lib.py:153] step: 685250, training_loss: 1.04814e+02
I1112 11:42:20.947218 140264174335808 run_lib.py:153] step: 685300, training_loss: 1.40280e+02
I1112 11:42:31.456171 140264174335808 run_lib.py:153] step: 685350, training_loss: 1.45569e+02
I1112 11:42:42.097250 140264174335808 run_lib.py:153] step: 685400, training_loss: 1.61079e+02
I1112 11:42:52.980006 140264174335808 run_lib.py:153] step: 685450, training_loss: 1.23963e+02
I1112 11:43:03.687169 140264174335808 run_lib.py:153] step: 685500, training_loss: 1.01230e+02
I1112 11:43:14.202901 140264174335808 run_lib.py:153] step: 685550, training_loss: 1.15823e+02
I1112 11:43:24.041867 140264174335808 run_lib.py:153] step: 685600, training_loss: 9.30524e+01
I1112 11:43:34.136280 140264174335808 run_lib.py:153] step: 685650, training_loss: 1.32289e+02
I1112 11:43:44.477413 140264174335808 run_lib.py:153] step: 685700, training_loss: 1.32733e+02
I1112 11:43:53.937138 140264174335808 run_lib.py:153] step: 685750, training_loss: 1.19260e+02
I1112 11:44:03.791008 140264174335808 run_lib.py:153] step: 685800, training_loss: 1.45524e+02
I1112 11:44:13.523997 140264174335808 run_lib.py:153] step: 685850, training_loss: 1.45346e+02
I1112 11:44:23.100895 140264174335808 run_lib.py:153] step: 685900, training_loss: 1.16230e+02
I1112 11:44:33.136796 140264174335808 run_lib.py:153] step: 685950, training_loss: 1.09557e+02
I1112 11:44:43.500652 140264174335808 run_lib.py:153] step: 686000, training_loss: 1.31990e+02
I1112 11:44:53.776211 140264174335808 run_lib.py:153] step: 686050, training_loss: 1.18235e+02
I1112 11:45:03.950491 140264174335808 run_lib.py:153] step: 686100, training_loss: 1.20868e+02
I1112 11:45:13.919030 140264174335808 run_lib.py:153] step: 686150, training_loss: 9.19293e+01
I1112 11:45:24.057475 140264174335808 run_lib.py:153] step: 686200, training_loss: 1.17026e+02
I1112 11:45:34.058485 140264174335808 run_lib.py:153] step: 686250, training_loss: 1.41702e+02
I1112 11:45:44.342103 140264174335808 run_lib.py:153] step: 686300, training_loss: 1.18838e+02
I1112 11:45:54.492373 140264174335808 run_lib.py:153] step: 686350, training_loss: 1.43363e+02
I1112 11:46:04.353885 140264174335808 run_lib.py:153] step: 686400, training_loss: 1.05938e+02
I1112 11:46:13.668113 140264174335808 run_lib.py:153] step: 686450, training_loss: 1.47876e+02
I1112 11:46:23.255774 140264174335808 run_lib.py:153] step: 686500, training_loss: 1.32497e+02
I1112 11:46:32.863725 140264174335808 run_lib.py:153] step: 686550, training_loss: 1.12059e+02
I1112 11:46:42.754546 140264174335808 run_lib.py:153] step: 686600, training_loss: 1.50861e+02
I1112 11:46:52.526753 140264174335808 run_lib.py:153] step: 686650, training_loss: 1.32965e+02
I1112 11:47:02.836069 140264174335808 run_lib.py:153] step: 686700, training_loss: 1.38092e+02
I1112 11:47:12.448391 140264174335808 run_lib.py:153] step: 686750, training_loss: 1.20541e+02
I1112 11:47:22.820482 140264174335808 run_lib.py:153] step: 686800, training_loss: 1.28488e+02
I1112 11:47:32.377433 140264174335808 run_lib.py:153] step: 686850, training_loss: 1.12458e+02
I1112 11:47:42.148018 140264174335808 run_lib.py:153] step: 686900, training_loss: 1.07780e+02
I1112 11:47:51.829698 140264174335808 run_lib.py:153] step: 686950, training_loss: 1.09736e+02
I1112 11:48:01.167457 140264174335808 run_lib.py:153] step: 687000, training_loss: 1.11091e+02
I1112 11:48:10.753546 140264174335808 run_lib.py:153] step: 687050, training_loss: 1.32135e+02
I1112 11:48:20.648141 140264174335808 run_lib.py:153] step: 687100, training_loss: 1.09331e+02
I1112 11:48:31.023581 140264174335808 run_lib.py:153] step: 687150, training_loss: 1.05771e+02
I1112 11:48:40.406461 140264174335808 run_lib.py:153] step: 687200, training_loss: 1.32726e+02
I1112 11:48:50.358240 140264174335808 run_lib.py:153] step: 687250, training_loss: 1.39875e+02
I1112 11:49:00.003699 140264174335808 run_lib.py:153] step: 687300, training_loss: 1.18065e+02
I1112 11:49:10.359757 140264174335808 run_lib.py:153] step: 687350, training_loss: 1.12131e+02
I1112 11:49:20.107635 140264174335808 run_lib.py:153] step: 687400, training_loss: 1.43455e+02
I1112 11:49:30.367480 140264174335808 run_lib.py:153] step: 687450, training_loss: 1.34542e+02
I1112 11:49:40.977038 140264174335808 run_lib.py:153] step: 687500, training_loss: 1.48798e+02
I1112 11:49:51.429779 140264174335808 run_lib.py:153] step: 687550, training_loss: 1.21986e+02
I1112 11:50:01.232325 140264174335808 run_lib.py:153] step: 687600, training_loss: 1.32122e+02
I1112 11:50:10.687222 140264174335808 run_lib.py:153] step: 687650, training_loss: 1.47236e+02
I1112 11:50:20.704283 140264174335808 run_lib.py:153] step: 687700, training_loss: 1.43517e+02
I1112 11:50:30.380571 140264174335808 run_lib.py:153] step: 687750, training_loss: 1.06585e+02
I1112 11:50:40.407344 140264174335808 run_lib.py:153] step: 687800, training_loss: 1.27250e+02
I1112 11:50:50.495263 140264174335808 run_lib.py:153] step: 687850, training_loss: 1.22810e+02
I1112 11:51:00.246667 140264174335808 run_lib.py:153] step: 687900, training_loss: 1.45196e+02
I1112 11:51:10.268771 140264174335808 run_lib.py:153] step: 687950, training_loss: 1.13883e+02
I1112 11:51:20.297616 140264174335808 run_lib.py:153] step: 688000, training_loss: 1.38239e+02
I1112 11:51:30.814895 140264174335808 run_lib.py:153] step: 688050, training_loss: 1.36846e+02
I1112 11:51:40.422299 140264174335808 run_lib.py:153] step: 688100, training_loss: 1.10332e+02
I1112 11:51:51.188286 140264174335808 run_lib.py:153] step: 688150, training_loss: 1.47026e+02
I1112 11:52:01.173432 140264174335808 run_lib.py:153] step: 688200, training_loss: 1.23818e+02
I1112 11:52:10.734229 140264174335808 run_lib.py:153] step: 688250, training_loss: 1.29087e+02
I1112 11:52:20.223480 140264174335808 run_lib.py:153] step: 688300, training_loss: 9.46503e+01
I1112 11:52:30.104269 140264174335808 run_lib.py:153] step: 688350, training_loss: 1.45236e+02
I1112 11:52:39.775727 140264174335808 run_lib.py:153] step: 688400, training_loss: 1.38348e+02
I1112 11:52:49.382070 140264174335808 run_lib.py:153] step: 688450, training_loss: 1.33485e+02
I1112 11:52:58.810313 140264174335808 run_lib.py:153] step: 688500, training_loss: 1.28674e+02
I1112 11:53:08.795886 140264174335808 run_lib.py:153] step: 688550, training_loss: 1.15675e+02
I1112 11:53:18.345899 140264174335808 run_lib.py:153] step: 688600, training_loss: 1.17678e+02
I1112 11:53:28.752016 140264174335808 run_lib.py:153] step: 688650, training_loss: 1.34593e+02
I1112 11:53:38.646869 140264174335808 run_lib.py:153] step: 688700, training_loss: 1.03340e+02
I1112 11:53:48.313415 140264174335808 run_lib.py:153] step: 688750, training_loss: 1.41704e+02
I1112 11:53:58.269737 140264174335808 run_lib.py:153] step: 688800, training_loss: 1.26428e+02
I1112 11:54:08.316301 140264174335808 run_lib.py:153] step: 688850, training_loss: 8.91884e+01
I1112 11:54:18.297097 140264174335808 run_lib.py:153] step: 688900, training_loss: 1.29020e+02
I1112 11:54:27.615659 140264174335808 run_lib.py:153] step: 688950, training_loss: 1.15467e+02
I1112 11:54:37.562266 140264174335808 run_lib.py:153] step: 689000, training_loss: 1.10825e+02
I1112 11:54:47.212780 140264174335808 run_lib.py:153] step: 689050, training_loss: 1.41966e+02
I1112 11:54:57.819430 140264174335808 run_lib.py:153] step: 689100, training_loss: 1.37135e+02
I1112 11:55:07.520220 140264174335808 run_lib.py:153] step: 689150, training_loss: 1.29152e+02
I1112 11:55:17.861740 140264174335808 run_lib.py:153] step: 689200, training_loss: 8.17330e+01
I1112 11:55:28.441052 140264174335808 run_lib.py:153] step: 689250, training_loss: 1.34689e+02
I1112 11:55:38.571386 140264174335808 run_lib.py:153] step: 689300, training_loss: 1.19218e+02
I1112 11:55:48.355154 140264174335808 run_lib.py:153] step: 689350, training_loss: 1.34749e+02
I1112 11:55:58.624805 140264174335808 run_lib.py:153] step: 689400, training_loss: 1.33053e+02
I1112 11:56:08.731251 140264174335808 run_lib.py:153] step: 689450, training_loss: 1.00713e+02
I1112 11:56:18.487365 140264174335808 run_lib.py:153] step: 689500, training_loss: 1.24702e+02
I1112 11:56:28.644981 140264174335808 run_lib.py:153] step: 689550, training_loss: 1.23909e+02
I1112 11:56:38.628665 140264174335808 run_lib.py:153] step: 689600, training_loss: 1.26418e+02
I1112 11:56:48.417400 140264174335808 run_lib.py:153] step: 689650, training_loss: 9.65434e+01
I1112 11:56:58.017374 140264174335808 run_lib.py:153] step: 689700, training_loss: 1.29285e+02
I1112 11:57:08.286605 140264174335808 run_lib.py:153] step: 689750, training_loss: 1.40638e+02
I1112 11:57:18.788313 140264174335808 run_lib.py:153] step: 689800, training_loss: 1.37748e+02
I1112 11:57:30.005523 140264174335808 run_lib.py:153] step: 689850, training_loss: 1.30925e+02
I1112 11:57:40.218944 140264174335808 run_lib.py:153] step: 689900, training_loss: 1.24744e+02
I1112 11:57:50.252755 140264174335808 run_lib.py:153] step: 689950, training_loss: 1.19559e+02
I1112 11:58:00.109585 140264174335808 run_lib.py:153] step: 690000, training_loss: 1.14748e+02
I1112 11:58:00.707182 140264174335808 run_lib.py:166] step: 690000, eval_loss: 1.37302e+02
I1112 11:58:10.049969 140264174335808 run_lib.py:153] step: 690050, training_loss: 1.31857e+02
I1112 11:58:20.528942 140264174335808 run_lib.py:153] step: 690100, training_loss: 1.01317e+02
I1112 11:58:30.730866 140264174335808 run_lib.py:153] step: 690150, training_loss: 1.06236e+02
I1112 11:58:40.604359 140264174335808 run_lib.py:153] step: 690200, training_loss: 1.33387e+02
I1112 11:58:51.177360 140264174335808 run_lib.py:153] step: 690250, training_loss: 1.32119e+02
I1112 11:59:01.241328 140264174335808 run_lib.py:153] step: 690300, training_loss: 1.26301e+02
I1112 11:59:10.586756 140264174335808 run_lib.py:153] step: 690350, training_loss: 9.51087e+01
I1112 11:59:20.460883 140264174335808 run_lib.py:153] step: 690400, training_loss: 1.08261e+02
I1112 11:59:30.335749 140264174335808 run_lib.py:153] step: 690450, training_loss: 1.26534e+02
I1112 11:59:40.402666 140264174335808 run_lib.py:153] step: 690500, training_loss: 1.47221e+02
I1112 11:59:50.299996 140264174335808 run_lib.py:153] step: 690550, training_loss: 1.05911e+02
I1112 12:00:00.571641 140264174335808 run_lib.py:153] step: 690600, training_loss: 1.33229e+02
I1112 12:00:10.363327 140264174335808 run_lib.py:153] step: 690650, training_loss: 1.25709e+02
I1112 12:00:19.818839 140264174335808 run_lib.py:153] step: 690700, training_loss: 1.08231e+02
I1112 12:00:29.719340 140264174335808 run_lib.py:153] step: 690750, training_loss: 1.47948e+02
I1112 12:00:39.639369 140264174335808 run_lib.py:153] step: 690800, training_loss: 1.46740e+02
I1112 12:00:50.012761 140264174335808 run_lib.py:153] step: 690850, training_loss: 1.23150e+02
I1112 12:01:00.188935 140264174335808 run_lib.py:153] step: 690900, training_loss: 1.40373e+02
I1112 12:01:09.713067 140264174335808 run_lib.py:153] step: 690950, training_loss: 1.31923e+02
I1112 12:01:19.291645 140264174335808 run_lib.py:153] step: 691000, training_loss: 1.29368e+02
I1112 12:01:29.219645 140264174335808 run_lib.py:153] step: 691050, training_loss: 1.20201e+02
I1112 12:01:38.992935 140264174335808 run_lib.py:153] step: 691100, training_loss: 1.16072e+02
I1112 12:01:48.963111 140264174335808 run_lib.py:153] step: 691150, training_loss: 1.30672e+02
I1112 12:01:58.811487 140264174335808 run_lib.py:153] step: 691200, training_loss: 1.39154e+02
I1112 12:02:08.991381 140264174335808 run_lib.py:153] step: 691250, training_loss: 1.01945e+02
I1112 12:02:19.142462 140264174335808 run_lib.py:153] step: 691300, training_loss: 1.41746e+02
I1112 12:02:29.702140 140264174335808 run_lib.py:153] step: 691350, training_loss: 1.08537e+02
I1112 12:02:39.970139 140264174335808 run_lib.py:153] step: 691400, training_loss: 1.38541e+02
I1112 12:02:50.226166 140264174335808 run_lib.py:153] step: 691450, training_loss: 1.13356e+02
I1112 12:02:59.718267 140264174335808 run_lib.py:153] step: 691500, training_loss: 1.44872e+02
I1112 12:03:09.952749 140264174335808 run_lib.py:153] step: 691550, training_loss: 1.08930e+02
I1112 12:03:19.737177 140264174335808 run_lib.py:153] step: 691600, training_loss: 1.32953e+02
I1112 12:03:29.396425 140264174335808 run_lib.py:153] step: 691650, training_loss: 1.33791e+02
I1112 12:03:39.556853 140264174335808 run_lib.py:153] step: 691700, training_loss: 1.35657e+02
I1112 12:03:49.268159 140264174335808 run_lib.py:153] step: 691750, training_loss: 1.50026e+02
I1112 12:03:59.909907 140264174335808 run_lib.py:153] step: 691800, training_loss: 1.33423e+02
I1112 12:04:10.483695 140264174335808 run_lib.py:153] step: 691850, training_loss: 1.10616e+02
I1112 12:04:20.756305 140264174335808 run_lib.py:153] step: 691900, training_loss: 1.06230e+02
I1112 12:04:31.366596 140264174335808 run_lib.py:153] step: 691950, training_loss: 1.61574e+02
I1112 12:04:41.798361 140264174335808 run_lib.py:153] step: 692000, training_loss: 1.25221e+02
I1112 12:04:51.867781 140264174335808 run_lib.py:153] step: 692050, training_loss: 1.10639e+02
I1112 12:05:01.676696 140264174335808 run_lib.py:153] step: 692100, training_loss: 1.34039e+02
I1112 12:05:11.802088 140264174335808 run_lib.py:153] step: 692150, training_loss: 1.36536e+02
I1112 12:05:22.451933 140264174335808 run_lib.py:153] step: 692200, training_loss: 1.05748e+02
I1112 12:05:32.575005 140264174335808 run_lib.py:153] step: 692250, training_loss: 1.40586e+02
I1112 12:05:42.667326 140264174335808 run_lib.py:153] step: 692300, training_loss: 1.33965e+02
I1112 12:05:52.703575 140264174335808 run_lib.py:153] step: 692350, training_loss: 1.20388e+02
I1112 12:06:03.027203 140264174335808 run_lib.py:153] step: 692400, training_loss: 1.14751e+02
I1112 12:06:13.160535 140264174335808 run_lib.py:153] step: 692450, training_loss: 1.24895e+02
I1112 12:06:23.476441 140264174335808 run_lib.py:153] step: 692500, training_loss: 9.27457e+01
I1112 12:06:33.898915 140264174335808 run_lib.py:153] step: 692550, training_loss: 1.19636e+02
I1112 12:06:43.424397 140264174335808 run_lib.py:153] step: 692600, training_loss: 1.48591e+02
I1112 12:06:53.901778 140264174335808 run_lib.py:153] step: 692650, training_loss: 1.22317e+02
I1112 12:07:03.885975 140264174335808 run_lib.py:153] step: 692700, training_loss: 1.09603e+02
I1112 12:07:13.899880 140264174335808 run_lib.py:153] step: 692750, training_loss: 1.11641e+02
I1112 12:07:23.465015 140264174335808 run_lib.py:153] step: 692800, training_loss: 1.38461e+02
I1112 12:07:33.751031 140264174335808 run_lib.py:153] step: 692850, training_loss: 9.14864e+01
I1112 12:07:43.831464 140264174335808 run_lib.py:153] step: 692900, training_loss: 1.30369e+02
I1112 12:07:54.506856 140264174335808 run_lib.py:153] step: 692950, training_loss: 1.13244e+02
I1112 12:08:03.867826 140264174335808 run_lib.py:153] step: 693000, training_loss: 1.12934e+02
I1112 12:08:13.751698 140264174335808 run_lib.py:153] step: 693050, training_loss: 1.30578e+02
I1112 12:08:23.837953 140264174335808 run_lib.py:153] step: 693100, training_loss: 1.09050e+02
I1112 12:08:33.904626 140264174335808 run_lib.py:153] step: 693150, training_loss: 1.35678e+02
I1112 12:08:44.625844 140264174335808 run_lib.py:153] step: 693200, training_loss: 1.60858e+02
I1112 12:08:54.525018 140264174335808 run_lib.py:153] step: 693250, training_loss: 9.77454e+01
I1112 12:09:05.070344 140264174335808 run_lib.py:153] step: 693300, training_loss: 1.13122e+02
I1112 12:09:15.173095 140264174335808 run_lib.py:153] step: 693350, training_loss: 1.16851e+02
I1112 12:09:25.013274 140264174335808 run_lib.py:153] step: 693400, training_loss: 1.22017e+02
I1112 12:09:35.515842 140264174335808 run_lib.py:153] step: 693450, training_loss: 1.58055e+02
I1112 12:09:45.727349 140264174335808 run_lib.py:153] step: 693500, training_loss: 1.46748e+02
I1112 12:09:56.727954 140264174335808 run_lib.py:153] step: 693550, training_loss: 1.37314e+02
I1112 12:10:07.769095 140264174335808 run_lib.py:153] step: 693600, training_loss: 1.25061e+02
I1112 12:10:18.325593 140264174335808 run_lib.py:153] step: 693650, training_loss: 1.17913e+02
I1112 12:10:28.784343 140264174335808 run_lib.py:153] step: 693700, training_loss: 1.18846e+02
I1112 12:10:39.312951 140264174335808 run_lib.py:153] step: 693750, training_loss: 1.12650e+02
I1112 12:10:50.105512 140264174335808 run_lib.py:153] step: 693800, training_loss: 1.51632e+02
I1112 12:10:59.496628 140264174335808 run_lib.py:153] step: 693850, training_loss: 1.14208e+02
I1112 12:11:10.040782 140264174335808 run_lib.py:153] step: 693900, training_loss: 1.53421e+02
I1112 12:11:20.327569 140264174335808 run_lib.py:153] step: 693950, training_loss: 1.38966e+02
I1112 12:11:30.223354 140264174335808 run_lib.py:153] step: 694000, training_loss: 1.15841e+02
I1112 12:11:40.972216 140264174335808 run_lib.py:153] step: 694050, training_loss: 1.19914e+02
I1112 12:11:51.892857 140264174335808 run_lib.py:153] step: 694100, training_loss: 1.02461e+02
I1112 12:12:02.031378 140264174335808 run_lib.py:153] step: 694150, training_loss: 1.28339e+02
I1112 12:12:12.381159 140264174335808 run_lib.py:153] step: 694200, training_loss: 1.63991e+02
I1112 12:12:22.917072 140264174335808 run_lib.py:153] step: 694250, training_loss: 1.44440e+02
I1112 12:12:32.769631 140264174335808 run_lib.py:153] step: 694300, training_loss: 8.45306e+01
I1112 12:12:42.705837 140264174335808 run_lib.py:153] step: 694350, training_loss: 1.19977e+02
I1112 12:12:52.976198 140264174335808 run_lib.py:153] step: 694400, training_loss: 1.06857e+02
I1112 12:13:03.192179 140264174335808 run_lib.py:153] step: 694450, training_loss: 9.83659e+01
I1112 12:13:12.627764 140264174335808 run_lib.py:153] step: 694500, training_loss: 1.07746e+02
I1112 12:13:22.689658 140264174335808 run_lib.py:153] step: 694550, training_loss: 1.42687e+02
I1112 12:13:32.580442 140264174335808 run_lib.py:153] step: 694600, training_loss: 1.15511e+02
I1112 12:13:42.529772 140264174335808 run_lib.py:153] step: 694650, training_loss: 1.04925e+02
I1112 12:13:52.414436 140264174335808 run_lib.py:153] step: 694700, training_loss: 1.10812e+02
I1112 12:14:02.335253 140264174335808 run_lib.py:153] step: 694750, training_loss: 1.40098e+02
I1112 12:14:12.026588 140264174335808 run_lib.py:153] step: 694800, training_loss: 1.33340e+02
I1112 12:14:21.646984 140264174335808 run_lib.py:153] step: 694850, training_loss: 1.06133e+02
I1112 12:14:31.625787 140264174335808 run_lib.py:153] step: 694900, training_loss: 1.33754e+02
I1112 12:14:41.138552 140264174335808 run_lib.py:153] step: 694950, training_loss: 1.35477e+02
I1112 12:14:51.206989 140264174335808 run_lib.py:153] step: 695000, training_loss: 1.54509e+02
I1112 12:14:51.310209 140264174335808 run_lib.py:166] step: 695000, eval_loss: 1.32476e+02
I1112 12:15:01.740341 140264174335808 run_lib.py:153] step: 695050, training_loss: 1.48074e+02
I1112 12:15:11.399694 140264174335808 run_lib.py:153] step: 695100, training_loss: 1.41994e+02
I1112 12:15:21.587482 140264174335808 run_lib.py:153] step: 695150, training_loss: 1.18699e+02
I1112 12:15:31.203605 140264174335808 run_lib.py:153] step: 695200, training_loss: 1.20226e+02
I1112 12:15:40.522121 140264174335808 run_lib.py:153] step: 695250, training_loss: 1.05961e+02
I1112 12:15:50.724593 140264174335808 run_lib.py:153] step: 695300, training_loss: 1.04706e+02
I1112 12:16:00.544425 140264174335808 run_lib.py:153] step: 695350, training_loss: 1.38036e+02
I1112 12:16:11.138154 140264174335808 run_lib.py:153] step: 695400, training_loss: 1.57744e+02
I1112 12:16:21.226929 140264174335808 run_lib.py:153] step: 695450, training_loss: 1.27138e+02
I1112 12:16:31.769526 140264174335808 run_lib.py:153] step: 695500, training_loss: 1.34841e+02
I1112 12:16:42.046862 140264174335808 run_lib.py:153] step: 695550, training_loss: 1.44236e+02
I1112 12:16:51.724092 140264174335808 run_lib.py:153] step: 695600, training_loss: 1.48896e+02
I1112 12:17:01.562394 140264174335808 run_lib.py:153] step: 695650, training_loss: 1.04508e+02
I1112 12:17:10.958506 140264174335808 run_lib.py:153] step: 695700, training_loss: 1.28599e+02
I1112 12:17:20.621368 140264174335808 run_lib.py:153] step: 695750, training_loss: 1.32060e+02
I1112 12:17:30.056888 140264174335808 run_lib.py:153] step: 695800, training_loss: 1.09887e+02
I1112 12:17:39.795087 140264174335808 run_lib.py:153] step: 695850, training_loss: 1.26800e+02
I1112 12:17:49.810829 140264174335808 run_lib.py:153] step: 695900, training_loss: 1.29913e+02
I1112 12:17:59.716852 140264174335808 run_lib.py:153] step: 695950, training_loss: 1.59377e+02
I1112 12:18:09.890585 140264174335808 run_lib.py:153] step: 696000, training_loss: 1.17185e+02
I1112 12:18:19.504236 140264174335808 run_lib.py:153] step: 696050, training_loss: 1.05196e+02
I1112 12:18:29.638253 140264174335808 run_lib.py:153] step: 696100, training_loss: 1.63796e+02
I1112 12:18:39.281395 140264174335808 run_lib.py:153] step: 696150, training_loss: 1.39229e+02
I1112 12:18:49.399272 140264174335808 run_lib.py:153] step: 696200, training_loss: 1.37516e+02
I1112 12:18:59.980602 140264174335808 run_lib.py:153] step: 696250, training_loss: 1.39558e+02
I1112 12:19:09.656990 140264174335808 run_lib.py:153] step: 696300, training_loss: 1.30963e+02
I1112 12:19:19.913747 140264174335808 run_lib.py:153] step: 696350, training_loss: 1.31828e+02
I1112 12:19:30.009308 140264174335808 run_lib.py:153] step: 696400, training_loss: 1.20449e+02
I1112 12:19:39.608682 140264174335808 run_lib.py:153] step: 696450, training_loss: 1.50445e+02
I1112 12:19:49.571499 140264174335808 run_lib.py:153] step: 696500, training_loss: 1.01630e+02
I1112 12:19:59.620698 140264174335808 run_lib.py:153] step: 696550, training_loss: 1.37215e+02
I1112 12:20:09.950864 140264174335808 run_lib.py:153] step: 696600, training_loss: 1.20251e+02
I1112 12:20:20.287588 140264174335808 run_lib.py:153] step: 696650, training_loss: 1.29591e+02
I1112 12:20:29.710706 140264174335808 run_lib.py:153] step: 696700, training_loss: 1.20560e+02
I1112 12:20:39.127711 140264174335808 run_lib.py:153] step: 696750, training_loss: 1.06050e+02
I1112 12:20:48.694689 140264174335808 run_lib.py:153] step: 696800, training_loss: 1.52792e+02
I1112 12:20:59.629848 140264174335808 run_lib.py:153] step: 696850, training_loss: 1.39447e+02
I1112 12:21:09.826865 140264174335808 run_lib.py:153] step: 696900, training_loss: 1.23806e+02
I1112 12:21:21.013586 140264174335808 run_lib.py:153] step: 696950, training_loss: 1.29598e+02
I1112 12:21:30.720304 140264174335808 run_lib.py:153] step: 697000, training_loss: 1.51203e+02
I1112 12:21:40.597256 140264174335808 run_lib.py:153] step: 697050, training_loss: 1.19305e+02
I1112 12:21:50.917896 140264174335808 run_lib.py:153] step: 697100, training_loss: 1.20650e+02
I1112 12:22:02.121981 140264174335808 run_lib.py:153] step: 697150, training_loss: 1.12861e+02
I1112 12:22:12.333441 140264174335808 run_lib.py:153] step: 697200, training_loss: 1.00916e+02
I1112 12:22:22.174156 140264174335808 run_lib.py:153] step: 697250, training_loss: 1.48052e+02
I1112 12:22:32.526897 140264174335808 run_lib.py:153] step: 697300, training_loss: 1.31098e+02
I1112 12:22:42.244887 140264174335808 run_lib.py:153] step: 697350, training_loss: 1.39377e+02
I1112 12:22:51.920578 140264174335808 run_lib.py:153] step: 697400, training_loss: 1.51231e+02
I1112 12:23:01.821888 140264174335808 run_lib.py:153] step: 697450, training_loss: 1.10785e+02
I1112 12:23:12.441015 140264174335808 run_lib.py:153] step: 697500, training_loss: 1.33098e+02
I1112 12:23:22.083915 140264174335808 run_lib.py:153] step: 697550, training_loss: 1.22585e+02
I1112 12:23:31.938689 140264174335808 run_lib.py:153] step: 697600, training_loss: 1.19056e+02
I1112 12:23:42.588443 140264174335808 run_lib.py:153] step: 697650, training_loss: 1.19099e+02
I1112 12:23:52.414927 140264174335808 run_lib.py:153] step: 697700, training_loss: 1.39346e+02
I1112 12:24:02.880879 140264174335808 run_lib.py:153] step: 697750, training_loss: 1.20248e+02
I1112 12:24:13.060532 140264174335808 run_lib.py:153] step: 697800, training_loss: 1.09683e+02
I1112 12:24:22.879189 140264174335808 run_lib.py:153] step: 697850, training_loss: 1.24768e+02
I1112 12:24:33.292841 140264174335808 run_lib.py:153] step: 697900, training_loss: 1.20150e+02
I1112 12:24:42.981796 140264174335808 run_lib.py:153] step: 697950, training_loss: 1.68150e+02
I1112 12:24:53.130250 140264174335808 run_lib.py:153] step: 698000, training_loss: 1.42418e+02
I1112 12:25:03.163941 140264174335808 run_lib.py:153] step: 698050, training_loss: 1.34071e+02
I1112 12:25:12.590986 140264174335808 run_lib.py:153] step: 698100, training_loss: 1.53219e+02
I1112 12:25:22.487866 140264174335808 run_lib.py:153] step: 698150, training_loss: 1.06499e+02
I1112 12:25:32.240532 140264174335808 run_lib.py:153] step: 698200, training_loss: 1.04807e+02
I1112 12:25:42.072078 140264174335808 run_lib.py:153] step: 698250, training_loss: 1.34727e+02
I1112 12:25:52.318550 140264174335808 run_lib.py:153] step: 698300, training_loss: 1.38392e+02
I1112 12:26:02.381769 140264174335808 run_lib.py:153] step: 698350, training_loss: 1.24293e+02
I1112 12:26:12.246536 140264174335808 run_lib.py:153] step: 698400, training_loss: 1.26141e+02
I1112 12:26:21.893945 140264174335808 run_lib.py:153] step: 698450, training_loss: 1.43677e+02
I1112 12:26:32.042334 140264174335808 run_lib.py:153] step: 698500, training_loss: 1.11848e+02
I1112 12:26:41.954092 140264174335808 run_lib.py:153] step: 698550, training_loss: 1.33960e+02
I1112 12:26:51.501010 140264174335808 run_lib.py:153] step: 698600, training_loss: 1.25748e+02
I1112 12:27:01.205480 140264174335808 run_lib.py:153] step: 698650, training_loss: 1.27657e+02
I1112 12:27:11.271733 140264174335808 run_lib.py:153] step: 698700, training_loss: 1.27363e+02
I1112 12:27:21.198845 140264174335808 run_lib.py:153] step: 698750, training_loss: 1.16353e+02
I1112 12:27:30.820553 140264174335808 run_lib.py:153] step: 698800, training_loss: 1.71217e+02
I1112 12:27:41.466706 140264174335808 run_lib.py:153] step: 698850, training_loss: 1.23870e+02
I1112 12:27:51.838057 140264174335808 run_lib.py:153] step: 698900, training_loss: 1.41529e+02
I1112 12:28:02.373340 140264174335808 run_lib.py:153] step: 698950, training_loss: 1.05309e+02
I1112 12:28:12.176603 140264174335808 run_lib.py:153] step: 699000, training_loss: 9.64985e+01
I1112 12:28:22.759534 140264174335808 run_lib.py:153] step: 699050, training_loss: 1.25323e+02
I1112 12:28:32.996596 140264174335808 run_lib.py:153] step: 699100, training_loss: 1.33762e+02
I1112 12:28:43.281946 140264174335808 run_lib.py:153] step: 699150, training_loss: 1.49628e+02
I1112 12:28:53.519717 140264174335808 run_lib.py:153] step: 699200, training_loss: 1.07784e+02
I1112 12:29:03.305348 140264174335808 run_lib.py:153] step: 699250, training_loss: 1.18505e+02
I1112 12:29:13.262770 140264174335808 run_lib.py:153] step: 699300, training_loss: 1.50207e+02
I1112 12:29:23.754714 140264174335808 run_lib.py:153] step: 699350, training_loss: 1.21235e+02
I1112 12:29:33.643879 140264174335808 run_lib.py:153] step: 699400, training_loss: 1.06630e+02
I1112 12:29:43.296609 140264174335808 run_lib.py:153] step: 699450, training_loss: 1.15529e+02
I1112 12:29:53.230247 140264174335808 run_lib.py:153] step: 699500, training_loss: 1.23456e+02
I1112 12:30:03.152808 140264174335808 run_lib.py:153] step: 699550, training_loss: 1.42687e+02
I1112 12:30:13.516619 140264174335808 run_lib.py:153] step: 699600, training_loss: 1.19905e+02
I1112 12:30:23.702404 140264174335808 run_lib.py:153] step: 699650, training_loss: 1.22658e+02
I1112 12:30:34.445460 140264174335808 run_lib.py:153] step: 699700, training_loss: 1.22422e+02
I1112 12:30:45.014892 140264174335808 run_lib.py:153] step: 699750, training_loss: 1.09787e+02
I1112 12:30:55.055070 140264174335808 run_lib.py:153] step: 699800, training_loss: 1.46854e+02
I1112 12:31:05.162093 140264174335808 run_lib.py:153] step: 699850, training_loss: 1.29997e+02
I1112 12:31:15.100589 140264174335808 run_lib.py:153] step: 699900, training_loss: 1.09972e+02
I1112 12:31:25.541552 140264174335808 run_lib.py:153] step: 699950, training_loss: 1.38066e+02
I1112 12:31:35.675228 140264174335808 run_lib.py:153] step: 700000, training_loss: 1.09108e+02
I1112 12:31:36.219460 140264174335808 run_lib.py:166] step: 700000, eval_loss: 1.13804e+02
I1112 12:31:46.328652 140264174335808 run_lib.py:153] step: 700050, training_loss: 1.22427e+02
I1112 12:31:56.804658 140264174335808 run_lib.py:153] step: 700100, training_loss: 1.12054e+02
I1112 12:32:06.908080 140264174335808 run_lib.py:153] step: 700150, training_loss: 1.00247e+02
I1112 12:32:16.789526 140264174335808 run_lib.py:153] step: 700200, training_loss: 1.28738e+02
I1112 12:32:26.983021 140264174335808 run_lib.py:153] step: 700250, training_loss: 1.23025e+02
I1112 12:32:37.143834 140264174335808 run_lib.py:153] step: 700300, training_loss: 1.38497e+02
I1112 12:32:46.727085 140264174335808 run_lib.py:153] step: 700350, training_loss: 1.11950e+02
I1112 12:32:57.026502 140264174335808 run_lib.py:153] step: 700400, training_loss: 1.43236e+02
I1112 12:33:06.832786 140264174335808 run_lib.py:153] step: 700450, training_loss: 1.17142e+02
I1112 12:33:16.527591 140264174335808 run_lib.py:153] step: 700500, training_loss: 1.39712e+02
I1112 12:33:26.414371 140264174335808 run_lib.py:153] step: 700550, training_loss: 1.18855e+02
I1112 12:33:36.831805 140264174335808 run_lib.py:153] step: 700600, training_loss: 1.15461e+02
I1112 12:33:47.427036 140264174335808 run_lib.py:153] step: 700650, training_loss: 1.20739e+02
I1112 12:33:57.240532 140264174335808 run_lib.py:153] step: 700700, training_loss: 1.21589e+02
I1112 12:34:06.942206 140264174335808 run_lib.py:153] step: 700750, training_loss: 1.05399e+02
I1112 12:34:17.565105 140264174335808 run_lib.py:153] step: 700800, training_loss: 1.20245e+02
I1112 12:34:27.044075 140264174335808 run_lib.py:153] step: 700850, training_loss: 1.11876e+02
I1112 12:34:37.001002 140264174335808 run_lib.py:153] step: 700900, training_loss: 1.25588e+02
I1112 12:34:46.396005 140264174335808 run_lib.py:153] step: 700950, training_loss: 1.27525e+02
I1112 12:34:56.171463 140264174335808 run_lib.py:153] step: 701000, training_loss: 1.33346e+02
I1112 12:35:06.212992 140264174335808 run_lib.py:153] step: 701050, training_loss: 1.36571e+02
I1112 12:35:15.451075 140264174335808 run_lib.py:153] step: 701100, training_loss: 1.33787e+02
I1112 12:35:24.898129 140264174335808 run_lib.py:153] step: 701150, training_loss: 1.28110e+02
I1112 12:35:34.475543 140264174335808 run_lib.py:153] step: 701200, training_loss: 1.23416e+02
I1112 12:35:45.158842 140264174335808 run_lib.py:153] step: 701250, training_loss: 1.08384e+02
I1112 12:35:55.103838 140264174335808 run_lib.py:153] step: 701300, training_loss: 1.24423e+02
I1112 12:36:05.476300 140264174335808 run_lib.py:153] step: 701350, training_loss: 1.24114e+02
I1112 12:36:15.468852 140264174335808 run_lib.py:153] step: 701400, training_loss: 1.50229e+02
I1112 12:36:25.531104 140264174335808 run_lib.py:153] step: 701450, training_loss: 1.41353e+02
I1112 12:36:35.517000 140264174335808 run_lib.py:153] step: 701500, training_loss: 1.18661e+02
I1112 12:36:44.995503 140264174335808 run_lib.py:153] step: 701550, training_loss: 1.11418e+02
I1112 12:36:54.626163 140264174335808 run_lib.py:153] step: 701600, training_loss: 1.10677e+02
I1112 12:37:04.129255 140264174335808 run_lib.py:153] step: 701650, training_loss: 1.08918e+02
I1112 12:37:13.561834 140264174335808 run_lib.py:153] step: 701700, training_loss: 1.36227e+02
I1112 12:37:23.591487 140264174335808 run_lib.py:153] step: 701750, training_loss: 1.46971e+02
I1112 12:37:33.197783 140264174335808 run_lib.py:153] step: 701800, training_loss: 1.12339e+02
I1112 12:37:42.989809 140264174335808 run_lib.py:153] step: 701850, training_loss: 1.25235e+02
I1112 12:37:52.444809 140264174335808 run_lib.py:153] step: 701900, training_loss: 1.60044e+02
I1112 12:38:02.573287 140264174335808 run_lib.py:153] step: 701950, training_loss: 9.66044e+01
I1112 12:38:12.393309 140264174335808 run_lib.py:153] step: 702000, training_loss: 1.16155e+02
I1112 12:38:22.576828 140264174335808 run_lib.py:153] step: 702050, training_loss: 1.37084e+02
I1112 12:38:32.834498 140264174335808 run_lib.py:153] step: 702100, training_loss: 1.38829e+02
I1112 12:38:43.309821 140264174335808 run_lib.py:153] step: 702150, training_loss: 1.44482e+02
I1112 12:38:52.720356 140264174335808 run_lib.py:153] step: 702200, training_loss: 9.56609e+01
I1112 12:39:02.500319 140264174335808 run_lib.py:153] step: 702250, training_loss: 1.07213e+02
I1112 12:39:12.460005 140264174335808 run_lib.py:153] step: 702300, training_loss: 1.26888e+02
I1112 12:39:22.648288 140264174335808 run_lib.py:153] step: 702350, training_loss: 1.28463e+02
I1112 12:39:33.159858 140264174335808 run_lib.py:153] step: 702400, training_loss: 1.35986e+02
I1112 12:39:43.460392 140264174335808 run_lib.py:153] step: 702450, training_loss: 1.19399e+02
I1112 12:39:52.994092 140264174335808 run_lib.py:153] step: 702500, training_loss: 1.15816e+02
I1112 12:40:03.176962 140264174335808 run_lib.py:153] step: 702550, training_loss: 1.08838e+02
I1112 12:40:12.874994 140264174335808 run_lib.py:153] step: 702600, training_loss: 1.12377e+02
I1112 12:40:22.626594 140264174335808 run_lib.py:153] step: 702650, training_loss: 1.43682e+02
I1112 12:40:32.205895 140264174335808 run_lib.py:153] step: 702700, training_loss: 1.06928e+02
I1112 12:40:41.721285 140264174335808 run_lib.py:153] step: 702750, training_loss: 1.41580e+02
I1112 12:40:51.294444 140264174335808 run_lib.py:153] step: 702800, training_loss: 1.42047e+02
I1112 12:41:01.156286 140264174335808 run_lib.py:153] step: 702850, training_loss: 1.26862e+02
I1112 12:41:11.050866 140264174335808 run_lib.py:153] step: 702900, training_loss: 1.48411e+02
I1112 12:41:21.268990 140264174335808 run_lib.py:153] step: 702950, training_loss: 1.11807e+02
I1112 12:41:31.917493 140264174335808 run_lib.py:153] step: 703000, training_loss: 1.02889e+02
I1112 12:41:41.853116 140264174335808 run_lib.py:153] step: 703050, training_loss: 9.33839e+01
I1112 12:41:52.443893 140264174335808 run_lib.py:153] step: 703100, training_loss: 9.70107e+01
I1112 12:42:02.808219 140264174335808 run_lib.py:153] step: 703150, training_loss: 1.37311e+02
I1112 12:42:12.844600 140264174335808 run_lib.py:153] step: 703200, training_loss: 1.28868e+02
I1112 12:42:22.526398 140264174335808 run_lib.py:153] step: 703250, training_loss: 1.48773e+02
I1112 12:42:33.278315 140264174335808 run_lib.py:153] step: 703300, training_loss: 1.14270e+02
I1112 12:42:43.303517 140264174335808 run_lib.py:153] step: 703350, training_loss: 1.34264e+02
I1112 12:42:52.932342 140264174335808 run_lib.py:153] step: 703400, training_loss: 1.14021e+02
I1112 12:43:03.249496 140264174335808 run_lib.py:153] step: 703450, training_loss: 1.22658e+02
I1112 12:43:13.176169 140264174335808 run_lib.py:153] step: 703500, training_loss: 1.24143e+02
I1112 12:43:23.175963 140264174335808 run_lib.py:153] step: 703550, training_loss: 1.34609e+02
I1112 12:43:32.751623 140264174335808 run_lib.py:153] step: 703600, training_loss: 1.33827e+02
I1112 12:43:43.099863 140264174335808 run_lib.py:153] step: 703650, training_loss: 1.30666e+02
I1112 12:43:53.375537 140264174335808 run_lib.py:153] step: 703700, training_loss: 1.31339e+02
I1112 12:44:02.870661 140264174335808 run_lib.py:153] step: 703750, training_loss: 1.42565e+02
I1112 12:44:12.764169 140264174335808 run_lib.py:153] step: 703800, training_loss: 1.49336e+02
I1112 12:44:23.319755 140264174335808 run_lib.py:153] step: 703850, training_loss: 1.49081e+02
I1112 12:44:33.872182 140264174335808 run_lib.py:153] step: 703900, training_loss: 1.02128e+02
I1112 12:44:44.314382 140264174335808 run_lib.py:153] step: 703950, training_loss: 1.34975e+02
I1112 12:44:54.004192 140264174335808 run_lib.py:153] step: 704000, training_loss: 1.10824e+02
I1112 12:45:03.351839 140264174335808 run_lib.py:153] step: 704050, training_loss: 1.25868e+02
I1112 12:45:14.104734 140264174335808 run_lib.py:153] step: 704100, training_loss: 1.17601e+02
I1112 12:45:24.244611 140264174335808 run_lib.py:153] step: 704150, training_loss: 1.35482e+02
I1112 12:45:33.535809 140264174335808 run_lib.py:153] step: 704200, training_loss: 1.27499e+02
I1112 12:45:43.365598 140264174335808 run_lib.py:153] step: 704250, training_loss: 1.08578e+02
I1112 12:45:52.947321 140264174335808 run_lib.py:153] step: 704300, training_loss: 1.17268e+02
I1112 12:46:02.776361 140264174335808 run_lib.py:153] step: 704350, training_loss: 1.24197e+02
I1112 12:46:12.255738 140264174335808 run_lib.py:153] step: 704400, training_loss: 1.10908e+02
I1112 12:46:21.843134 140264174335808 run_lib.py:153] step: 704450, training_loss: 1.26523e+02
I1112 12:46:32.007516 140264174335808 run_lib.py:153] step: 704500, training_loss: 1.13206e+02
I1112 12:46:41.910651 140264174335808 run_lib.py:153] step: 704550, training_loss: 1.13169e+02
I1112 12:46:51.624210 140264174335808 run_lib.py:153] step: 704600, training_loss: 1.23710e+02
I1112 12:47:01.621096 140264174335808 run_lib.py:153] step: 704650, training_loss: 1.47974e+02
I1112 12:47:11.988819 140264174335808 run_lib.py:153] step: 704700, training_loss: 1.38472e+02
I1112 12:47:22.334373 140264174335808 run_lib.py:153] step: 704750, training_loss: 1.15559e+02
I1112 12:47:32.570389 140264174335808 run_lib.py:153] step: 704800, training_loss: 1.40277e+02
I1112 12:47:42.754950 140264174335808 run_lib.py:153] step: 704850, training_loss: 1.36732e+02
I1112 12:47:53.200420 140264174335808 run_lib.py:153] step: 704900, training_loss: 1.09983e+02
I1112 12:48:03.231353 140264174335808 run_lib.py:153] step: 704950, training_loss: 1.37768e+02
I1112 12:48:12.983963 140264174335808 run_lib.py:153] step: 705000, training_loss: 1.59297e+02
I1112 12:48:13.086434 140264174335808 run_lib.py:166] step: 705000, eval_loss: 1.18316e+02
I1112 12:48:22.568030 140264174335808 run_lib.py:153] step: 705050, training_loss: 1.30446e+02
I1112 12:48:32.442181 140264174335808 run_lib.py:153] step: 705100, training_loss: 1.19671e+02
I1112 12:48:42.566940 140264174335808 run_lib.py:153] step: 705150, training_loss: 1.52872e+02
I1112 12:48:52.558067 140264174335808 run_lib.py:153] step: 705200, training_loss: 1.40202e+02
I1112 12:49:02.915113 140264174335808 run_lib.py:153] step: 705250, training_loss: 1.24495e+02
I1112 12:49:12.386881 140264174335808 run_lib.py:153] step: 705300, training_loss: 1.22725e+02
I1112 12:49:22.238018 140264174335808 run_lib.py:153] step: 705350, training_loss: 1.24287e+02
I1112 12:49:32.278682 140264174335808 run_lib.py:153] step: 705400, training_loss: 1.30674e+02
I1112 12:49:41.906932 140264174335808 run_lib.py:153] step: 705450, training_loss: 1.07344e+02
I1112 12:49:51.380858 140264174335808 run_lib.py:153] step: 705500, training_loss: 1.27534e+02
I1112 12:50:01.572758 140264174335808 run_lib.py:153] step: 705550, training_loss: 1.15016e+02
I1112 12:50:11.521504 140264174335808 run_lib.py:153] step: 705600, training_loss: 1.36450e+02
I1112 12:50:21.647166 140264174335808 run_lib.py:153] step: 705650, training_loss: 9.65806e+01
I1112 12:50:31.394363 140264174335808 run_lib.py:153] step: 705700, training_loss: 1.01191e+02
I1112 12:50:41.410196 140264174335808 run_lib.py:153] step: 705750, training_loss: 1.35179e+02
I1112 12:50:51.355457 140264174335808 run_lib.py:153] step: 705800, training_loss: 1.44963e+02
I1112 12:51:01.201683 140264174335808 run_lib.py:153] step: 705850, training_loss: 1.67985e+02
I1112 12:51:10.372396 140264174335808 run_lib.py:153] step: 705900, training_loss: 1.42228e+02
I1112 12:51:21.012027 140264174335808 run_lib.py:153] step: 705950, training_loss: 8.82297e+01
I1112 12:51:31.084907 140264174335808 run_lib.py:153] step: 706000, training_loss: 1.13177e+02
I1112 12:51:41.156862 140264174335808 run_lib.py:153] step: 706050, training_loss: 1.31503e+02
I1112 12:51:51.484520 140264174335808 run_lib.py:153] step: 706100, training_loss: 1.43818e+02
I1112 12:52:01.490048 140264174335808 run_lib.py:153] step: 706150, training_loss: 1.55915e+02
I1112 12:52:11.192580 140264174335808 run_lib.py:153] step: 706200, training_loss: 1.21034e+02
I1112 12:52:21.863016 140264174335808 run_lib.py:153] step: 706250, training_loss: 1.24216e+02
I1112 12:52:32.614944 140264174335808 run_lib.py:153] step: 706300, training_loss: 1.27874e+02
I1112 12:52:42.390954 140264174335808 run_lib.py:153] step: 706350, training_loss: 1.16513e+02
I1112 12:52:52.247936 140264174335808 run_lib.py:153] step: 706400, training_loss: 1.30391e+02
I1112 12:53:02.121811 140264174335808 run_lib.py:153] step: 706450, training_loss: 1.41611e+02
I1112 12:53:11.878828 140264174335808 run_lib.py:153] step: 706500, training_loss: 1.39452e+02
I1112 12:53:22.067227 140264174335808 run_lib.py:153] step: 706550, training_loss: 1.23040e+02
I1112 12:53:32.533175 140264174335808 run_lib.py:153] step: 706600, training_loss: 1.25001e+02
I1112 12:53:43.085358 140264174335808 run_lib.py:153] step: 706650, training_loss: 1.29632e+02
I1112 12:53:53.474692 140264174335808 run_lib.py:153] step: 706700, training_loss: 1.01385e+02
I1112 12:54:03.071943 140264174335808 run_lib.py:153] step: 706750, training_loss: 1.35005e+02
I1112 12:54:13.219307 140264174335808 run_lib.py:153] step: 706800, training_loss: 1.14595e+02
I1112 12:54:22.861666 140264174335808 run_lib.py:153] step: 706850, training_loss: 1.14032e+02
I1112 12:54:33.104658 140264174335808 run_lib.py:153] step: 706900, training_loss: 8.37785e+01
I1112 12:54:43.095973 140264174335808 run_lib.py:153] step: 706950, training_loss: 1.19979e+02
I1112 12:54:53.055865 140264174335808 run_lib.py:153] step: 707000, training_loss: 8.83766e+01
I1112 12:55:03.187915 140264174335808 run_lib.py:153] step: 707050, training_loss: 1.30245e+02
I1112 12:55:12.915150 140264174335808 run_lib.py:153] step: 707100, training_loss: 9.05118e+01
I1112 12:55:23.099995 140264174335808 run_lib.py:153] step: 707150, training_loss: 1.28860e+02
I1112 12:55:32.628910 140264174335808 run_lib.py:153] step: 707200, training_loss: 1.34115e+02
I1112 12:55:42.514908 140264174335808 run_lib.py:153] step: 707250, training_loss: 1.29907e+02
I1112 12:55:53.044357 140264174335808 run_lib.py:153] step: 707300, training_loss: 1.18206e+02
I1112 12:56:02.314396 140264174335808 run_lib.py:153] step: 707350, training_loss: 1.35702e+02
I1112 12:56:11.926932 140264174335808 run_lib.py:153] step: 707400, training_loss: 1.17278e+02
I1112 12:56:22.136097 140264174335808 run_lib.py:153] step: 707450, training_loss: 1.10000e+02
I1112 12:56:32.059263 140264174335808 run_lib.py:153] step: 707500, training_loss: 1.26093e+02
I1112 12:56:42.028251 140264174335808 run_lib.py:153] step: 707550, training_loss: 1.48190e+02
I1112 12:56:51.982599 140264174335808 run_lib.py:153] step: 707600, training_loss: 1.21147e+02
I1112 12:57:02.450987 140264174335808 run_lib.py:153] step: 707650, training_loss: 1.03709e+02
I1112 12:57:13.031936 140264174335808 run_lib.py:153] step: 707700, training_loss: 1.45398e+02
I1112 12:57:23.041221 140264174335808 run_lib.py:153] step: 707750, training_loss: 1.32944e+02
I1112 12:57:33.519682 140264174335808 run_lib.py:153] step: 707800, training_loss: 1.24070e+02
I1112 12:57:43.123596 140264174335808 run_lib.py:153] step: 707850, training_loss: 1.33836e+02
I1112 12:57:53.098390 140264174335808 run_lib.py:153] step: 707900, training_loss: 1.40269e+02
I1112 12:58:02.870961 140264174335808 run_lib.py:153] step: 707950, training_loss: 9.87402e+01
I1112 12:58:12.974279 140264174335808 run_lib.py:153] step: 708000, training_loss: 1.64949e+02
I1112 12:58:23.257553 140264174335808 run_lib.py:153] step: 708050, training_loss: 1.20792e+02
I1112 12:58:33.519138 140264174335808 run_lib.py:153] step: 708100, training_loss: 1.40721e+02
I1112 12:58:43.106294 140264174335808 run_lib.py:153] step: 708150, training_loss: 1.58169e+02
I1112 12:58:52.744673 140264174335808 run_lib.py:153] step: 708200, training_loss: 1.12244e+02
I1112 12:59:03.232359 140264174335808 run_lib.py:153] step: 708250, training_loss: 8.86780e+01
I1112 12:59:14.143585 140264174335808 run_lib.py:153] step: 708300, training_loss: 1.58890e+02
I1112 12:59:24.340424 140264174335808 run_lib.py:153] step: 708350, training_loss: 1.11941e+02
I1112 12:59:35.114123 140264174335808 run_lib.py:153] step: 708400, training_loss: 1.28100e+02
I1112 12:59:44.921157 140264174335808 run_lib.py:153] step: 708450, training_loss: 1.30119e+02
I1112 12:59:54.310242 140264174335808 run_lib.py:153] step: 708500, training_loss: 9.77945e+01
I1112 13:00:03.721569 140264174335808 run_lib.py:153] step: 708550, training_loss: 1.18651e+02
I1112 13:00:13.457495 140264174335808 run_lib.py:153] step: 708600, training_loss: 1.24705e+02
I1112 13:00:22.748329 140264174335808 run_lib.py:153] step: 708650, training_loss: 1.06379e+02
I1112 13:00:33.099725 140264174335808 run_lib.py:153] step: 708700, training_loss: 1.38371e+02
I1112 13:00:43.574206 140264174335808 run_lib.py:153] step: 708750, training_loss: 1.23701e+02
I1112 13:00:54.171617 140264174335808 run_lib.py:153] step: 708800, training_loss: 1.58003e+02
I1112 13:01:03.900500 140264174335808 run_lib.py:153] step: 708850, training_loss: 1.05092e+02
I1112 13:01:14.426222 140264174335808 run_lib.py:153] step: 708900, training_loss: 1.02594e+02
I1112 13:01:24.532088 140264174335808 run_lib.py:153] step: 708950, training_loss: 1.22926e+02
I1112 13:01:34.464650 140264174335808 run_lib.py:153] step: 709000, training_loss: 1.71691e+02
I1112 13:01:44.160079 140264174335808 run_lib.py:153] step: 709050, training_loss: 1.23835e+02
I1112 13:01:54.216861 140264174335808 run_lib.py:153] step: 709100, training_loss: 1.16870e+02
I1112 13:02:04.107029 140264174335808 run_lib.py:153] step: 709150, training_loss: 1.30452e+02
I1112 13:02:13.739112 140264174335808 run_lib.py:153] step: 709200, training_loss: 1.29607e+02
I1112 13:02:23.763876 140264174335808 run_lib.py:153] step: 709250, training_loss: 1.08354e+02
I1112 13:02:33.494199 140264174335808 run_lib.py:153] step: 709300, training_loss: 1.24609e+02
I1112 13:02:42.708743 140264174335808 run_lib.py:153] step: 709350, training_loss: 1.43094e+02
I1112 13:02:51.946422 140264174335808 run_lib.py:153] step: 709400, training_loss: 1.01272e+02
I1112 13:03:01.344662 140264174335808 run_lib.py:153] step: 709450, training_loss: 1.27465e+02
I1112 13:03:10.961214 140264174335808 run_lib.py:153] step: 709500, training_loss: 1.15117e+02
I1112 13:03:20.152946 140264174335808 run_lib.py:153] step: 709550, training_loss: 1.20700e+02
I1112 13:03:29.442044 140264174335808 run_lib.py:153] step: 709600, training_loss: 1.30390e+02
I1112 13:03:39.164370 140264174335808 run_lib.py:153] step: 709650, training_loss: 1.25542e+02
I1112 13:03:49.268377 140264174335808 run_lib.py:153] step: 709700, training_loss: 1.31221e+02
I1112 13:03:59.229143 140264174335808 run_lib.py:153] step: 709750, training_loss: 1.31645e+02
I1112 13:04:08.884070 140264174335808 run_lib.py:153] step: 709800, training_loss: 1.21575e+02
I1112 13:04:18.708936 140264174335808 run_lib.py:153] step: 709850, training_loss: 1.36737e+02
I1112 13:04:28.744413 140264174335808 run_lib.py:153] step: 709900, training_loss: 1.04388e+02
I1112 13:04:38.257281 140264174335808 run_lib.py:153] step: 709950, training_loss: 1.20761e+02
I1112 13:04:47.894915 140264174335808 run_lib.py:153] step: 710000, training_loss: 9.90759e+01
I1112 13:04:48.472457 140264174335808 run_lib.py:166] step: 710000, eval_loss: 1.10439e+02
I1112 13:04:58.440017 140264174335808 run_lib.py:153] step: 710050, training_loss: 9.85980e+01
I1112 13:05:08.884060 140264174335808 run_lib.py:153] step: 710100, training_loss: 1.34718e+02
I1112 13:05:18.658374 140264174335808 run_lib.py:153] step: 710150, training_loss: 1.16550e+02
I1112 13:05:28.175983 140264174335808 run_lib.py:153] step: 710200, training_loss: 1.17875e+02
I1112 13:05:38.006994 140264174335808 run_lib.py:153] step: 710250, training_loss: 1.17923e+02
I1112 13:05:48.402049 140264174335808 run_lib.py:153] step: 710300, training_loss: 1.33949e+02
I1112 13:05:58.480515 140264174335808 run_lib.py:153] step: 710350, training_loss: 1.75097e+02
I1112 13:06:09.115347 140264174335808 run_lib.py:153] step: 710400, training_loss: 1.17128e+02
I1112 13:06:19.556720 140264174335808 run_lib.py:153] step: 710450, training_loss: 1.31410e+02
I1112 13:06:29.578552 140264174335808 run_lib.py:153] step: 710500, training_loss: 9.64397e+01
I1112 13:06:39.606082 140264174335808 run_lib.py:153] step: 710550, training_loss: 1.26873e+02
I1112 13:06:50.248575 140264174335808 run_lib.py:153] step: 710600, training_loss: 1.27300e+02
I1112 13:07:01.051819 140264174335808 run_lib.py:153] step: 710650, training_loss: 1.32664e+02
I1112 13:07:11.279277 140264174335808 run_lib.py:153] step: 710700, training_loss: 1.19942e+02
I1112 13:07:21.803129 140264174335808 run_lib.py:153] step: 710750, training_loss: 1.54921e+02
I1112 13:07:31.955487 140264174335808 run_lib.py:153] step: 710800, training_loss: 1.18630e+02
I1112 13:07:42.218638 140264174335808 run_lib.py:153] step: 710850, training_loss: 1.19053e+02
I1112 13:07:52.498137 140264174335808 run_lib.py:153] step: 710900, training_loss: 1.44109e+02
I1112 13:08:02.681005 140264174335808 run_lib.py:153] step: 710950, training_loss: 1.57284e+02
I1112 13:08:13.188567 140264174335808 run_lib.py:153] step: 711000, training_loss: 1.15439e+02
I1112 13:08:23.162801 140264174335808 run_lib.py:153] step: 711050, training_loss: 1.22543e+02
I1112 13:08:33.488356 140264174335808 run_lib.py:153] step: 711100, training_loss: 1.22480e+02
I1112 13:08:43.560799 140264174335808 run_lib.py:153] step: 711150, training_loss: 1.26691e+02
I1112 13:08:54.140484 140264174335808 run_lib.py:153] step: 711200, training_loss: 1.55403e+02
I1112 13:09:04.540812 140264174335808 run_lib.py:153] step: 711250, training_loss: 1.25371e+02
I1112 13:09:14.094918 140264174335808 run_lib.py:153] step: 711300, training_loss: 1.22071e+02
I1112 13:09:24.466152 140264174335808 run_lib.py:153] step: 711350, training_loss: 1.17036e+02
I1112 13:09:33.985220 140264174335808 run_lib.py:153] step: 711400, training_loss: 1.21905e+02
I1112 13:09:43.721131 140264174335808 run_lib.py:153] step: 711450, training_loss: 1.02329e+02
I1112 13:09:54.262583 140264174335808 run_lib.py:153] step: 711500, training_loss: 1.47122e+02
I1112 13:10:04.029507 140264174335808 run_lib.py:153] step: 711550, training_loss: 1.22267e+02
I1112 13:10:14.325932 140264174335808 run_lib.py:153] step: 711600, training_loss: 1.17357e+02
I1112 13:10:24.506078 140264174335808 run_lib.py:153] step: 711650, training_loss: 1.15389e+02
I1112 13:10:34.164798 140264174335808 run_lib.py:153] step: 711700, training_loss: 1.20468e+02
I1112 13:10:44.334061 140264174335808 run_lib.py:153] step: 711750, training_loss: 1.12529e+02
I1112 13:10:53.999635 140264174335808 run_lib.py:153] step: 711800, training_loss: 1.23636e+02
I1112 13:11:03.559179 140264174335808 run_lib.py:153] step: 711850, training_loss: 1.25263e+02
I1112 13:11:13.075301 140264174335808 run_lib.py:153] step: 711900, training_loss: 1.14917e+02
I1112 13:11:23.127540 140264174335808 run_lib.py:153] step: 711950, training_loss: 1.38590e+02
I1112 13:11:32.653361 140264174335808 run_lib.py:153] step: 712000, training_loss: 1.05383e+02
I1112 13:11:42.431317 140264174335808 run_lib.py:153] step: 712050, training_loss: 1.35115e+02
I1112 13:11:52.091630 140264174335808 run_lib.py:153] step: 712100, training_loss: 1.24931e+02
I1112 13:12:01.582419 140264174335808 run_lib.py:153] step: 712150, training_loss: 1.26115e+02
I1112 13:12:11.835358 140264174335808 run_lib.py:153] step: 712200, training_loss: 1.41067e+02
I1112 13:12:22.086899 140264174335808 run_lib.py:153] step: 712250, training_loss: 1.38355e+02
I1112 13:12:31.457622 140264174335808 run_lib.py:153] step: 712300, training_loss: 1.47763e+02
I1112 13:12:41.262249 140264174335808 run_lib.py:153] step: 712350, training_loss: 1.31639e+02
I1112 13:12:51.269217 140264174335808 run_lib.py:153] step: 712400, training_loss: 1.83520e+02
I1112 13:13:01.006477 140264174335808 run_lib.py:153] step: 712450, training_loss: 1.22799e+02
I1112 13:13:11.592114 140264174335808 run_lib.py:153] step: 712500, training_loss: 1.16193e+02
I1112 13:13:22.066411 140264174335808 run_lib.py:153] step: 712550, training_loss: 1.27849e+02
I1112 13:13:31.800068 140264174335808 run_lib.py:153] step: 712600, training_loss: 1.01036e+02
I1112 13:13:41.579467 140264174335808 run_lib.py:153] step: 712650, training_loss: 1.32549e+02
I1112 13:13:51.356228 140264174335808 run_lib.py:153] step: 712700, training_loss: 1.28107e+02
I1112 13:14:01.805455 140264174335808 run_lib.py:153] step: 712750, training_loss: 1.28628e+02
I1112 13:14:12.226158 140264174335808 run_lib.py:153] step: 712800, training_loss: 1.09514e+02
I1112 13:14:21.753724 140264174335808 run_lib.py:153] step: 712850, training_loss: 1.34192e+02
I1112 13:14:32.435033 140264174335808 run_lib.py:153] step: 712900, training_loss: 1.13591e+02
I1112 13:14:42.881276 140264174335808 run_lib.py:153] step: 712950, training_loss: 1.28032e+02
I1112 13:14:53.008823 140264174335808 run_lib.py:153] step: 713000, training_loss: 1.39936e+02
I1112 13:15:03.117429 140264174335808 run_lib.py:153] step: 713050, training_loss: 1.06589e+02
I1112 13:15:13.264559 140264174335808 run_lib.py:153] step: 713100, training_loss: 1.17871e+02
I1112 13:15:23.193935 140264174335808 run_lib.py:153] step: 713150, training_loss: 1.55617e+02
I1112 13:15:32.543656 140264174335808 run_lib.py:153] step: 713200, training_loss: 9.27787e+01
I1112 13:15:42.194294 140264174335808 run_lib.py:153] step: 713250, training_loss: 1.36992e+02
I1112 13:15:52.347150 140264174335808 run_lib.py:153] step: 713300, training_loss: 1.06738e+02
I1112 13:16:02.468103 140264174335808 run_lib.py:153] step: 713350, training_loss: 1.10603e+02
I1112 13:16:12.545870 140264174335808 run_lib.py:153] step: 713400, training_loss: 1.37241e+02
I1112 13:16:22.050132 140264174335808 run_lib.py:153] step: 713450, training_loss: 1.32801e+02
I1112 13:16:31.774085 140264174335808 run_lib.py:153] step: 713500, training_loss: 1.37443e+02
I1112 13:16:41.186100 140264174335808 run_lib.py:153] step: 713550, training_loss: 1.14122e+02
I1112 13:16:51.564275 140264174335808 run_lib.py:153] step: 713600, training_loss: 1.09078e+02
I1112 13:17:01.808403 140264174335808 run_lib.py:153] step: 713650, training_loss: 1.23171e+02
I1112 13:17:11.398914 140264174335808 run_lib.py:153] step: 713700, training_loss: 1.18804e+02
I1112 13:17:21.021786 140264174335808 run_lib.py:153] step: 713750, training_loss: 1.26550e+02
I1112 13:17:30.424997 140264174335808 run_lib.py:153] step: 713800, training_loss: 1.26047e+02
I1112 13:17:40.028936 140264174335808 run_lib.py:153] step: 713850, training_loss: 9.32559e+01
I1112 13:17:50.264286 140264174335808 run_lib.py:153] step: 713900, training_loss: 1.30206e+02
I1112 13:18:00.188287 140264174335808 run_lib.py:153] step: 713950, training_loss: 1.10433e+02
I1112 13:18:09.584380 140264174335808 run_lib.py:153] step: 714000, training_loss: 1.20395e+02
I1112 13:18:19.564336 140264174335808 run_lib.py:153] step: 714050, training_loss: 1.58212e+02
I1112 13:18:29.635099 140264174335808 run_lib.py:153] step: 714100, training_loss: 1.44474e+02
I1112 13:18:40.144767 140264174335808 run_lib.py:153] step: 714150, training_loss: 1.35318e+02
I1112 13:18:50.307147 140264174335808 run_lib.py:153] step: 714200, training_loss: 1.15296e+02
I1112 13:19:00.596371 140264174335808 run_lib.py:153] step: 714250, training_loss: 1.40807e+02
I1112 13:19:10.874912 140264174335808 run_lib.py:153] step: 714300, training_loss: 1.39969e+02
I1112 13:19:21.522852 140264174335808 run_lib.py:153] step: 714350, training_loss: 1.32214e+02
I1112 13:19:31.294329 140264174335808 run_lib.py:153] step: 714400, training_loss: 1.33422e+02
I1112 13:19:41.319655 140264174335808 run_lib.py:153] step: 714450, training_loss: 1.44634e+02
I1112 13:19:51.083042 140264174335808 run_lib.py:153] step: 714500, training_loss: 1.50357e+02
I1112 13:20:00.955460 140264174335808 run_lib.py:153] step: 714550, training_loss: 1.21192e+02
I1112 13:20:10.463084 140264174335808 run_lib.py:153] step: 714600, training_loss: 1.30701e+02
I1112 13:20:20.075122 140264174335808 run_lib.py:153] step: 714650, training_loss: 1.01766e+02
I1112 13:20:30.152862 140264174335808 run_lib.py:153] step: 714700, training_loss: 1.35804e+02
I1112 13:20:39.904021 140264174335808 run_lib.py:153] step: 714750, training_loss: 1.31309e+02
I1112 13:20:49.977117 140264174335808 run_lib.py:153] step: 714800, training_loss: 1.29117e+02
I1112 13:21:00.377393 140264174335808 run_lib.py:153] step: 714850, training_loss: 1.34914e+02
I1112 13:21:11.228482 140264174335808 run_lib.py:153] step: 714900, training_loss: 1.14115e+02
I1112 13:21:21.380070 140264174335808 run_lib.py:153] step: 714950, training_loss: 1.26516e+02
I1112 13:21:31.484609 140264174335808 run_lib.py:153] step: 715000, training_loss: 1.41211e+02
I1112 13:21:31.585751 140264174335808 run_lib.py:166] step: 715000, eval_loss: 1.18383e+02
I1112 13:21:41.325532 140264174335808 run_lib.py:153] step: 715050, training_loss: 1.35801e+02
I1112 13:21:51.179594 140264174335808 run_lib.py:153] step: 715100, training_loss: 1.35391e+02
I1112 13:22:01.538811 140264174335808 run_lib.py:153] step: 715150, training_loss: 1.12783e+02
I1112 13:22:12.059187 140264174335808 run_lib.py:153] step: 715200, training_loss: 1.49112e+02
I1112 13:22:22.604598 140264174335808 run_lib.py:153] step: 715250, training_loss: 1.43529e+02
I1112 13:22:33.174896 140264174335808 run_lib.py:153] step: 715300, training_loss: 1.20259e+02
I1112 13:22:43.696197 140264174335808 run_lib.py:153] step: 715350, training_loss: 1.34382e+02
I1112 13:22:54.281659 140264174335808 run_lib.py:153] step: 715400, training_loss: 1.36814e+02
I1112 13:23:04.284924 140264174335808 run_lib.py:153] step: 715450, training_loss: 1.30377e+02
I1112 13:23:15.026694 140264174335808 run_lib.py:153] step: 715500, training_loss: 1.37033e+02
I1112 13:23:25.118346 140264174335808 run_lib.py:153] step: 715550, training_loss: 1.66262e+02
I1112 13:23:35.266628 140264174335808 run_lib.py:153] step: 715600, training_loss: 1.29566e+02
I1112 13:23:44.790132 140264174335808 run_lib.py:153] step: 715650, training_loss: 1.27183e+02
I1112 13:23:54.920549 140264174335808 run_lib.py:153] step: 715700, training_loss: 1.51203e+02
I1112 13:24:04.620215 140264174335808 run_lib.py:153] step: 715750, training_loss: 1.65163e+02
I1112 13:24:14.146833 140264174335808 run_lib.py:153] step: 715800, training_loss: 1.21604e+02
I1112 13:24:23.958347 140264174335808 run_lib.py:153] step: 715850, training_loss: 1.33640e+02
I1112 13:24:34.029354 140264174335808 run_lib.py:153] step: 715900, training_loss: 1.18861e+02
I1112 13:24:43.692470 140264174335808 run_lib.py:153] step: 715950, training_loss: 1.18234e+02
I1112 13:24:53.862366 140264174335808 run_lib.py:153] step: 716000, training_loss: 1.38768e+02
I1112 13:25:03.504471 140264174335808 run_lib.py:153] step: 716050, training_loss: 8.98969e+01
I1112 13:25:13.208989 140264174335808 run_lib.py:153] step: 716100, training_loss: 1.16286e+02
I1112 13:25:23.443974 140264174335808 run_lib.py:153] step: 716150, training_loss: 1.46221e+02
I1112 13:25:32.803296 140264174335808 run_lib.py:153] step: 716200, training_loss: 1.36911e+02
I1112 13:25:43.285572 140264174335808 run_lib.py:153] step: 716250, training_loss: 1.29101e+02
I1112 13:25:52.955893 140264174335808 run_lib.py:153] step: 716300, training_loss: 1.22002e+02
I1112 13:26:03.723590 140264174335808 run_lib.py:153] step: 716350, training_loss: 1.05760e+02
I1112 13:26:14.043493 140264174335808 run_lib.py:153] step: 716400, training_loss: 1.07472e+02
I1112 13:26:23.528901 140264174335808 run_lib.py:153] step: 716450, training_loss: 1.49379e+02
I1112 13:26:33.529753 140264174335808 run_lib.py:153] step: 716500, training_loss: 1.15140e+02
I1112 13:26:44.276632 140264174335808 run_lib.py:153] step: 716550, training_loss: 1.07985e+02
I1112 13:26:53.876152 140264174335808 run_lib.py:153] step: 716600, training_loss: 1.15968e+02
I1112 13:27:03.644101 140264174335808 run_lib.py:153] step: 716650, training_loss: 1.21943e+02
I1112 13:27:13.548327 140264174335808 run_lib.py:153] step: 716700, training_loss: 1.77678e+02
I1112 13:27:23.729700 140264174335808 run_lib.py:153] step: 716750, training_loss: 1.04939e+02
I1112 13:27:33.577100 140264174335808 run_lib.py:153] step: 716800, training_loss: 1.18375e+02
I1112 13:27:43.872204 140264174335808 run_lib.py:153] step: 716850, training_loss: 1.29556e+02
I1112 13:27:54.471596 140264174335808 run_lib.py:153] step: 716900, training_loss: 1.33231e+02
I1112 13:28:05.070164 140264174335808 run_lib.py:153] step: 716950, training_loss: 1.19028e+02
I1112 13:28:14.657657 140264174335808 run_lib.py:153] step: 717000, training_loss: 9.29405e+01
I1112 13:28:25.189388 140264174335808 run_lib.py:153] step: 717050, training_loss: 1.32481e+02
I1112 13:28:35.222245 140264174335808 run_lib.py:153] step: 717100, training_loss: 1.35250e+02
I1112 13:28:45.766380 140264174335808 run_lib.py:153] step: 717150, training_loss: 1.13645e+02
I1112 13:28:56.572257 140264174335808 run_lib.py:153] step: 717200, training_loss: 1.13796e+02
I1112 13:29:06.586023 140264174335808 run_lib.py:153] step: 717250, training_loss: 1.22330e+02
I1112 13:29:17.460063 140264174335808 run_lib.py:153] step: 717300, training_loss: 1.08772e+02
I1112 13:29:27.696293 140264174335808 run_lib.py:153] step: 717350, training_loss: 1.35571e+02
I1112 13:29:38.331780 140264174335808 run_lib.py:153] step: 717400, training_loss: 1.16942e+02
I1112 13:29:49.150520 140264174335808 run_lib.py:153] step: 717450, training_loss: 1.24546e+02
I1112 13:29:59.316863 140264174335808 run_lib.py:153] step: 717500, training_loss: 1.17007e+02
I1112 13:30:09.297523 140264174335808 run_lib.py:153] step: 717550, training_loss: 1.33488e+02
I1112 13:30:19.608232 140264174335808 run_lib.py:153] step: 717600, training_loss: 1.05693e+02
I1112 13:30:29.641141 140264174335808 run_lib.py:153] step: 717650, training_loss: 1.09482e+02
I1112 13:30:39.525128 140264174335808 run_lib.py:153] step: 717700, training_loss: 1.53941e+02
I1112 13:30:49.067642 140264174335808 run_lib.py:153] step: 717750, training_loss: 1.03836e+02
I1112 13:30:59.079727 140264174335808 run_lib.py:153] step: 717800, training_loss: 1.16305e+02
I1112 13:31:09.562100 140264174335808 run_lib.py:153] step: 717850, training_loss: 1.33270e+02
I1112 13:31:20.433459 140264174335808 run_lib.py:153] step: 717900, training_loss: 1.51398e+02
I1112 13:31:30.614909 140264174335808 run_lib.py:153] step: 717950, training_loss: 1.27294e+02
I1112 13:31:39.811057 140264174335808 run_lib.py:153] step: 718000, training_loss: 1.60162e+02
I1112 13:31:49.814927 140264174335808 run_lib.py:153] step: 718050, training_loss: 1.25514e+02
I1112 13:32:00.559679 140264174335808 run_lib.py:153] step: 718100, training_loss: 1.43634e+02
I1112 13:32:10.876614 140264174335808 run_lib.py:153] step: 718150, training_loss: 9.30574e+01
I1112 13:32:20.910131 140264174335808 run_lib.py:153] step: 718200, training_loss: 1.31224e+02
I1112 13:32:31.466227 140264174335808 run_lib.py:153] step: 718250, training_loss: 1.50000e+02
I1112 13:32:41.206391 140264174335808 run_lib.py:153] step: 718300, training_loss: 1.28578e+02
I1112 13:32:51.596295 140264174335808 run_lib.py:153] step: 718350, training_loss: 1.11831e+02
I1112 13:33:01.904478 140264174335808 run_lib.py:153] step: 718400, training_loss: 9.58132e+01
I1112 13:33:12.229654 140264174335808 run_lib.py:153] step: 718450, training_loss: 9.06470e+01
I1112 13:33:22.094925 140264174335808 run_lib.py:153] step: 718500, training_loss: 1.22469e+02
I1112 13:33:31.758022 140264174335808 run_lib.py:153] step: 718550, training_loss: 1.36892e+02
I1112 13:33:42.117286 140264174335808 run_lib.py:153] step: 718600, training_loss: 1.22117e+02
I1112 13:33:51.682018 140264174335808 run_lib.py:153] step: 718650, training_loss: 1.22364e+02
I1112 13:34:01.518173 140264174335808 run_lib.py:153] step: 718700, training_loss: 1.25159e+02
I1112 13:34:11.329758 140264174335808 run_lib.py:153] step: 718750, training_loss: 1.27993e+02
I1112 13:34:21.035537 140264174335808 run_lib.py:153] step: 718800, training_loss: 1.07056e+02
I1112 13:34:31.090391 140264174335808 run_lib.py:153] step: 718850, training_loss: 1.26885e+02
I1112 13:34:41.323255 140264174335808 run_lib.py:153] step: 718900, training_loss: 1.11793e+02
I1112 13:34:51.250106 140264174335808 run_lib.py:153] step: 718950, training_loss: 1.10696e+02
I1112 13:35:01.096885 140264174335808 run_lib.py:153] step: 719000, training_loss: 1.18659e+02
I1112 13:35:11.394974 140264174335808 run_lib.py:153] step: 719050, training_loss: 8.64867e+01
I1112 13:35:20.670302 140264174335808 run_lib.py:153] step: 719100, training_loss: 1.41997e+02
I1112 13:35:30.950191 140264174335808 run_lib.py:153] step: 719150, training_loss: 1.11576e+02
I1112 13:35:40.273507 140264174335808 run_lib.py:153] step: 719200, training_loss: 1.20842e+02
I1112 13:35:50.517378 140264174335808 run_lib.py:153] step: 719250, training_loss: 1.08242e+02
I1112 13:36:01.204607 140264174335808 run_lib.py:153] step: 719300, training_loss: 1.22384e+02
I1112 13:36:11.711213 140264174335808 run_lib.py:153] step: 719350, training_loss: 1.03577e+02
I1112 13:36:21.879201 140264174335808 run_lib.py:153] step: 719400, training_loss: 1.37874e+02
I1112 13:36:32.252839 140264174335808 run_lib.py:153] step: 719450, training_loss: 1.01086e+02
I1112 13:36:42.428539 140264174335808 run_lib.py:153] step: 719500, training_loss: 1.15475e+02
I1112 13:36:52.334561 140264174335808 run_lib.py:153] step: 719550, training_loss: 1.29831e+02
I1112 13:37:02.136636 140264174335808 run_lib.py:153] step: 719600, training_loss: 1.59985e+02
I1112 13:37:12.463269 140264174335808 run_lib.py:153] step: 719650, training_loss: 1.02574e+02
I1112 13:37:22.419681 140264174335808 run_lib.py:153] step: 719700, training_loss: 1.48122e+02
I1112 13:37:32.319534 140264174335808 run_lib.py:153] step: 719750, training_loss: 1.31892e+02
I1112 13:37:42.342113 140264174335808 run_lib.py:153] step: 719800, training_loss: 9.13445e+01
I1112 13:37:52.650449 140264174335808 run_lib.py:153] step: 719850, training_loss: 1.28975e+02
I1112 13:38:02.144155 140264174335808 run_lib.py:153] step: 719900, training_loss: 1.30629e+02
I1112 13:38:12.875036 140264174335808 run_lib.py:153] step: 719950, training_loss: 1.26132e+02
I1112 13:38:22.917187 140264174335808 run_lib.py:153] step: 720000, training_loss: 1.27518e+02
I1112 13:38:23.488770 140264174335808 run_lib.py:166] step: 720000, eval_loss: 1.02886e+02
I1112 13:38:34.036398 140264174335808 run_lib.py:153] step: 720050, training_loss: 1.20321e+02
I1112 13:38:44.178591 140264174335808 run_lib.py:153] step: 720100, training_loss: 1.31190e+02
I1112 13:38:54.372318 140264174335808 run_lib.py:153] step: 720150, training_loss: 1.31880e+02
I1112 13:39:04.457138 140264174335808 run_lib.py:153] step: 720200, training_loss: 1.34411e+02
I1112 13:39:14.233747 140264174335808 run_lib.py:153] step: 720250, training_loss: 1.42100e+02
I1112 13:39:24.954350 140264174335808 run_lib.py:153] step: 720300, training_loss: 1.29172e+02
I1112 13:39:35.006950 140264174335808 run_lib.py:153] step: 720350, training_loss: 1.16793e+02
I1112 13:39:45.166110 140264174335808 run_lib.py:153] step: 720400, training_loss: 1.21115e+02
I1112 13:39:55.429980 140264174335808 run_lib.py:153] step: 720450, training_loss: 1.55355e+02
I1112 13:40:05.694893 140264174335808 run_lib.py:153] step: 720500, training_loss: 1.27014e+02
I1112 13:40:15.652093 140264174335808 run_lib.py:153] step: 720550, training_loss: 1.30497e+02
I1112 13:40:25.890371 140264174335808 run_lib.py:153] step: 720600, training_loss: 1.38179e+02
I1112 13:40:35.923485 140264174335808 run_lib.py:153] step: 720650, training_loss: 1.30838e+02
I1112 13:40:45.551981 140264174335808 run_lib.py:153] step: 720700, training_loss: 1.01626e+02
I1112 13:40:55.551976 140264174335808 run_lib.py:153] step: 720750, training_loss: 1.44576e+02
I1112 13:41:06.559169 140264174335808 run_lib.py:153] step: 720800, training_loss: 1.37820e+02
I1112 13:41:15.898926 140264174335808 run_lib.py:153] step: 720850, training_loss: 1.32241e+02
I1112 13:41:25.649291 140264174335808 run_lib.py:153] step: 720900, training_loss: 9.98260e+01
I1112 13:41:35.260507 140264174335808 run_lib.py:153] step: 720950, training_loss: 1.00222e+02
I1112 13:41:45.482007 140264174335808 run_lib.py:153] step: 721000, training_loss: 1.54738e+02
I1112 13:41:55.055396 140264174335808 run_lib.py:153] step: 721050, training_loss: 1.12008e+02
I1112 13:42:05.658265 140264174335808 run_lib.py:153] step: 721100, training_loss: 1.10434e+02
I1112 13:42:16.311444 140264174335808 run_lib.py:153] step: 721150, training_loss: 1.21789e+02
I1112 13:42:25.983174 140264174335808 run_lib.py:153] step: 721200, training_loss: 9.61130e+01
I1112 13:42:35.505306 140264174335808 run_lib.py:153] step: 721250, training_loss: 1.46369e+02
I1112 13:42:45.727838 140264174335808 run_lib.py:153] step: 721300, training_loss: 1.05646e+02
I1112 13:42:55.609220 140264174335808 run_lib.py:153] step: 721350, training_loss: 9.63664e+01
I1112 13:43:05.174853 140264174335808 run_lib.py:153] step: 721400, training_loss: 1.21575e+02
I1112 13:43:14.802848 140264174335808 run_lib.py:153] step: 721450, training_loss: 1.20816e+02
I1112 13:43:24.454923 140264174335808 run_lib.py:153] step: 721500, training_loss: 1.21252e+02
I1112 13:43:34.502178 140264174335808 run_lib.py:153] step: 721550, training_loss: 1.18837e+02
I1112 13:43:44.522334 140264174335808 run_lib.py:153] step: 721600, training_loss: 1.34171e+02
I1112 13:43:54.069995 140264174335808 run_lib.py:153] step: 721650, training_loss: 1.23913e+02
I1112 13:44:03.680156 140264174335808 run_lib.py:153] step: 721700, training_loss: 1.17634e+02
I1112 13:44:13.373962 140264174335808 run_lib.py:153] step: 721750, training_loss: 1.09157e+02
I1112 13:44:22.962234 140264174335808 run_lib.py:153] step: 721800, training_loss: 1.22146e+02
I1112 13:44:33.254720 140264174335808 run_lib.py:153] step: 721850, training_loss: 1.47141e+02
I1112 13:44:42.822613 140264174335808 run_lib.py:153] step: 721900, training_loss: 1.49280e+02
I1112 13:44:52.457500 140264174335808 run_lib.py:153] step: 721950, training_loss: 1.29477e+02
I1112 13:45:02.523452 140264174335808 run_lib.py:153] step: 722000, training_loss: 1.15198e+02
I1112 13:45:12.545582 140264174335808 run_lib.py:153] step: 722050, training_loss: 1.70893e+02
I1112 13:45:21.958172 140264174335808 run_lib.py:153] step: 722100, training_loss: 1.27454e+02
I1112 13:45:31.434687 140264174335808 run_lib.py:153] step: 722150, training_loss: 1.29600e+02
I1112 13:45:41.209552 140264174335808 run_lib.py:153] step: 722200, training_loss: 1.29423e+02
I1112 13:45:50.656787 140264174335808 run_lib.py:153] step: 722250, training_loss: 9.46852e+01
I1112 13:46:00.750183 140264174335808 run_lib.py:153] step: 722300, training_loss: 1.35582e+02
I1112 13:46:11.071445 140264174335808 run_lib.py:153] step: 722350, training_loss: 1.25630e+02
I1112 13:46:21.421768 140264174335808 run_lib.py:153] step: 722400, training_loss: 1.16602e+02
I1112 13:46:31.542078 140264174335808 run_lib.py:153] step: 722450, training_loss: 1.11080e+02
I1112 13:46:41.241523 140264174335808 run_lib.py:153] step: 722500, training_loss: 1.44250e+02
I1112 13:46:50.631469 140264174335808 run_lib.py:153] step: 722550, training_loss: 1.13488e+02
I1112 13:47:00.832664 140264174335808 run_lib.py:153] step: 722600, training_loss: 1.37517e+02
I1112 13:47:11.357129 140264174335808 run_lib.py:153] step: 722650, training_loss: 1.52109e+02
I1112 13:47:22.048404 140264174335808 run_lib.py:153] step: 722700, training_loss: 1.40710e+02
I1112 13:47:32.355965 140264174335808 run_lib.py:153] step: 722750, training_loss: 1.28014e+02
I1112 13:47:42.725780 140264174335808 run_lib.py:153] step: 722800, training_loss: 1.11894e+02
I1112 13:47:53.102483 140264174335808 run_lib.py:153] step: 722850, training_loss: 1.06481e+02
I1112 13:48:02.967187 140264174335808 run_lib.py:153] step: 722900, training_loss: 1.07872e+02
I1112 13:48:12.790232 140264174335808 run_lib.py:153] step: 722950, training_loss: 1.32812e+02
I1112 13:48:23.249380 140264174335808 run_lib.py:153] step: 723000, training_loss: 1.27890e+02
I1112 13:48:32.941863 140264174335808 run_lib.py:153] step: 723050, training_loss: 1.45788e+02
I1112 13:48:42.686985 140264174335808 run_lib.py:153] step: 723100, training_loss: 1.31259e+02
I1112 13:48:53.136117 140264174335808 run_lib.py:153] step: 723150, training_loss: 1.29062e+02
I1112 13:49:02.982746 140264174335808 run_lib.py:153] step: 723200, training_loss: 1.07055e+02
I1112 13:49:13.288285 140264174335808 run_lib.py:153] step: 723250, training_loss: 9.13667e+01
I1112 13:49:24.056781 140264174335808 run_lib.py:153] step: 723300, training_loss: 1.29761e+02
I1112 13:49:34.777848 140264174335808 run_lib.py:153] step: 723350, training_loss: 1.26609e+02
I1112 13:49:45.344395 140264174335808 run_lib.py:153] step: 723400, training_loss: 1.04539e+02
I1112 13:49:55.056961 140264174335808 run_lib.py:153] step: 723450, training_loss: 1.49457e+02
I1112 13:50:05.066074 140264174335808 run_lib.py:153] step: 723500, training_loss: 1.36533e+02
I1112 13:50:15.245218 140264174335808 run_lib.py:153] step: 723550, training_loss: 1.25906e+02
I1112 13:50:25.081702 140264174335808 run_lib.py:153] step: 723600, training_loss: 1.24846e+02
I1112 13:50:35.215123 140264174335808 run_lib.py:153] step: 723650, training_loss: 1.32861e+02
I1112 13:50:44.839279 140264174335808 run_lib.py:153] step: 723700, training_loss: 1.48335e+02
I1112 13:50:54.655993 140264174335808 run_lib.py:153] step: 723750, training_loss: 1.16953e+02
I1112 13:51:04.972450 140264174335808 run_lib.py:153] step: 723800, training_loss: 9.97869e+01
I1112 13:51:14.945133 140264174335808 run_lib.py:153] step: 723850, training_loss: 1.56374e+02
I1112 13:51:24.721139 140264174335808 run_lib.py:153] step: 723900, training_loss: 1.26788e+02
I1112 13:51:35.641102 140264174335808 run_lib.py:153] step: 723950, training_loss: 1.18100e+02
I1112 13:51:46.046557 140264174335808 run_lib.py:153] step: 724000, training_loss: 1.23005e+02
I1112 13:51:56.927011 140264174335808 run_lib.py:153] step: 724050, training_loss: 1.24227e+02
I1112 13:52:07.325580 140264174335808 run_lib.py:153] step: 724100, training_loss: 9.53706e+01
I1112 13:52:18.449196 140264174335808 run_lib.py:153] step: 724150, training_loss: 1.33479e+02
I1112 13:52:29.623553 140264174335808 run_lib.py:153] step: 724200, training_loss: 1.22542e+02
I1112 13:52:39.509997 140264174335808 run_lib.py:153] step: 724250, training_loss: 1.25818e+02
I1112 13:52:49.847623 140264174335808 run_lib.py:153] step: 724300, training_loss: 1.20475e+02
I1112 13:52:59.422034 140264174335808 run_lib.py:153] step: 724350, training_loss: 1.70207e+02
I1112 13:53:09.658111 140264174335808 run_lib.py:153] step: 724400, training_loss: 1.33887e+02
I1112 13:53:19.563292 140264174335808 run_lib.py:153] step: 724450, training_loss: 1.49613e+02
I1112 13:53:29.903160 140264174335808 run_lib.py:153] step: 724500, training_loss: 1.25454e+02
I1112 13:53:39.875213 140264174335808 run_lib.py:153] step: 724550, training_loss: 9.37729e+01
I1112 13:53:49.841407 140264174335808 run_lib.py:153] step: 724600, training_loss: 1.36331e+02
I1112 13:53:59.583366 140264174335808 run_lib.py:153] step: 724650, training_loss: 1.39140e+02
I1112 13:54:09.508214 140264174335808 run_lib.py:153] step: 724700, training_loss: 1.33478e+02
I1112 13:54:19.957709 140264174335808 run_lib.py:153] step: 724750, training_loss: 1.46649e+02
I1112 13:54:29.597725 140264174335808 run_lib.py:153] step: 724800, training_loss: 1.30832e+02
I1112 13:54:40.019877 140264174335808 run_lib.py:153] step: 724850, training_loss: 7.82728e+01
I1112 13:54:50.002891 140264174335808 run_lib.py:153] step: 724900, training_loss: 1.40838e+02
I1112 13:54:59.963529 140264174335808 run_lib.py:153] step: 724950, training_loss: 1.16888e+02
I1112 13:55:10.763365 140264174335808 run_lib.py:153] step: 725000, training_loss: 1.57641e+02
I1112 13:55:10.864792 140264174335808 run_lib.py:166] step: 725000, eval_loss: 1.30394e+02
I1112 13:55:20.810296 140264174335808 run_lib.py:153] step: 725050, training_loss: 1.15684e+02
I1112 13:55:30.776322 140264174335808 run_lib.py:153] step: 725100, training_loss: 1.27754e+02
I1112 13:55:40.173302 140264174335808 run_lib.py:153] step: 725150, training_loss: 1.12109e+02
I1112 13:55:49.822771 140264174335808 run_lib.py:153] step: 725200, training_loss: 1.31288e+02
I1112 13:55:59.522591 140264174335808 run_lib.py:153] step: 725250, training_loss: 1.35354e+02
I1112 13:56:09.860882 140264174335808 run_lib.py:153] step: 725300, training_loss: 1.44614e+02
I1112 13:56:20.347973 140264174335808 run_lib.py:153] step: 725350, training_loss: 1.42365e+02
I1112 13:56:30.318065 140264174335808 run_lib.py:153] step: 725400, training_loss: 1.19501e+02
I1112 13:56:40.896824 140264174335808 run_lib.py:153] step: 725450, training_loss: 1.07565e+02
I1112 13:56:50.977049 140264174335808 run_lib.py:153] step: 725500, training_loss: 1.26231e+02
I1112 13:57:00.835312 140264174335808 run_lib.py:153] step: 725550, training_loss: 1.11364e+02
I1112 13:57:10.413536 140264174335808 run_lib.py:153] step: 725600, training_loss: 1.45512e+02
I1112 13:57:20.178501 140264174335808 run_lib.py:153] step: 725650, training_loss: 1.47405e+02
I1112 13:57:29.596641 140264174335808 run_lib.py:153] step: 725700, training_loss: 1.50290e+02
I1112 13:57:39.690464 140264174335808 run_lib.py:153] step: 725750, training_loss: 1.16082e+02
I1112 13:57:49.550919 140264174335808 run_lib.py:153] step: 725800, training_loss: 1.26492e+02
I1112 13:57:59.365668 140264174335808 run_lib.py:153] step: 725850, training_loss: 1.25666e+02
I1112 13:58:09.541894 140264174335808 run_lib.py:153] step: 725900, training_loss: 1.45748e+02
I1112 13:58:19.393699 140264174335808 run_lib.py:153] step: 725950, training_loss: 1.21323e+02
I1112 13:58:29.449923 140264174335808 run_lib.py:153] step: 726000, training_loss: 1.30822e+02
I1112 13:58:39.597954 140264174335808 run_lib.py:153] step: 726050, training_loss: 1.34475e+02
I1112 13:58:49.815326 140264174335808 run_lib.py:153] step: 726100, training_loss: 1.18786e+02
I1112 13:58:59.409439 140264174335808 run_lib.py:153] step: 726150, training_loss: 1.01311e+02
I1112 13:59:09.433823 140264174335808 run_lib.py:153] step: 726200, training_loss: 1.25150e+02
I1112 13:59:19.054591 140264174335808 run_lib.py:153] step: 726250, training_loss: 1.41966e+02
I1112 13:59:28.994431 140264174335808 run_lib.py:153] step: 726300, training_loss: 1.13514e+02
I1112 13:59:39.689469 140264174335808 run_lib.py:153] step: 726350, training_loss: 1.09920e+02
I1112 13:59:49.897582 140264174335808 run_lib.py:153] step: 726400, training_loss: 1.05232e+02
I1112 13:59:59.833580 140264174335808 run_lib.py:153] step: 726450, training_loss: 1.34823e+02
I1112 14:00:09.460754 140264174335808 run_lib.py:153] step: 726500, training_loss: 1.31868e+02
I1112 14:00:19.277075 140264174335808 run_lib.py:153] step: 726550, training_loss: 1.11678e+02
I1112 14:00:29.085017 140264174335808 run_lib.py:153] step: 726600, training_loss: 1.53843e+02
I1112 14:00:39.038652 140264174335808 run_lib.py:153] step: 726650, training_loss: 1.39577e+02
I1112 14:00:49.499661 140264174335808 run_lib.py:153] step: 726700, training_loss: 1.01661e+02
I1112 14:00:59.471366 140264174335808 run_lib.py:153] step: 726750, training_loss: 1.40482e+02
I1112 14:01:09.844828 140264174335808 run_lib.py:153] step: 726800, training_loss: 1.34145e+02
I1112 14:01:20.160005 140264174335808 run_lib.py:153] step: 726850, training_loss: 1.42052e+02
I1112 14:01:29.859356 140264174335808 run_lib.py:153] step: 726900, training_loss: 9.41725e+01
I1112 14:01:40.450262 140264174335808 run_lib.py:153] step: 726950, training_loss: 1.42695e+02
I1112 14:01:50.586657 140264174335808 run_lib.py:153] step: 727000, training_loss: 1.11831e+02
I1112 14:02:00.762319 140264174335808 run_lib.py:153] step: 727050, training_loss: 1.55565e+02
I1112 14:02:10.725427 140264174335808 run_lib.py:153] step: 727100, training_loss: 1.28872e+02
I1112 14:02:20.957565 140264174335808 run_lib.py:153] step: 727150, training_loss: 1.27975e+02
I1112 14:02:30.365693 140264174335808 run_lib.py:153] step: 727200, training_loss: 1.34066e+02
I1112 14:02:40.170622 140264174335808 run_lib.py:153] step: 727250, training_loss: 1.29588e+02
I1112 14:02:50.367840 140264174335808 run_lib.py:153] step: 727300, training_loss: 1.02939e+02
I1112 14:03:00.760593 140264174335808 run_lib.py:153] step: 727350, training_loss: 1.12101e+02
I1112 14:03:10.614731 140264174335808 run_lib.py:153] step: 727400, training_loss: 1.28705e+02
I1112 14:03:21.038352 140264174335808 run_lib.py:153] step: 727450, training_loss: 1.06331e+02
I1112 14:03:31.111316 140264174335808 run_lib.py:153] step: 727500, training_loss: 1.21696e+02
I1112 14:03:41.551577 140264174335808 run_lib.py:153] step: 727550, training_loss: 1.53537e+02
I1112 14:03:52.139572 140264174335808 run_lib.py:153] step: 727600, training_loss: 9.71310e+01
I1112 14:04:02.561931 140264174335808 run_lib.py:153] step: 727650, training_loss: 1.00067e+02
I1112 14:04:12.878335 140264174335808 run_lib.py:153] step: 727700, training_loss: 1.44668e+02
I1112 14:04:22.660038 140264174335808 run_lib.py:153] step: 727750, training_loss: 1.25444e+02
I1112 14:04:33.062269 140264174335808 run_lib.py:153] step: 727800, training_loss: 1.15331e+02
I1112 14:04:42.831398 140264174335808 run_lib.py:153] step: 727850, training_loss: 1.25101e+02
I1112 14:04:52.467125 140264174335808 run_lib.py:153] step: 727900, training_loss: 1.13401e+02
I1112 14:05:02.495115 140264174335808 run_lib.py:153] step: 727950, training_loss: 1.26284e+02
I1112 14:05:12.012339 140264174335808 run_lib.py:153] step: 728000, training_loss: 1.39452e+02
I1112 14:05:22.270650 140264174335808 run_lib.py:153] step: 728050, training_loss: 1.24860e+02
I1112 14:05:32.937385 140264174335808 run_lib.py:153] step: 728100, training_loss: 8.76067e+01
I1112 14:05:43.206651 140264174335808 run_lib.py:153] step: 728150, training_loss: 1.14801e+02
I1112 14:05:53.908638 140264174335808 run_lib.py:153] step: 728200, training_loss: 1.50677e+02
I1112 14:06:03.906423 140264174335808 run_lib.py:153] step: 728250, training_loss: 1.33649e+02
I1112 14:06:13.590606 140264174335808 run_lib.py:153] step: 728300, training_loss: 1.30530e+02
I1112 14:06:23.977282 140264174335808 run_lib.py:153] step: 728350, training_loss: 1.17988e+02
I1112 14:06:33.954345 140264174335808 run_lib.py:153] step: 728400, training_loss: 1.18094e+02
I1112 14:06:43.934652 140264174335808 run_lib.py:153] step: 728450, training_loss: 1.46093e+02
I1112 14:06:53.586844 140264174335808 run_lib.py:153] step: 728500, training_loss: 1.23313e+02
I1112 14:07:04.091192 140264174335808 run_lib.py:153] step: 728550, training_loss: 1.11165e+02
I1112 14:07:14.170288 140264174335808 run_lib.py:153] step: 728600, training_loss: 1.16371e+02
I1112 14:07:23.934423 140264174335808 run_lib.py:153] step: 728650, training_loss: 1.08829e+02
I1112 14:07:33.886225 140264174335808 run_lib.py:153] step: 728700, training_loss: 1.27722e+02
I1112 14:07:43.659246 140264174335808 run_lib.py:153] step: 728750, training_loss: 1.22635e+02
I1112 14:07:52.950894 140264174335808 run_lib.py:153] step: 728800, training_loss: 1.16138e+02
I1112 14:08:02.596613 140264174335808 run_lib.py:153] step: 728850, training_loss: 1.21024e+02
I1112 14:08:12.857106 140264174335808 run_lib.py:153] step: 728900, training_loss: 1.45434e+02
I1112 14:08:22.518142 140264174335808 run_lib.py:153] step: 728950, training_loss: 1.33737e+02
I1112 14:08:32.888285 140264174335808 run_lib.py:153] step: 729000, training_loss: 1.05625e+02
I1112 14:08:42.490946 140264174335808 run_lib.py:153] step: 729050, training_loss: 1.20657e+02
I1112 14:08:53.213872 140264174335808 run_lib.py:153] step: 729100, training_loss: 1.25778e+02
I1112 14:09:03.191131 140264174335808 run_lib.py:153] step: 729150, training_loss: 1.18834e+02
I1112 14:09:12.519064 140264174335808 run_lib.py:153] step: 729200, training_loss: 1.17714e+02
I1112 14:09:22.418741 140264174335808 run_lib.py:153] step: 729250, training_loss: 1.18942e+02
I1112 14:09:33.329205 140264174335808 run_lib.py:153] step: 729300, training_loss: 9.36604e+01
I1112 14:09:43.745642 140264174335808 run_lib.py:153] step: 729350, training_loss: 1.35387e+02
I1112 14:09:53.815850 140264174335808 run_lib.py:153] step: 729400, training_loss: 1.06994e+02
I1112 14:10:03.736837 140264174335808 run_lib.py:153] step: 729450, training_loss: 1.51833e+02
I1112 14:10:13.930745 140264174335808 run_lib.py:153] step: 729500, training_loss: 1.36086e+02
I1112 14:10:23.273683 140264174335808 run_lib.py:153] step: 729550, training_loss: 1.04917e+02
I1112 14:10:32.626844 140264174335808 run_lib.py:153] step: 729600, training_loss: 1.30988e+02
I1112 14:10:42.021795 140264174335808 run_lib.py:153] step: 729650, training_loss: 1.25855e+02
I1112 14:10:51.326508 140264174335808 run_lib.py:153] step: 729700, training_loss: 1.43596e+02
I1112 14:11:00.559281 140264174335808 run_lib.py:153] step: 729750, training_loss: 1.26258e+02
I1112 14:11:10.161798 140264174335808 run_lib.py:153] step: 729800, training_loss: 1.43048e+02
I1112 14:11:20.108687 140264174335808 run_lib.py:153] step: 729850, training_loss: 1.27632e+02
I1112 14:11:30.459143 140264174335808 run_lib.py:153] step: 729900, training_loss: 8.72102e+01
I1112 14:11:41.111753 140264174335808 run_lib.py:153] step: 729950, training_loss: 1.15260e+02
I1112 14:11:51.535521 140264174335808 run_lib.py:153] step: 730000, training_loss: 1.23489e+02
I1112 14:11:52.086277 140264174335808 run_lib.py:166] step: 730000, eval_loss: 1.12820e+02
I1112 14:12:01.818397 140264174335808 run_lib.py:153] step: 730050, training_loss: 1.42946e+02
I1112 14:12:11.693248 140264174335808 run_lib.py:153] step: 730100, training_loss: 1.14037e+02
I1112 14:12:20.895249 140264174335808 run_lib.py:153] step: 730150, training_loss: 1.18100e+02
I1112 14:12:30.615131 140264174335808 run_lib.py:153] step: 730200, training_loss: 1.49261e+02
I1112 14:12:40.224549 140264174335808 run_lib.py:153] step: 730250, training_loss: 1.15503e+02
I1112 14:12:49.520370 140264174335808 run_lib.py:153] step: 730300, training_loss: 1.43484e+02
I1112 14:12:59.155520 140264174335808 run_lib.py:153] step: 730350, training_loss: 1.36140e+02
I1112 14:13:08.984937 140264174335808 run_lib.py:153] step: 730400, training_loss: 1.44818e+02
I1112 14:13:18.958830 140264174335808 run_lib.py:153] step: 730450, training_loss: 1.36824e+02
I1112 14:13:29.010516 140264174335808 run_lib.py:153] step: 730500, training_loss: 1.24808e+02
I1112 14:13:39.311374 140264174335808 run_lib.py:153] step: 730550, training_loss: 1.22331e+02
I1112 14:13:49.235410 140264174335808 run_lib.py:153] step: 730600, training_loss: 1.64099e+02
I1112 14:13:58.766299 140264174335808 run_lib.py:153] step: 730650, training_loss: 1.07377e+02
I1112 14:14:08.044331 140264174335808 run_lib.py:153] step: 730700, training_loss: 1.18499e+02
I1112 14:14:17.923088 140264174335808 run_lib.py:153] step: 730750, training_loss: 1.44615e+02
I1112 14:14:28.053293 140264174335808 run_lib.py:153] step: 730800, training_loss: 9.73949e+01
I1112 14:14:37.341567 140264174335808 run_lib.py:153] step: 730850, training_loss: 1.58247e+02
I1112 14:14:47.306238 140264174335808 run_lib.py:153] step: 730900, training_loss: 1.05136e+02
I1112 14:14:57.019278 140264174335808 run_lib.py:153] step: 730950, training_loss: 1.13035e+02
I1112 14:15:06.348808 140264174335808 run_lib.py:153] step: 731000, training_loss: 1.21678e+02
I1112 14:15:16.116453 140264174335808 run_lib.py:153] step: 731050, training_loss: 1.28205e+02
I1112 14:15:26.896934 140264174335808 run_lib.py:153] step: 731100, training_loss: 1.27396e+02
I1112 14:15:36.939216 140264174335808 run_lib.py:153] step: 731150, training_loss: 1.07649e+02
I1112 14:15:47.115221 140264174335808 run_lib.py:153] step: 731200, training_loss: 1.07095e+02
I1112 14:15:57.252326 140264174335808 run_lib.py:153] step: 731250, training_loss: 1.16344e+02
I1112 14:16:07.907195 140264174335808 run_lib.py:153] step: 731300, training_loss: 1.32016e+02
I1112 14:16:17.413925 140264174335808 run_lib.py:153] step: 731350, training_loss: 1.34140e+02
I1112 14:16:28.003050 140264174335808 run_lib.py:153] step: 731400, training_loss: 9.74253e+01
I1112 14:16:38.024605 140264174335808 run_lib.py:153] step: 731450, training_loss: 1.06242e+02
I1112 14:16:47.841513 140264174335808 run_lib.py:153] step: 731500, training_loss: 1.02648e+02
I1112 14:16:57.368396 140264174335808 run_lib.py:153] step: 731550, training_loss: 1.47038e+02
I1112 14:17:07.336174 140264174335808 run_lib.py:153] step: 731600, training_loss: 1.16864e+02
I1112 14:17:17.842266 140264174335808 run_lib.py:153] step: 731650, training_loss: 1.41016e+02
I1112 14:17:27.858570 140264174335808 run_lib.py:153] step: 731700, training_loss: 9.66526e+01
I1112 14:17:38.132018 140264174335808 run_lib.py:153] step: 731750, training_loss: 1.17530e+02
I1112 14:17:47.500140 140264174335808 run_lib.py:153] step: 731800, training_loss: 1.29749e+02
I1112 14:17:57.529251 140264174335808 run_lib.py:153] step: 731850, training_loss: 1.36811e+02
I1112 14:18:07.482943 140264174335808 run_lib.py:153] step: 731900, training_loss: 1.38892e+02
I1112 14:18:17.679679 140264174335808 run_lib.py:153] step: 731950, training_loss: 1.05511e+02
I1112 14:18:27.520818 140264174335808 run_lib.py:153] step: 732000, training_loss: 1.16846e+02
I1112 14:18:38.158823 140264174335808 run_lib.py:153] step: 732050, training_loss: 1.47476e+02
I1112 14:18:48.229791 140264174335808 run_lib.py:153] step: 732100, training_loss: 1.19970e+02
I1112 14:18:58.303418 140264174335808 run_lib.py:153] step: 732150, training_loss: 1.07882e+02
I1112 14:19:08.113350 140264174335808 run_lib.py:153] step: 732200, training_loss: 1.37888e+02
I1112 14:19:18.550056 140264174335808 run_lib.py:153] step: 732250, training_loss: 1.18943e+02
I1112 14:19:29.475941 140264174335808 run_lib.py:153] step: 732300, training_loss: 1.23218e+02
I1112 14:19:40.104372 140264174335808 run_lib.py:153] step: 732350, training_loss: 1.31540e+02
I1112 14:19:50.677134 140264174335808 run_lib.py:153] step: 732400, training_loss: 1.26089e+02
I1112 14:20:01.293478 140264174335808 run_lib.py:153] step: 732450, training_loss: 9.71436e+01
I1112 14:20:11.543741 140264174335808 run_lib.py:153] step: 732500, training_loss: 1.20870e+02
I1112 14:20:21.091892 140264174335808 run_lib.py:153] step: 732550, training_loss: 1.15406e+02
I1112 14:20:31.261989 140264174335808 run_lib.py:153] step: 732600, training_loss: 1.26743e+02
I1112 14:20:41.397887 140264174335808 run_lib.py:153] step: 732650, training_loss: 1.25131e+02
I1112 14:20:51.925962 140264174335808 run_lib.py:153] step: 732700, training_loss: 1.05604e+02
I1112 14:21:02.746426 140264174335808 run_lib.py:153] step: 732750, training_loss: 1.21645e+02
I1112 14:21:13.111667 140264174335808 run_lib.py:153] step: 732800, training_loss: 1.45456e+02
I1112 14:21:23.489894 140264174335808 run_lib.py:153] step: 732850, training_loss: 1.14236e+02
I1112 14:21:32.942798 140264174335808 run_lib.py:153] step: 732900, training_loss: 1.33826e+02
I1112 14:21:42.512134 140264174335808 run_lib.py:153] step: 732950, training_loss: 1.24362e+02
I1112 14:21:52.833039 140264174335808 run_lib.py:153] step: 733000, training_loss: 1.14848e+02
I1112 14:22:02.885242 140264174335808 run_lib.py:153] step: 733050, training_loss: 1.15740e+02
I1112 14:22:13.127303 140264174335808 run_lib.py:153] step: 733100, training_loss: 1.18160e+02
I1112 14:22:22.912806 140264174335808 run_lib.py:153] step: 733150, training_loss: 1.16823e+02
I1112 14:22:32.943752 140264174335808 run_lib.py:153] step: 733200, training_loss: 1.34281e+02
I1112 14:22:42.428513 140264174335808 run_lib.py:153] step: 733250, training_loss: 1.40812e+02
I1112 14:22:52.510757 140264174335808 run_lib.py:153] step: 733300, training_loss: 1.23697e+02
I1112 14:23:02.687009 140264174335808 run_lib.py:153] step: 733350, training_loss: 1.30179e+02
I1112 14:23:12.606805 140264174335808 run_lib.py:153] step: 733400, training_loss: 1.39070e+02
I1112 14:23:23.172563 140264174335808 run_lib.py:153] step: 733450, training_loss: 1.35289e+02
I1112 14:23:32.975633 140264174335808 run_lib.py:153] step: 733500, training_loss: 1.11172e+02
I1112 14:23:43.444368 140264174335808 run_lib.py:153] step: 733550, training_loss: 8.79164e+01
I1112 14:23:53.506161 140264174335808 run_lib.py:153] step: 733600, training_loss: 1.32761e+02
I1112 14:24:03.546425 140264174335808 run_lib.py:153] step: 733650, training_loss: 1.27085e+02
I1112 14:24:13.338161 140264174335808 run_lib.py:153] step: 733700, training_loss: 1.10966e+02
I1112 14:24:23.999594 140264174335808 run_lib.py:153] step: 733750, training_loss: 1.02703e+02
I1112 14:24:33.657173 140264174335808 run_lib.py:153] step: 733800, training_loss: 1.74180e+02
I1112 14:24:43.998517 140264174335808 run_lib.py:153] step: 733850, training_loss: 1.38869e+02
I1112 14:24:54.893013 140264174335808 run_lib.py:153] step: 733900, training_loss: 1.24575e+02
I1112 14:25:04.813025 140264174335808 run_lib.py:153] step: 733950, training_loss: 1.13521e+02
I1112 14:25:15.503995 140264174335808 run_lib.py:153] step: 734000, training_loss: 1.16866e+02
I1112 14:25:25.610113 140264174335808 run_lib.py:153] step: 734050, training_loss: 1.38590e+02
I1112 14:25:35.546954 140264174335808 run_lib.py:153] step: 734100, training_loss: 8.59385e+01
I1112 14:25:45.792329 140264174335808 run_lib.py:153] step: 734150, training_loss: 1.32245e+02
I1112 14:25:55.484112 140264174335808 run_lib.py:153] step: 734200, training_loss: 1.11931e+02
I1112 14:26:05.592187 140264174335808 run_lib.py:153] step: 734250, training_loss: 1.38444e+02
I1112 14:26:15.481408 140264174335808 run_lib.py:153] step: 734300, training_loss: 1.21629e+02
I1112 14:26:25.471143 140264174335808 run_lib.py:153] step: 734350, training_loss: 1.32455e+02
I1112 14:26:35.864675 140264174335808 run_lib.py:153] step: 734400, training_loss: 1.46829e+02
I1112 14:26:46.000028 140264174335808 run_lib.py:153] step: 734450, training_loss: 1.02038e+02
I1112 14:26:56.025534 140264174335808 run_lib.py:153] step: 734500, training_loss: 1.19061e+02
I1112 14:27:05.857433 140264174335808 run_lib.py:153] step: 734550, training_loss: 1.66886e+02
I1112 14:27:16.211105 140264174335808 run_lib.py:153] step: 734600, training_loss: 1.33846e+02
I1112 14:27:26.060613 140264174335808 run_lib.py:153] step: 734650, training_loss: 1.14349e+02
I1112 14:27:35.966861 140264174335808 run_lib.py:153] step: 734700, training_loss: 1.27177e+02
I1112 14:27:46.097445 140264174335808 run_lib.py:153] step: 734750, training_loss: 1.10701e+02
I1112 14:27:56.061893 140264174335808 run_lib.py:153] step: 734800, training_loss: 1.28690e+02
I1112 14:28:06.546011 140264174335808 run_lib.py:153] step: 734850, training_loss: 1.49313e+02
I1112 14:28:16.952770 140264174335808 run_lib.py:153] step: 734900, training_loss: 1.19894e+02
I1112 14:28:26.818385 140264174335808 run_lib.py:153] step: 734950, training_loss: 1.29926e+02
I1112 14:28:37.059615 140264174335808 run_lib.py:153] step: 735000, training_loss: 1.31572e+02
I1112 14:28:37.196960 140264174335808 run_lib.py:166] step: 735000, eval_loss: 1.08332e+02
I1112 14:28:47.018750 140264174335808 run_lib.py:153] step: 735050, training_loss: 1.68467e+02
I1112 14:28:56.562916 140264174335808 run_lib.py:153] step: 735100, training_loss: 1.69825e+02
I1112 14:29:06.166315 140264174335808 run_lib.py:153] step: 735150, training_loss: 1.24865e+02
I1112 14:29:16.085475 140264174335808 run_lib.py:153] step: 735200, training_loss: 1.27506e+02
I1112 14:29:25.576555 140264174335808 run_lib.py:153] step: 735250, training_loss: 1.06615e+02
I1112 14:29:35.101840 140264174335808 run_lib.py:153] step: 735300, training_loss: 1.46514e+02
I1112 14:29:45.136891 140264174335808 run_lib.py:153] step: 735350, training_loss: 1.20649e+02
I1112 14:29:55.513262 140264174335808 run_lib.py:153] step: 735400, training_loss: 1.10685e+02
I1112 14:30:05.467174 140264174335808 run_lib.py:153] step: 735450, training_loss: 1.34261e+02
I1112 14:30:15.180313 140264174335808 run_lib.py:153] step: 735500, training_loss: 1.17472e+02
I1112 14:30:24.735625 140264174335808 run_lib.py:153] step: 735550, training_loss: 1.45392e+02
I1112 14:30:35.033804 140264174335808 run_lib.py:153] step: 735600, training_loss: 1.27456e+02
I1112 14:30:45.492633 140264174335808 run_lib.py:153] step: 735650, training_loss: 1.02471e+02
I1112 14:30:55.563648 140264174335808 run_lib.py:153] step: 735700, training_loss: 1.00363e+02
I1112 14:31:05.558084 140264174335808 run_lib.py:153] step: 735750, training_loss: 1.25163e+02
I1112 14:31:15.610239 140264174335808 run_lib.py:153] step: 735800, training_loss: 1.38721e+02
I1112 14:31:26.353397 140264174335808 run_lib.py:153] step: 735850, training_loss: 1.15781e+02
I1112 14:31:36.185554 140264174335808 run_lib.py:153] step: 735900, training_loss: 1.52852e+02
I1112 14:31:46.349599 140264174335808 run_lib.py:153] step: 735950, training_loss: 1.34262e+02
I1112 14:31:56.735684 140264174335808 run_lib.py:153] step: 736000, training_loss: 1.45917e+02
I1112 14:32:06.777590 140264174335808 run_lib.py:153] step: 736050, training_loss: 1.39515e+02
I1112 14:32:17.016357 140264174335808 run_lib.py:153] step: 736100, training_loss: 1.16025e+02
I1112 14:32:27.172344 140264174335808 run_lib.py:153] step: 736150, training_loss: 1.10321e+02
I1112 14:32:37.160868 140264174335808 run_lib.py:153] step: 736200, training_loss: 1.36043e+02
I1112 14:32:47.882431 140264174335808 run_lib.py:153] step: 736250, training_loss: 1.12167e+02
I1112 14:32:58.404073 140264174335808 run_lib.py:153] step: 736300, training_loss: 1.21734e+02
I1112 14:33:08.309439 140264174335808 run_lib.py:153] step: 736350, training_loss: 1.23177e+02
I1112 14:33:18.092998 140264174335808 run_lib.py:153] step: 736400, training_loss: 1.35340e+02
I1112 14:33:27.926890 140264174335808 run_lib.py:153] step: 736450, training_loss: 1.46571e+02
I1112 14:33:38.115684 140264174335808 run_lib.py:153] step: 736500, training_loss: 1.08276e+02
I1112 14:33:47.951495 140264174335808 run_lib.py:153] step: 736550, training_loss: 1.29547e+02
I1112 14:33:58.186605 140264174335808 run_lib.py:153] step: 736600, training_loss: 1.34448e+02
I1112 14:34:08.057985 140264174335808 run_lib.py:153] step: 736650, training_loss: 1.24782e+02
I1112 14:34:18.134314 140264174335808 run_lib.py:153] step: 736700, training_loss: 1.29839e+02
I1112 14:34:27.835230 140264174335808 run_lib.py:153] step: 736750, training_loss: 1.28097e+02
I1112 14:34:38.020953 140264174335808 run_lib.py:153] step: 736800, training_loss: 1.07601e+02
I1112 14:34:48.405150 140264174335808 run_lib.py:153] step: 736850, training_loss: 1.39414e+02
I1112 14:34:58.432950 140264174335808 run_lib.py:153] step: 736900, training_loss: 1.40663e+02
I1112 14:35:08.570519 140264174335808 run_lib.py:153] step: 736950, training_loss: 1.09586e+02
I1112 14:35:18.608937 140264174335808 run_lib.py:153] step: 737000, training_loss: 9.53753e+01
I1112 14:35:29.146833 140264174335808 run_lib.py:153] step: 737050, training_loss: 1.32776e+02
I1112 14:35:39.504502 140264174335808 run_lib.py:153] step: 737100, training_loss: 1.53664e+02
I1112 14:35:49.917738 140264174335808 run_lib.py:153] step: 737150, training_loss: 1.19804e+02
I1112 14:35:59.403212 140264174335808 run_lib.py:153] step: 737200, training_loss: 1.38629e+02
I1112 14:36:09.159265 140264174335808 run_lib.py:153] step: 737250, training_loss: 1.39899e+02
I1112 14:36:18.822252 140264174335808 run_lib.py:153] step: 737300, training_loss: 9.60826e+01
I1112 14:36:28.995940 140264174335808 run_lib.py:153] step: 737350, training_loss: 1.13174e+02
I1112 14:36:39.679044 140264174335808 run_lib.py:153] step: 737400, training_loss: 1.01551e+02
I1112 14:36:49.894609 140264174335808 run_lib.py:153] step: 737450, training_loss: 1.45489e+02
I1112 14:37:00.200709 140264174335808 run_lib.py:153] step: 737500, training_loss: 9.84905e+01
I1112 14:37:10.633934 140264174335808 run_lib.py:153] step: 737550, training_loss: 1.37129e+02
I1112 14:37:20.831895 140264174335808 run_lib.py:153] step: 737600, training_loss: 1.16571e+02
I1112 14:37:31.363160 140264174335808 run_lib.py:153] step: 737650, training_loss: 1.12033e+02
I1112 14:37:41.399591 140264174335808 run_lib.py:153] step: 737700, training_loss: 1.29798e+02
I1112 14:37:52.089242 140264174335808 run_lib.py:153] step: 737750, training_loss: 1.24338e+02
I1112 14:38:01.963734 140264174335808 run_lib.py:153] step: 737800, training_loss: 1.55320e+02
I1112 14:38:12.295787 140264174335808 run_lib.py:153] step: 737850, training_loss: 1.12046e+02
I1112 14:38:22.113284 140264174335808 run_lib.py:153] step: 737900, training_loss: 1.49956e+02
I1112 14:38:32.106579 140264174335808 run_lib.py:153] step: 737950, training_loss: 1.14153e+02
I1112 14:38:42.360054 140264174335808 run_lib.py:153] step: 738000, training_loss: 1.32088e+02
I1112 14:38:52.431167 140264174335808 run_lib.py:153] step: 738050, training_loss: 1.07664e+02
I1112 14:39:03.412514 140264174335808 run_lib.py:153] step: 738100, training_loss: 1.30375e+02
I1112 14:39:13.842130 140264174335808 run_lib.py:153] step: 738150, training_loss: 1.31581e+02
I1112 14:39:24.207723 140264174335808 run_lib.py:153] step: 738200, training_loss: 1.05718e+02
I1112 14:39:34.686590 140264174335808 run_lib.py:153] step: 738250, training_loss: 1.31219e+02
I1112 14:39:45.386700 140264174335808 run_lib.py:153] step: 738300, training_loss: 1.56759e+02
I1112 14:39:56.122338 140264174335808 run_lib.py:153] step: 738350, training_loss: 1.10432e+02
I1112 14:40:05.847095 140264174335808 run_lib.py:153] step: 738400, training_loss: 1.58877e+02
I1112 14:40:15.265522 140264174335808 run_lib.py:153] step: 738450, training_loss: 1.36642e+02
I1112 14:40:25.077474 140264174335808 run_lib.py:153] step: 738500, training_loss: 1.38455e+02
I1112 14:40:34.292116 140264174335808 run_lib.py:153] step: 738550, training_loss: 1.20689e+02
I1112 14:40:44.077101 140264174335808 run_lib.py:153] step: 738600, training_loss: 1.07343e+02
I1112 14:40:53.768294 140264174335808 run_lib.py:153] step: 738650, training_loss: 1.41862e+02
I1112 14:41:03.492839 140264174335808 run_lib.py:153] step: 738700, training_loss: 1.39576e+02
I1112 14:41:13.289712 140264174335808 run_lib.py:153] step: 738750, training_loss: 1.09730e+02
I1112 14:41:23.703500 140264174335808 run_lib.py:153] step: 738800, training_loss: 1.22232e+02
I1112 14:41:33.364047 140264174335808 run_lib.py:153] step: 738850, training_loss: 1.28720e+02
I1112 14:41:43.505980 140264174335808 run_lib.py:153] step: 738900, training_loss: 1.31507e+02
I1112 14:41:53.233393 140264174335808 run_lib.py:153] step: 738950, training_loss: 1.22149e+02
I1112 14:42:03.424346 140264174335808 run_lib.py:153] step: 739000, training_loss: 1.13960e+02
I1112 14:42:13.864149 140264174335808 run_lib.py:153] step: 739050, training_loss: 1.22402e+02
I1112 14:42:24.145946 140264174335808 run_lib.py:153] step: 739100, training_loss: 1.27307e+02
I1112 14:42:34.492046 140264174335808 run_lib.py:153] step: 739150, training_loss: 1.28790e+02
I1112 14:42:44.440272 140264174335808 run_lib.py:153] step: 739200, training_loss: 1.45753e+02
I1112 14:42:54.002264 140264174335808 run_lib.py:153] step: 739250, training_loss: 1.07491e+02
I1112 14:43:04.586172 140264174335808 run_lib.py:153] step: 739300, training_loss: 1.47207e+02
I1112 14:43:14.998954 140264174335808 run_lib.py:153] step: 739350, training_loss: 1.41367e+02
I1112 14:43:25.880426 140264174335808 run_lib.py:153] step: 739400, training_loss: 1.10463e+02
I1112 14:43:36.392615 140264174335808 run_lib.py:153] step: 739450, training_loss: 1.24754e+02
I1112 14:43:47.202752 140264174335808 run_lib.py:153] step: 739500, training_loss: 1.35461e+02
I1112 14:43:57.261648 140264174335808 run_lib.py:153] step: 739550, training_loss: 1.53242e+02
I1112 14:44:07.321963 140264174335808 run_lib.py:153] step: 739600, training_loss: 1.31094e+02
I1112 14:44:17.723007 140264174335808 run_lib.py:153] step: 739650, training_loss: 1.38102e+02
I1112 14:44:27.645352 140264174335808 run_lib.py:153] step: 739700, training_loss: 1.36095e+02
I1112 14:44:37.963483 140264174335808 run_lib.py:153] step: 739750, training_loss: 1.29858e+02
I1112 14:44:47.990647 140264174335808 run_lib.py:153] step: 739800, training_loss: 1.35069e+02
I1112 14:44:57.446645 140264174335808 run_lib.py:153] step: 739850, training_loss: 1.15851e+02
I1112 14:45:07.321393 140264174335808 run_lib.py:153] step: 739900, training_loss: 1.23216e+02
I1112 14:45:18.222222 140264174335808 run_lib.py:153] step: 739950, training_loss: 1.09254e+02
I1112 14:45:28.405592 140264174335808 run_lib.py:153] step: 740000, training_loss: 1.27813e+02
I1112 14:45:28.940886 140264174335808 run_lib.py:166] step: 740000, eval_loss: 1.02569e+02
I1112 14:45:38.636822 140264174335808 run_lib.py:153] step: 740050, training_loss: 8.90739e+01
I1112 14:45:48.306797 140264174335808 run_lib.py:153] step: 740100, training_loss: 1.05939e+02
I1112 14:45:58.277835 140264174335808 run_lib.py:153] step: 740150, training_loss: 1.41717e+02
I1112 14:46:08.103885 140264174335808 run_lib.py:153] step: 740200, training_loss: 1.35720e+02
I1112 14:46:18.535500 140264174335808 run_lib.py:153] step: 740250, training_loss: 1.47266e+02
I1112 14:46:28.520559 140264174335808 run_lib.py:153] step: 740300, training_loss: 1.13744e+02
I1112 14:46:38.025619 140264174335808 run_lib.py:153] step: 740350, training_loss: 1.15689e+02
I1112 14:46:48.032321 140264174335808 run_lib.py:153] step: 740400, training_loss: 1.32039e+02
I1112 14:46:58.374654 140264174335808 run_lib.py:153] step: 740450, training_loss: 1.71689e+02
I1112 14:47:08.324770 140264174335808 run_lib.py:153] step: 740500, training_loss: 1.14210e+02
I1112 14:47:18.912586 140264174335808 run_lib.py:153] step: 740550, training_loss: 1.05193e+02
I1112 14:47:28.557033 140264174335808 run_lib.py:153] step: 740600, training_loss: 1.34594e+02
I1112 14:47:38.452763 140264174335808 run_lib.py:153] step: 740650, training_loss: 1.19000e+02
I1112 14:47:48.874670 140264174335808 run_lib.py:153] step: 740700, training_loss: 1.40366e+02
I1112 14:47:59.496279 140264174335808 run_lib.py:153] step: 740750, training_loss: 1.27055e+02
I1112 14:48:09.493231 140264174335808 run_lib.py:153] step: 740800, training_loss: 1.05347e+02
I1112 14:48:19.021425 140264174335808 run_lib.py:153] step: 740850, training_loss: 9.92313e+01
I1112 14:48:28.824183 140264174335808 run_lib.py:153] step: 740900, training_loss: 1.14609e+02
I1112 14:48:40.145472 140264174335808 run_lib.py:153] step: 740950, training_loss: 1.09464e+02
I1112 14:48:50.451509 140264174335808 run_lib.py:153] step: 741000, training_loss: 1.37364e+02
I1112 14:49:00.420795 140264174335808 run_lib.py:153] step: 741050, training_loss: 1.39703e+02
I1112 14:49:10.869805 140264174335808 run_lib.py:153] step: 741100, training_loss: 1.14603e+02
I1112 14:49:21.281095 140264174335808 run_lib.py:153] step: 741150, training_loss: 1.38448e+02
I1112 14:49:32.240541 140264174335808 run_lib.py:153] step: 741200, training_loss: 9.95486e+01
I1112 14:49:42.026434 140264174335808 run_lib.py:153] step: 741250, training_loss: 1.17168e+02
I1112 14:49:52.371520 140264174335808 run_lib.py:153] step: 741300, training_loss: 1.31990e+02
I1112 14:50:03.465645 140264174335808 run_lib.py:153] step: 741350, training_loss: 1.41243e+02
I1112 14:50:13.811627 140264174335808 run_lib.py:153] step: 741400, training_loss: 1.24920e+02
I1112 14:50:24.053506 140264174335808 run_lib.py:153] step: 741450, training_loss: 1.35180e+02
I1112 14:50:33.718958 140264174335808 run_lib.py:153] step: 741500, training_loss: 1.37850e+02
I1112 14:50:43.411547 140264174335808 run_lib.py:153] step: 741550, training_loss: 1.20490e+02
I1112 14:50:53.598110 140264174335808 run_lib.py:153] step: 741600, training_loss: 1.29077e+02
I1112 14:51:03.698495 140264174335808 run_lib.py:153] step: 741650, training_loss: 1.45445e+02
I1112 14:51:13.595229 140264174335808 run_lib.py:153] step: 741700, training_loss: 1.39533e+02
I1112 14:51:23.847642 140264174335808 run_lib.py:153] step: 741750, training_loss: 1.37567e+02
I1112 14:51:33.973608 140264174335808 run_lib.py:153] step: 741800, training_loss: 1.27872e+02
I1112 14:51:44.052735 140264174335808 run_lib.py:153] step: 741850, training_loss: 1.26562e+02
I1112 14:51:54.052624 140264174335808 run_lib.py:153] step: 741900, training_loss: 1.31050e+02
I1112 14:52:05.248769 140264174335808 run_lib.py:153] step: 741950, training_loss: 1.00196e+02
I1112 14:52:15.834490 140264174335808 run_lib.py:153] step: 742000, training_loss: 1.32967e+02
I1112 14:52:26.203824 140264174335808 run_lib.py:153] step: 742050, training_loss: 1.47161e+02
I1112 14:52:36.293745 140264174335808 run_lib.py:153] step: 742100, training_loss: 1.18391e+02
I1112 14:52:46.988395 140264174335808 run_lib.py:153] step: 742150, training_loss: 1.10355e+02
I1112 14:52:56.637149 140264174335808 run_lib.py:153] step: 742200, training_loss: 1.36336e+02
I1112 14:53:06.361128 140264174335808 run_lib.py:153] step: 742250, training_loss: 1.46926e+02
I1112 14:53:16.621672 140264174335808 run_lib.py:153] step: 742300, training_loss: 1.08638e+02
I1112 14:53:26.386951 140264174335808 run_lib.py:153] step: 742350, training_loss: 1.21649e+02
I1112 14:53:36.022623 140264174335808 run_lib.py:153] step: 742400, training_loss: 1.38811e+02
I1112 14:53:46.753830 140264174335808 run_lib.py:153] step: 742450, training_loss: 1.27641e+02
I1112 14:53:56.718101 140264174335808 run_lib.py:153] step: 742500, training_loss: 1.09356e+02
I1112 14:54:07.177241 140264174335808 run_lib.py:153] step: 742550, training_loss: 1.50861e+02
I1112 14:54:18.087628 140264174335808 run_lib.py:153] step: 742600, training_loss: 1.20099e+02
I1112 14:54:28.222588 140264174335808 run_lib.py:153] step: 742650, training_loss: 1.68984e+02
I1112 14:54:37.634152 140264174335808 run_lib.py:153] step: 742700, training_loss: 1.34343e+02
I1112 14:54:47.684038 140264174335808 run_lib.py:153] step: 742750, training_loss: 1.21657e+02
I1112 14:54:57.401314 140264174335808 run_lib.py:153] step: 742800, training_loss: 1.62662e+02
I1112 14:55:07.039977 140264174335808 run_lib.py:153] step: 742850, training_loss: 1.23357e+02
I1112 14:55:17.459072 140264174335808 run_lib.py:153] step: 742900, training_loss: 1.26958e+02
I1112 14:55:28.018054 140264174335808 run_lib.py:153] step: 742950, training_loss: 1.34750e+02
I1112 14:55:38.010050 140264174335808 run_lib.py:153] step: 743000, training_loss: 1.15364e+02
I1112 14:55:47.532828 140264174335808 run_lib.py:153] step: 743050, training_loss: 1.25531e+02
I1112 14:55:57.704575 140264174335808 run_lib.py:153] step: 743100, training_loss: 1.03498e+02
I1112 14:56:08.010191 140264174335808 run_lib.py:153] step: 743150, training_loss: 1.34315e+02
I1112 14:56:18.129337 140264174335808 run_lib.py:153] step: 743200, training_loss: 1.19791e+02
I1112 14:56:27.970484 140264174335808 run_lib.py:153] step: 743250, training_loss: 1.04492e+02
I1112 14:56:38.518401 140264174335808 run_lib.py:153] step: 743300, training_loss: 1.60405e+02
I1112 14:56:48.582862 140264174335808 run_lib.py:153] step: 743350, training_loss: 1.10814e+02
I1112 14:56:59.111321 140264174335808 run_lib.py:153] step: 743400, training_loss: 1.37663e+02
I1112 14:57:08.942376 140264174335808 run_lib.py:153] step: 743450, training_loss: 1.23989e+02
I1112 14:57:19.034457 140264174335808 run_lib.py:153] step: 743500, training_loss: 9.86384e+01
I1112 14:57:28.883460 140264174335808 run_lib.py:153] step: 743550, training_loss: 1.05885e+02
I1112 14:57:39.058603 140264174335808 run_lib.py:153] step: 743600, training_loss: 1.32873e+02
I1112 14:57:48.956151 140264174335808 run_lib.py:153] step: 743650, training_loss: 1.32164e+02
I1112 14:57:58.725512 140264174335808 run_lib.py:153] step: 743700, training_loss: 1.35977e+02
I1112 14:58:09.200088 140264174335808 run_lib.py:153] step: 743750, training_loss: 1.26005e+02
I1112 14:58:19.561578 140264174335808 run_lib.py:153] step: 743800, training_loss: 1.48330e+02
I1112 14:58:29.741041 140264174335808 run_lib.py:153] step: 743850, training_loss: 1.21840e+02
I1112 14:58:40.063467 140264174335808 run_lib.py:153] step: 743900, training_loss: 1.12312e+02
I1112 14:58:50.511434 140264174335808 run_lib.py:153] step: 743950, training_loss: 1.25090e+02
I1112 14:59:00.669823 140264174335808 run_lib.py:153] step: 744000, training_loss: 9.69011e+01
I1112 14:59:10.366480 140264174335808 run_lib.py:153] step: 744050, training_loss: 1.63388e+02
I1112 14:59:20.918294 140264174335808 run_lib.py:153] step: 744100, training_loss: 1.27328e+02
I1112 14:59:31.287197 140264174335808 run_lib.py:153] step: 744150, training_loss: 1.12531e+02
I1112 14:59:40.755354 140264174335808 run_lib.py:153] step: 744200, training_loss: 1.47123e+02
I1112 14:59:50.668218 140264174335808 run_lib.py:153] step: 744250, training_loss: 1.37059e+02
I1112 15:00:01.015304 140264174335808 run_lib.py:153] step: 744300, training_loss: 9.70889e+01
I1112 15:00:10.529124 140264174335808 run_lib.py:153] step: 744350, training_loss: 1.19037e+02
I1112 15:00:20.721558 140264174335808 run_lib.py:153] step: 744400, training_loss: 1.30757e+02
I1112 15:00:30.670409 140264174335808 run_lib.py:153] step: 744450, training_loss: 1.63872e+02
I1112 15:00:41.237279 140264174335808 run_lib.py:153] step: 744500, training_loss: 1.13129e+02
I1112 15:00:51.512220 140264174335808 run_lib.py:153] step: 744550, training_loss: 1.27534e+02
I1112 15:01:02.362976 140264174335808 run_lib.py:153] step: 744600, training_loss: 1.32077e+02
I1112 15:01:12.789987 140264174335808 run_lib.py:153] step: 744650, training_loss: 1.09196e+02
I1112 15:01:23.242022 140264174335808 run_lib.py:153] step: 744700, training_loss: 1.19416e+02
I1112 15:01:33.811959 140264174335808 run_lib.py:153] step: 744750, training_loss: 1.54831e+02
I1112 15:01:43.659236 140264174335808 run_lib.py:153] step: 744800, training_loss: 1.37444e+02
I1112 15:01:53.734340 140264174335808 run_lib.py:153] step: 744850, training_loss: 1.14290e+02
I1112 15:02:04.311115 140264174335808 run_lib.py:153] step: 744900, training_loss: 1.26057e+02
I1112 15:02:14.599675 140264174335808 run_lib.py:153] step: 744950, training_loss: 1.31391e+02
I1112 15:02:24.889031 140264174335808 run_lib.py:153] step: 745000, training_loss: 1.28110e+02
I1112 15:02:25.001018 140264174335808 run_lib.py:166] step: 745000, eval_loss: 1.37553e+02
I1112 15:02:35.024774 140264174335808 run_lib.py:153] step: 745050, training_loss: 1.36838e+02
I1112 15:02:44.751729 140264174335808 run_lib.py:153] step: 745100, training_loss: 1.18656e+02
I1112 15:02:54.706183 140264174335808 run_lib.py:153] step: 745150, training_loss: 1.24136e+02
I1112 15:03:05.847783 140264174335808 run_lib.py:153] step: 745200, training_loss: 1.07916e+02
I1112 15:03:16.640601 140264174335808 run_lib.py:153] step: 745250, training_loss: 1.09823e+02
I1112 15:03:27.344502 140264174335808 run_lib.py:153] step: 745300, training_loss: 1.25495e+02
I1112 15:03:36.833023 140264174335808 run_lib.py:153] step: 745350, training_loss: 1.27810e+02
I1112 15:03:47.429049 140264174335808 run_lib.py:153] step: 745400, training_loss: 1.03056e+02
I1112 15:03:57.678323 140264174335808 run_lib.py:153] step: 745450, training_loss: 1.24329e+02
I1112 15:04:07.516842 140264174335808 run_lib.py:153] step: 745500, training_loss: 1.23210e+02
I1112 15:04:17.498278 140264174335808 run_lib.py:153] step: 745550, training_loss: 9.21998e+01
I1112 15:04:28.226009 140264174335808 run_lib.py:153] step: 745600, training_loss: 1.27207e+02
I1112 15:04:38.838268 140264174335808 run_lib.py:153] step: 745650, training_loss: 1.36176e+02
I1112 15:04:48.618469 140264174335808 run_lib.py:153] step: 745700, training_loss: 1.19811e+02
I1112 15:04:58.785297 140264174335808 run_lib.py:153] step: 745750, training_loss: 1.23913e+02
I1112 15:05:08.363866 140264174335808 run_lib.py:153] step: 745800, training_loss: 1.16017e+02
I1112 15:05:18.476812 140264174335808 run_lib.py:153] step: 745850, training_loss: 1.10211e+02
I1112 15:05:29.164145 140264174335808 run_lib.py:153] step: 745900, training_loss: 1.41988e+02
I1112 15:05:39.340062 140264174335808 run_lib.py:153] step: 745950, training_loss: 1.10009e+02
I1112 15:05:49.568138 140264174335808 run_lib.py:153] step: 746000, training_loss: 1.21305e+02
I1112 15:05:59.991245 140264174335808 run_lib.py:153] step: 746050, training_loss: 1.20650e+02
I1112 15:06:10.035132 140264174335808 run_lib.py:153] step: 746100, training_loss: 1.27539e+02
I1112 15:06:21.167168 140264174335808 run_lib.py:153] step: 746150, training_loss: 1.42311e+02
I1112 15:06:31.575717 140264174335808 run_lib.py:153] step: 746200, training_loss: 1.68468e+02
I1112 15:06:41.656518 140264174335808 run_lib.py:153] step: 746250, training_loss: 1.00721e+02
I1112 15:06:52.162528 140264174335808 run_lib.py:153] step: 746300, training_loss: 1.26539e+02
I1112 15:07:02.443472 140264174335808 run_lib.py:153] step: 746350, training_loss: 9.73983e+01
I1112 15:07:13.039953 140264174335808 run_lib.py:153] step: 746400, training_loss: 1.21273e+02
I1112 15:07:23.739488 140264174335808 run_lib.py:153] step: 746450, training_loss: 1.32287e+02
I1112 15:07:34.290203 140264174335808 run_lib.py:153] step: 746500, training_loss: 1.19522e+02
I1112 15:07:44.722121 140264174335808 run_lib.py:153] step: 746550, training_loss: 1.47166e+02
I1112 15:07:54.679940 140264174335808 run_lib.py:153] step: 746600, training_loss: 1.29923e+02
I1112 15:08:05.025802 140264174335808 run_lib.py:153] step: 746650, training_loss: 1.27177e+02
I1112 15:08:15.671785 140264174335808 run_lib.py:153] step: 746700, training_loss: 1.37448e+02
I1112 15:08:26.146786 140264174335808 run_lib.py:153] step: 746750, training_loss: 1.12630e+02
I1112 15:08:36.754537 140264174335808 run_lib.py:153] step: 746800, training_loss: 1.39496e+02
I1112 15:08:46.987563 140264174335808 run_lib.py:153] step: 746850, training_loss: 1.26861e+02
I1112 15:08:56.842231 140264174335808 run_lib.py:153] step: 746900, training_loss: 1.25928e+02
I1112 15:09:07.122250 140264174335808 run_lib.py:153] step: 746950, training_loss: 1.45692e+02
I1112 15:09:17.495431 140264174335808 run_lib.py:153] step: 747000, training_loss: 1.30515e+02
I1112 15:09:27.976365 140264174335808 run_lib.py:153] step: 747050, training_loss: 1.40426e+02
I1112 15:09:37.888006 140264174335808 run_lib.py:153] step: 747100, training_loss: 1.23512e+02
I1112 15:09:47.793503 140264174335808 run_lib.py:153] step: 747150, training_loss: 1.08075e+02
I1112 15:09:57.665399 140264174335808 run_lib.py:153] step: 747200, training_loss: 1.74434e+02
I1112 15:10:07.508339 140264174335808 run_lib.py:153] step: 747250, training_loss: 1.32776e+02
I1112 15:10:17.823671 140264174335808 run_lib.py:153] step: 747300, training_loss: 1.55441e+02
I1112 15:10:27.405567 140264174335808 run_lib.py:153] step: 747350, training_loss: 1.16564e+02
I1112 15:10:37.127228 140264174335808 run_lib.py:153] step: 747400, training_loss: 1.24667e+02
I1112 15:10:47.123674 140264174335808 run_lib.py:153] step: 747450, training_loss: 1.40937e+02
I1112 15:10:56.735517 140264174335808 run_lib.py:153] step: 747500, training_loss: 1.51490e+02
I1112 15:11:06.884483 140264174335808 run_lib.py:153] step: 747550, training_loss: 1.36829e+02
I1112 15:11:17.497971 140264174335808 run_lib.py:153] step: 747600, training_loss: 1.29278e+02
I1112 15:11:27.923384 140264174335808 run_lib.py:153] step: 747650, training_loss: 1.36710e+02
I1112 15:11:39.000756 140264174335808 run_lib.py:153] step: 747700, training_loss: 1.19985e+02
I1112 15:11:49.271731 140264174335808 run_lib.py:153] step: 747750, training_loss: 1.05852e+02
I1112 15:11:59.682307 140264174335808 run_lib.py:153] step: 747800, training_loss: 1.41411e+02
I1112 15:12:09.121033 140264174335808 run_lib.py:153] step: 747850, training_loss: 1.13344e+02
I1112 15:12:20.041884 140264174335808 run_lib.py:153] step: 747900, training_loss: 1.44685e+02
I1112 15:12:29.975009 140264174335808 run_lib.py:153] step: 747950, training_loss: 1.35511e+02
I1112 15:12:40.583885 140264174335808 run_lib.py:153] step: 748000, training_loss: 1.14120e+02
I1112 15:12:50.663693 140264174335808 run_lib.py:153] step: 748050, training_loss: 1.01634e+02
I1112 15:13:01.547092 140264174335808 run_lib.py:153] step: 748100, training_loss: 1.37001e+02
I1112 15:13:12.128493 140264174335808 run_lib.py:153] step: 748150, training_loss: 1.23893e+02
I1112 15:13:22.757449 140264174335808 run_lib.py:153] step: 748200, training_loss: 1.32913e+02
I1112 15:13:33.381372 140264174335808 run_lib.py:153] step: 748250, training_loss: 1.46292e+02
I1112 15:13:43.143938 140264174335808 run_lib.py:153] step: 748300, training_loss: 1.48202e+02
I1112 15:13:53.609641 140264174335808 run_lib.py:153] step: 748350, training_loss: 1.13195e+02
I1112 15:14:03.580854 140264174335808 run_lib.py:153] step: 748400, training_loss: 1.13097e+02
I1112 15:14:13.866152 140264174335808 run_lib.py:153] step: 748450, training_loss: 1.14663e+02
I1112 15:14:23.705291 140264174335808 run_lib.py:153] step: 748500, training_loss: 1.22523e+02
I1112 15:14:33.565771 140264174335808 run_lib.py:153] step: 748550, training_loss: 1.13645e+02
I1112 15:14:43.732987 140264174335808 run_lib.py:153] step: 748600, training_loss: 1.17057e+02
I1112 15:14:53.973758 140264174335808 run_lib.py:153] step: 748650, training_loss: 1.25340e+02
I1112 15:15:03.460076 140264174335808 run_lib.py:153] step: 748700, training_loss: 1.40688e+02
I1112 15:15:13.215995 140264174335808 run_lib.py:153] step: 748750, training_loss: 1.25444e+02
I1112 15:15:23.290929 140264174335808 run_lib.py:153] step: 748800, training_loss: 1.18725e+02
I1112 15:15:33.193778 140264174335808 run_lib.py:153] step: 748850, training_loss: 9.29957e+01
I1112 15:15:43.168969 140264174335808 run_lib.py:153] step: 748900, training_loss: 1.19784e+02
I1112 15:15:52.954377 140264174335808 run_lib.py:153] step: 748950, training_loss: 1.29525e+02
I1112 15:16:02.295219 140264174335808 run_lib.py:153] step: 749000, training_loss: 1.21055e+02
I1112 15:16:12.191143 140264174335808 run_lib.py:153] step: 749050, training_loss: 1.49555e+02
I1112 15:16:22.768854 140264174335808 run_lib.py:153] step: 749100, training_loss: 1.22433e+02
I1112 15:16:32.368036 140264174335808 run_lib.py:153] step: 749150, training_loss: 1.31729e+02
I1112 15:16:41.768845 140264174335808 run_lib.py:153] step: 749200, training_loss: 1.28327e+02
I1112 15:16:52.166932 140264174335808 run_lib.py:153] step: 749250, training_loss: 1.20237e+02
I1112 15:17:01.560724 140264174335808 run_lib.py:153] step: 749300, training_loss: 1.56443e+02
I1112 15:17:11.381175 140264174335808 run_lib.py:153] step: 749350, training_loss: 1.37136e+02
I1112 15:17:20.813943 140264174335808 run_lib.py:153] step: 749400, training_loss: 1.65892e+02
I1112 15:17:30.644598 140264174335808 run_lib.py:153] step: 749450, training_loss: 1.38324e+02
I1112 15:17:40.731426 140264174335808 run_lib.py:153] step: 749500, training_loss: 1.17677e+02
I1112 15:17:50.260569 140264174335808 run_lib.py:153] step: 749550, training_loss: 1.59750e+02
I1112 15:17:59.984566 140264174335808 run_lib.py:153] step: 749600, training_loss: 1.30525e+02
I1112 15:18:09.963832 140264174335808 run_lib.py:153] step: 749650, training_loss: 1.13224e+02
I1112 15:18:19.790758 140264174335808 run_lib.py:153] step: 749700, training_loss: 1.27378e+02
I1112 15:18:29.548384 140264174335808 run_lib.py:153] step: 749750, training_loss: 1.23208e+02
I1112 15:18:39.336879 140264174335808 run_lib.py:153] step: 749800, training_loss: 1.05818e+02
I1112 15:18:49.499331 140264174335808 run_lib.py:153] step: 749850, training_loss: 9.66431e+01
I1112 15:18:59.138085 140264174335808 run_lib.py:153] step: 749900, training_loss: 1.49439e+02
I1112 15:19:09.132527 140264174335808 run_lib.py:153] step: 749950, training_loss: 8.71540e+01
I1112 15:19:19.536146 140264174335808 run_lib.py:153] step: 750000, training_loss: 1.25617e+02
I1112 15:19:20.082123 140264174335808 run_lib.py:166] step: 750000, eval_loss: 1.09324e+02
I1112 15:19:30.644569 140264174335808 run_lib.py:153] step: 750050, training_loss: 1.19520e+02
I1112 15:19:41.153233 140264174335808 run_lib.py:153] step: 750100, training_loss: 1.26302e+02
I1112 15:19:50.949006 140264174335808 run_lib.py:153] step: 750150, training_loss: 1.21710e+02
I1112 15:20:00.652301 140264174335808 run_lib.py:153] step: 750200, training_loss: 1.41772e+02
I1112 15:20:10.363701 140264174335808 run_lib.py:153] step: 750250, training_loss: 1.28287e+02
I1112 15:20:20.065194 140264174335808 run_lib.py:153] step: 750300, training_loss: 1.20890e+02
I1112 15:20:29.300725 140264174335808 run_lib.py:153] step: 750350, training_loss: 1.12933e+02
I1112 15:20:39.344144 140264174335808 run_lib.py:153] step: 750400, training_loss: 1.18439e+02
I1112 15:20:49.112148 140264174335808 run_lib.py:153] step: 750450, training_loss: 1.44822e+02
I1112 15:20:59.099706 140264174335808 run_lib.py:153] step: 750500, training_loss: 1.31581e+02
I1112 15:21:08.476212 140264174335808 run_lib.py:153] step: 750550, training_loss: 1.34616e+02
I1112 15:21:18.259912 140264174335808 run_lib.py:153] step: 750600, training_loss: 1.25565e+02
I1112 15:21:28.658053 140264174335808 run_lib.py:153] step: 750650, training_loss: 1.57537e+02
I1112 15:21:38.489792 140264174335808 run_lib.py:153] step: 750700, training_loss: 1.03195e+02
I1112 15:21:47.782656 140264174335808 run_lib.py:153] step: 750750, training_loss: 1.46291e+02
I1112 15:21:57.065147 140264174335808 run_lib.py:153] step: 750800, training_loss: 1.33690e+02
I1112 15:22:06.744750 140264174335808 run_lib.py:153] step: 750850, training_loss: 1.21749e+02
I1112 15:22:16.803264 140264174335808 run_lib.py:153] step: 750900, training_loss: 1.36965e+02
I1112 15:22:26.409346 140264174335808 run_lib.py:153] step: 750950, training_loss: 1.41225e+02
I1112 15:22:36.082217 140264174335808 run_lib.py:153] step: 751000, training_loss: 1.22146e+02
I1112 15:22:45.791473 140264174335808 run_lib.py:153] step: 751050, training_loss: 1.33527e+02
I1112 15:22:56.061580 140264174335808 run_lib.py:153] step: 751100, training_loss: 1.21343e+02
I1112 15:23:05.631843 140264174335808 run_lib.py:153] step: 751150, training_loss: 1.22398e+02
I1112 15:23:15.283692 140264174335808 run_lib.py:153] step: 751200, training_loss: 1.49853e+02
I1112 15:23:25.778777 140264174335808 run_lib.py:153] step: 751250, training_loss: 1.16591e+02
I1112 15:23:35.796215 140264174335808 run_lib.py:153] step: 751300, training_loss: 1.17220e+02
I1112 15:23:45.117202 140264174335808 run_lib.py:153] step: 751350, training_loss: 1.20293e+02
I1112 15:23:54.813069 140264174335808 run_lib.py:153] step: 751400, training_loss: 1.27021e+02
I1112 15:24:04.529170 140264174335808 run_lib.py:153] step: 751450, training_loss: 1.20170e+02
I1112 15:24:14.232534 140264174335808 run_lib.py:153] step: 751500, training_loss: 1.31699e+02
I1112 15:24:23.704117 140264174335808 run_lib.py:153] step: 751550, training_loss: 1.20989e+02
I1112 15:24:33.521836 140264174335808 run_lib.py:153] step: 751600, training_loss: 1.19930e+02
I1112 15:24:42.985093 140264174335808 run_lib.py:153] step: 751650, training_loss: 1.19506e+02
I1112 15:24:52.652563 140264174335808 run_lib.py:153] step: 751700, training_loss: 1.00675e+02
I1112 15:25:02.065836 140264174335808 run_lib.py:153] step: 751750, training_loss: 1.02805e+02
I1112 15:25:12.465398 140264174335808 run_lib.py:153] step: 751800, training_loss: 1.11387e+02
I1112 15:25:22.284694 140264174335808 run_lib.py:153] step: 751850, training_loss: 1.13193e+02
I1112 15:25:32.400196 140264174335808 run_lib.py:153] step: 751900, training_loss: 1.36147e+02
I1112 15:25:42.217723 140264174335808 run_lib.py:153] step: 751950, training_loss: 1.09022e+02
I1112 15:25:52.316626 140264174335808 run_lib.py:153] step: 752000, training_loss: 1.38606e+02
I1112 15:26:01.783182 140264174335808 run_lib.py:153] step: 752050, training_loss: 1.39131e+02
I1112 15:26:11.107675 140264174335808 run_lib.py:153] step: 752100, training_loss: 1.26889e+02
I1112 15:26:20.594034 140264174335808 run_lib.py:153] step: 752150, training_loss: 8.41607e+01
I1112 15:26:30.213139 140264174335808 run_lib.py:153] step: 752200, training_loss: 1.49702e+02
I1112 15:26:40.054574 140264174335808 run_lib.py:153] step: 752250, training_loss: 1.21028e+02
I1112 15:26:50.232512 140264174335808 run_lib.py:153] step: 752300, training_loss: 1.03710e+02
I1112 15:27:01.120524 140264174335808 run_lib.py:153] step: 752350, training_loss: 1.34507e+02
I1112 15:27:11.478263 140264174335808 run_lib.py:153] step: 752400, training_loss: 1.06552e+02
I1112 15:27:21.979391 140264174335808 run_lib.py:153] step: 752450, training_loss: 1.19537e+02
I1112 15:27:32.172822 140264174335808 run_lib.py:153] step: 752500, training_loss: 1.37370e+02
I1112 15:27:41.661986 140264174335808 run_lib.py:153] step: 752550, training_loss: 1.46361e+02
I1112 15:27:52.709448 140264174335808 run_lib.py:153] step: 752600, training_loss: 1.49337e+02
I1112 15:28:02.674745 140264174335808 run_lib.py:153] step: 752650, training_loss: 1.31146e+02
I1112 15:28:12.768467 140264174335808 run_lib.py:153] step: 752700, training_loss: 1.08746e+02
I1112 15:28:22.563406 140264174335808 run_lib.py:153] step: 752750, training_loss: 1.38826e+02
I1112 15:28:33.120583 140264174335808 run_lib.py:153] step: 752800, training_loss: 1.34615e+02
I1112 15:28:43.139231 140264174335808 run_lib.py:153] step: 752850, training_loss: 1.32478e+02
I1112 15:28:53.256391 140264174335808 run_lib.py:153] step: 752900, training_loss: 1.41530e+02
I1112 15:29:03.731050 140264174335808 run_lib.py:153] step: 752950, training_loss: 1.19143e+02
I1112 15:29:14.246309 140264174335808 run_lib.py:153] step: 753000, training_loss: 1.33777e+02
I1112 15:29:24.874470 140264174335808 run_lib.py:153] step: 753050, training_loss: 1.22047e+02
I1112 15:29:35.311812 140264174335808 run_lib.py:153] step: 753100, training_loss: 1.20995e+02
I1112 15:29:46.183411 140264174335808 run_lib.py:153] step: 753150, training_loss: 1.32573e+02
I1112 15:29:55.455623 140264174335808 run_lib.py:153] step: 753200, training_loss: 1.07432e+02
I1112 15:30:04.891926 140264174335808 run_lib.py:153] step: 753250, training_loss: 1.52508e+02
I1112 15:30:14.410294 140264174335808 run_lib.py:153] step: 753300, training_loss: 1.04417e+02
I1112 15:30:24.080872 140264174335808 run_lib.py:153] step: 753350, training_loss: 1.29690e+02
I1112 15:30:33.873276 140264174335808 run_lib.py:153] step: 753400, training_loss: 1.58783e+02
I1112 15:30:44.071442 140264174335808 run_lib.py:153] step: 753450, training_loss: 1.29991e+02
I1112 15:30:53.711613 140264174335808 run_lib.py:153] step: 753500, training_loss: 1.07747e+02
I1112 15:31:03.402410 140264174335808 run_lib.py:153] step: 753550, training_loss: 1.18924e+02
I1112 15:31:13.073981 140264174335808 run_lib.py:153] step: 753600, training_loss: 1.33136e+02
I1112 15:31:23.185882 140264174335808 run_lib.py:153] step: 753650, training_loss: 1.48148e+02
I1112 15:31:33.130825 140264174335808 run_lib.py:153] step: 753700, training_loss: 1.24017e+02
I1112 15:31:43.072241 140264174335808 run_lib.py:153] step: 753750, training_loss: 1.33523e+02
I1112 15:31:53.378173 140264174335808 run_lib.py:153] step: 753800, training_loss: 1.40208e+02
I1112 15:32:02.945172 140264174335808 run_lib.py:153] step: 753850, training_loss: 1.28418e+02
I1112 15:32:12.934019 140264174335808 run_lib.py:153] step: 753900, training_loss: 1.28621e+02
I1112 15:32:22.897833 140264174335808 run_lib.py:153] step: 753950, training_loss: 1.10524e+02
I1112 15:32:32.316731 140264174335808 run_lib.py:153] step: 754000, training_loss: 1.20916e+02
I1112 15:32:42.045919 140264174335808 run_lib.py:153] step: 754050, training_loss: 1.64735e+02
I1112 15:32:51.770967 140264174335808 run_lib.py:153] step: 754100, training_loss: 1.15540e+02
I1112 15:33:01.621324 140264174335808 run_lib.py:153] step: 754150, training_loss: 1.21078e+02
I1112 15:33:11.834404 140264174335808 run_lib.py:153] step: 754200, training_loss: 9.48551e+01
I1112 15:33:21.116304 140264174335808 run_lib.py:153] step: 754250, training_loss: 1.22061e+02
I1112 15:33:31.525364 140264174335808 run_lib.py:153] step: 754300, training_loss: 1.14953e+02
I1112 15:33:41.639491 140264174335808 run_lib.py:153] step: 754350, training_loss: 1.15270e+02
I1112 15:33:52.007571 140264174335808 run_lib.py:153] step: 754400, training_loss: 1.06593e+02
I1112 15:34:02.072041 140264174335808 run_lib.py:153] step: 754450, training_loss: 1.15935e+02
I1112 15:34:12.163396 140264174335808 run_lib.py:153] step: 754500, training_loss: 1.06810e+02
I1112 15:34:22.701795 140264174335808 run_lib.py:153] step: 754550, training_loss: 1.31803e+02
I1112 15:34:32.360592 140264174335808 run_lib.py:153] step: 754600, training_loss: 1.31136e+02
I1112 15:34:42.662711 140264174335808 run_lib.py:153] step: 754650, training_loss: 1.11753e+02
I1112 15:34:53.112858 140264174335808 run_lib.py:153] step: 754700, training_loss: 1.26789e+02
I1112 15:35:03.327083 140264174335808 run_lib.py:153] step: 754750, training_loss: 1.30191e+02
I1112 15:35:13.346294 140264174335808 run_lib.py:153] step: 754800, training_loss: 9.80228e+01
I1112 15:35:23.775980 140264174335808 run_lib.py:153] step: 754850, training_loss: 1.38243e+02
I1112 15:35:33.285777 140264174335808 run_lib.py:153] step: 754900, training_loss: 1.22075e+02
I1112 15:35:43.190509 140264174335808 run_lib.py:153] step: 754950, training_loss: 1.16951e+02
I1112 15:35:53.143833 140264174335808 run_lib.py:153] step: 755000, training_loss: 1.53166e+02
I1112 15:35:53.279060 140264174335808 run_lib.py:166] step: 755000, eval_loss: 1.33972e+02
I1112 15:36:03.902740 140264174335808 run_lib.py:153] step: 755050, training_loss: 1.03284e+02
I1112 15:36:13.974694 140264174335808 run_lib.py:153] step: 755100, training_loss: 1.15784e+02
I1112 15:36:24.021556 140264174335808 run_lib.py:153] step: 755150, training_loss: 1.28827e+02
I1112 15:36:34.365599 140264174335808 run_lib.py:153] step: 755200, training_loss: 1.34171e+02
I1112 15:36:44.835840 140264174335808 run_lib.py:153] step: 755250, training_loss: 1.43019e+02
I1112 15:36:55.069299 140264174335808 run_lib.py:153] step: 755300, training_loss: 1.20753e+02
I1112 15:37:05.083035 140264174335808 run_lib.py:153] step: 755350, training_loss: 1.26046e+02
I1112 15:37:15.020651 140264174335808 run_lib.py:153] step: 755400, training_loss: 1.26233e+02
I1112 15:37:25.676166 140264174335808 run_lib.py:153] step: 755450, training_loss: 1.27649e+02
I1112 15:37:35.728861 140264174335808 run_lib.py:153] step: 755500, training_loss: 1.85277e+02
I1112 15:37:45.737332 140264174335808 run_lib.py:153] step: 755550, training_loss: 1.27452e+02
I1112 15:37:56.268945 140264174335808 run_lib.py:153] step: 755600, training_loss: 1.24335e+02
I1112 15:38:06.954439 140264174335808 run_lib.py:153] step: 755650, training_loss: 1.14468e+02
I1112 15:38:17.003524 140264174335808 run_lib.py:153] step: 755700, training_loss: 1.24598e+02
I1112 15:38:27.217918 140264174335808 run_lib.py:153] step: 755750, training_loss: 1.06221e+02
I1112 15:38:37.553542 140264174335808 run_lib.py:153] step: 755800, training_loss: 1.30536e+02
I1112 15:38:48.322532 140264174335808 run_lib.py:153] step: 755850, training_loss: 1.47258e+02
I1112 15:38:58.488009 140264174335808 run_lib.py:153] step: 755900, training_loss: 1.27354e+02
I1112 15:39:09.435226 140264174335808 run_lib.py:153] step: 755950, training_loss: 1.25825e+02
I1112 15:39:20.098346 140264174335808 run_lib.py:153] step: 756000, training_loss: 1.25799e+02
I1112 15:39:30.723379 140264174335808 run_lib.py:153] step: 756050, training_loss: 1.60112e+02
I1112 15:39:41.265393 140264174335808 run_lib.py:153] step: 756100, training_loss: 9.99946e+01
I1112 15:39:50.789155 140264174335808 run_lib.py:153] step: 756150, training_loss: 1.47289e+02
I1112 15:40:00.822684 140264174335808 run_lib.py:153] step: 756200, training_loss: 1.08794e+02
I1112 15:40:11.474627 140264174335808 run_lib.py:153] step: 756250, training_loss: 1.31083e+02
I1112 15:40:21.057929 140264174335808 run_lib.py:153] step: 756300, training_loss: 1.09950e+02
I1112 15:40:31.144000 140264174335808 run_lib.py:153] step: 756350, training_loss: 1.16455e+02
I1112 15:40:41.512444 140264174335808 run_lib.py:153] step: 756400, training_loss: 1.00514e+02
I1112 15:40:51.582522 140264174335808 run_lib.py:153] step: 756450, training_loss: 1.70616e+02
I1112 15:41:01.588012 140264174335808 run_lib.py:153] step: 756500, training_loss: 1.26873e+02
I1112 15:41:11.433121 140264174335808 run_lib.py:153] step: 756550, training_loss: 1.30257e+02
I1112 15:41:21.795923 140264174335808 run_lib.py:153] step: 756600, training_loss: 1.34309e+02
I1112 15:41:31.636091 140264174335808 run_lib.py:153] step: 756650, training_loss: 1.49158e+02
I1112 15:41:42.039676 140264174335808 run_lib.py:153] step: 756700, training_loss: 1.38396e+02
I1112 15:41:52.455207 140264174335808 run_lib.py:153] step: 756750, training_loss: 1.26230e+02
I1112 15:42:03.019786 140264174335808 run_lib.py:153] step: 756800, training_loss: 1.27996e+02
I1112 15:42:13.757317 140264174335808 run_lib.py:153] step: 756850, training_loss: 1.23417e+02
I1112 15:42:23.515702 140264174335808 run_lib.py:153] step: 756900, training_loss: 1.08118e+02
I1112 15:42:33.290075 140264174335808 run_lib.py:153] step: 756950, training_loss: 1.25575e+02
I1112 15:42:43.087913 140264174335808 run_lib.py:153] step: 757000, training_loss: 1.45637e+02
I1112 15:42:52.735973 140264174335808 run_lib.py:153] step: 757050, training_loss: 1.36101e+02
I1112 15:43:02.642186 140264174335808 run_lib.py:153] step: 757100, training_loss: 1.27159e+02
I1112 15:43:12.562131 140264174335808 run_lib.py:153] step: 757150, training_loss: 1.21369e+02
I1112 15:43:22.269553 140264174335808 run_lib.py:153] step: 757200, training_loss: 1.36259e+02
I1112 15:43:32.237390 140264174335808 run_lib.py:153] step: 757250, training_loss: 1.18657e+02
I1112 15:43:41.672339 140264174335808 run_lib.py:153] step: 757300, training_loss: 1.13267e+02
I1112 15:43:51.465678 140264174335808 run_lib.py:153] step: 757350, training_loss: 1.22564e+02
I1112 15:44:00.942130 140264174335808 run_lib.py:153] step: 757400, training_loss: 1.41624e+02
I1112 15:44:10.400264 140264174335808 run_lib.py:153] step: 757450, training_loss: 1.15069e+02
I1112 15:44:20.098286 140264174335808 run_lib.py:153] step: 757500, training_loss: 1.62852e+02
I1112 15:44:29.664515 140264174335808 run_lib.py:153] step: 757550, training_loss: 1.01966e+02
I1112 15:44:39.691470 140264174335808 run_lib.py:153] step: 757600, training_loss: 1.18233e+02
I1112 15:44:48.995095 140264174335808 run_lib.py:153] step: 757650, training_loss: 1.44078e+02
I1112 15:44:58.913317 140264174335808 run_lib.py:153] step: 757700, training_loss: 1.14175e+02
I1112 15:45:09.342849 140264174335808 run_lib.py:153] step: 757750, training_loss: 1.40794e+02
I1112 15:45:19.752311 140264174335808 run_lib.py:153] step: 757800, training_loss: 1.37522e+02
I1112 15:45:30.154146 140264174335808 run_lib.py:153] step: 757850, training_loss: 1.23718e+02
I1112 15:45:40.565672 140264174335808 run_lib.py:153] step: 757900, training_loss: 1.21269e+02
I1112 15:45:51.675462 140264174335808 run_lib.py:153] step: 757950, training_loss: 1.55906e+02
I1112 15:46:02.196167 140264174335808 run_lib.py:153] step: 758000, training_loss: 1.26512e+02
I1112 15:46:12.052131 140264174335808 run_lib.py:153] step: 758050, training_loss: 1.23881e+02
I1112 15:46:22.582167 140264174335808 run_lib.py:153] step: 758100, training_loss: 1.28113e+02
I1112 15:46:32.509041 140264174335808 run_lib.py:153] step: 758150, training_loss: 1.54127e+02
I1112 15:46:42.343436 140264174335808 run_lib.py:153] step: 758200, training_loss: 1.12356e+02
I1112 15:46:52.601062 140264174335808 run_lib.py:153] step: 758250, training_loss: 1.61524e+02
I1112 15:47:02.502337 140264174335808 run_lib.py:153] step: 758300, training_loss: 1.34000e+02
I1112 15:47:13.435006 140264174335808 run_lib.py:153] step: 758350, training_loss: 1.12920e+02
I1112 15:47:23.757624 140264174335808 run_lib.py:153] step: 758400, training_loss: 1.08895e+02
I1112 15:47:33.790623 140264174335808 run_lib.py:153] step: 758450, training_loss: 1.19871e+02
I1112 15:47:43.445897 140264174335808 run_lib.py:153] step: 758500, training_loss: 9.58672e+01
I1112 15:47:54.052521 140264174335808 run_lib.py:153] step: 758550, training_loss: 1.39510e+02
I1112 15:48:04.002458 140264174335808 run_lib.py:153] step: 758600, training_loss: 1.78708e+02
I1112 15:48:13.769726 140264174335808 run_lib.py:153] step: 758650, training_loss: 1.04696e+02
I1112 15:48:23.515099 140264174335808 run_lib.py:153] step: 758700, training_loss: 1.34784e+02
I1112 15:48:33.907768 140264174335808 run_lib.py:153] step: 758750, training_loss: 1.06744e+02
I1112 15:48:43.824648 140264174335808 run_lib.py:153] step: 758800, training_loss: 1.11702e+02
I1112 15:48:54.301565 140264174335808 run_lib.py:153] step: 758850, training_loss: 1.06933e+02
I1112 15:49:04.625027 140264174335808 run_lib.py:153] step: 758900, training_loss: 1.08699e+02
I1112 15:49:14.889880 140264174335808 run_lib.py:153] step: 758950, training_loss: 1.26651e+02
I1112 15:49:25.101816 140264174335808 run_lib.py:153] step: 759000, training_loss: 1.29279e+02
I1112 15:49:35.065160 140264174335808 run_lib.py:153] step: 759050, training_loss: 1.16272e+02
I1112 15:49:45.358284 140264174335808 run_lib.py:153] step: 759100, training_loss: 1.19311e+02
I1112 15:49:55.669776 140264174335808 run_lib.py:153] step: 759150, training_loss: 1.11176e+02
I1112 15:50:06.717439 140264174335808 run_lib.py:153] step: 759200, training_loss: 1.46264e+02
I1112 15:50:17.030478 140264174335808 run_lib.py:153] step: 759250, training_loss: 1.12832e+02
I1112 15:50:27.077759 140264174335808 run_lib.py:153] step: 759300, training_loss: 1.37756e+02
I1112 15:50:37.140620 140264174335808 run_lib.py:153] step: 759350, training_loss: 1.08982e+02
I1112 15:50:46.838428 140264174335808 run_lib.py:153] step: 759400, training_loss: 1.04545e+02
I1112 15:50:56.746162 140264174335808 run_lib.py:153] step: 759450, training_loss: 9.39975e+01
I1112 15:51:06.521795 140264174335808 run_lib.py:153] step: 759500, training_loss: 1.13402e+02
I1112 15:51:16.504693 140264174335808 run_lib.py:153] step: 759550, training_loss: 1.46975e+02
I1112 15:51:26.103898 140264174335808 run_lib.py:153] step: 759600, training_loss: 1.22768e+02
I1112 15:51:35.445122 140264174335808 run_lib.py:153] step: 759650, training_loss: 1.24125e+02
I1112 15:51:45.223387 140264174335808 run_lib.py:153] step: 759700, training_loss: 1.25155e+02
I1112 15:51:55.718651 140264174335808 run_lib.py:153] step: 759750, training_loss: 1.32367e+02
I1112 15:52:05.407382 140264174335808 run_lib.py:153] step: 759800, training_loss: 1.31290e+02
I1112 15:52:15.141632 140264174335808 run_lib.py:153] step: 759850, training_loss: 1.13191e+02
I1112 15:52:25.105404 140264174335808 run_lib.py:153] step: 759900, training_loss: 1.07542e+02
I1112 15:52:35.665244 140264174335808 run_lib.py:153] step: 759950, training_loss: 1.15134e+02
I1112 15:52:45.479685 140264174335808 run_lib.py:153] step: 760000, training_loss: 1.38480e+02
I1112 15:52:46.003896 140264174335808 run_lib.py:166] step: 760000, eval_loss: 1.11727e+02
I1112 15:52:56.774544 140264174335808 run_lib.py:153] step: 760050, training_loss: 1.17788e+02
I1112 15:53:07.475392 140264174335808 run_lib.py:153] step: 760100, training_loss: 1.38924e+02
I1112 15:53:18.081712 140264174335808 run_lib.py:153] step: 760150, training_loss: 1.15735e+02
I1112 15:53:29.157425 140264174335808 run_lib.py:153] step: 760200, training_loss: 1.13622e+02
I1112 15:53:39.263733 140264174335808 run_lib.py:153] step: 760250, training_loss: 1.08896e+02
I1112 15:53:48.720587 140264174335808 run_lib.py:153] step: 760300, training_loss: 9.98898e+01
I1112 15:53:58.028520 140264174335808 run_lib.py:153] step: 760350, training_loss: 1.33570e+02
I1112 15:54:08.049942 140264174335808 run_lib.py:153] step: 760400, training_loss: 1.17300e+02
I1112 15:54:18.653258 140264174335808 run_lib.py:153] step: 760450, training_loss: 1.24643e+02
I1112 15:54:28.609183 140264174335808 run_lib.py:153] step: 760500, training_loss: 1.12898e+02
I1112 15:54:39.017971 140264174335808 run_lib.py:153] step: 760550, training_loss: 1.29026e+02
I1112 15:54:49.286483 140264174335808 run_lib.py:153] step: 760600, training_loss: 1.38789e+02
I1112 15:54:58.784380 140264174335808 run_lib.py:153] step: 760650, training_loss: 1.38992e+02
I1112 15:55:09.011479 140264174335808 run_lib.py:153] step: 760700, training_loss: 9.39479e+01
I1112 15:55:19.848326 140264174335808 run_lib.py:153] step: 760750, training_loss: 1.30309e+02
I1112 15:55:30.759998 140264174335808 run_lib.py:153] step: 760800, training_loss: 1.17458e+02
I1112 15:55:40.684620 140264174335808 run_lib.py:153] step: 760850, training_loss: 1.35524e+02
I1112 15:55:50.953229 140264174335808 run_lib.py:153] step: 760900, training_loss: 1.19615e+02
I1112 15:56:00.417261 140264174335808 run_lib.py:153] step: 760950, training_loss: 1.06791e+02
I1112 15:56:10.092157 140264174335808 run_lib.py:153] step: 761000, training_loss: 1.12227e+02
I1112 15:56:19.611374 140264174335808 run_lib.py:153] step: 761050, training_loss: 1.23519e+02
I1112 15:56:29.133696 140264174335808 run_lib.py:153] step: 761100, training_loss: 1.45104e+02
I1112 15:56:39.105615 140264174335808 run_lib.py:153] step: 761150, training_loss: 1.09352e+02
I1112 15:56:49.132689 140264174335808 run_lib.py:153] step: 761200, training_loss: 1.28015e+02
I1112 15:56:58.597083 140264174335808 run_lib.py:153] step: 761250, training_loss: 1.37119e+02
I1112 15:57:08.054183 140264174335808 run_lib.py:153] step: 761300, training_loss: 1.30212e+02
I1112 15:57:18.053731 140264174335808 run_lib.py:153] step: 761350, training_loss: 1.27164e+02
I1112 15:57:27.652273 140264174335808 run_lib.py:153] step: 761400, training_loss: 1.19677e+02
I1112 15:57:38.277312 140264174335808 run_lib.py:153] step: 761450, training_loss: 9.70703e+01
I1112 15:57:48.340782 140264174335808 run_lib.py:153] step: 761500, training_loss: 9.83323e+01
I1112 15:57:58.308577 140264174335808 run_lib.py:153] step: 761550, training_loss: 1.69243e+02
I1112 15:58:07.593212 140264174335808 run_lib.py:153] step: 761600, training_loss: 1.36863e+02
I1112 15:58:17.915354 140264174335808 run_lib.py:153] step: 761650, training_loss: 1.14294e+02
I1112 15:58:27.484871 140264174335808 run_lib.py:153] step: 761700, training_loss: 1.06110e+02
I1112 15:58:37.103047 140264174335808 run_lib.py:153] step: 761750, training_loss: 1.08844e+02
I1112 15:58:47.441193 140264174335808 run_lib.py:153] step: 761800, training_loss: 1.31541e+02
I1112 15:58:58.066501 140264174335808 run_lib.py:153] step: 761850, training_loss: 1.35698e+02
I1112 15:59:07.421067 140264174335808 run_lib.py:153] step: 761900, training_loss: 9.56644e+01
I1112 15:59:17.624098 140264174335808 run_lib.py:153] step: 761950, training_loss: 1.14501e+02
I1112 15:59:26.944502 140264174335808 run_lib.py:153] step: 762000, training_loss: 1.35728e+02
I1112 15:59:36.929970 140264174335808 run_lib.py:153] step: 762050, training_loss: 1.37357e+02
I1112 15:59:46.567918 140264174335808 run_lib.py:153] step: 762100, training_loss: 1.18004e+02
I1112 15:59:57.503173 140264174335808 run_lib.py:153] step: 762150, training_loss: 1.28109e+02
I1112 16:00:08.076134 140264174335808 run_lib.py:153] step: 762200, training_loss: 1.27024e+02
I1112 16:00:18.488635 140264174335808 run_lib.py:153] step: 762250, training_loss: 1.35507e+02
I1112 16:00:28.471485 140264174335808 run_lib.py:153] step: 762300, training_loss: 1.14790e+02
I1112 16:00:37.924038 140264174335808 run_lib.py:153] step: 762350, training_loss: 9.48999e+01
I1112 16:00:47.909503 140264174335808 run_lib.py:153] step: 762400, training_loss: 1.21699e+02
I1112 16:00:57.968117 140264174335808 run_lib.py:153] step: 762450, training_loss: 1.19813e+02
I1112 16:01:07.784903 140264174335808 run_lib.py:153] step: 762500, training_loss: 1.21276e+02
I1112 16:01:17.536958 140264174335808 run_lib.py:153] step: 762550, training_loss: 1.22606e+02
I1112 16:01:26.800832 140264174335808 run_lib.py:153] step: 762600, training_loss: 1.28476e+02
I1112 16:01:37.084485 140264174335808 run_lib.py:153] step: 762650, training_loss: 1.35115e+02
I1112 16:01:47.338901 140264174335808 run_lib.py:153] step: 762700, training_loss: 9.99656e+01
I1112 16:01:57.807748 140264174335808 run_lib.py:153] step: 762750, training_loss: 1.36091e+02
I1112 16:02:07.656224 140264174335808 run_lib.py:153] step: 762800, training_loss: 1.39286e+02
I1112 16:02:18.385703 140264174335808 run_lib.py:153] step: 762850, training_loss: 1.30204e+02
I1112 16:02:28.341328 140264174335808 run_lib.py:153] step: 762900, training_loss: 1.25056e+02
I1112 16:02:38.129950 140264174335808 run_lib.py:153] step: 762950, training_loss: 1.41560e+02
I1112 16:02:48.512326 140264174335808 run_lib.py:153] step: 763000, training_loss: 1.22701e+02
I1112 16:02:58.847638 140264174335808 run_lib.py:153] step: 763050, training_loss: 1.40100e+02
I1112 16:03:08.372001 140264174335808 run_lib.py:153] step: 763100, training_loss: 1.29597e+02
I1112 16:03:18.142648 140264174335808 run_lib.py:153] step: 763150, training_loss: 1.20394e+02
I1112 16:03:28.998791 140264174335808 run_lib.py:153] step: 763200, training_loss: 1.06577e+02
I1112 16:03:39.381332 140264174335808 run_lib.py:153] step: 763250, training_loss: 1.08916e+02
I1112 16:03:49.550557 140264174335808 run_lib.py:153] step: 763300, training_loss: 1.48514e+02
I1112 16:03:59.404602 140264174335808 run_lib.py:153] step: 763350, training_loss: 1.10517e+02
I1112 16:04:10.332073 140264174335808 run_lib.py:153] step: 763400, training_loss: 1.21104e+02
I1112 16:04:20.630870 140264174335808 run_lib.py:153] step: 763450, training_loss: 1.04869e+02
I1112 16:04:30.832370 140264174335808 run_lib.py:153] step: 763500, training_loss: 1.18591e+02
I1112 16:04:40.443517 140264174335808 run_lib.py:153] step: 763550, training_loss: 1.29196e+02
I1112 16:04:50.658699 140264174335808 run_lib.py:153] step: 763600, training_loss: 1.24754e+02
I1112 16:05:00.728672 140264174335808 run_lib.py:153] step: 763650, training_loss: 1.43794e+02
I1112 16:05:10.309262 140264174335808 run_lib.py:153] step: 763700, training_loss: 1.47631e+02
I1112 16:05:19.763904 140264174335808 run_lib.py:153] step: 763750, training_loss: 1.07874e+02
I1112 16:05:29.577467 140264174335808 run_lib.py:153] step: 763800, training_loss: 1.11513e+02
I1112 16:05:39.840556 140264174335808 run_lib.py:153] step: 763850, training_loss: 1.18663e+02
I1112 16:05:49.648330 140264174335808 run_lib.py:153] step: 763900, training_loss: 1.41647e+02
I1112 16:05:59.066206 140264174335808 run_lib.py:153] step: 763950, training_loss: 1.55655e+02
I1112 16:06:08.731100 140264174335808 run_lib.py:153] step: 764000, training_loss: 1.21066e+02
I1112 16:06:19.382349 140264174335808 run_lib.py:153] step: 764050, training_loss: 1.43450e+02
I1112 16:06:29.705328 140264174335808 run_lib.py:153] step: 764100, training_loss: 1.20385e+02
I1112 16:06:39.931960 140264174335808 run_lib.py:153] step: 764150, training_loss: 1.30033e+02
I1112 16:06:50.089658 140264174335808 run_lib.py:153] step: 764200, training_loss: 1.27694e+02
I1112 16:07:00.093259 140264174335808 run_lib.py:153] step: 764250, training_loss: 1.29629e+02
I1112 16:07:10.503354 140264174335808 run_lib.py:153] step: 764300, training_loss: 1.10354e+02
I1112 16:07:20.234586 140264174335808 run_lib.py:153] step: 764350, training_loss: 1.41521e+02
I1112 16:07:30.543540 140264174335808 run_lib.py:153] step: 764400, training_loss: 1.42041e+02
I1112 16:07:39.966065 140264174335808 run_lib.py:153] step: 764450, training_loss: 1.51846e+02
I1112 16:07:49.484386 140264174335808 run_lib.py:153] step: 764500, training_loss: 1.26273e+02
I1112 16:07:59.048129 140264174335808 run_lib.py:153] step: 764550, training_loss: 1.21116e+02
I1112 16:08:08.937972 140264174335808 run_lib.py:153] step: 764600, training_loss: 1.46980e+02
I1112 16:08:19.338545 140264174335808 run_lib.py:153] step: 764650, training_loss: 1.08873e+02
I1112 16:08:29.601518 140264174335808 run_lib.py:153] step: 764700, training_loss: 1.02722e+02
I1112 16:08:39.824203 140264174335808 run_lib.py:153] step: 764750, training_loss: 1.33571e+02
I1112 16:08:49.831582 140264174335808 run_lib.py:153] step: 764800, training_loss: 1.25965e+02
I1112 16:08:59.649692 140264174335808 run_lib.py:153] step: 764850, training_loss: 1.59330e+02
I1112 16:09:09.825823 140264174335808 run_lib.py:153] step: 764900, training_loss: 1.07805e+02
I1112 16:09:20.317820 140264174335808 run_lib.py:153] step: 764950, training_loss: 1.23053e+02
I1112 16:09:31.141378 140264174335808 run_lib.py:153] step: 765000, training_loss: 1.19289e+02
I1112 16:09:31.282784 140264174335808 run_lib.py:166] step: 765000, eval_loss: 1.42520e+02
I1112 16:09:41.511988 140264174335808 run_lib.py:153] step: 765050, training_loss: 1.40897e+02
I1112 16:09:51.051069 140264174335808 run_lib.py:153] step: 765100, training_loss: 1.51225e+02
I1112 16:10:01.089838 140264174335808 run_lib.py:153] step: 765150, training_loss: 1.31782e+02
I1112 16:10:11.796547 140264174335808 run_lib.py:153] step: 765200, training_loss: 1.71177e+02
I1112 16:10:21.741699 140264174335808 run_lib.py:153] step: 765250, training_loss: 1.36719e+02
I1112 16:10:31.567398 140264174335808 run_lib.py:153] step: 765300, training_loss: 1.06940e+02
I1112 16:10:42.340272 140264174335808 run_lib.py:153] step: 765350, training_loss: 1.03821e+02
I1112 16:10:52.952075 140264174335808 run_lib.py:153] step: 765400, training_loss: 8.95256e+01
I1112 16:11:02.735425 140264174335808 run_lib.py:153] step: 765450, training_loss: 1.11223e+02
I1112 16:11:12.897138 140264174335808 run_lib.py:153] step: 765500, training_loss: 1.16184e+02
I1112 16:11:23.454341 140264174335808 run_lib.py:153] step: 765550, training_loss: 1.37000e+02
I1112 16:11:33.639770 140264174335808 run_lib.py:153] step: 765600, training_loss: 1.26320e+02
I1112 16:11:43.345579 140264174335808 run_lib.py:153] step: 765650, training_loss: 1.31386e+02
I1112 16:11:53.513818 140264174335808 run_lib.py:153] step: 765700, training_loss: 1.10199e+02
I1112 16:12:04.147613 140264174335808 run_lib.py:153] step: 765750, training_loss: 1.24224e+02
I1112 16:12:14.757281 140264174335808 run_lib.py:153] step: 765800, training_loss: 1.21445e+02
I1112 16:12:25.927161 140264174335808 run_lib.py:153] step: 765850, training_loss: 1.14641e+02
I1112 16:12:35.913742 140264174335808 run_lib.py:153] step: 765900, training_loss: 1.25122e+02
I1112 16:12:46.523360 140264174335808 run_lib.py:153] step: 765950, training_loss: 1.16888e+02
I1112 16:12:56.461240 140264174335808 run_lib.py:153] step: 766000, training_loss: 1.36756e+02
I1112 16:13:07.198746 140264174335808 run_lib.py:153] step: 766050, training_loss: 1.38106e+02
I1112 16:13:17.468487 140264174335808 run_lib.py:153] step: 766100, training_loss: 1.11862e+02
I1112 16:13:26.876780 140264174335808 run_lib.py:153] step: 766150, training_loss: 1.34617e+02
I1112 16:13:37.381536 140264174335808 run_lib.py:153] step: 766200, training_loss: 1.21109e+02
I1112 16:13:47.562785 140264174335808 run_lib.py:153] step: 766250, training_loss: 1.34966e+02
I1112 16:13:57.717773 140264174335808 run_lib.py:153] step: 766300, training_loss: 1.32786e+02
I1112 16:14:07.353231 140264174335808 run_lib.py:153] step: 766350, training_loss: 1.09861e+02
I1112 16:14:17.248060 140264174335808 run_lib.py:153] step: 766400, training_loss: 1.35662e+02
I1112 16:14:27.027771 140264174335808 run_lib.py:153] step: 766450, training_loss: 1.34630e+02
I1112 16:14:36.633753 140264174335808 run_lib.py:153] step: 766500, training_loss: 1.36871e+02
I1112 16:14:46.603897 140264174335808 run_lib.py:153] step: 766550, training_loss: 1.40580e+02
I1112 16:14:57.067196 140264174335808 run_lib.py:153] step: 766600, training_loss: 1.23686e+02
I1112 16:15:07.091950 140264174335808 run_lib.py:153] step: 766650, training_loss: 1.26237e+02
I1112 16:15:17.026252 140264174335808 run_lib.py:153] step: 766700, training_loss: 1.07906e+02
I1112 16:15:26.712045 140264174335808 run_lib.py:153] step: 766750, training_loss: 1.34235e+02
I1112 16:15:36.419151 140264174335808 run_lib.py:153] step: 766800, training_loss: 1.24038e+02
I1112 16:15:46.572422 140264174335808 run_lib.py:153] step: 766850, training_loss: 1.37510e+02
I1112 16:15:56.483523 140264174335808 run_lib.py:153] step: 766900, training_loss: 1.21317e+02
I1112 16:16:06.171361 140264174335808 run_lib.py:153] step: 766950, training_loss: 9.90587e+01
I1112 16:16:16.015338 140264174335808 run_lib.py:153] step: 767000, training_loss: 1.42264e+02
I1112 16:16:25.682507 140264174335808 run_lib.py:153] step: 767050, training_loss: 1.15594e+02
I1112 16:16:35.777201 140264174335808 run_lib.py:153] step: 767100, training_loss: 1.16695e+02
I1112 16:16:45.512767 140264174335808 run_lib.py:153] step: 767150, training_loss: 1.44605e+02
I1112 16:16:54.771980 140264174335808 run_lib.py:153] step: 767200, training_loss: 1.56517e+02
I1112 16:17:04.049371 140264174335808 run_lib.py:153] step: 767250, training_loss: 1.18669e+02
I1112 16:17:13.413708 140264174335808 run_lib.py:153] step: 767300, training_loss: 1.37253e+02
I1112 16:17:22.889725 140264174335808 run_lib.py:153] step: 767350, training_loss: 1.12225e+02
I1112 16:17:32.498708 140264174335808 run_lib.py:153] step: 767400, training_loss: 1.17906e+02
I1112 16:17:42.745681 140264174335808 run_lib.py:153] step: 767450, training_loss: 1.29005e+02
I1112 16:17:52.643749 140264174335808 run_lib.py:153] step: 767500, training_loss: 9.88813e+01
I1112 16:18:02.003128 140264174335808 run_lib.py:153] step: 767550, training_loss: 1.22006e+02
I1112 16:18:12.077082 140264174335808 run_lib.py:153] step: 767600, training_loss: 1.23264e+02
I1112 16:18:21.803205 140264174335808 run_lib.py:153] step: 767650, training_loss: 1.01486e+02
I1112 16:18:32.459073 140264174335808 run_lib.py:153] step: 767700, training_loss: 1.00303e+02
I1112 16:18:43.676450 140264174335808 run_lib.py:153] step: 767750, training_loss: 1.44413e+02
I1112 16:18:54.162005 140264174335808 run_lib.py:153] step: 767800, training_loss: 1.22640e+02
I1112 16:19:03.963952 140264174335808 run_lib.py:153] step: 767850, training_loss: 1.33704e+02
I1112 16:19:14.597624 140264174335808 run_lib.py:153] step: 767900, training_loss: 1.09679e+02
I1112 16:19:24.518905 140264174335808 run_lib.py:153] step: 767950, training_loss: 1.01779e+02
I1112 16:19:34.226901 140264174335808 run_lib.py:153] step: 768000, training_loss: 1.51953e+02
I1112 16:19:44.220349 140264174335808 run_lib.py:153] step: 768050, training_loss: 1.26926e+02
I1112 16:19:54.127650 140264174335808 run_lib.py:153] step: 768100, training_loss: 1.38650e+02
I1112 16:20:03.775767 140264174335808 run_lib.py:153] step: 768150, training_loss: 1.43031e+02
I1112 16:20:13.377070 140264174335808 run_lib.py:153] step: 768200, training_loss: 1.24942e+02
I1112 16:20:23.171277 140264174335808 run_lib.py:153] step: 768250, training_loss: 1.36276e+02
I1112 16:20:33.496978 140264174335808 run_lib.py:153] step: 768300, training_loss: 1.27591e+02
I1112 16:20:43.682011 140264174335808 run_lib.py:153] step: 768350, training_loss: 1.24491e+02
I1112 16:20:53.680165 140264174335808 run_lib.py:153] step: 768400, training_loss: 1.05038e+02
I1112 16:21:04.039672 140264174335808 run_lib.py:153] step: 768450, training_loss: 1.35645e+02
I1112 16:21:13.652631 140264174335808 run_lib.py:153] step: 768500, training_loss: 1.43057e+02
I1112 16:21:23.389627 140264174335808 run_lib.py:153] step: 768550, training_loss: 9.01663e+01
I1112 16:21:33.329957 140264174335808 run_lib.py:153] step: 768600, training_loss: 1.18631e+02
I1112 16:21:43.922655 140264174335808 run_lib.py:153] step: 768650, training_loss: 1.30844e+02
I1112 16:21:53.724512 140264174335808 run_lib.py:153] step: 768700, training_loss: 1.04738e+02
I1112 16:22:03.805855 140264174335808 run_lib.py:153] step: 768750, training_loss: 1.12937e+02
I1112 16:22:13.548514 140264174335808 run_lib.py:153] step: 768800, training_loss: 1.13166e+02
I1112 16:22:23.935875 140264174335808 run_lib.py:153] step: 768850, training_loss: 1.29020e+02
I1112 16:22:34.019103 140264174335808 run_lib.py:153] step: 768900, training_loss: 1.07037e+02
I1112 16:22:44.004692 140264174335808 run_lib.py:153] step: 768950, training_loss: 1.31968e+02
I1112 16:22:54.675024 140264174335808 run_lib.py:153] step: 769000, training_loss: 1.43503e+02
I1112 16:23:05.101491 140264174335808 run_lib.py:153] step: 769050, training_loss: 1.20454e+02
I1112 16:23:15.312936 140264174335808 run_lib.py:153] step: 769100, training_loss: 9.32957e+01
I1112 16:23:24.632439 140264174335808 run_lib.py:153] step: 769150, training_loss: 1.45614e+02
I1112 16:23:34.460204 140264174335808 run_lib.py:153] step: 769200, training_loss: 9.40993e+01
I1112 16:23:44.259001 140264174335808 run_lib.py:153] step: 769250, training_loss: 1.02807e+02
I1112 16:23:54.491473 140264174335808 run_lib.py:153] step: 769300, training_loss: 1.46220e+02
I1112 16:24:04.522451 140264174335808 run_lib.py:153] step: 769350, training_loss: 1.69425e+02
I1112 16:24:15.088574 140264174335808 run_lib.py:153] step: 769400, training_loss: 1.02610e+02
I1112 16:24:25.397093 140264174335808 run_lib.py:153] step: 769450, training_loss: 1.25797e+02
I1112 16:24:35.935034 140264174335808 run_lib.py:153] step: 769500, training_loss: 1.04509e+02
I1112 16:24:45.999320 140264174335808 run_lib.py:153] step: 769550, training_loss: 1.25845e+02
I1112 16:24:55.934467 140264174335808 run_lib.py:153] step: 769600, training_loss: 1.13923e+02
I1112 16:25:05.320286 140264174335808 run_lib.py:153] step: 769650, training_loss: 1.07375e+02
I1112 16:25:15.210595 140264174335808 run_lib.py:153] step: 769700, training_loss: 1.25261e+02
I1112 16:25:25.370241 140264174335808 run_lib.py:153] step: 769750, training_loss: 1.50257e+02
I1112 16:25:35.782242 140264174335808 run_lib.py:153] step: 769800, training_loss: 1.33149e+02
I1112 16:25:46.249391 140264174335808 run_lib.py:153] step: 769850, training_loss: 1.49405e+02
I1112 16:25:56.958687 140264174335808 run_lib.py:153] step: 769900, training_loss: 1.53303e+02
I1112 16:26:06.766849 140264174335808 run_lib.py:153] step: 769950, training_loss: 1.03094e+02
I1112 16:26:16.384842 140264174335808 run_lib.py:153] step: 770000, training_loss: 1.07275e+02
I1112 16:26:16.955888 140264174335808 run_lib.py:166] step: 770000, eval_loss: 1.40633e+02
I1112 16:26:26.876556 140264174335808 run_lib.py:153] step: 770050, training_loss: 1.82497e+02
I1112 16:26:38.166152 140264174335808 run_lib.py:153] step: 770100, training_loss: 1.36038e+02
I1112 16:26:48.668600 140264174335808 run_lib.py:153] step: 770150, training_loss: 9.62186e+01
I1112 16:26:59.140810 140264174335808 run_lib.py:153] step: 770200, training_loss: 1.46321e+02
I1112 16:27:09.770116 140264174335808 run_lib.py:153] step: 770250, training_loss: 1.14511e+02
I1112 16:27:19.660866 140264174335808 run_lib.py:153] step: 770300, training_loss: 1.23383e+02
I1112 16:27:29.684166 140264174335808 run_lib.py:153] step: 770350, training_loss: 1.37748e+02
I1112 16:27:40.116267 140264174335808 run_lib.py:153] step: 770400, training_loss: 1.33908e+02
I1112 16:27:49.775614 140264174335808 run_lib.py:153] step: 770450, training_loss: 1.07669e+02
I1112 16:28:00.025663 140264174335808 run_lib.py:153] step: 770500, training_loss: 1.24656e+02
I1112 16:28:10.386306 140264174335808 run_lib.py:153] step: 770550, training_loss: 1.26303e+02
I1112 16:28:20.442711 140264174335808 run_lib.py:153] step: 770600, training_loss: 1.25121e+02
I1112 16:28:31.019753 140264174335808 run_lib.py:153] step: 770650, training_loss: 1.33103e+02
I1112 16:28:41.429296 140264174335808 run_lib.py:153] step: 770700, training_loss: 1.35285e+02
I1112 16:28:52.002092 140264174335808 run_lib.py:153] step: 770750, training_loss: 1.49866e+02
I1112 16:29:01.586458 140264174335808 run_lib.py:153] step: 770800, training_loss: 1.24485e+02
I1112 16:29:11.952911 140264174335808 run_lib.py:153] step: 770850, training_loss: 1.57894e+02
I1112 16:29:22.587003 140264174335808 run_lib.py:153] step: 770900, training_loss: 1.12101e+02
I1112 16:29:32.757147 140264174335808 run_lib.py:153] step: 770950, training_loss: 1.37790e+02
I1112 16:29:43.031119 140264174335808 run_lib.py:153] step: 771000, training_loss: 1.19365e+02
I1112 16:29:53.540532 140264174335808 run_lib.py:153] step: 771050, training_loss: 1.56283e+02
I1112 16:30:03.995166 140264174335808 run_lib.py:153] step: 771100, training_loss: 1.25419e+02
I1112 16:30:14.407263 140264174335808 run_lib.py:153] step: 771150, training_loss: 1.37555e+02
I1112 16:30:24.521073 140264174335808 run_lib.py:153] step: 771200, training_loss: 1.44033e+02
I1112 16:30:34.000788 140264174335808 run_lib.py:153] step: 771250, training_loss: 1.39881e+02
I1112 16:30:44.094455 140264174335808 run_lib.py:153] step: 771300, training_loss: 1.44823e+02
I1112 16:30:53.848566 140264174335808 run_lib.py:153] step: 771350, training_loss: 1.19529e+02
I1112 16:31:04.045604 140264174335808 run_lib.py:153] step: 771400, training_loss: 1.14031e+02
I1112 16:31:14.812974 140264174335808 run_lib.py:153] step: 771450, training_loss: 1.31037e+02
I1112 16:31:25.619097 140264174335808 run_lib.py:153] step: 771500, training_loss: 1.11921e+02
I1112 16:31:35.262120 140264174335808 run_lib.py:153] step: 771550, training_loss: 1.24067e+02
I1112 16:31:46.051005 140264174335808 run_lib.py:153] step: 771600, training_loss: 1.33291e+02
I1112 16:31:56.509294 140264174335808 run_lib.py:153] step: 771650, training_loss: 1.33092e+02
I1112 16:32:06.406330 140264174335808 run_lib.py:153] step: 771700, training_loss: 1.43514e+02
I1112 16:32:16.055292 140264174335808 run_lib.py:153] step: 771750, training_loss: 1.15097e+02
I1112 16:32:26.414480 140264174335808 run_lib.py:153] step: 771800, training_loss: 1.31492e+02
I1112 16:32:35.991079 140264174335808 run_lib.py:153] step: 771850, training_loss: 1.16434e+02
I1112 16:32:45.985249 140264174335808 run_lib.py:153] step: 771900, training_loss: 1.45648e+02
I1112 16:32:56.820132 140264174335808 run_lib.py:153] step: 771950, training_loss: 1.00822e+02
I1112 16:33:07.527285 140264174335808 run_lib.py:153] step: 772000, training_loss: 1.44685e+02
I1112 16:33:17.631405 140264174335808 run_lib.py:153] step: 772050, training_loss: 1.46247e+02
I1112 16:33:27.769265 140264174335808 run_lib.py:153] step: 772100, training_loss: 1.32539e+02
I1112 16:33:38.855478 140264174335808 run_lib.py:153] step: 772150, training_loss: 1.66337e+02
I1112 16:33:48.678539 140264174335808 run_lib.py:153] step: 772200, training_loss: 1.00140e+02
I1112 16:33:59.010373 140264174335808 run_lib.py:153] step: 772250, training_loss: 1.33645e+02
I1112 16:34:08.870949 140264174335808 run_lib.py:153] step: 772300, training_loss: 1.39706e+02
I1112 16:34:19.186981 140264174335808 run_lib.py:153] step: 772350, training_loss: 1.35019e+02
I1112 16:34:29.414401 140264174335808 run_lib.py:153] step: 772400, training_loss: 1.16660e+02
I1112 16:34:40.674791 140264174335808 run_lib.py:153] step: 772450, training_loss: 1.03794e+02
I1112 16:34:51.005407 140264174335808 run_lib.py:153] step: 772500, training_loss: 1.20383e+02
I1112 16:35:01.140727 140264174335808 run_lib.py:153] step: 772550, training_loss: 1.28059e+02
I1112 16:35:11.505227 140264174335808 run_lib.py:153] step: 772600, training_loss: 1.12649e+02
I1112 16:35:21.587118 140264174335808 run_lib.py:153] step: 772650, training_loss: 1.62449e+02
I1112 16:35:32.261755 140264174335808 run_lib.py:153] step: 772700, training_loss: 1.34034e+02
I1112 16:35:42.734524 140264174335808 run_lib.py:153] step: 772750, training_loss: 1.10814e+02
I1112 16:35:52.925640 140264174335808 run_lib.py:153] step: 772800, training_loss: 1.29335e+02
I1112 16:36:03.341279 140264174335808 run_lib.py:153] step: 772850, training_loss: 1.36634e+02
I1112 16:36:13.146246 140264174335808 run_lib.py:153] step: 772900, training_loss: 9.43275e+01
I1112 16:36:23.511391 140264174335808 run_lib.py:153] step: 772950, training_loss: 1.49064e+02
I1112 16:36:33.581508 140264174335808 run_lib.py:153] step: 773000, training_loss: 1.40179e+02
I1112 16:36:43.633417 140264174335808 run_lib.py:153] step: 773050, training_loss: 1.15659e+02
I1112 16:36:54.223851 140264174335808 run_lib.py:153] step: 773100, training_loss: 1.48952e+02
I1112 16:37:03.687728 140264174335808 run_lib.py:153] step: 773150, training_loss: 1.05609e+02
I1112 16:37:14.010124 140264174335808 run_lib.py:153] step: 773200, training_loss: 1.49671e+02
I1112 16:37:23.695182 140264174335808 run_lib.py:153] step: 773250, training_loss: 1.32049e+02
I1112 16:37:33.986293 140264174335808 run_lib.py:153] step: 773300, training_loss: 1.15402e+02
I1112 16:37:44.129198 140264174335808 run_lib.py:153] step: 773350, training_loss: 1.37381e+02
I1112 16:37:53.700971 140264174335808 run_lib.py:153] step: 773400, training_loss: 1.32695e+02
I1112 16:38:04.186356 140264174335808 run_lib.py:153] step: 773450, training_loss: 1.56555e+02
I1112 16:38:14.519050 140264174335808 run_lib.py:153] step: 773500, training_loss: 1.26270e+02
I1112 16:38:24.361461 140264174335808 run_lib.py:153] step: 773550, training_loss: 1.47258e+02
I1112 16:38:34.359090 140264174335808 run_lib.py:153] step: 773600, training_loss: 1.26566e+02
I1112 16:38:44.431544 140264174335808 run_lib.py:153] step: 773650, training_loss: 1.34092e+02
I1112 16:38:54.521764 140264174335808 run_lib.py:153] step: 773700, training_loss: 1.31727e+02
I1112 16:39:04.953408 140264174335808 run_lib.py:153] step: 773750, training_loss: 1.03833e+02
I1112 16:39:14.931007 140264174335808 run_lib.py:153] step: 773800, training_loss: 1.13434e+02
I1112 16:39:25.118300 140264174335808 run_lib.py:153] step: 773850, training_loss: 1.55151e+02
I1112 16:39:35.201154 140264174335808 run_lib.py:153] step: 773900, training_loss: 1.32030e+02
I1112 16:39:44.643830 140264174335808 run_lib.py:153] step: 773950, training_loss: 1.47817e+02
I1112 16:39:54.591237 140264174335808 run_lib.py:153] step: 774000, training_loss: 1.30178e+02
I1112 16:40:04.923657 140264174335808 run_lib.py:153] step: 774050, training_loss: 1.32825e+02
I1112 16:40:14.892978 140264174335808 run_lib.py:153] step: 774100, training_loss: 1.37439e+02
I1112 16:40:25.982801 140264174335808 run_lib.py:153] step: 774150, training_loss: 1.49680e+02
I1112 16:40:36.501655 140264174335808 run_lib.py:153] step: 774200, training_loss: 1.06624e+02
I1112 16:40:46.234750 140264174335808 run_lib.py:153] step: 774250, training_loss: 1.38319e+02
I1112 16:40:55.946877 140264174335808 run_lib.py:153] step: 774300, training_loss: 1.51471e+02
I1112 16:41:06.121387 140264174335808 run_lib.py:153] step: 774350, training_loss: 1.22378e+02
I1112 16:41:15.980107 140264174335808 run_lib.py:153] step: 774400, training_loss: 8.90331e+01
I1112 16:41:25.514763 140264174335808 run_lib.py:153] step: 774450, training_loss: 1.33487e+02
I1112 16:41:35.538655 140264174335808 run_lib.py:153] step: 774500, training_loss: 1.87628e+02
I1112 16:41:45.918303 140264174335808 run_lib.py:153] step: 774550, training_loss: 1.08605e+02
I1112 16:41:56.249193 140264174335808 run_lib.py:153] step: 774600, training_loss: 1.22144e+02
I1112 16:42:06.513214 140264174335808 run_lib.py:153] step: 774650, training_loss: 1.45273e+02
I1112 16:42:16.646681 140264174335808 run_lib.py:153] step: 774700, training_loss: 1.18276e+02
I1112 16:42:27.265284 140264174335808 run_lib.py:153] step: 774750, training_loss: 1.18316e+02
I1112 16:42:37.970067 140264174335808 run_lib.py:153] step: 774800, training_loss: 1.19698e+02
I1112 16:42:48.243958 140264174335808 run_lib.py:153] step: 774850, training_loss: 1.37890e+02
I1112 16:42:58.510304 140264174335808 run_lib.py:153] step: 774900, training_loss: 1.29673e+02
I1112 16:43:08.500494 140264174335808 run_lib.py:153] step: 774950, training_loss: 1.04328e+02
I1112 16:43:18.468876 140264174335808 run_lib.py:153] step: 775000, training_loss: 1.06784e+02
I1112 16:43:18.575806 140264174335808 run_lib.py:166] step: 775000, eval_loss: 1.17773e+02
I1112 16:43:28.576680 140264174335808 run_lib.py:153] step: 775050, training_loss: 1.31356e+02
I1112 16:43:39.160685 140264174335808 run_lib.py:153] step: 775100, training_loss: 1.13611e+02
I1112 16:43:49.276838 140264174335808 run_lib.py:153] step: 775150, training_loss: 1.36098e+02
I1112 16:43:59.374965 140264174335808 run_lib.py:153] step: 775200, training_loss: 1.01996e+02
I1112 16:44:09.129091 140264174335808 run_lib.py:153] step: 775250, training_loss: 1.15796e+02
I1112 16:44:18.967419 140264174335808 run_lib.py:153] step: 775300, training_loss: 1.22924e+02
I1112 16:44:28.826231 140264174335808 run_lib.py:153] step: 775350, training_loss: 1.50418e+02
I1112 16:44:39.672717 140264174335808 run_lib.py:153] step: 775400, training_loss: 1.14788e+02
I1112 16:44:49.411169 140264174335808 run_lib.py:153] step: 775450, training_loss: 1.31553e+02
I1112 16:44:59.272074 140264174335808 run_lib.py:153] step: 775500, training_loss: 1.28245e+02
I1112 16:45:09.782163 140264174335808 run_lib.py:153] step: 775550, training_loss: 1.22831e+02
I1112 16:45:19.580823 140264174335808 run_lib.py:153] step: 775600, training_loss: 1.38075e+02
I1112 16:45:29.049334 140264174335808 run_lib.py:153] step: 775650, training_loss: 1.56906e+02
I1112 16:45:39.212568 140264174335808 run_lib.py:153] step: 775700, training_loss: 1.49964e+02
I1112 16:45:49.295358 140264174335808 run_lib.py:153] step: 775750, training_loss: 1.25165e+02
I1112 16:45:59.878734 140264174335808 run_lib.py:153] step: 775800, training_loss: 1.36507e+02
I1112 16:46:09.647181 140264174335808 run_lib.py:153] step: 775850, training_loss: 1.12975e+02
I1112 16:46:19.596762 140264174335808 run_lib.py:153] step: 775900, training_loss: 1.15999e+02
I1112 16:46:29.223549 140264174335808 run_lib.py:153] step: 775950, training_loss: 1.03064e+02
I1112 16:46:39.633854 140264174335808 run_lib.py:153] step: 776000, training_loss: 1.15172e+02
I1112 16:46:49.451601 140264174335808 run_lib.py:153] step: 776050, training_loss: 1.02840e+02
I1112 16:46:59.655522 140264174335808 run_lib.py:153] step: 776100, training_loss: 1.34636e+02
I1112 16:47:09.832743 140264174335808 run_lib.py:153] step: 776150, training_loss: 1.14370e+02
I1112 16:47:19.112081 140264174335808 run_lib.py:153] step: 776200, training_loss: 1.09673e+02
I1112 16:47:29.151903 140264174335808 run_lib.py:153] step: 776250, training_loss: 1.49412e+02
I1112 16:47:39.233538 140264174335808 run_lib.py:153] step: 776300, training_loss: 1.13267e+02
I1112 16:47:48.932922 140264174335808 run_lib.py:153] step: 776350, training_loss: 1.20153e+02
I1112 16:47:58.220788 140264174335808 run_lib.py:153] step: 776400, training_loss: 1.18728e+02
I1112 16:48:07.725091 140264174335808 run_lib.py:153] step: 776450, training_loss: 1.22748e+02
I1112 16:48:17.566254 140264174335808 run_lib.py:153] step: 776500, training_loss: 1.38736e+02
I1112 16:48:27.320568 140264174335808 run_lib.py:153] step: 776550, training_loss: 1.56645e+02
I1112 16:48:37.672722 140264174335808 run_lib.py:153] step: 776600, training_loss: 1.37157e+02
I1112 16:48:48.172956 140264174335808 run_lib.py:153] step: 776650, training_loss: 1.09242e+02
I1112 16:48:57.929967 140264174335808 run_lib.py:153] step: 776700, training_loss: 1.43581e+02
I1112 16:49:08.068738 140264174335808 run_lib.py:153] step: 776750, training_loss: 1.46639e+02
I1112 16:49:18.439771 140264174335808 run_lib.py:153] step: 776800, training_loss: 1.48227e+02
I1112 16:49:28.705455 140264174335808 run_lib.py:153] step: 776850, training_loss: 9.32087e+01
I1112 16:49:38.179494 140264174335808 run_lib.py:153] step: 776900, training_loss: 1.18707e+02
I1112 16:49:48.500623 140264174335808 run_lib.py:153] step: 776950, training_loss: 9.71121e+01
I1112 16:49:58.522503 140264174335808 run_lib.py:153] step: 777000, training_loss: 1.34247e+02
I1112 16:50:09.257538 140264174335808 run_lib.py:153] step: 777050, training_loss: 1.25594e+02
I1112 16:50:19.583439 140264174335808 run_lib.py:153] step: 777100, training_loss: 1.32321e+02
I1112 16:50:29.774772 140264174335808 run_lib.py:153] step: 777150, training_loss: 1.44638e+02
I1112 16:50:39.698927 140264174335808 run_lib.py:153] step: 777200, training_loss: 1.29155e+02
I1112 16:50:50.297780 140264174335808 run_lib.py:153] step: 777250, training_loss: 1.14231e+02
I1112 16:51:00.812714 140264174335808 run_lib.py:153] step: 777300, training_loss: 1.25357e+02
I1112 16:51:11.133028 140264174335808 run_lib.py:153] step: 777350, training_loss: 1.35016e+02
I1112 16:51:21.409142 140264174335808 run_lib.py:153] step: 777400, training_loss: 1.47047e+02
I1112 16:51:31.497081 140264174335808 run_lib.py:153] step: 777450, training_loss: 9.78764e+01
I1112 16:51:42.040380 140264174335808 run_lib.py:153] step: 777500, training_loss: 1.08870e+02
I1112 16:51:52.253924 140264174335808 run_lib.py:153] step: 777550, training_loss: 1.04446e+02
I1112 16:52:02.790532 140264174335808 run_lib.py:153] step: 777600, training_loss: 1.29190e+02
I1112 16:52:12.516788 140264174335808 run_lib.py:153] step: 777650, training_loss: 1.22925e+02
I1112 16:52:23.030619 140264174335808 run_lib.py:153] step: 777700, training_loss: 1.18966e+02
I1112 16:52:33.481001 140264174335808 run_lib.py:153] step: 777750, training_loss: 1.23228e+02
I1112 16:52:43.145674 140264174335808 run_lib.py:153] step: 777800, training_loss: 1.12635e+02
I1112 16:52:53.505506 140264174335808 run_lib.py:153] step: 777850, training_loss: 1.00669e+02
I1112 16:53:03.730429 140264174335808 run_lib.py:153] step: 777900, training_loss: 1.33949e+02
I1112 16:53:13.341011 140264174335808 run_lib.py:153] step: 777950, training_loss: 1.16699e+02
I1112 16:53:23.562546 140264174335808 run_lib.py:153] step: 778000, training_loss: 1.15607e+02
I1112 16:53:33.568119 140264174335808 run_lib.py:153] step: 778050, training_loss: 1.12257e+02
I1112 16:53:43.022279 140264174335808 run_lib.py:153] step: 778100, training_loss: 1.23121e+02
I1112 16:53:53.230351 140264174335808 run_lib.py:153] step: 778150, training_loss: 1.28288e+02
I1112 16:54:03.409378 140264174335808 run_lib.py:153] step: 778200, training_loss: 1.42761e+02
I1112 16:54:14.186101 140264174335808 run_lib.py:153] step: 778250, training_loss: 1.27937e+02
I1112 16:54:24.175282 140264174335808 run_lib.py:153] step: 778300, training_loss: 1.19700e+02
I1112 16:54:34.667348 140264174335808 run_lib.py:153] step: 778350, training_loss: 1.07138e+02
I1112 16:54:44.654051 140264174335808 run_lib.py:153] step: 778400, training_loss: 9.65691e+01
I1112 16:54:54.900316 140264174335808 run_lib.py:153] step: 778450, training_loss: 1.43462e+02
I1112 16:55:04.238522 140264174335808 run_lib.py:153] step: 778500, training_loss: 1.58454e+02
I1112 16:55:14.880039 140264174335808 run_lib.py:153] step: 778550, training_loss: 1.06438e+02
I1112 16:55:24.579565 140264174335808 run_lib.py:153] step: 778600, training_loss: 1.26208e+02
I1112 16:55:34.862462 140264174335808 run_lib.py:153] step: 778650, training_loss: 1.20203e+02
I1112 16:55:45.302884 140264174335808 run_lib.py:153] step: 778700, training_loss: 1.40506e+02
I1112 16:55:55.485457 140264174335808 run_lib.py:153] step: 778750, training_loss: 1.29411e+02
I1112 16:56:05.536549 140264174335808 run_lib.py:153] step: 778800, training_loss: 1.30036e+02
I1112 16:56:15.981818 140264174335808 run_lib.py:153] step: 778850, training_loss: 1.07502e+02
I1112 16:56:26.170120 140264174335808 run_lib.py:153] step: 778900, training_loss: 1.14440e+02
I1112 16:56:35.602174 140264174335808 run_lib.py:153] step: 778950, training_loss: 1.25615e+02
I1112 16:56:45.303224 140264174335808 run_lib.py:153] step: 779000, training_loss: 1.42096e+02
I1112 16:56:54.850157 140264174335808 run_lib.py:153] step: 779050, training_loss: 9.66561e+01
I1112 16:57:04.974210 140264174335808 run_lib.py:153] step: 779100, training_loss: 1.00386e+02
I1112 16:57:15.167534 140264174335808 run_lib.py:153] step: 779150, training_loss: 1.49224e+02
I1112 16:57:24.604349 140264174335808 run_lib.py:153] step: 779200, training_loss: 1.37308e+02
I1112 16:57:34.613526 140264174335808 run_lib.py:153] step: 779250, training_loss: 1.00336e+02
I1112 16:57:44.340092 140264174335808 run_lib.py:153] step: 779300, training_loss: 1.23780e+02
I1112 16:57:54.444294 140264174335808 run_lib.py:153] step: 779350, training_loss: 1.21966e+02
I1112 16:58:04.348513 140264174335808 run_lib.py:153] step: 779400, training_loss: 1.09675e+02
I1112 16:58:14.185056 140264174335808 run_lib.py:153] step: 779450, training_loss: 1.28313e+02
I1112 16:58:24.789744 140264174335808 run_lib.py:153] step: 779500, training_loss: 1.25641e+02
I1112 16:58:34.604429 140264174335808 run_lib.py:153] step: 779550, training_loss: 1.18449e+02
I1112 16:58:44.532258 140264174335808 run_lib.py:153] step: 779600, training_loss: 1.33627e+02
I1112 16:58:54.355596 140264174335808 run_lib.py:153] step: 779650, training_loss: 1.05652e+02
I1112 16:59:04.035038 140264174335808 run_lib.py:153] step: 779700, training_loss: 1.32184e+02
I1112 16:59:13.965793 140264174335808 run_lib.py:153] step: 779750, training_loss: 1.16397e+02
I1112 16:59:24.100270 140264174335808 run_lib.py:153] step: 779800, training_loss: 1.53774e+02
I1112 16:59:34.045674 140264174335808 run_lib.py:153] step: 779850, training_loss: 1.13669e+02
I1112 16:59:43.933450 140264174335808 run_lib.py:153] step: 779900, training_loss: 1.18751e+02
I1112 16:59:53.570546 140264174335808 run_lib.py:153] step: 779950, training_loss: 1.29097e+02
I1112 17:00:02.887309 140264174335808 run_lib.py:153] step: 780000, training_loss: 1.44288e+02
I1112 17:00:03.414861 140264174335808 run_lib.py:166] step: 780000, eval_loss: 1.38957e+02
I1112 17:00:13.139662 140264174335808 run_lib.py:153] step: 780050, training_loss: 1.15185e+02
I1112 17:00:22.801605 140264174335808 run_lib.py:153] step: 780100, training_loss: 1.27571e+02
I1112 17:00:33.080097 140264174335808 run_lib.py:153] step: 780150, training_loss: 1.45527e+02
I1112 17:00:43.295070 140264174335808 run_lib.py:153] step: 780200, training_loss: 1.30353e+02
I1112 17:00:53.404264 140264174335808 run_lib.py:153] step: 780250, training_loss: 1.29477e+02
I1112 17:01:03.973514 140264174335808 run_lib.py:153] step: 780300, training_loss: 1.37741e+02
I1112 17:01:14.114383 140264174335808 run_lib.py:153] step: 780350, training_loss: 9.93032e+01
I1112 17:01:24.379673 140264174335808 run_lib.py:153] step: 780400, training_loss: 1.42484e+02
I1112 17:01:34.627502 140264174335808 run_lib.py:153] step: 780450, training_loss: 1.31361e+02
I1112 17:01:44.846177 140264174335808 run_lib.py:153] step: 780500, training_loss: 1.22862e+02
I1112 17:01:54.826955 140264174335808 run_lib.py:153] step: 780550, training_loss: 1.61856e+02
I1112 17:02:04.359084 140264174335808 run_lib.py:153] step: 780600, training_loss: 1.25302e+02
I1112 17:02:14.552031 140264174335808 run_lib.py:153] step: 780650, training_loss: 1.34878e+02
I1112 17:02:24.974238 140264174335808 run_lib.py:153] step: 780700, training_loss: 1.31881e+02
I1112 17:02:35.475510 140264174335808 run_lib.py:153] step: 780750, training_loss: 9.56089e+01
I1112 17:02:45.894726 140264174335808 run_lib.py:153] step: 780800, training_loss: 1.32427e+02
I1112 17:02:56.000013 140264174335808 run_lib.py:153] step: 780850, training_loss: 1.06809e+02
I1112 17:03:05.802945 140264174335808 run_lib.py:153] step: 780900, training_loss: 1.26581e+02
I1112 17:03:15.694221 140264174335808 run_lib.py:153] step: 780950, training_loss: 1.10906e+02
I1112 17:03:24.954175 140264174335808 run_lib.py:153] step: 781000, training_loss: 1.22598e+02
I1112 17:03:34.575072 140264174335808 run_lib.py:153] step: 781050, training_loss: 1.33748e+02
I1112 17:03:45.351871 140264174335808 run_lib.py:153] step: 781100, training_loss: 1.27832e+02
I1112 17:03:54.726962 140264174335808 run_lib.py:153] step: 781150, training_loss: 1.25922e+02
I1112 17:04:04.751869 140264174335808 run_lib.py:153] step: 781200, training_loss: 1.34715e+02
I1112 17:04:14.393710 140264174335808 run_lib.py:153] step: 781250, training_loss: 9.88113e+01
I1112 17:04:23.772710 140264174335808 run_lib.py:153] step: 781300, training_loss: 1.16440e+02
I1112 17:04:33.012402 140264174335808 run_lib.py:153] step: 781350, training_loss: 1.52449e+02
I1112 17:04:42.897825 140264174335808 run_lib.py:153] step: 781400, training_loss: 1.05944e+02
I1112 17:04:52.604283 140264174335808 run_lib.py:153] step: 781450, training_loss: 1.40620e+02
I1112 17:05:02.534802 140264174335808 run_lib.py:153] step: 781500, training_loss: 1.38417e+02
I1112 17:05:12.066510 140264174335808 run_lib.py:153] step: 781550, training_loss: 1.26065e+02
I1112 17:05:21.749556 140264174335808 run_lib.py:153] step: 781600, training_loss: 1.06675e+02
I1112 17:05:31.609042 140264174335808 run_lib.py:153] step: 781650, training_loss: 1.30277e+02
I1112 17:05:41.590263 140264174335808 run_lib.py:153] step: 781700, training_loss: 1.15808e+02
I1112 17:05:51.127721 140264174335808 run_lib.py:153] step: 781750, training_loss: 1.09363e+02
I1112 17:06:01.044541 140264174335808 run_lib.py:153] step: 781800, training_loss: 1.18762e+02
I1112 17:06:10.984962 140264174335808 run_lib.py:153] step: 781850, training_loss: 1.09411e+02
I1112 17:06:21.124671 140264174335808 run_lib.py:153] step: 781900, training_loss: 1.17667e+02
I1112 17:06:30.892244 140264174335808 run_lib.py:153] step: 781950, training_loss: 1.62366e+02
I1112 17:06:41.185245 140264174335808 run_lib.py:153] step: 782000, training_loss: 1.24949e+02
I1112 17:06:51.481281 140264174335808 run_lib.py:153] step: 782050, training_loss: 1.21234e+02
I1112 17:07:01.366347 140264174335808 run_lib.py:153] step: 782100, training_loss: 1.14034e+02
I1112 17:07:11.512847 140264174335808 run_lib.py:153] step: 782150, training_loss: 1.20001e+02
I1112 17:07:20.938930 140264174335808 run_lib.py:153] step: 782200, training_loss: 1.36621e+02
I1112 17:07:31.147398 140264174335808 run_lib.py:153] step: 782250, training_loss: 1.53986e+02
I1112 17:07:41.336886 140264174335808 run_lib.py:153] step: 782300, training_loss: 1.45165e+02
I1112 17:07:51.823433 140264174335808 run_lib.py:153] step: 782350, training_loss: 1.35632e+02
I1112 17:08:02.543510 140264174335808 run_lib.py:153] step: 782400, training_loss: 1.16682e+02
I1112 17:08:12.917814 140264174335808 run_lib.py:153] step: 782450, training_loss: 8.92748e+01
I1112 17:08:23.456560 140264174335808 run_lib.py:153] step: 782500, training_loss: 9.84248e+01
I1112 17:08:33.928083 140264174335808 run_lib.py:153] step: 782550, training_loss: 1.29265e+02
I1112 17:08:43.508905 140264174335808 run_lib.py:153] step: 782600, training_loss: 1.10661e+02
I1112 17:08:53.310690 140264174335808 run_lib.py:153] step: 782650, training_loss: 9.56243e+01
I1112 17:09:03.302095 140264174335808 run_lib.py:153] step: 782700, training_loss: 1.34599e+02
I1112 17:09:13.958184 140264174335808 run_lib.py:153] step: 782750, training_loss: 1.34751e+02
I1112 17:09:23.741220 140264174335808 run_lib.py:153] step: 782800, training_loss: 1.33721e+02
I1112 17:09:34.199948 140264174335808 run_lib.py:153] step: 782850, training_loss: 1.24588e+02
I1112 17:09:43.746565 140264174335808 run_lib.py:153] step: 782900, training_loss: 1.10614e+02
I1112 17:09:54.329746 140264174335808 run_lib.py:153] step: 782950, training_loss: 1.76345e+02
I1112 17:10:05.220343 140264174335808 run_lib.py:153] step: 783000, training_loss: 1.08380e+02
I1112 17:10:15.233264 140264174335808 run_lib.py:153] step: 783050, training_loss: 1.38344e+02
I1112 17:10:25.199782 140264174335808 run_lib.py:153] step: 783100, training_loss: 8.88390e+01
I1112 17:10:35.026957 140264174335808 run_lib.py:153] step: 783150, training_loss: 1.19874e+02
I1112 17:10:44.781208 140264174335808 run_lib.py:153] step: 783200, training_loss: 9.31519e+01
I1112 17:10:55.139403 140264174335808 run_lib.py:153] step: 783250, training_loss: 1.22911e+02
I1112 17:11:05.651626 140264174335808 run_lib.py:153] step: 783300, training_loss: 1.05028e+02
I1112 17:11:15.268498 140264174335808 run_lib.py:153] step: 783350, training_loss: 1.34233e+02
I1112 17:11:26.057353 140264174335808 run_lib.py:153] step: 783400, training_loss: 1.50983e+02
I1112 17:11:35.629010 140264174335808 run_lib.py:153] step: 783450, training_loss: 1.32300e+02
I1112 17:11:45.110226 140264174335808 run_lib.py:153] step: 783500, training_loss: 1.24741e+02
I1112 17:11:55.450657 140264174335808 run_lib.py:153] step: 783550, training_loss: 1.18190e+02
I1112 17:12:06.106076 140264174335808 run_lib.py:153] step: 783600, training_loss: 1.18518e+02
I1112 17:12:15.537330 140264174335808 run_lib.py:153] step: 783650, training_loss: 1.49401e+02
I1112 17:12:25.798565 140264174335808 run_lib.py:153] step: 783700, training_loss: 1.49030e+02
I1112 17:12:36.269012 140264174335808 run_lib.py:153] step: 783750, training_loss: 1.28186e+02
I1112 17:12:46.215021 140264174335808 run_lib.py:153] step: 783800, training_loss: 1.26779e+02
I1112 17:12:57.004185 140264174335808 run_lib.py:153] step: 783850, training_loss: 1.31307e+02
I1112 17:13:07.444258 140264174335808 run_lib.py:153] step: 783900, training_loss: 1.09836e+02
I1112 17:13:17.640105 140264174335808 run_lib.py:153] step: 783950, training_loss: 1.25290e+02
I1112 17:13:27.310450 140264174335808 run_lib.py:153] step: 784000, training_loss: 1.69726e+02
I1112 17:13:38.218917 140264174335808 run_lib.py:153] step: 784050, training_loss: 1.23645e+02
I1112 17:13:48.633971 140264174335808 run_lib.py:153] step: 784100, training_loss: 1.41738e+02
I1112 17:13:58.786495 140264174335808 run_lib.py:153] step: 784150, training_loss: 1.18164e+02
I1112 17:14:08.636031 140264174335808 run_lib.py:153] step: 784200, training_loss: 1.02930e+02
I1112 17:14:18.931076 140264174335808 run_lib.py:153] step: 784250, training_loss: 1.04591e+02
I1112 17:14:29.101448 140264174335808 run_lib.py:153] step: 784300, training_loss: 1.34548e+02
I1112 17:14:38.912694 140264174335808 run_lib.py:153] step: 784350, training_loss: 1.31290e+02
I1112 17:14:48.699720 140264174335808 run_lib.py:153] step: 784400, training_loss: 1.22538e+02
I1112 17:14:58.987340 140264174335808 run_lib.py:153] step: 784450, training_loss: 1.26302e+02
I1112 17:15:08.923605 140264174335808 run_lib.py:153] step: 784500, training_loss: 1.20576e+02
I1112 17:15:18.727924 140264174335808 run_lib.py:153] step: 784550, training_loss: 1.52875e+02
I1112 17:15:28.825144 140264174335808 run_lib.py:153] step: 784600, training_loss: 1.61980e+02
I1112 17:15:39.325905 140264174335808 run_lib.py:153] step: 784650, training_loss: 1.36148e+02
I1112 17:15:49.921872 140264174335808 run_lib.py:153] step: 784700, training_loss: 1.17255e+02
I1112 17:16:00.452091 140264174335808 run_lib.py:153] step: 784750, training_loss: 1.21170e+02
I1112 17:16:11.075295 140264174335808 run_lib.py:153] step: 784800, training_loss: 1.25381e+02
I1112 17:16:21.653113 140264174335808 run_lib.py:153] step: 784850, training_loss: 1.47847e+02
I1112 17:16:31.940215 140264174335808 run_lib.py:153] step: 784900, training_loss: 1.38248e+02
I1112 17:16:42.156108 140264174335808 run_lib.py:153] step: 784950, training_loss: 1.43820e+02
I1112 17:16:52.163363 140264174335808 run_lib.py:153] step: 785000, training_loss: 1.22268e+02
I1112 17:16:52.263740 140264174335808 run_lib.py:166] step: 785000, eval_loss: 1.05744e+02
I1112 17:17:02.544314 140264174335808 run_lib.py:153] step: 785050, training_loss: 1.35261e+02
I1112 17:17:12.823993 140264174335808 run_lib.py:153] step: 785100, training_loss: 1.21262e+02
I1112 17:17:22.746870 140264174335808 run_lib.py:153] step: 785150, training_loss: 1.33978e+02
I1112 17:17:32.699636 140264174335808 run_lib.py:153] step: 785200, training_loss: 1.23705e+02
I1112 17:17:42.244957 140264174335808 run_lib.py:153] step: 785250, training_loss: 1.35929e+02
I1112 17:17:52.059270 140264174335808 run_lib.py:153] step: 785300, training_loss: 1.41816e+02
I1112 17:18:02.310758 140264174335808 run_lib.py:153] step: 785350, training_loss: 1.29997e+02
I1112 17:18:12.278670 140264174335808 run_lib.py:153] step: 785400, training_loss: 1.13149e+02
I1112 17:18:22.201186 140264174335808 run_lib.py:153] step: 785450, training_loss: 1.21031e+02
I1112 17:18:32.258838 140264174335808 run_lib.py:153] step: 785500, training_loss: 8.46629e+01
I1112 17:18:42.367076 140264174335808 run_lib.py:153] step: 785550, training_loss: 1.22471e+02
I1112 17:18:53.131149 140264174335808 run_lib.py:153] step: 785600, training_loss: 1.19377e+02
I1112 17:19:02.602325 140264174335808 run_lib.py:153] step: 785650, training_loss: 9.33281e+01
I1112 17:19:13.083311 140264174335808 run_lib.py:153] step: 785700, training_loss: 1.23729e+02
I1112 17:19:23.504385 140264174335808 run_lib.py:153] step: 785750, training_loss: 1.28016e+02
I1112 17:19:33.583827 140264174335808 run_lib.py:153] step: 785800, training_loss: 1.38050e+02
I1112 17:19:43.913612 140264174335808 run_lib.py:153] step: 785850, training_loss: 8.44395e+01
I1112 17:19:53.518052 140264174335808 run_lib.py:153] step: 785900, training_loss: 1.29529e+02
I1112 17:20:04.069131 140264174335808 run_lib.py:153] step: 785950, training_loss: 1.37400e+02
I1112 17:20:13.910237 140264174335808 run_lib.py:153] step: 786000, training_loss: 1.09394e+02
I1112 17:20:23.871439 140264174335808 run_lib.py:153] step: 786050, training_loss: 1.63041e+02
I1112 17:20:34.062404 140264174335808 run_lib.py:153] step: 786100, training_loss: 9.89767e+01
I1112 17:20:44.637709 140264174335808 run_lib.py:153] step: 786150, training_loss: 1.16782e+02
I1112 17:20:54.406104 140264174335808 run_lib.py:153] step: 786200, training_loss: 1.18722e+02
I1112 17:21:04.638902 140264174335808 run_lib.py:153] step: 786250, training_loss: 1.17829e+02
I1112 17:21:14.155287 140264174335808 run_lib.py:153] step: 786300, training_loss: 1.17408e+02
I1112 17:21:23.968010 140264174335808 run_lib.py:153] step: 786350, training_loss: 1.27428e+02
I1112 17:21:34.685771 140264174335808 run_lib.py:153] step: 786400, training_loss: 1.04543e+02
I1112 17:21:44.856602 140264174335808 run_lib.py:153] step: 786450, training_loss: 1.28323e+02
I1112 17:21:54.887530 140264174335808 run_lib.py:153] step: 786500, training_loss: 1.44503e+02
I1112 17:22:05.247143 140264174335808 run_lib.py:153] step: 786550, training_loss: 1.28540e+02
I1112 17:22:15.341953 140264174335808 run_lib.py:153] step: 786600, training_loss: 1.23856e+02
I1112 17:22:25.582172 140264174335808 run_lib.py:153] step: 786650, training_loss: 1.16464e+02
I1112 17:22:35.439133 140264174335808 run_lib.py:153] step: 786700, training_loss: 1.50132e+02
I1112 17:22:45.033366 140264174335808 run_lib.py:153] step: 786750, training_loss: 1.05059e+02
I1112 17:22:55.352178 140264174335808 run_lib.py:153] step: 786800, training_loss: 1.41145e+02
I1112 17:23:05.596720 140264174335808 run_lib.py:153] step: 786850, training_loss: 1.25918e+02
I1112 17:23:15.330955 140264174335808 run_lib.py:153] step: 786900, training_loss: 1.35244e+02
I1112 17:23:26.426637 140264174335808 run_lib.py:153] step: 786950, training_loss: 8.60682e+01
I1112 17:23:36.647601 140264174335808 run_lib.py:153] step: 787000, training_loss: 1.40079e+02
I1112 17:23:47.050166 140264174335808 run_lib.py:153] step: 787050, training_loss: 1.18170e+02
I1112 17:23:56.994734 140264174335808 run_lib.py:153] step: 787100, training_loss: 1.22986e+02
I1112 17:24:07.507549 140264174335808 run_lib.py:153] step: 787150, training_loss: 1.12683e+02
I1112 17:24:17.880935 140264174335808 run_lib.py:153] step: 787200, training_loss: 1.01175e+02
I1112 17:24:28.803589 140264174335808 run_lib.py:153] step: 787250, training_loss: 1.14177e+02
I1112 17:24:39.335468 140264174335808 run_lib.py:153] step: 787300, training_loss: 1.19464e+02
I1112 17:24:49.581610 140264174335808 run_lib.py:153] step: 787350, training_loss: 1.10304e+02
I1112 17:25:00.065920 140264174335808 run_lib.py:153] step: 787400, training_loss: 1.52396e+02
I1112 17:25:10.401228 140264174335808 run_lib.py:153] step: 787450, training_loss: 1.30664e+02
I1112 17:25:20.708506 140264174335808 run_lib.py:153] step: 787500, training_loss: 1.24581e+02
I1112 17:25:30.706360 140264174335808 run_lib.py:153] step: 787550, training_loss: 1.21304e+02
I1112 17:25:40.982403 140264174335808 run_lib.py:153] step: 787600, training_loss: 1.21987e+02
I1112 17:25:50.895343 140264174335808 run_lib.py:153] step: 787650, training_loss: 9.75201e+01
I1112 17:26:01.416520 140264174335808 run_lib.py:153] step: 787700, training_loss: 1.32669e+02
I1112 17:26:11.760607 140264174335808 run_lib.py:153] step: 787750, training_loss: 1.03746e+02
I1112 17:26:22.534554 140264174335808 run_lib.py:153] step: 787800, training_loss: 1.12208e+02
I1112 17:26:33.313276 140264174335808 run_lib.py:153] step: 787850, training_loss: 1.08427e+02
I1112 17:26:43.824085 140264174335808 run_lib.py:153] step: 787900, training_loss: 1.47887e+02
I1112 17:26:54.103783 140264174335808 run_lib.py:153] step: 787950, training_loss: 1.30093e+02
I1112 17:27:04.080889 140264174335808 run_lib.py:153] step: 788000, training_loss: 1.14803e+02
I1112 17:27:14.345258 140264174335808 run_lib.py:153] step: 788050, training_loss: 1.21791e+02
I1112 17:27:23.789588 140264174335808 run_lib.py:153] step: 788100, training_loss: 1.06318e+02
I1112 17:27:34.480724 140264174335808 run_lib.py:153] step: 788150, training_loss: 1.31492e+02
I1112 17:27:43.994046 140264174335808 run_lib.py:153] step: 788200, training_loss: 1.16903e+02
I1112 17:27:54.720986 140264174335808 run_lib.py:153] step: 788250, training_loss: 1.17292e+02
I1112 17:28:04.196354 140264174335808 run_lib.py:153] step: 788300, training_loss: 1.47705e+02
I1112 17:28:14.405590 140264174335808 run_lib.py:153] step: 788350, training_loss: 1.31947e+02
I1112 17:28:24.220658 140264174335808 run_lib.py:153] step: 788400, training_loss: 1.37668e+02
I1112 17:28:34.399137 140264174335808 run_lib.py:153] step: 788450, training_loss: 1.14002e+02
I1112 17:28:44.651465 140264174335808 run_lib.py:153] step: 788500, training_loss: 1.08960e+02
I1112 17:28:54.692114 140264174335808 run_lib.py:153] step: 788550, training_loss: 1.01164e+02
I1112 17:29:04.467752 140264174335808 run_lib.py:153] step: 788600, training_loss: 1.21366e+02
I1112 17:29:14.029072 140264174335808 run_lib.py:153] step: 788650, training_loss: 1.26465e+02
I1112 17:29:24.012994 140264174335808 run_lib.py:153] step: 788700, training_loss: 1.38490e+02
I1112 17:29:34.576329 140264174335808 run_lib.py:153] step: 788750, training_loss: 1.34256e+02
I1112 17:29:45.037035 140264174335808 run_lib.py:153] step: 788800, training_loss: 1.31714e+02
I1112 17:29:55.220546 140264174335808 run_lib.py:153] step: 788850, training_loss: 1.21478e+02
I1112 17:30:05.456174 140264174335808 run_lib.py:153] step: 788900, training_loss: 1.34731e+02
I1112 17:30:15.762725 140264174335808 run_lib.py:153] step: 788950, training_loss: 1.41271e+02
I1112 17:30:25.587152 140264174335808 run_lib.py:153] step: 789000, training_loss: 1.21803e+02
I1112 17:30:35.859788 140264174335808 run_lib.py:153] step: 789050, training_loss: 1.29127e+02
I1112 17:30:46.199142 140264174335808 run_lib.py:153] step: 789100, training_loss: 1.02816e+02
I1112 17:30:56.438749 140264174335808 run_lib.py:153] step: 789150, training_loss: 1.11689e+02
I1112 17:31:06.363389 140264174335808 run_lib.py:153] step: 789200, training_loss: 1.06074e+02
I1112 17:31:15.811135 140264174335808 run_lib.py:153] step: 789250, training_loss: 1.10094e+02
I1112 17:31:25.147109 140264174335808 run_lib.py:153] step: 789300, training_loss: 1.10061e+02
I1112 17:31:35.125027 140264174335808 run_lib.py:153] step: 789350, training_loss: 1.40737e+02
I1112 17:31:45.285467 140264174335808 run_lib.py:153] step: 789400, training_loss: 1.24358e+02
I1112 17:31:54.591919 140264174335808 run_lib.py:153] step: 789450, training_loss: 1.47557e+02
I1112 17:32:04.875940 140264174335808 run_lib.py:153] step: 789500, training_loss: 1.33516e+02
I1112 17:32:14.971263 140264174335808 run_lib.py:153] step: 789550, training_loss: 1.33227e+02
I1112 17:32:24.892692 140264174335808 run_lib.py:153] step: 789600, training_loss: 1.17740e+02
I1112 17:32:34.460376 140264174335808 run_lib.py:153] step: 789650, training_loss: 1.42879e+02
I1112 17:32:43.999357 140264174335808 run_lib.py:153] step: 789700, training_loss: 1.29011e+02
I1112 17:32:53.932326 140264174335808 run_lib.py:153] step: 789750, training_loss: 1.32919e+02
I1112 17:33:03.725329 140264174335808 run_lib.py:153] step: 789800, training_loss: 1.29791e+02
I1112 17:33:13.242419 140264174335808 run_lib.py:153] step: 789850, training_loss: 1.12966e+02
I1112 17:33:23.450693 140264174335808 run_lib.py:153] step: 789900, training_loss: 1.28376e+02
I1112 17:33:32.881118 140264174335808 run_lib.py:153] step: 789950, training_loss: 1.09377e+02
I1112 17:33:43.141001 140264174335808 run_lib.py:153] step: 790000, training_loss: 1.44335e+02
I1112 17:33:43.713299 140264174335808 run_lib.py:166] step: 790000, eval_loss: 1.09359e+02
I1112 17:33:53.409746 140264174335808 run_lib.py:153] step: 790050, training_loss: 1.31357e+02
I1112 17:34:03.242401 140264174335808 run_lib.py:153] step: 790100, training_loss: 1.05519e+02
I1112 17:34:13.211125 140264174335808 run_lib.py:153] step: 790150, training_loss: 1.13278e+02
I1112 17:34:23.237944 140264174335808 run_lib.py:153] step: 790200, training_loss: 9.17566e+01
I1112 17:34:33.905846 140264174335808 run_lib.py:153] step: 790250, training_loss: 1.14860e+02
I1112 17:34:43.570013 140264174335808 run_lib.py:153] step: 790300, training_loss: 1.19915e+02
I1112 17:34:53.374905 140264174335808 run_lib.py:153] step: 790350, training_loss: 1.31094e+02
I1112 17:35:04.350941 140264174335808 run_lib.py:153] step: 790400, training_loss: 1.17300e+02
I1112 17:35:14.682227 140264174335808 run_lib.py:153] step: 790450, training_loss: 1.25910e+02
I1112 17:35:24.430656 140264174335808 run_lib.py:153] step: 790500, training_loss: 1.18903e+02
I1112 17:35:34.292468 140264174335808 run_lib.py:153] step: 790550, training_loss: 1.43363e+02
I1112 17:35:44.692225 140264174335808 run_lib.py:153] step: 790600, training_loss: 1.14063e+02
I1112 17:35:54.359305 140264174335808 run_lib.py:153] step: 790650, training_loss: 1.26293e+02
I1112 17:36:03.964331 140264174335808 run_lib.py:153] step: 790700, training_loss: 1.21377e+02
I1112 17:36:14.086224 140264174335808 run_lib.py:153] step: 790750, training_loss: 9.48140e+01
I1112 17:36:23.626926 140264174335808 run_lib.py:153] step: 790800, training_loss: 1.13426e+02
I1112 17:36:32.894693 140264174335808 run_lib.py:153] step: 790850, training_loss: 1.16548e+02
I1112 17:36:42.681896 140264174335808 run_lib.py:153] step: 790900, training_loss: 1.29616e+02
I1112 17:36:52.807030 140264174335808 run_lib.py:153] step: 790950, training_loss: 1.04559e+02
I1112 17:37:03.085396 140264174335808 run_lib.py:153] step: 791000, training_loss: 1.54037e+02
I1112 17:37:13.572663 140264174335808 run_lib.py:153] step: 791050, training_loss: 1.19361e+02
I1112 17:37:23.974042 140264174335808 run_lib.py:153] step: 791100, training_loss: 1.15415e+02
I1112 17:37:34.135479 140264174335808 run_lib.py:153] step: 791150, training_loss: 1.43692e+02
I1112 17:37:44.101132 140264174335808 run_lib.py:153] step: 791200, training_loss: 1.27124e+02
I1112 17:37:53.728635 140264174335808 run_lib.py:153] step: 791250, training_loss: 1.19377e+02
I1112 17:38:03.481256 140264174335808 run_lib.py:153] step: 791300, training_loss: 1.14083e+02
I1112 17:38:13.654752 140264174335808 run_lib.py:153] step: 791350, training_loss: 1.34756e+02
I1112 17:38:23.397641 140264174335808 run_lib.py:153] step: 791400, training_loss: 1.32470e+02
I1112 17:38:33.191909 140264174335808 run_lib.py:153] step: 791450, training_loss: 1.43409e+02
I1112 17:38:42.924229 140264174335808 run_lib.py:153] step: 791500, training_loss: 1.53983e+02
I1112 17:38:52.595739 140264174335808 run_lib.py:153] step: 791550, training_loss: 1.05687e+02
I1112 17:39:02.607368 140264174335808 run_lib.py:153] step: 791600, training_loss: 1.25512e+02
I1112 17:39:12.769079 140264174335808 run_lib.py:153] step: 791650, training_loss: 1.17224e+02
I1112 17:39:22.544880 140264174335808 run_lib.py:153] step: 791700, training_loss: 1.31133e+02
I1112 17:39:32.409960 140264174335808 run_lib.py:153] step: 791750, training_loss: 1.41355e+02
I1112 17:39:42.158704 140264174335808 run_lib.py:153] step: 791800, training_loss: 1.65558e+02
I1112 17:39:51.987929 140264174335808 run_lib.py:153] step: 791850, training_loss: 1.07221e+02
I1112 17:40:01.662187 140264174335808 run_lib.py:153] step: 791900, training_loss: 1.34620e+02
I1112 17:40:11.190985 140264174335808 run_lib.py:153] step: 791950, training_loss: 1.17071e+02
I1112 17:40:21.638102 140264174335808 run_lib.py:153] step: 792000, training_loss: 1.21006e+02
I1112 17:40:31.466829 140264174335808 run_lib.py:153] step: 792050, training_loss: 8.93954e+01
I1112 17:40:41.083213 140264174335808 run_lib.py:153] step: 792100, training_loss: 1.32476e+02
I1112 17:40:50.965224 140264174335808 run_lib.py:153] step: 792150, training_loss: 1.10468e+02
I1112 17:41:00.246864 140264174335808 run_lib.py:153] step: 792200, training_loss: 1.29691e+02
I1112 17:41:10.157557 140264174335808 run_lib.py:153] step: 792250, training_loss: 1.33601e+02
I1112 17:41:20.261325 140264174335808 run_lib.py:153] step: 792300, training_loss: 1.35180e+02
I1112 17:41:29.952848 140264174335808 run_lib.py:153] step: 792350, training_loss: 1.09470e+02
I1112 17:41:39.640185 140264174335808 run_lib.py:153] step: 792400, training_loss: 1.21940e+02
I1112 17:41:50.137973 140264174335808 run_lib.py:153] step: 792450, training_loss: 1.44729e+02
I1112 17:42:00.830600 140264174335808 run_lib.py:153] step: 792500, training_loss: 1.09126e+02
I1112 17:42:10.504149 140264174335808 run_lib.py:153] step: 792550, training_loss: 1.32227e+02
I1112 17:42:20.881290 140264174335808 run_lib.py:153] step: 792600, training_loss: 1.29269e+02
I1112 17:42:30.648610 140264174335808 run_lib.py:153] step: 792650, training_loss: 1.30819e+02
I1112 17:42:40.349386 140264174335808 run_lib.py:153] step: 792700, training_loss: 1.26621e+02
I1112 17:42:50.306492 140264174335808 run_lib.py:153] step: 792750, training_loss: 9.69803e+01
I1112 17:43:00.920635 140264174335808 run_lib.py:153] step: 792800, training_loss: 1.50976e+02
I1112 17:43:11.392345 140264174335808 run_lib.py:153] step: 792850, training_loss: 1.44987e+02
I1112 17:43:22.408442 140264174335808 run_lib.py:153] step: 792900, training_loss: 1.28677e+02
I1112 17:43:32.594797 140264174335808 run_lib.py:153] step: 792950, training_loss: 1.29892e+02
I1112 17:43:43.445857 140264174335808 run_lib.py:153] step: 793000, training_loss: 1.22001e+02
I1112 17:43:53.409847 140264174335808 run_lib.py:153] step: 793050, training_loss: 1.48407e+02
I1112 17:44:02.988701 140264174335808 run_lib.py:153] step: 793100, training_loss: 1.09021e+02
I1112 17:44:12.487377 140264174335808 run_lib.py:153] step: 793150, training_loss: 1.32384e+02
I1112 17:44:22.620489 140264174335808 run_lib.py:153] step: 793200, training_loss: 1.12161e+02
I1112 17:44:32.729306 140264174335808 run_lib.py:153] step: 793250, training_loss: 1.34402e+02
I1112 17:44:43.100042 140264174335808 run_lib.py:153] step: 793300, training_loss: 1.15045e+02
I1112 17:44:53.953507 140264174335808 run_lib.py:153] step: 793350, training_loss: 1.07004e+02
I1112 17:45:04.777320 140264174335808 run_lib.py:153] step: 793400, training_loss: 1.00882e+02
I1112 17:45:15.368837 140264174335808 run_lib.py:153] step: 793450, training_loss: 9.99069e+01
I1112 17:45:25.688217 140264174335808 run_lib.py:153] step: 793500, training_loss: 1.33865e+02
I1112 17:45:35.443950 140264174335808 run_lib.py:153] step: 793550, training_loss: 1.15480e+02
I1112 17:45:45.733830 140264174335808 run_lib.py:153] step: 793600, training_loss: 1.03847e+02
I1112 17:45:55.465787 140264174335808 run_lib.py:153] step: 793650, training_loss: 1.40151e+02
I1112 17:46:05.189926 140264174335808 run_lib.py:153] step: 793700, training_loss: 1.26532e+02
I1112 17:46:14.627309 140264174335808 run_lib.py:153] step: 793750, training_loss: 1.26776e+02
I1112 17:46:24.102848 140264174335808 run_lib.py:153] step: 793800, training_loss: 1.30968e+02
I1112 17:46:33.985469 140264174335808 run_lib.py:153] step: 793850, training_loss: 1.41302e+02
I1112 17:46:43.864638 140264174335808 run_lib.py:153] step: 793900, training_loss: 1.24068e+02
I1112 17:46:53.624509 140264174335808 run_lib.py:153] step: 793950, training_loss: 1.22202e+02
I1112 17:47:04.416961 140264174335808 run_lib.py:153] step: 794000, training_loss: 1.45606e+02
I1112 17:47:14.934923 140264174335808 run_lib.py:153] step: 794050, training_loss: 1.29045e+02
I1112 17:47:25.258985 140264174335808 run_lib.py:153] step: 794100, training_loss: 1.41501e+02
I1112 17:47:35.221795 140264174335808 run_lib.py:153] step: 794150, training_loss: 1.23215e+02
I1112 17:47:44.771262 140264174335808 run_lib.py:153] step: 794200, training_loss: 9.98361e+01
I1112 17:47:55.064946 140264174335808 run_lib.py:153] step: 794250, training_loss: 1.13884e+02
I1112 17:48:05.311997 140264174335808 run_lib.py:153] step: 794300, training_loss: 1.05778e+02
I1112 17:48:15.027158 140264174335808 run_lib.py:153] step: 794350, training_loss: 1.29345e+02
I1112 17:48:24.696526 140264174335808 run_lib.py:153] step: 794400, training_loss: 1.29673e+02
I1112 17:48:34.472037 140264174335808 run_lib.py:153] step: 794450, training_loss: 1.18847e+02
I1112 17:48:44.228895 140264174335808 run_lib.py:153] step: 794500, training_loss: 1.32046e+02
I1112 17:48:54.411502 140264174335808 run_lib.py:153] step: 794550, training_loss: 1.40446e+02
I1112 17:49:03.811898 140264174335808 run_lib.py:153] step: 794600, training_loss: 1.45629e+02
I1112 17:49:13.301451 140264174335808 run_lib.py:153] step: 794650, training_loss: 1.22997e+02
I1112 17:49:23.872402 140264174335808 run_lib.py:153] step: 794700, training_loss: 1.14667e+02
I1112 17:49:34.212265 140264174335808 run_lib.py:153] step: 794750, training_loss: 1.39532e+02
I1112 17:49:44.693353 140264174335808 run_lib.py:153] step: 794800, training_loss: 1.33799e+02
I1112 17:49:54.856612 140264174335808 run_lib.py:153] step: 794850, training_loss: 9.94094e+01
I1112 17:50:04.519332 140264174335808 run_lib.py:153] step: 794900, training_loss: 1.47845e+02
I1112 17:50:14.504001 140264174335808 run_lib.py:153] step: 794950, training_loss: 1.17225e+02
I1112 17:50:24.618366 140264174335808 run_lib.py:153] step: 795000, training_loss: 7.94358e+01
I1112 17:50:24.761518 140264174335808 run_lib.py:166] step: 795000, eval_loss: 1.16117e+02
I1112 17:50:34.680500 140264174335808 run_lib.py:153] step: 795050, training_loss: 1.02321e+02
I1112 17:50:45.232701 140264174335808 run_lib.py:153] step: 795100, training_loss: 1.21758e+02
I1112 17:50:55.750140 140264174335808 run_lib.py:153] step: 795150, training_loss: 1.37252e+02
I1112 17:51:05.805011 140264174335808 run_lib.py:153] step: 795200, training_loss: 1.50911e+02
I1112 17:51:16.271463 140264174335808 run_lib.py:153] step: 795250, training_loss: 1.20214e+02
I1112 17:51:25.966385 140264174335808 run_lib.py:153] step: 795300, training_loss: 1.37328e+02
I1112 17:51:36.676020 140264174335808 run_lib.py:153] step: 795350, training_loss: 1.46554e+02
I1112 17:51:47.048311 140264174335808 run_lib.py:153] step: 795400, training_loss: 9.95726e+01
I1112 17:51:56.709771 140264174335808 run_lib.py:153] step: 795450, training_loss: 1.19562e+02
I1112 17:52:06.639338 140264174335808 run_lib.py:153] step: 795500, training_loss: 1.53245e+02
I1112 17:52:16.399018 140264174335808 run_lib.py:153] step: 795550, training_loss: 1.05878e+02
I1112 17:52:26.663935 140264174335808 run_lib.py:153] step: 795600, training_loss: 1.64829e+02
I1112 17:52:36.293266 140264174335808 run_lib.py:153] step: 795650, training_loss: 1.60713e+02
I1112 17:52:45.831588 140264174335808 run_lib.py:153] step: 795700, training_loss: 1.31544e+02
I1112 17:52:55.510018 140264174335808 run_lib.py:153] step: 795750, training_loss: 1.24279e+02
I1112 17:53:04.962107 140264174335808 run_lib.py:153] step: 795800, training_loss: 1.49234e+02
I1112 17:53:14.481819 140264174335808 run_lib.py:153] step: 795850, training_loss: 1.16406e+02
I1112 17:53:24.078408 140264174335808 run_lib.py:153] step: 795900, training_loss: 1.39245e+02
I1112 17:53:33.354895 140264174335808 run_lib.py:153] step: 795950, training_loss: 1.62966e+02
I1112 17:53:43.002546 140264174335808 run_lib.py:153] step: 796000, training_loss: 1.39991e+02
I1112 17:53:52.647799 140264174335808 run_lib.py:153] step: 796050, training_loss: 1.05746e+02
I1112 17:54:02.887003 140264174335808 run_lib.py:153] step: 796100, training_loss: 1.22969e+02
I1112 17:54:13.362589 140264174335808 run_lib.py:153] step: 796150, training_loss: 1.18235e+02
I1112 17:54:23.185219 140264174335808 run_lib.py:153] step: 796200, training_loss: 1.38950e+02
I1112 17:54:33.814546 140264174335808 run_lib.py:153] step: 796250, training_loss: 1.04985e+02
I1112 17:54:43.226357 140264174335808 run_lib.py:153] step: 796300, training_loss: 1.49393e+02
I1112 17:54:52.653939 140264174335808 run_lib.py:153] step: 796350, training_loss: 1.18941e+02
I1112 17:55:02.174269 140264174335808 run_lib.py:153] step: 796400, training_loss: 1.25355e+02
I1112 17:55:12.342048 140264174335808 run_lib.py:153] step: 796450, training_loss: 1.42143e+02
I1112 17:55:22.037461 140264174335808 run_lib.py:153] step: 796500, training_loss: 1.24472e+02
I1112 17:55:31.642911 140264174335808 run_lib.py:153] step: 796550, training_loss: 1.43162e+02
I1112 17:55:41.346873 140264174335808 run_lib.py:153] step: 796600, training_loss: 1.12258e+02
I1112 17:55:50.921975 140264174335808 run_lib.py:153] step: 796650, training_loss: 1.01715e+02
I1112 17:56:00.902100 140264174335808 run_lib.py:153] step: 796700, training_loss: 1.15280e+02
I1112 17:56:10.709615 140264174335808 run_lib.py:153] step: 796750, training_loss: 1.36980e+02
I1112 17:56:21.021438 140264174335808 run_lib.py:153] step: 796800, training_loss: 1.42198e+02
I1112 17:56:30.963715 140264174335808 run_lib.py:153] step: 796850, training_loss: 1.41421e+02
I1112 17:56:40.662503 140264174335808 run_lib.py:153] step: 796900, training_loss: 1.33694e+02
I1112 17:56:50.692853 140264174335808 run_lib.py:153] step: 796950, training_loss: 1.38354e+02
I1112 17:57:01.078492 140264174335808 run_lib.py:153] step: 797000, training_loss: 1.21351e+02
I1112 17:57:10.894093 140264174335808 run_lib.py:153] step: 797050, training_loss: 1.23370e+02
I1112 17:57:20.931712 140264174335808 run_lib.py:153] step: 797100, training_loss: 1.25738e+02
I1112 17:57:30.979709 140264174335808 run_lib.py:153] step: 797150, training_loss: 1.16759e+02
I1112 17:57:41.450673 140264174335808 run_lib.py:153] step: 797200, training_loss: 1.31912e+02
I1112 17:57:51.492069 140264174335808 run_lib.py:153] step: 797250, training_loss: 1.28518e+02
I1112 17:58:01.328407 140264174335808 run_lib.py:153] step: 797300, training_loss: 1.11428e+02
I1112 17:58:11.763199 140264174335808 run_lib.py:153] step: 797350, training_loss: 1.50186e+02
I1112 17:58:22.390973 140264174335808 run_lib.py:153] step: 797400, training_loss: 1.17470e+02
I1112 17:58:32.755901 140264174335808 run_lib.py:153] step: 797450, training_loss: 1.26927e+02
I1112 17:58:42.910376 140264174335808 run_lib.py:153] step: 797500, training_loss: 1.18189e+02
I1112 17:58:53.139441 140264174335808 run_lib.py:153] step: 797550, training_loss: 1.26030e+02
I1112 17:59:03.344878 140264174335808 run_lib.py:153] step: 797600, training_loss: 1.03013e+02
I1112 17:59:13.596426 140264174335808 run_lib.py:153] step: 797650, training_loss: 1.21770e+02
I1112 17:59:23.812690 140264174335808 run_lib.py:153] step: 797700, training_loss: 1.59284e+02
I1112 17:59:34.377836 140264174335808 run_lib.py:153] step: 797750, training_loss: 1.19226e+02
I1112 17:59:44.188831 140264174335808 run_lib.py:153] step: 797800, training_loss: 1.18734e+02
I1112 17:59:54.364096 140264174335808 run_lib.py:153] step: 797850, training_loss: 1.49663e+02
I1112 18:00:04.663064 140264174335808 run_lib.py:153] step: 797900, training_loss: 1.34682e+02
I1112 18:00:14.640639 140264174335808 run_lib.py:153] step: 797950, training_loss: 1.31676e+02
I1112 18:00:24.295715 140264174335808 run_lib.py:153] step: 798000, training_loss: 9.63321e+01
I1112 18:00:33.769757 140264174335808 run_lib.py:153] step: 798050, training_loss: 1.12157e+02
I1112 18:00:43.566830 140264174335808 run_lib.py:153] step: 798100, training_loss: 1.19422e+02
I1112 18:00:54.120656 140264174335808 run_lib.py:153] step: 798150, training_loss: 1.09302e+02
I1112 18:01:04.014463 140264174335808 run_lib.py:153] step: 798200, training_loss: 1.29436e+02
I1112 18:01:13.920319 140264174335808 run_lib.py:153] step: 798250, training_loss: 1.18021e+02
I1112 18:01:23.856069 140264174335808 run_lib.py:153] step: 798300, training_loss: 1.25798e+02
I1112 18:01:33.591792 140264174335808 run_lib.py:153] step: 798350, training_loss: 1.11455e+02
I1112 18:01:44.050112 140264174335808 run_lib.py:153] step: 798400, training_loss: 1.08626e+02
I1112 18:01:53.725781 140264174335808 run_lib.py:153] step: 798450, training_loss: 9.01304e+01
I1112 18:02:03.885827 140264174335808 run_lib.py:153] step: 798500, training_loss: 1.20630e+02
I1112 18:02:14.450241 140264174335808 run_lib.py:153] step: 798550, training_loss: 1.42508e+02
I1112 18:02:25.043738 140264174335808 run_lib.py:153] step: 798600, training_loss: 1.36284e+02
I1112 18:02:35.110941 140264174335808 run_lib.py:153] step: 798650, training_loss: 1.21958e+02
I1112 18:02:45.059002 140264174335808 run_lib.py:153] step: 798700, training_loss: 1.51420e+02
I1112 18:02:55.064415 140264174335808 run_lib.py:153] step: 798750, training_loss: 1.10087e+02
I1112 18:03:04.623988 140264174335808 run_lib.py:153] step: 798800, training_loss: 1.38665e+02
I1112 18:03:14.622023 140264174335808 run_lib.py:153] step: 798850, training_loss: 1.67313e+02
I1112 18:03:24.449210 140264174335808 run_lib.py:153] step: 798900, training_loss: 1.10729e+02
I1112 18:03:33.894883 140264174335808 run_lib.py:153] step: 798950, training_loss: 1.15184e+02
I1112 18:03:43.840095 140264174335808 run_lib.py:153] step: 799000, training_loss: 1.02195e+02
I1112 18:03:53.367901 140264174335808 run_lib.py:153] step: 799050, training_loss: 1.12173e+02
I1112 18:04:03.521260 140264174335808 run_lib.py:153] step: 799100, training_loss: 1.07429e+02
I1112 18:04:13.203402 140264174335808 run_lib.py:153] step: 799150, training_loss: 1.20150e+02
I1112 18:04:22.771377 140264174335808 run_lib.py:153] step: 799200, training_loss: 1.21885e+02
I1112 18:04:32.103783 140264174335808 run_lib.py:153] step: 799250, training_loss: 1.06256e+02
I1112 18:04:41.417846 140264174335808 run_lib.py:153] step: 799300, training_loss: 1.51531e+02
I1112 18:04:50.911065 140264174335808 run_lib.py:153] step: 799350, training_loss: 1.10003e+02
I1112 18:05:00.417151 140264174335808 run_lib.py:153] step: 799400, training_loss: 1.36732e+02
I1112 18:05:10.388417 140264174335808 run_lib.py:153] step: 799450, training_loss: 1.23894e+02
I1112 18:05:20.202659 140264174335808 run_lib.py:153] step: 799500, training_loss: 1.04586e+02
I1112 18:05:29.974149 140264174335808 run_lib.py:153] step: 799550, training_loss: 1.15912e+02
I1112 18:05:39.299726 140264174335808 run_lib.py:153] step: 799600, training_loss: 1.06504e+02
I1112 18:05:48.836523 140264174335808 run_lib.py:153] step: 799650, training_loss: 1.41932e+02
I1112 18:05:58.300818 140264174335808 run_lib.py:153] step: 799700, training_loss: 1.19385e+02
I1112 18:06:08.138879 140264174335808 run_lib.py:153] step: 799750, training_loss: 1.23542e+02
I1112 18:06:17.989979 140264174335808 run_lib.py:153] step: 799800, training_loss: 1.57854e+02
I1112 18:06:27.561638 140264174335808 run_lib.py:153] step: 799850, training_loss: 9.80398e+01
I1112 18:06:37.067949 140264174335808 run_lib.py:153] step: 799900, training_loss: 8.85480e+01
I1112 18:06:46.550301 140264174335808 run_lib.py:153] step: 799950, training_loss: 1.37863e+02
I1112 18:06:56.235012 140264174335808 run_lib.py:153] step: 800000, training_loss: 1.10817e+02
I1112 18:06:56.778991 140264174335808 run_lib.py:166] step: 800000, eval_loss: 1.10079e+02
I1112 18:07:06.767144 140264174335808 run_lib.py:153] step: 800050, training_loss: 1.09261e+02
I1112 18:07:16.370516 140264174335808 run_lib.py:153] step: 800100, training_loss: 1.43984e+02
I1112 18:07:26.333464 140264174335808 run_lib.py:153] step: 800150, training_loss: 1.30599e+02
I1112 18:07:35.790683 140264174335808 run_lib.py:153] step: 800200, training_loss: 1.31366e+02
I1112 18:07:46.387885 140264174335808 run_lib.py:153] step: 800250, training_loss: 1.41125e+02
I1112 18:07:56.163426 140264174335808 run_lib.py:153] step: 800300, training_loss: 9.13199e+01
I1112 18:08:06.371475 140264174335808 run_lib.py:153] step: 800350, training_loss: 1.39705e+02
I1112 18:08:16.623366 140264174335808 run_lib.py:153] step: 800400, training_loss: 1.24867e+02
I1112 18:08:26.817707 140264174335808 run_lib.py:153] step: 800450, training_loss: 1.17118e+02
I1112 18:08:36.543355 140264174335808 run_lib.py:153] step: 800500, training_loss: 1.47184e+02
I1112 18:08:46.059503 140264174335808 run_lib.py:153] step: 800550, training_loss: 1.34066e+02
I1112 18:08:55.306324 140264174335808 run_lib.py:153] step: 800600, training_loss: 1.35636e+02
I1112 18:09:04.958150 140264174335808 run_lib.py:153] step: 800650, training_loss: 1.44756e+02
I1112 18:09:14.582854 140264174335808 run_lib.py:153] step: 800700, training_loss: 1.28501e+02
I1112 18:09:24.434025 140264174335808 run_lib.py:153] step: 800750, training_loss: 1.14446e+02
I1112 18:09:34.084368 140264174335808 run_lib.py:153] step: 800800, training_loss: 1.57431e+02
I1112 18:09:43.407347 140264174335808 run_lib.py:153] step: 800850, training_loss: 1.40724e+02
I1112 18:09:53.500682 140264174335808 run_lib.py:153] step: 800900, training_loss: 1.50030e+02
I1112 18:10:02.881077 140264174335808 run_lib.py:153] step: 800950, training_loss: 1.16056e+02
I1112 18:10:12.834803 140264174335808 run_lib.py:153] step: 801000, training_loss: 1.34807e+02
I1112 18:10:22.593805 140264174335808 run_lib.py:153] step: 801050, training_loss: 1.08192e+02
I1112 18:10:32.956867 140264174335808 run_lib.py:153] step: 801100, training_loss: 1.37098e+02
I1112 18:10:42.799922 140264174335808 run_lib.py:153] step: 801150, training_loss: 1.37277e+02
I1112 18:10:52.170262 140264174335808 run_lib.py:153] step: 801200, training_loss: 1.00542e+02
I1112 18:11:01.990931 140264174335808 run_lib.py:153] step: 801250, training_loss: 1.03027e+02
I1112 18:11:12.071691 140264174335808 run_lib.py:153] step: 801300, training_loss: 1.16059e+02
I1112 18:11:22.273937 140264174335808 run_lib.py:153] step: 801350, training_loss: 1.25520e+02
I1112 18:11:32.022498 140264174335808 run_lib.py:153] step: 801400, training_loss: 1.46668e+02
I1112 18:11:42.756461 140264174335808 run_lib.py:153] step: 801450, training_loss: 1.43109e+02
I1112 18:11:53.014694 140264174335808 run_lib.py:153] step: 801500, training_loss: 1.23375e+02
I1112 18:12:02.770691 140264174335808 run_lib.py:153] step: 801550, training_loss: 1.05383e+02
I1112 18:12:12.505301 140264174335808 run_lib.py:153] step: 801600, training_loss: 1.11070e+02
I1112 18:12:22.408884 140264174335808 run_lib.py:153] step: 801650, training_loss: 1.33143e+02
I1112 18:12:32.212311 140264174335808 run_lib.py:153] step: 801700, training_loss: 1.21191e+02
I1112 18:12:42.151391 140264174335808 run_lib.py:153] step: 801750, training_loss: 1.20978e+02
I1112 18:12:51.473493 140264174335808 run_lib.py:153] step: 801800, training_loss: 1.34002e+02
I1112 18:13:01.170637 140264174335808 run_lib.py:153] step: 801850, training_loss: 1.04773e+02
I1112 18:13:10.715055 140264174335808 run_lib.py:153] step: 801900, training_loss: 1.04312e+02
I1112 18:13:20.053474 140264174335808 run_lib.py:153] step: 801950, training_loss: 1.36700e+02
I1112 18:13:29.261162 140264174335808 run_lib.py:153] step: 802000, training_loss: 1.12798e+02
I1112 18:13:38.782184 140264174335808 run_lib.py:153] step: 802050, training_loss: 1.01433e+02
I1112 18:13:48.587705 140264174335808 run_lib.py:153] step: 802100, training_loss: 1.26747e+02
I1112 18:13:58.591866 140264174335808 run_lib.py:153] step: 802150, training_loss: 1.08763e+02
I1112 18:14:08.734281 140264174335808 run_lib.py:153] step: 802200, training_loss: 1.25918e+02
I1112 18:14:19.317448 140264174335808 run_lib.py:153] step: 802250, training_loss: 1.24551e+02
I1112 18:14:29.331811 140264174335808 run_lib.py:153] step: 802300, training_loss: 1.38004e+02
I1112 18:14:40.362606 140264174335808 run_lib.py:153] step: 802350, training_loss: 1.20785e+02
I1112 18:14:50.790324 140264174335808 run_lib.py:153] step: 802400, training_loss: 1.27591e+02
I1112 18:15:01.056616 140264174335808 run_lib.py:153] step: 802450, training_loss: 1.21272e+02
I1112 18:15:10.865922 140264174335808 run_lib.py:153] step: 802500, training_loss: 1.18824e+02
I1112 18:15:20.105810 140264174335808 run_lib.py:153] step: 802550, training_loss: 1.23150e+02
I1112 18:15:29.414213 140264174335808 run_lib.py:153] step: 802600, training_loss: 1.10241e+02
I1112 18:15:39.233996 140264174335808 run_lib.py:153] step: 802650, training_loss: 1.23362e+02
I1112 18:15:48.657921 140264174335808 run_lib.py:153] step: 802700, training_loss: 1.49857e+02
I1112 18:15:57.876642 140264174335808 run_lib.py:153] step: 802750, training_loss: 1.45095e+02
I1112 18:16:07.398912 140264174335808 run_lib.py:153] step: 802800, training_loss: 1.25379e+02
I1112 18:16:16.907554 140264174335808 run_lib.py:153] step: 802850, training_loss: 1.07952e+02
I1112 18:16:26.545326 140264174335808 run_lib.py:153] step: 802900, training_loss: 1.30956e+02
I1112 18:16:37.414583 140264174335808 run_lib.py:153] step: 802950, training_loss: 1.36653e+02
I1112 18:16:47.049768 140264174335808 run_lib.py:153] step: 803000, training_loss: 1.25566e+02
I1112 18:16:56.930623 140264174335808 run_lib.py:153] step: 803050, training_loss: 1.11350e+02
I1112 18:17:07.761908 140264174335808 run_lib.py:153] step: 803100, training_loss: 1.02905e+02
I1112 18:17:18.294670 140264174335808 run_lib.py:153] step: 803150, training_loss: 1.29293e+02
I1112 18:17:27.988407 140264174335808 run_lib.py:153] step: 803200, training_loss: 1.23931e+02
I1112 18:17:38.051500 140264174335808 run_lib.py:153] step: 803250, training_loss: 1.26377e+02
I1112 18:17:48.463701 140264174335808 run_lib.py:153] step: 803300, training_loss: 1.34947e+02
I1112 18:17:58.221941 140264174335808 run_lib.py:153] step: 803350, training_loss: 1.20666e+02
I1112 18:18:07.743652 140264174335808 run_lib.py:153] step: 803400, training_loss: 1.23066e+02
I1112 18:18:18.025462 140264174335808 run_lib.py:153] step: 803450, training_loss: 1.51402e+02
I1112 18:18:28.631664 140264174335808 run_lib.py:153] step: 803500, training_loss: 1.30024e+02
I1112 18:18:39.147010 140264174335808 run_lib.py:153] step: 803550, training_loss: 1.49184e+02
I1112 18:18:49.125973 140264174335808 run_lib.py:153] step: 803600, training_loss: 1.08292e+02
I1112 18:18:58.909207 140264174335808 run_lib.py:153] step: 803650, training_loss: 9.51591e+01
I1112 18:19:08.841710 140264174335808 run_lib.py:153] step: 803700, training_loss: 1.29091e+02
I1112 18:19:19.002038 140264174335808 run_lib.py:153] step: 803750, training_loss: 1.30380e+02
I1112 18:19:29.039746 140264174335808 run_lib.py:153] step: 803800, training_loss: 1.36730e+02
I1112 18:19:38.837040 140264174335808 run_lib.py:153] step: 803850, training_loss: 1.21725e+02
I1112 18:19:49.467579 140264174335808 run_lib.py:153] step: 803900, training_loss: 1.47186e+02
I1112 18:19:59.220490 140264174335808 run_lib.py:153] step: 803950, training_loss: 1.08826e+02
I1112 18:20:09.449038 140264174335808 run_lib.py:153] step: 804000, training_loss: 9.58857e+01
I1112 18:20:20.237853 140264174335808 run_lib.py:153] step: 804050, training_loss: 1.14410e+02
I1112 18:20:30.105813 140264174335808 run_lib.py:153] step: 804100, training_loss: 1.25339e+02
I1112 18:20:40.520004 140264174335808 run_lib.py:153] step: 804150, training_loss: 1.27855e+02
I1112 18:20:50.284710 140264174335808 run_lib.py:153] step: 804200, training_loss: 1.37850e+02
I1112 18:20:59.726938 140264174335808 run_lib.py:153] step: 804250, training_loss: 1.05252e+02
I1112 18:21:09.384418 140264174335808 run_lib.py:153] step: 804300, training_loss: 1.40780e+02
I1112 18:21:18.703554 140264174335808 run_lib.py:153] step: 804350, training_loss: 1.32803e+02
I1112 18:21:28.227984 140264174335808 run_lib.py:153] step: 804400, training_loss: 1.25125e+02
I1112 18:21:37.893344 140264174335808 run_lib.py:153] step: 804450, training_loss: 1.47639e+02
I1112 18:21:47.232737 140264174335808 run_lib.py:153] step: 804500, training_loss: 1.18221e+02
I1112 18:21:56.645806 140264174335808 run_lib.py:153] step: 804550, training_loss: 1.27620e+02
I1112 18:22:06.294403 140264174335808 run_lib.py:153] step: 804600, training_loss: 1.30612e+02
I1112 18:22:16.391246 140264174335808 run_lib.py:153] step: 804650, training_loss: 1.38122e+02
I1112 18:22:27.232819 140264174335808 run_lib.py:153] step: 804700, training_loss: 1.14105e+02
I1112 18:22:36.843427 140264174335808 run_lib.py:153] step: 804750, training_loss: 1.26897e+02
I1112 18:22:46.406638 140264174335808 run_lib.py:153] step: 804800, training_loss: 1.44628e+02
I1112 18:22:55.675783 140264174335808 run_lib.py:153] step: 804850, training_loss: 1.31328e+02
I1112 18:23:05.173014 140264174335808 run_lib.py:153] step: 804900, training_loss: 1.41183e+02
I1112 18:23:14.634543 140264174335808 run_lib.py:153] step: 804950, training_loss: 1.37265e+02
I1112 18:23:24.685061 140264174335808 run_lib.py:153] step: 805000, training_loss: 1.40480e+02
I1112 18:23:24.823438 140264174335808 run_lib.py:166] step: 805000, eval_loss: 1.15211e+02
I1112 18:23:34.759943 140264174335808 run_lib.py:153] step: 805050, training_loss: 1.21129e+02
I1112 18:23:44.418689 140264174335808 run_lib.py:153] step: 805100, training_loss: 1.09712e+02
I1112 18:23:55.139428 140264174335808 run_lib.py:153] step: 805150, training_loss: 1.43553e+02
I1112 18:24:04.927022 140264174335808 run_lib.py:153] step: 805200, training_loss: 1.32326e+02
I1112 18:24:14.978696 140264174335808 run_lib.py:153] step: 805250, training_loss: 9.89944e+01
I1112 18:24:24.522628 140264174335808 run_lib.py:153] step: 805300, training_loss: 1.39865e+02
I1112 18:24:34.877996 140264174335808 run_lib.py:153] step: 805350, training_loss: 1.15672e+02
I1112 18:24:45.241649 140264174335808 run_lib.py:153] step: 805400, training_loss: 1.26259e+02
I1112 18:24:54.670318 140264174335808 run_lib.py:153] step: 805450, training_loss: 1.16828e+02
I1112 18:25:04.668385 140264174335808 run_lib.py:153] step: 805500, training_loss: 1.15327e+02
I1112 18:25:15.021502 140264174335808 run_lib.py:153] step: 805550, training_loss: 1.27230e+02
I1112 18:25:24.681472 140264174335808 run_lib.py:153] step: 805600, training_loss: 1.20622e+02
I1112 18:25:34.700296 140264174335808 run_lib.py:153] step: 805650, training_loss: 1.23827e+02
I1112 18:25:44.574528 140264174335808 run_lib.py:153] step: 805700, training_loss: 1.51062e+02
I1112 18:25:54.765727 140264174335808 run_lib.py:153] step: 805750, training_loss: 1.14838e+02
I1112 18:26:05.147552 140264174335808 run_lib.py:153] step: 805800, training_loss: 1.27982e+02
I1112 18:26:15.161977 140264174335808 run_lib.py:153] step: 805850, training_loss: 1.30295e+02
I1112 18:26:24.680094 140264174335808 run_lib.py:153] step: 805900, training_loss: 9.60497e+01
I1112 18:26:34.683972 140264174335808 run_lib.py:153] step: 805950, training_loss: 1.17144e+02
I1112 18:26:44.989466 140264174335808 run_lib.py:153] step: 806000, training_loss: 1.21577e+02
I1112 18:26:54.502674 140264174335808 run_lib.py:153] step: 806050, training_loss: 1.51040e+02
I1112 18:27:04.023245 140264174335808 run_lib.py:153] step: 806100, training_loss: 1.12186e+02
I1112 18:27:13.458014 140264174335808 run_lib.py:153] step: 806150, training_loss: 1.50554e+02
I1112 18:27:22.795008 140264174335808 run_lib.py:153] step: 806200, training_loss: 1.40939e+02
I1112 18:27:32.399677 140264174335808 run_lib.py:153] step: 806250, training_loss: 1.41271e+02
I1112 18:27:42.420711 140264174335808 run_lib.py:153] step: 806300, training_loss: 1.08006e+02
I1112 18:27:52.088895 140264174335808 run_lib.py:153] step: 806350, training_loss: 1.15823e+02
I1112 18:28:01.692808 140264174335808 run_lib.py:153] step: 806400, training_loss: 1.50148e+02
I1112 18:28:11.255512 140264174335808 run_lib.py:153] step: 806450, training_loss: 1.06142e+02
I1112 18:28:21.073843 140264174335808 run_lib.py:153] step: 806500, training_loss: 1.12532e+02
I1112 18:28:30.783553 140264174335808 run_lib.py:153] step: 806550, training_loss: 1.22980e+02
I1112 18:28:41.074223 140264174335808 run_lib.py:153] step: 806600, training_loss: 1.27746e+02
I1112 18:28:50.571632 140264174335808 run_lib.py:153] step: 806650, training_loss: 1.40830e+02
I1112 18:29:00.899578 140264174335808 run_lib.py:153] step: 806700, training_loss: 1.57469e+02
I1112 18:29:10.717772 140264174335808 run_lib.py:153] step: 806750, training_loss: 1.37844e+02
I1112 18:29:20.707974 140264174335808 run_lib.py:153] step: 806800, training_loss: 1.05114e+02
I1112 18:29:30.368263 140264174335808 run_lib.py:153] step: 806850, training_loss: 1.36500e+02
I1112 18:29:39.946568 140264174335808 run_lib.py:153] step: 806900, training_loss: 1.42569e+02
I1112 18:29:50.350132 140264174335808 run_lib.py:153] step: 806950, training_loss: 1.19956e+02
I1112 18:30:00.268612 140264174335808 run_lib.py:153] step: 807000, training_loss: 1.21606e+02
I1112 18:30:10.690176 140264174335808 run_lib.py:153] step: 807050, training_loss: 1.37547e+02
I1112 18:30:20.762187 140264174335808 run_lib.py:153] step: 807100, training_loss: 1.22703e+02
I1112 18:30:31.110956 140264174335808 run_lib.py:153] step: 807150, training_loss: 1.39778e+02
I1112 18:30:41.329226 140264174335808 run_lib.py:153] step: 807200, training_loss: 1.30096e+02
I1112 18:30:51.068353 140264174335808 run_lib.py:153] step: 807250, training_loss: 1.30323e+02
I1112 18:31:00.559911 140264174335808 run_lib.py:153] step: 807300, training_loss: 1.21991e+02
I1112 18:31:10.870929 140264174335808 run_lib.py:153] step: 807350, training_loss: 1.13697e+02
I1112 18:31:21.047596 140264174335808 run_lib.py:153] step: 807400, training_loss: 1.56974e+02
I1112 18:31:31.222199 140264174335808 run_lib.py:153] step: 807450, training_loss: 1.19728e+02
I1112 18:31:41.309850 140264174335808 run_lib.py:153] step: 807500, training_loss: 1.28275e+02
I1112 18:31:51.097085 140264174335808 run_lib.py:153] step: 807550, training_loss: 7.61129e+01
I1112 18:32:00.958134 140264174335808 run_lib.py:153] step: 807600, training_loss: 1.48064e+02
I1112 18:32:10.532088 140264174335808 run_lib.py:153] step: 807650, training_loss: 1.27218e+02
I1112 18:32:20.675143 140264174335808 run_lib.py:153] step: 807700, training_loss: 1.38808e+02
I1112 18:32:30.358950 140264174335808 run_lib.py:153] step: 807750, training_loss: 1.15272e+02
I1112 18:32:39.983970 140264174335808 run_lib.py:153] step: 807800, training_loss: 1.11940e+02
I1112 18:32:49.480809 140264174335808 run_lib.py:153] step: 807850, training_loss: 1.05113e+02
I1112 18:33:00.010680 140264174335808 run_lib.py:153] step: 807900, training_loss: 1.27858e+02
I1112 18:33:09.913226 140264174335808 run_lib.py:153] step: 807950, training_loss: 1.19734e+02
I1112 18:33:19.783416 140264174335808 run_lib.py:153] step: 808000, training_loss: 1.34623e+02
I1112 18:33:29.141631 140264174335808 run_lib.py:153] step: 808050, training_loss: 1.05983e+02
I1112 18:33:39.034476 140264174335808 run_lib.py:153] step: 808100, training_loss: 1.35703e+02
I1112 18:33:48.559919 140264174335808 run_lib.py:153] step: 808150, training_loss: 1.21132e+02
I1112 18:33:58.152801 140264174335808 run_lib.py:153] step: 808200, training_loss: 1.40618e+02
I1112 18:34:08.843677 140264174335808 run_lib.py:153] step: 808250, training_loss: 1.41187e+02
I1112 18:34:18.305048 140264174335808 run_lib.py:153] step: 808300, training_loss: 1.31042e+02
I1112 18:34:28.352411 140264174335808 run_lib.py:153] step: 808350, training_loss: 1.57873e+02
I1112 18:34:37.769100 140264174335808 run_lib.py:153] step: 808400, training_loss: 9.97993e+01
I1112 18:34:47.307204 140264174335808 run_lib.py:153] step: 808450, training_loss: 1.05408e+02
I1112 18:34:56.536590 140264174335808 run_lib.py:153] step: 808500, training_loss: 1.33184e+02
I1112 18:35:05.891410 140264174335808 run_lib.py:153] step: 808550, training_loss: 1.26864e+02
I1112 18:35:15.633967 140264174335808 run_lib.py:153] step: 808600, training_loss: 1.33617e+02
I1112 18:35:26.149031 140264174335808 run_lib.py:153] step: 808650, training_loss: 1.39611e+02
I1112 18:35:36.833207 140264174335808 run_lib.py:153] step: 808700, training_loss: 1.34690e+02
I1112 18:35:47.367978 140264174335808 run_lib.py:153] step: 808750, training_loss: 1.17931e+02
I1112 18:35:56.800024 140264174335808 run_lib.py:153] step: 808800, training_loss: 1.28647e+02
I1112 18:36:07.278515 140264174335808 run_lib.py:153] step: 808850, training_loss: 1.35549e+02
I1112 18:36:17.541165 140264174335808 run_lib.py:153] step: 808900, training_loss: 1.49371e+02
I1112 18:36:28.129121 140264174335808 run_lib.py:153] step: 808950, training_loss: 9.33629e+01
I1112 18:36:38.234356 140264174335808 run_lib.py:153] step: 809000, training_loss: 1.12520e+02
I1112 18:36:49.256757 140264174335808 run_lib.py:153] step: 809050, training_loss: 1.04478e+02
I1112 18:36:59.752721 140264174335808 run_lib.py:153] step: 809100, training_loss: 1.18299e+02
I1112 18:37:09.674539 140264174335808 run_lib.py:153] step: 809150, training_loss: 1.27969e+02
I1112 18:37:19.875241 140264174335808 run_lib.py:153] step: 809200, training_loss: 1.47188e+02
I1112 18:37:30.107205 140264174335808 run_lib.py:153] step: 809250, training_loss: 1.24540e+02
I1112 18:37:40.714508 140264174335808 run_lib.py:153] step: 809300, training_loss: 1.16313e+02
I1112 18:37:50.241915 140264174335808 run_lib.py:153] step: 809350, training_loss: 1.48929e+02
I1112 18:37:59.696449 140264174335808 run_lib.py:153] step: 809400, training_loss: 1.59413e+02
I1112 18:38:09.615661 140264174335808 run_lib.py:153] step: 809450, training_loss: 1.13677e+02
I1112 18:38:19.818852 140264174335808 run_lib.py:153] step: 809500, training_loss: 1.15069e+02
I1112 18:38:29.680434 140264174335808 run_lib.py:153] step: 809550, training_loss: 1.08926e+02
I1112 18:38:39.939517 140264174335808 run_lib.py:153] step: 809600, training_loss: 1.01939e+02
I1112 18:38:49.958403 140264174335808 run_lib.py:153] step: 809650, training_loss: 1.31217e+02
I1112 18:39:00.424494 140264174335808 run_lib.py:153] step: 809700, training_loss: 1.22668e+02
I1112 18:39:09.951343 140264174335808 run_lib.py:153] step: 809750, training_loss: 1.42920e+02
I1112 18:39:20.121543 140264174335808 run_lib.py:153] step: 809800, training_loss: 1.60267e+02
I1112 18:39:31.211167 140264174335808 run_lib.py:153] step: 809850, training_loss: 1.05382e+02
I1112 18:39:41.618024 140264174335808 run_lib.py:153] step: 809900, training_loss: 1.33960e+02
I1112 18:39:52.265890 140264174335808 run_lib.py:153] step: 809950, training_loss: 1.37525e+02
I1112 18:40:02.717330 140264174335808 run_lib.py:153] step: 810000, training_loss: 1.40328e+02
I1112 18:40:03.342229 140264174335808 run_lib.py:166] step: 810000, eval_loss: 1.09112e+02
I1112 18:40:13.401202 140264174335808 run_lib.py:153] step: 810050, training_loss: 1.52246e+02
I1112 18:40:23.934272 140264174335808 run_lib.py:153] step: 810100, training_loss: 1.36552e+02
I1112 18:40:34.123673 140264174335808 run_lib.py:153] step: 810150, training_loss: 1.07480e+02
I1112 18:40:43.908677 140264174335808 run_lib.py:153] step: 810200, training_loss: 1.12787e+02
I1112 18:40:53.476274 140264174335808 run_lib.py:153] step: 810250, training_loss: 1.22062e+02
I1112 18:41:03.175508 140264174335808 run_lib.py:153] step: 810300, training_loss: 1.52591e+02
I1112 18:41:13.248321 140264174335808 run_lib.py:153] step: 810350, training_loss: 1.13925e+02
I1112 18:41:23.307785 140264174335808 run_lib.py:153] step: 810400, training_loss: 1.20289e+02
I1112 18:41:33.831204 140264174335808 run_lib.py:153] step: 810450, training_loss: 1.21355e+02
I1112 18:41:44.852586 140264174335808 run_lib.py:153] step: 810500, training_loss: 1.26051e+02
I1112 18:41:55.616740 140264174335808 run_lib.py:153] step: 810550, training_loss: 1.41111e+02
I1112 18:42:06.721054 140264174335808 run_lib.py:153] step: 810600, training_loss: 1.15617e+02
I1112 18:42:17.047993 140264174335808 run_lib.py:153] step: 810650, training_loss: 1.25390e+02
I1112 18:42:27.008528 140264174335808 run_lib.py:153] step: 810700, training_loss: 1.39174e+02
I1112 18:42:36.928929 140264174335808 run_lib.py:153] step: 810750, training_loss: 1.14768e+02
I1112 18:42:47.542010 140264174335808 run_lib.py:153] step: 810800, training_loss: 1.27906e+02
I1112 18:42:57.462726 140264174335808 run_lib.py:153] step: 810850, training_loss: 1.13393e+02
I1112 18:43:08.086059 140264174335808 run_lib.py:153] step: 810900, training_loss: 1.17765e+02
I1112 18:43:18.544636 140264174335808 run_lib.py:153] step: 810950, training_loss: 1.65970e+02
I1112 18:43:28.861262 140264174335808 run_lib.py:153] step: 811000, training_loss: 1.43539e+02
I1112 18:43:39.281352 140264174335808 run_lib.py:153] step: 811050, training_loss: 1.36025e+02
I1112 18:43:49.507829 140264174335808 run_lib.py:153] step: 811100, training_loss: 1.19255e+02
I1112 18:43:59.648416 140264174335808 run_lib.py:153] step: 811150, training_loss: 1.23751e+02
I1112 18:44:10.416908 140264174335808 run_lib.py:153] step: 811200, training_loss: 1.22780e+02
I1112 18:44:20.410859 140264174335808 run_lib.py:153] step: 811250, training_loss: 1.34714e+02
I1112 18:44:30.488439 140264174335808 run_lib.py:153] step: 811300, training_loss: 1.40694e+02
I1112 18:44:40.484875 140264174335808 run_lib.py:153] step: 811350, training_loss: 1.12873e+02
I1112 18:44:50.579081 140264174335808 run_lib.py:153] step: 811400, training_loss: 1.27725e+02
I1112 18:45:00.697465 140264174335808 run_lib.py:153] step: 811450, training_loss: 1.15751e+02
I1112 18:45:11.176712 140264174335808 run_lib.py:153] step: 811500, training_loss: 1.60142e+02
I1112 18:45:21.527588 140264174335808 run_lib.py:153] step: 811550, training_loss: 1.19898e+02
I1112 18:45:31.311668 140264174335808 run_lib.py:153] step: 811600, training_loss: 1.12235e+02
I1112 18:45:41.640533 140264174335808 run_lib.py:153] step: 811650, training_loss: 1.31731e+02
I1112 18:45:51.486447 140264174335808 run_lib.py:153] step: 811700, training_loss: 1.33992e+02
I1112 18:46:02.241128 140264174335808 run_lib.py:153] step: 811750, training_loss: 1.15215e+02
I1112 18:46:12.627959 140264174335808 run_lib.py:153] step: 811800, training_loss: 9.62247e+01
I1112 18:46:23.091607 140264174335808 run_lib.py:153] step: 811850, training_loss: 1.37816e+02
I1112 18:46:32.976835 140264174335808 run_lib.py:153] step: 811900, training_loss: 1.20077e+02
I1112 18:46:43.045037 140264174335808 run_lib.py:153] step: 811950, training_loss: 9.48710e+01
I1112 18:46:53.190434 140264174335808 run_lib.py:153] step: 812000, training_loss: 1.26218e+02
I1112 18:47:03.081785 140264174335808 run_lib.py:153] step: 812050, training_loss: 1.54803e+02
I1112 18:47:13.143987 140264174335808 run_lib.py:153] step: 812100, training_loss: 1.12483e+02
I1112 18:47:23.368504 140264174335808 run_lib.py:153] step: 812150, training_loss: 1.36426e+02
I1112 18:47:34.026050 140264174335808 run_lib.py:153] step: 812200, training_loss: 1.38037e+02
I1112 18:47:43.556743 140264174335808 run_lib.py:153] step: 812250, training_loss: 1.33973e+02
I1112 18:47:53.279413 140264174335808 run_lib.py:153] step: 812300, training_loss: 1.36338e+02
I1112 18:48:03.129606 140264174335808 run_lib.py:153] step: 812350, training_loss: 1.06478e+02
I1112 18:48:13.166265 140264174335808 run_lib.py:153] step: 812400, training_loss: 1.08973e+02
I1112 18:48:22.736348 140264174335808 run_lib.py:153] step: 812450, training_loss: 1.25464e+02
I1112 18:48:33.696432 140264174335808 run_lib.py:153] step: 812500, training_loss: 1.26231e+02
I1112 18:48:43.399430 140264174335808 run_lib.py:153] step: 812550, training_loss: 1.34523e+02
I1112 18:48:53.657435 140264174335808 run_lib.py:153] step: 812600, training_loss: 1.29374e+02
I1112 18:49:03.900749 140264174335808 run_lib.py:153] step: 812650, training_loss: 1.27047e+02
I1112 18:49:13.467023 140264174335808 run_lib.py:153] step: 812700, training_loss: 9.36980e+01
I1112 18:49:22.915228 140264174335808 run_lib.py:153] step: 812750, training_loss: 1.35451e+02
I1112 18:49:32.604876 140264174335808 run_lib.py:153] step: 812800, training_loss: 1.24705e+02
I1112 18:49:42.142320 140264174335808 run_lib.py:153] step: 812850, training_loss: 1.07697e+02
I1112 18:49:52.452635 140264174335808 run_lib.py:153] step: 812900, training_loss: 1.11660e+02
I1112 18:50:02.940986 140264174335808 run_lib.py:153] step: 812950, training_loss: 1.20498e+02
I1112 18:50:13.185850 140264174335808 run_lib.py:153] step: 813000, training_loss: 1.35830e+02
I1112 18:50:23.144709 140264174335808 run_lib.py:153] step: 813050, training_loss: 1.08371e+02
I1112 18:50:33.385826 140264174335808 run_lib.py:153] step: 813100, training_loss: 1.13342e+02
I1112 18:50:43.190550 140264174335808 run_lib.py:153] step: 813150, training_loss: 1.03899e+02
I1112 18:50:53.239407 140264174335808 run_lib.py:153] step: 813200, training_loss: 1.19093e+02
I1112 18:51:04.043214 140264174335808 run_lib.py:153] step: 813250, training_loss: 9.82451e+01
I1112 18:51:13.843789 140264174335808 run_lib.py:153] step: 813300, training_loss: 1.16760e+02
I1112 18:51:24.062901 140264174335808 run_lib.py:153] step: 813350, training_loss: 1.23039e+02
I1112 18:51:33.948874 140264174335808 run_lib.py:153] step: 813400, training_loss: 1.44789e+02
I1112 18:51:43.823432 140264174335808 run_lib.py:153] step: 813450, training_loss: 1.30228e+02
I1112 18:51:54.232752 140264174335808 run_lib.py:153] step: 813500, training_loss: 1.14123e+02
I1112 18:52:04.443335 140264174335808 run_lib.py:153] step: 813550, training_loss: 1.46313e+02
I1112 18:52:15.467529 140264174335808 run_lib.py:153] step: 813600, training_loss: 1.55736e+02
I1112 18:52:25.577991 140264174335808 run_lib.py:153] step: 813650, training_loss: 9.40852e+01
I1112 18:52:35.558717 140264174335808 run_lib.py:153] step: 813700, training_loss: 1.11627e+02
I1112 18:52:45.896072 140264174335808 run_lib.py:153] step: 813750, training_loss: 9.75885e+01
I1112 18:52:55.851662 140264174335808 run_lib.py:153] step: 813800, training_loss: 1.24040e+02
I1112 18:53:05.916095 140264174335808 run_lib.py:153] step: 813850, training_loss: 1.07211e+02
I1112 18:53:15.735609 140264174335808 run_lib.py:153] step: 813900, training_loss: 1.21951e+02
I1112 18:53:25.382001 140264174335808 run_lib.py:153] step: 813950, training_loss: 1.12868e+02
I1112 18:53:35.099938 140264174335808 run_lib.py:153] step: 814000, training_loss: 1.38759e+02
I1112 18:53:45.443500 140264174335808 run_lib.py:153] step: 814050, training_loss: 1.17278e+02
I1112 18:53:56.735458 140264174335808 run_lib.py:153] step: 814100, training_loss: 1.13943e+02
I1112 18:54:07.005932 140264174335808 run_lib.py:153] step: 814150, training_loss: 1.39026e+02
I1112 18:54:17.231199 140264174335808 run_lib.py:153] step: 814200, training_loss: 1.34669e+02
I1112 18:54:27.384243 140264174335808 run_lib.py:153] step: 814250, training_loss: 9.79402e+01
I1112 18:54:37.592721 140264174335808 run_lib.py:153] step: 814300, training_loss: 1.09716e+02
I1112 18:54:47.982158 140264174335808 run_lib.py:153] step: 814350, training_loss: 1.44573e+02
I1112 18:54:58.305645 140264174335808 run_lib.py:153] step: 814400, training_loss: 1.34272e+02
I1112 18:55:08.500946 140264174335808 run_lib.py:153] step: 814450, training_loss: 1.00875e+02
I1112 18:55:18.418972 140264174335808 run_lib.py:153] step: 814500, training_loss: 1.20788e+02
I1112 18:55:27.941961 140264174335808 run_lib.py:153] step: 814550, training_loss: 1.23976e+02
I1112 18:55:38.061137 140264174335808 run_lib.py:153] step: 814600, training_loss: 1.40108e+02
I1112 18:55:48.038082 140264174335808 run_lib.py:153] step: 814650, training_loss: 1.08908e+02
I1112 18:55:58.046576 140264174335808 run_lib.py:153] step: 814700, training_loss: 1.17446e+02
I1112 18:56:08.530442 140264174335808 run_lib.py:153] step: 814750, training_loss: 1.00115e+02
I1112 18:56:19.010735 140264174335808 run_lib.py:153] step: 814800, training_loss: 1.49435e+02
I1112 18:56:29.255757 140264174335808 run_lib.py:153] step: 814850, training_loss: 1.03536e+02
I1112 18:56:38.728485 140264174335808 run_lib.py:153] step: 814900, training_loss: 1.19383e+02
I1112 18:56:48.641451 140264174335808 run_lib.py:153] step: 814950, training_loss: 1.24202e+02
I1112 18:56:58.874372 140264174335808 run_lib.py:153] step: 815000, training_loss: 1.11689e+02
I1112 18:56:59.022416 140264174335808 run_lib.py:166] step: 815000, eval_loss: 1.57326e+02
I1112 18:57:08.729722 140264174335808 run_lib.py:153] step: 815050, training_loss: 1.22953e+02
I1112 18:57:18.513931 140264174335808 run_lib.py:153] step: 815100, training_loss: 1.33245e+02
I1112 18:57:28.330195 140264174335808 run_lib.py:153] step: 815150, training_loss: 1.30466e+02
I1112 18:57:38.733973 140264174335808 run_lib.py:153] step: 815200, training_loss: 1.39459e+02
I1112 18:57:48.733626 140264174335808 run_lib.py:153] step: 815250, training_loss: 1.24823e+02
I1112 18:57:58.381902 140264174335808 run_lib.py:153] step: 815300, training_loss: 1.20797e+02
I1112 18:58:07.802364 140264174335808 run_lib.py:153] step: 815350, training_loss: 1.20000e+02
I1112 18:58:17.781187 140264174335808 run_lib.py:153] step: 815400, training_loss: 1.26392e+02
I1112 18:58:27.994618 140264174335808 run_lib.py:153] step: 815450, training_loss: 1.49777e+02
I1112 18:58:37.996450 140264174335808 run_lib.py:153] step: 815500, training_loss: 1.34525e+02
I1112 18:58:48.151623 140264174335808 run_lib.py:153] step: 815550, training_loss: 1.33500e+02
I1112 18:58:58.109597 140264174335808 run_lib.py:153] step: 815600, training_loss: 1.29394e+02
I1112 18:59:07.828195 140264174335808 run_lib.py:153] step: 815650, training_loss: 1.17459e+02
I1112 18:59:17.446322 140264174335808 run_lib.py:153] step: 815700, training_loss: 1.15329e+02
I1112 18:59:27.914757 140264174335808 run_lib.py:153] step: 815750, training_loss: 1.04568e+02
I1112 18:59:38.288903 140264174335808 run_lib.py:153] step: 815800, training_loss: 1.05106e+02
I1112 18:59:48.034514 140264174335808 run_lib.py:153] step: 815850, training_loss: 1.13702e+02
I1112 18:59:57.690078 140264174335808 run_lib.py:153] step: 815900, training_loss: 1.66588e+02
I1112 19:00:08.311108 140264174335808 run_lib.py:153] step: 815950, training_loss: 1.20466e+02
I1112 19:00:19.346272 140264174335808 run_lib.py:153] step: 816000, training_loss: 1.56985e+02
I1112 19:00:29.782180 140264174335808 run_lib.py:153] step: 816050, training_loss: 1.01367e+02
I1112 19:00:39.694014 140264174335808 run_lib.py:153] step: 816100, training_loss: 1.34144e+02
I1112 19:00:49.616089 140264174335808 run_lib.py:153] step: 816150, training_loss: 1.21314e+02
I1112 19:00:59.727767 140264174335808 run_lib.py:153] step: 816200, training_loss: 1.36550e+02
I1112 19:01:10.141378 140264174335808 run_lib.py:153] step: 816250, training_loss: 1.40706e+02
I1112 19:01:19.735240 140264174335808 run_lib.py:153] step: 816300, training_loss: 1.21280e+02
I1112 19:01:30.029734 140264174335808 run_lib.py:153] step: 816350, training_loss: 1.56325e+02
I1112 19:01:40.897522 140264174335808 run_lib.py:153] step: 816400, training_loss: 1.37411e+02
I1112 19:01:50.605194 140264174335808 run_lib.py:153] step: 816450, training_loss: 1.43448e+02
I1112 19:02:00.987642 140264174335808 run_lib.py:153] step: 816500, training_loss: 1.00056e+02
I1112 19:02:10.528831 140264174335808 run_lib.py:153] step: 816550, training_loss: 1.45296e+02
I1112 19:02:20.169904 140264174335808 run_lib.py:153] step: 816600, training_loss: 1.41751e+02
I1112 19:02:29.998229 140264174335808 run_lib.py:153] step: 816650, training_loss: 1.43664e+02
I1112 19:02:40.509032 140264174335808 run_lib.py:153] step: 816700, training_loss: 1.54467e+02
I1112 19:02:50.592964 140264174335808 run_lib.py:153] step: 816750, training_loss: 1.19081e+02
I1112 19:03:00.859247 140264174335808 run_lib.py:153] step: 816800, training_loss: 1.32571e+02
I1112 19:03:11.373736 140264174335808 run_lib.py:153] step: 816850, training_loss: 1.41264e+02
I1112 19:03:22.259265 140264174335808 run_lib.py:153] step: 816900, training_loss: 1.00744e+02
I1112 19:03:32.658122 140264174335808 run_lib.py:153] step: 816950, training_loss: 1.54435e+02
I1112 19:03:43.042804 140264174335808 run_lib.py:153] step: 817000, training_loss: 1.10441e+02
I1112 19:03:53.412321 140264174335808 run_lib.py:153] step: 817050, training_loss: 1.32211e+02
I1112 19:04:03.773067 140264174335808 run_lib.py:153] step: 817100, training_loss: 1.50031e+02
I1112 19:04:14.711519 140264174335808 run_lib.py:153] step: 817150, training_loss: 1.38148e+02
I1112 19:04:24.871200 140264174335808 run_lib.py:153] step: 817200, training_loss: 1.40936e+02
I1112 19:04:34.912127 140264174335808 run_lib.py:153] step: 817250, training_loss: 1.22707e+02
I1112 19:04:44.857778 140264174335808 run_lib.py:153] step: 817300, training_loss: 1.42472e+02
I1112 19:04:54.822659 140264174335808 run_lib.py:153] step: 817350, training_loss: 1.05861e+02
I1112 19:05:04.463150 140264174335808 run_lib.py:153] step: 817400, training_loss: 1.06222e+02
I1112 19:05:13.811366 140264174335808 run_lib.py:153] step: 817450, training_loss: 1.50876e+02
I1112 19:05:24.090590 140264174335808 run_lib.py:153] step: 817500, training_loss: 1.01500e+02
I1112 19:05:34.668882 140264174335808 run_lib.py:153] step: 817550, training_loss: 1.20168e+02
I1112 19:05:45.495587 140264174335808 run_lib.py:153] step: 817600, training_loss: 1.28055e+02
I1112 19:05:55.960840 140264174335808 run_lib.py:153] step: 817650, training_loss: 1.07940e+02
I1112 19:06:05.458948 140264174335808 run_lib.py:153] step: 817700, training_loss: 1.08283e+02
I1112 19:06:16.181951 140264174335808 run_lib.py:153] step: 817750, training_loss: 1.45973e+02
I1112 19:06:26.853008 140264174335808 run_lib.py:153] step: 817800, training_loss: 1.06958e+02
I1112 19:06:37.198374 140264174335808 run_lib.py:153] step: 817850, training_loss: 1.27312e+02
I1112 19:06:47.043655 140264174335808 run_lib.py:153] step: 817900, training_loss: 1.16531e+02
I1112 19:06:57.578716 140264174335808 run_lib.py:153] step: 817950, training_loss: 1.15610e+02
I1112 19:07:08.450378 140264174335808 run_lib.py:153] step: 818000, training_loss: 1.03809e+02
I1112 19:07:18.819711 140264174335808 run_lib.py:153] step: 818050, training_loss: 1.39270e+02
I1112 19:07:29.222338 140264174335808 run_lib.py:153] step: 818100, training_loss: 1.30303e+02
I1112 19:07:38.900695 140264174335808 run_lib.py:153] step: 818150, training_loss: 1.17528e+02
I1112 19:07:49.049183 140264174335808 run_lib.py:153] step: 818200, training_loss: 1.09541e+02
I1112 19:07:59.580312 140264174335808 run_lib.py:153] step: 818250, training_loss: 1.05967e+02
I1112 19:08:09.317659 140264174335808 run_lib.py:153] step: 818300, training_loss: 1.13944e+02
I1112 19:08:20.181521 140264174335808 run_lib.py:153] step: 818350, training_loss: 1.41123e+02
I1112 19:08:30.155853 140264174335808 run_lib.py:153] step: 818400, training_loss: 9.84131e+01
I1112 19:08:40.478468 140264174335808 run_lib.py:153] step: 818450, training_loss: 9.85261e+01
I1112 19:08:50.747030 140264174335808 run_lib.py:153] step: 818500, training_loss: 1.07339e+02
I1112 19:09:01.263186 140264174335808 run_lib.py:153] step: 818550, training_loss: 1.30872e+02
I1112 19:09:11.339645 140264174335808 run_lib.py:153] step: 818600, training_loss: 1.13938e+02
I1112 19:09:21.547805 140264174335808 run_lib.py:153] step: 818650, training_loss: 1.34646e+02
I1112 19:09:31.931379 140264174335808 run_lib.py:153] step: 818700, training_loss: 1.16573e+02
I1112 19:09:41.949683 140264174335808 run_lib.py:153] step: 818750, training_loss: 1.34167e+02
I1112 19:09:51.964529 140264174335808 run_lib.py:153] step: 818800, training_loss: 1.05451e+02
I1112 19:10:02.348882 140264174335808 run_lib.py:153] step: 818850, training_loss: 1.11576e+02
I1112 19:10:12.246817 140264174335808 run_lib.py:153] step: 818900, training_loss: 1.34064e+02
I1112 19:10:22.550663 140264174335808 run_lib.py:153] step: 818950, training_loss: 1.44352e+02
I1112 19:10:32.684121 140264174335808 run_lib.py:153] step: 819000, training_loss: 1.43877e+02
I1112 19:10:42.906158 140264174335808 run_lib.py:153] step: 819050, training_loss: 1.03645e+02
I1112 19:10:53.529918 140264174335808 run_lib.py:153] step: 819100, training_loss: 1.25846e+02
I1112 19:11:04.089903 140264174335808 run_lib.py:153] step: 819150, training_loss: 1.14017e+02
I1112 19:11:17.107885 140264174335808 run_lib.py:153] step: 819200, training_loss: 1.16903e+02
I1112 19:11:35.210154 140264174335808 run_lib.py:153] step: 819250, training_loss: 1.50449e+02
I1112 19:11:53.424193 140264174335808 run_lib.py:153] step: 819300, training_loss: 1.18147e+02
I1112 19:12:11.777826 140264174335808 run_lib.py:153] step: 819350, training_loss: 1.22517e+02
I1112 19:12:30.206786 140264174335808 run_lib.py:153] step: 819400, training_loss: 1.17754e+02
I1112 19:12:48.452687 140264174335808 run_lib.py:153] step: 819450, training_loss: 1.15313e+02
I1112 19:13:07.079369 140264174335808 run_lib.py:153] step: 819500, training_loss: 1.34046e+02
I1112 19:13:25.364022 140264174335808 run_lib.py:153] step: 819550, training_loss: 1.23309e+02
I1112 19:13:43.683992 140264174335808 run_lib.py:153] step: 819600, training_loss: 1.27926e+02
I1112 19:14:01.829025 140264174335808 run_lib.py:153] step: 819650, training_loss: 1.38465e+02
I1112 19:14:20.186155 140264174335808 run_lib.py:153] step: 819700, training_loss: 1.11091e+02
I1112 19:14:38.424938 140264174335808 run_lib.py:153] step: 819750, training_loss: 1.01894e+02
I1112 19:14:56.718291 140264174335808 run_lib.py:153] step: 819800, training_loss: 1.17695e+02
I1112 19:15:15.021093 140264174335808 run_lib.py:153] step: 819850, training_loss: 1.13749e+02
I1112 19:15:33.420864 140264174335808 run_lib.py:153] step: 819900, training_loss: 1.22842e+02
I1112 19:15:51.722722 140264174335808 run_lib.py:153] step: 819950, training_loss: 1.30313e+02
I1112 19:16:10.352535 140264174335808 run_lib.py:153] step: 820000, training_loss: 1.24300e+02
I1112 19:16:10.958987 140264174335808 run_lib.py:166] step: 820000, eval_loss: 1.10233e+02
I1112 19:16:29.504947 140264174335808 run_lib.py:153] step: 820050, training_loss: 1.41261e+02
I1112 19:16:48.159792 140264174335808 run_lib.py:153] step: 820100, training_loss: 1.39315e+02
I1112 19:17:06.526923 140264174335808 run_lib.py:153] step: 820150, training_loss: 1.14997e+02
I1112 19:17:24.917418 140264174335808 run_lib.py:153] step: 820200, training_loss: 9.93485e+01
I1112 19:17:43.360759 140264174335808 run_lib.py:153] step: 820250, training_loss: 1.07896e+02
I1112 19:18:01.873311 140264174335808 run_lib.py:153] step: 820300, training_loss: 1.13427e+02
I1112 19:18:20.244342 140264174335808 run_lib.py:153] step: 820350, training_loss: 1.28418e+02
I1112 19:18:38.931100 140264174335808 run_lib.py:153] step: 820400, training_loss: 1.32158e+02
I1112 19:18:57.294795 140264174335808 run_lib.py:153] step: 820450, training_loss: 9.40505e+01
I1112 19:19:15.634063 140264174335808 run_lib.py:153] step: 820500, training_loss: 1.09090e+02
I1112 19:19:34.015819 140264174335808 run_lib.py:153] step: 820550, training_loss: 1.35450e+02
I1112 19:19:52.445350 140264174335808 run_lib.py:153] step: 820600, training_loss: 1.30292e+02
I1112 19:20:10.903976 140264174335808 run_lib.py:153] step: 820650, training_loss: 1.23064e+02
I1112 19:20:29.289476 140264174335808 run_lib.py:153] step: 820700, training_loss: 1.24701e+02
I1112 19:20:47.685843 140264174335808 run_lib.py:153] step: 820750, training_loss: 1.17970e+02
I1112 19:21:06.194430 140264174335808 run_lib.py:153] step: 820800, training_loss: 1.24343e+02
I1112 19:21:24.655444 140264174335808 run_lib.py:153] step: 820850, training_loss: 1.09209e+02
I1112 19:21:42.998095 140264174335808 run_lib.py:153] step: 820900, training_loss: 1.21729e+02
I1112 19:22:01.298428 140264174335808 run_lib.py:153] step: 820950, training_loss: 1.59482e+02
I1112 19:22:19.656618 140264174335808 run_lib.py:153] step: 821000, training_loss: 1.15538e+02
I1112 19:22:38.072860 140264174335808 run_lib.py:153] step: 821050, training_loss: 1.10372e+02
I1112 19:22:56.694462 140264174335808 run_lib.py:153] step: 821100, training_loss: 1.09218e+02
I1112 19:23:15.271862 140264174335808 run_lib.py:153] step: 821150, training_loss: 1.43582e+02
I1112 19:23:33.896944 140264174335808 run_lib.py:153] step: 821200, training_loss: 1.03374e+02
I1112 19:23:52.425188 140264174335808 run_lib.py:153] step: 821250, training_loss: 1.23382e+02
I1112 19:24:10.867562 140264174335808 run_lib.py:153] step: 821300, training_loss: 1.50222e+02
I1112 19:24:29.329858 140264174335808 run_lib.py:153] step: 821350, training_loss: 1.07403e+02
I1112 19:24:47.767236 140264174335808 run_lib.py:153] step: 821400, training_loss: 1.23344e+02
I1112 19:25:06.220855 140264174335808 run_lib.py:153] step: 821450, training_loss: 1.21643e+02
I1112 19:25:25.029449 140264174335808 run_lib.py:153] step: 821500, training_loss: 1.31641e+02
I1112 19:25:43.343859 140264174335808 run_lib.py:153] step: 821550, training_loss: 1.61772e+02
I1112 19:26:01.802028 140264174335808 run_lib.py:153] step: 821600, training_loss: 1.34881e+02
I1112 19:26:20.165612 140264174335808 run_lib.py:153] step: 821650, training_loss: 1.40050e+02
I1112 19:26:38.540051 140264174335808 run_lib.py:153] step: 821700, training_loss: 1.13889e+02
I1112 19:26:56.990418 140264174335808 run_lib.py:153] step: 821750, training_loss: 1.18397e+02
I1112 19:27:15.435103 140264174335808 run_lib.py:153] step: 821800, training_loss: 1.35293e+02
I1112 19:27:34.046223 140264174335808 run_lib.py:153] step: 821850, training_loss: 1.42744e+02
I1112 19:27:52.459985 140264174335808 run_lib.py:153] step: 821900, training_loss: 1.04881e+02
I1112 19:28:10.926465 140264174335808 run_lib.py:153] step: 821950, training_loss: 1.66148e+02
I1112 19:28:29.479859 140264174335808 run_lib.py:153] step: 822000, training_loss: 1.33777e+02
I1112 19:28:47.950470 140264174335808 run_lib.py:153] step: 822050, training_loss: 8.93931e+01
I1112 19:29:06.393371 140264174335808 run_lib.py:153] step: 822100, training_loss: 1.44371e+02
I1112 19:29:25.053205 140264174335808 run_lib.py:153] step: 822150, training_loss: 1.13450e+02
I1112 19:29:43.499106 140264174335808 run_lib.py:153] step: 822200, training_loss: 1.20818e+02
I1112 19:30:01.871585 140264174335808 run_lib.py:153] step: 822250, training_loss: 1.42565e+02
I1112 19:30:20.339499 140264174335808 run_lib.py:153] step: 822300, training_loss: 1.09516e+02
I1112 19:30:38.776034 140264174335808 run_lib.py:153] step: 822350, training_loss: 1.41576e+02
I1112 19:30:57.284386 140264174335808 run_lib.py:153] step: 822400, training_loss: 1.16314e+02
I1112 19:31:15.784106 140264174335808 run_lib.py:153] step: 822450, training_loss: 1.13678e+02
I1112 19:31:34.146025 140264174335808 run_lib.py:153] step: 822500, training_loss: 1.15221e+02
I1112 19:31:52.544575 140264174335808 run_lib.py:153] step: 822550, training_loss: 1.38997e+02
I1112 19:32:10.949599 140264174335808 run_lib.py:153] step: 822600, training_loss: 1.18433e+02
I1112 19:32:29.393295 140264174335808 run_lib.py:153] step: 822650, training_loss: 1.04624e+02
I1112 19:32:47.769533 140264174335808 run_lib.py:153] step: 822700, training_loss: 1.05146e+02
I1112 19:33:06.218677 140264174335808 run_lib.py:153] step: 822750, training_loss: 1.12273e+02
I1112 19:33:24.593507 140264174335808 run_lib.py:153] step: 822800, training_loss: 1.24940e+02
I1112 19:33:42.887875 140264174335808 run_lib.py:153] step: 822850, training_loss: 1.11096e+02
I1112 19:34:01.284553 140264174335808 run_lib.py:153] step: 822900, training_loss: 9.76035e+01
I1112 19:34:13.120880 140264174335808 run_lib.py:153] step: 822950, training_loss: 1.22751e+02
I1112 19:34:23.650507 140264174335808 run_lib.py:153] step: 823000, training_loss: 1.27993e+02
I1112 19:34:34.079875 140264174335808 run_lib.py:153] step: 823050, training_loss: 1.13407e+02
I1112 19:34:44.748019 140264174335808 run_lib.py:153] step: 823100, training_loss: 1.34387e+02
I1112 19:34:54.779809 140264174335808 run_lib.py:153] step: 823150, training_loss: 1.25175e+02
I1112 19:35:04.470687 140264174335808 run_lib.py:153] step: 823200, training_loss: 1.17631e+02
I1112 19:35:14.245566 140264174335808 run_lib.py:153] step: 823250, training_loss: 1.27596e+02
I1112 19:35:24.910273 140264174335808 run_lib.py:153] step: 823300, training_loss: 1.15788e+02
I1112 19:35:34.753682 140264174335808 run_lib.py:153] step: 823350, training_loss: 1.02861e+02
I1112 19:35:44.041020 140264174335808 run_lib.py:153] step: 823400, training_loss: 1.18469e+02
I1112 19:35:54.131978 140264174335808 run_lib.py:153] step: 823450, training_loss: 1.41447e+02
I1112 19:36:03.421382 140264174335808 run_lib.py:153] step: 823500, training_loss: 1.59157e+02
I1112 19:36:12.825973 140264174335808 run_lib.py:153] step: 823550, training_loss: 1.35804e+02
I1112 19:36:22.629246 140264174335808 run_lib.py:153] step: 823600, training_loss: 1.24322e+02
I1112 19:36:31.884518 140264174335808 run_lib.py:153] step: 823650, training_loss: 1.14907e+02
I1112 19:36:41.521669 140264174335808 run_lib.py:153] step: 823700, training_loss: 1.39141e+02
I1112 19:36:51.125586 140264174335808 run_lib.py:153] step: 823750, training_loss: 1.03852e+02
I1112 19:37:00.948341 140264174335808 run_lib.py:153] step: 823800, training_loss: 1.18004e+02
I1112 19:37:10.381512 140264174335808 run_lib.py:153] step: 823850, training_loss: 1.48310e+02
I1112 19:37:20.896571 140264174335808 run_lib.py:153] step: 823900, training_loss: 1.07396e+02
I1112 19:37:31.265842 140264174335808 run_lib.py:153] step: 823950, training_loss: 1.29485e+02
I1112 19:37:40.707708 140264174335808 run_lib.py:153] step: 824000, training_loss: 1.29738e+02
I1112 19:37:50.337168 140264174335808 run_lib.py:153] step: 824050, training_loss: 1.35254e+02
I1112 19:37:59.999134 140264174335808 run_lib.py:153] step: 824100, training_loss: 1.30890e+02
I1112 19:38:10.311671 140264174335808 run_lib.py:153] step: 824150, training_loss: 1.70253e+02
I1112 19:38:20.274508 140264174335808 run_lib.py:153] step: 824200, training_loss: 1.23202e+02
I1112 19:38:30.430664 140264174335808 run_lib.py:153] step: 824250, training_loss: 1.26373e+02
I1112 19:38:41.021809 140264174335808 run_lib.py:153] step: 824300, training_loss: 1.45374e+02
I1112 19:38:51.159413 140264174335808 run_lib.py:153] step: 824350, training_loss: 1.22226e+02
I1112 19:39:01.591338 140264174335808 run_lib.py:153] step: 824400, training_loss: 1.28943e+02
I1112 19:39:12.032805 140264174335808 run_lib.py:153] step: 824450, training_loss: 1.31642e+02
I1112 19:39:22.563665 140264174335808 run_lib.py:153] step: 824500, training_loss: 1.20143e+02
I1112 19:39:32.700451 140264174335808 run_lib.py:153] step: 824550, training_loss: 1.27998e+02
I1112 19:39:42.908616 140264174335808 run_lib.py:153] step: 824600, training_loss: 1.03319e+02
I1112 19:39:53.547518 140264174335808 run_lib.py:153] step: 824650, training_loss: 1.16249e+02
I1112 19:40:03.875704 140264174335808 run_lib.py:153] step: 824700, training_loss: 1.35336e+02
I1112 19:40:13.676845 140264174335808 run_lib.py:153] step: 824750, training_loss: 1.21504e+02
I1112 19:40:23.820504 140264174335808 run_lib.py:153] step: 824800, training_loss: 1.28896e+02
I1112 19:40:34.252213 140264174335808 run_lib.py:153] step: 824850, training_loss: 1.46112e+02
I1112 19:40:45.003441 140264174335808 run_lib.py:153] step: 824900, training_loss: 1.27674e+02
I1112 19:40:55.142718 140264174335808 run_lib.py:153] step: 824950, training_loss: 1.35396e+02
I1112 19:41:05.382054 140264174335808 run_lib.py:153] step: 825000, training_loss: 1.47840e+02
I1112 19:41:05.492924 140264174335808 run_lib.py:166] step: 825000, eval_loss: 1.21570e+02
I1112 19:41:15.808595 140264174335808 run_lib.py:153] step: 825050, training_loss: 1.06873e+02
I1112 19:41:25.623497 140264174335808 run_lib.py:153] step: 825100, training_loss: 1.42353e+02
I1112 19:41:36.024893 140264174335808 run_lib.py:153] step: 825150, training_loss: 1.13585e+02
I1112 19:41:46.951528 140264174335808 run_lib.py:153] step: 825200, training_loss: 1.14057e+02
I1112 19:41:57.360576 140264174335808 run_lib.py:153] step: 825250, training_loss: 1.09037e+02
I1112 19:42:07.074228 140264174335808 run_lib.py:153] step: 825300, training_loss: 1.41134e+02
I1112 19:42:16.666412 140264174335808 run_lib.py:153] step: 825350, training_loss: 1.28023e+02
I1112 19:42:26.513354 140264174335808 run_lib.py:153] step: 825400, training_loss: 1.47978e+02
I1112 19:42:36.663668 140264174335808 run_lib.py:153] step: 825450, training_loss: 1.44712e+02
I1112 19:42:46.946612 140264174335808 run_lib.py:153] step: 825500, training_loss: 1.11079e+02
I1112 19:42:57.409797 140264174335808 run_lib.py:153] step: 825550, training_loss: 9.27177e+01
I1112 19:43:07.482322 140264174335808 run_lib.py:153] step: 825600, training_loss: 1.47633e+02
I1112 19:43:17.832003 140264174335808 run_lib.py:153] step: 825650, training_loss: 1.22943e+02
I1112 19:43:27.888382 140264174335808 run_lib.py:153] step: 825700, training_loss: 1.22924e+02
I1112 19:43:38.304013 140264174335808 run_lib.py:153] step: 825750, training_loss: 1.01200e+02
I1112 19:43:49.060170 140264174335808 run_lib.py:153] step: 825800, training_loss: 1.08174e+02
I1112 19:43:59.378082 140264174335808 run_lib.py:153] step: 825850, training_loss: 1.40115e+02
I1112 19:44:09.109781 140264174335808 run_lib.py:153] step: 825900, training_loss: 1.05082e+02
I1112 19:44:19.184754 140264174335808 run_lib.py:153] step: 825950, training_loss: 1.56847e+02
I1112 19:44:29.467753 140264174335808 run_lib.py:153] step: 826000, training_loss: 1.12147e+02
I1112 19:44:39.693776 140264174335808 run_lib.py:153] step: 826050, training_loss: 1.12636e+02
I1112 19:44:49.127069 140264174335808 run_lib.py:153] step: 826100, training_loss: 1.45937e+02
I1112 19:44:58.978402 140264174335808 run_lib.py:153] step: 826150, training_loss: 1.35484e+02
I1112 19:45:09.028281 140264174335808 run_lib.py:153] step: 826200, training_loss: 1.26123e+02
I1112 19:45:18.833332 140264174335808 run_lib.py:153] step: 826250, training_loss: 1.33653e+02
I1112 19:45:29.068607 140264174335808 run_lib.py:153] step: 826300, training_loss: 1.37927e+02
I1112 19:45:39.832413 140264174335808 run_lib.py:153] step: 826350, training_loss: 1.16541e+02
I1112 19:45:50.181397 140264174335808 run_lib.py:153] step: 826400, training_loss: 9.46599e+01
I1112 19:46:00.254077 140264174335808 run_lib.py:153] step: 826450, training_loss: 1.39870e+02
I1112 19:46:10.830395 140264174335808 run_lib.py:153] step: 826500, training_loss: 1.35746e+02
I1112 19:46:20.446578 140264174335808 run_lib.py:153] step: 826550, training_loss: 9.71300e+01
I1112 19:46:31.135419 140264174335808 run_lib.py:153] step: 826600, training_loss: 1.41393e+02
I1112 19:46:41.229758 140264174335808 run_lib.py:153] step: 826650, training_loss: 1.34786e+02
I1112 19:46:50.968472 140264174335808 run_lib.py:153] step: 826700, training_loss: 1.15971e+02
I1112 19:47:00.908252 140264174335808 run_lib.py:153] step: 826750, training_loss: 1.11470e+02
I1112 19:47:11.059325 140264174335808 run_lib.py:153] step: 826800, training_loss: 1.04867e+02
I1112 19:47:21.386621 140264174335808 run_lib.py:153] step: 826850, training_loss: 1.44033e+02
I1112 19:47:32.037696 140264174335808 run_lib.py:153] step: 826900, training_loss: 1.13348e+02
I1112 19:47:42.377253 140264174335808 run_lib.py:153] step: 826950, training_loss: 1.44689e+02
I1112 19:47:52.388170 140264174335808 run_lib.py:153] step: 827000, training_loss: 1.11545e+02
I1112 19:48:02.728607 140264174335808 run_lib.py:153] step: 827050, training_loss: 1.15813e+02
I1112 19:48:13.254968 140264174335808 run_lib.py:153] step: 827100, training_loss: 1.28517e+02
I1112 19:48:24.131246 140264174335808 run_lib.py:153] step: 827150, training_loss: 1.31506e+02
I1112 19:48:33.812266 140264174335808 run_lib.py:153] step: 827200, training_loss: 1.71105e+02
I1112 19:48:44.281771 140264174335808 run_lib.py:153] step: 827250, training_loss: 1.35066e+02
I1112 19:48:54.125102 140264174335808 run_lib.py:153] step: 827300, training_loss: 1.18537e+02
I1112 19:49:03.950808 140264174335808 run_lib.py:153] step: 827350, training_loss: 8.71548e+01
I1112 19:49:14.352322 140264174335808 run_lib.py:153] step: 827400, training_loss: 1.47584e+02
I1112 19:49:24.796387 140264174335808 run_lib.py:153] step: 827450, training_loss: 1.03042e+02
I1112 19:49:35.460687 140264174335808 run_lib.py:153] step: 827500, training_loss: 1.17313e+02
I1112 19:49:44.984404 140264174335808 run_lib.py:153] step: 827550, training_loss: 1.50265e+02
I1112 19:49:55.711174 140264174335808 run_lib.py:153] step: 827600, training_loss: 1.23972e+02
I1112 19:50:06.775909 140264174335808 run_lib.py:153] step: 827650, training_loss: 1.21950e+02
I1112 19:50:16.864120 140264174335808 run_lib.py:153] step: 827700, training_loss: 1.30818e+02
I1112 19:50:27.388882 140264174335808 run_lib.py:153] step: 827750, training_loss: 1.07907e+02
I1112 19:50:37.705007 140264174335808 run_lib.py:153] step: 827800, training_loss: 1.12408e+02
I1112 19:50:47.550800 140264174335808 run_lib.py:153] step: 827850, training_loss: 1.32723e+02
I1112 19:50:57.621070 140264174335808 run_lib.py:153] step: 827900, training_loss: 1.42561e+02
I1112 19:51:08.083315 140264174335808 run_lib.py:153] step: 827950, training_loss: 1.06706e+02
I1112 19:51:18.328103 140264174335808 run_lib.py:153] step: 828000, training_loss: 1.15432e+02
I1112 19:51:28.018254 140264174335808 run_lib.py:153] step: 828050, training_loss: 1.24066e+02
I1112 19:51:37.851463 140264174335808 run_lib.py:153] step: 828100, training_loss: 1.32193e+02
I1112 19:51:47.462861 140264174335808 run_lib.py:153] step: 828150, training_loss: 1.55274e+02
I1112 19:51:57.614181 140264174335808 run_lib.py:153] step: 828200, training_loss: 1.11509e+02
I1112 19:52:08.021333 140264174335808 run_lib.py:153] step: 828250, training_loss: 1.53009e+02
I1112 19:52:18.652666 140264174335808 run_lib.py:153] step: 828300, training_loss: 1.16960e+02
I1112 19:52:29.363323 140264174335808 run_lib.py:153] step: 828350, training_loss: 1.12744e+02
I1112 19:52:39.261435 140264174335808 run_lib.py:153] step: 828400, training_loss: 9.23726e+01
I1112 19:52:48.791882 140264174335808 run_lib.py:153] step: 828450, training_loss: 1.28362e+02
I1112 19:52:59.048705 140264174335808 run_lib.py:153] step: 828500, training_loss: 1.56472e+02
I1112 19:53:09.095935 140264174335808 run_lib.py:153] step: 828550, training_loss: 1.31920e+02
I1112 19:53:19.323091 140264174335808 run_lib.py:153] step: 828600, training_loss: 1.47148e+02
I1112 19:53:29.604058 140264174335808 run_lib.py:153] step: 828650, training_loss: 1.36128e+02
I1112 19:53:39.572618 140264174335808 run_lib.py:153] step: 828700, training_loss: 1.25911e+02
I1112 19:53:50.024562 140264174335808 run_lib.py:153] step: 828750, training_loss: 1.18180e+02
I1112 19:54:01.055083 140264174335808 run_lib.py:153] step: 828800, training_loss: 1.49671e+02
I1112 19:54:11.259809 140264174335808 run_lib.py:153] step: 828850, training_loss: 1.22233e+02
I1112 19:54:21.439969 140264174335808 run_lib.py:153] step: 828900, training_loss: 1.30996e+02
I1112 19:54:32.186624 140264174335808 run_lib.py:153] step: 828950, training_loss: 1.10412e+02
I1112 19:54:42.620500 140264174335808 run_lib.py:153] step: 829000, training_loss: 1.13860e+02
I1112 19:54:52.286720 140264174335808 run_lib.py:153] step: 829050, training_loss: 1.48405e+02
I1112 19:55:02.413378 140264174335808 run_lib.py:153] step: 829100, training_loss: 1.44792e+02
I1112 19:55:12.577462 140264174335808 run_lib.py:153] step: 829150, training_loss: 1.27422e+02
I1112 19:55:22.522150 140264174335808 run_lib.py:153] step: 829200, training_loss: 1.13349e+02
I1112 19:55:33.049658 140264174335808 run_lib.py:153] step: 829250, training_loss: 1.35868e+02
I1112 19:55:42.905864 140264174335808 run_lib.py:153] step: 829300, training_loss: 1.20299e+02
I1112 19:55:53.309432 140264174335808 run_lib.py:153] step: 829350, training_loss: 1.26240e+02
I1112 19:56:03.361244 140264174335808 run_lib.py:153] step: 829400, training_loss: 1.06829e+02
I1112 19:56:13.381047 140264174335808 run_lib.py:153] step: 829450, training_loss: 1.47708e+02
I1112 19:56:23.393013 140264174335808 run_lib.py:153] step: 829500, training_loss: 1.43283e+02
I1112 19:56:33.565686 140264174335808 run_lib.py:153] step: 829550, training_loss: 1.48427e+02
I1112 19:56:43.720133 140264174335808 run_lib.py:153] step: 829600, training_loss: 1.46038e+02
I1112 19:56:54.100705 140264174335808 run_lib.py:153] step: 829650, training_loss: 1.14218e+02
I1112 19:57:04.318211 140264174335808 run_lib.py:153] step: 829700, training_loss: 1.18973e+02
I1112 19:57:14.819535 140264174335808 run_lib.py:153] step: 829750, training_loss: 1.28502e+02
I1112 19:57:24.460492 140264174335808 run_lib.py:153] step: 829800, training_loss: 1.55003e+02
I1112 19:57:34.546941 140264174335808 run_lib.py:153] step: 829850, training_loss: 1.04461e+02
I1112 19:57:44.669483 140264174335808 run_lib.py:153] step: 829900, training_loss: 1.31342e+02
I1112 19:57:54.259601 140264174335808 run_lib.py:153] step: 829950, training_loss: 1.48611e+02
I1112 19:58:04.807522 140264174335808 run_lib.py:153] step: 830000, training_loss: 1.57869e+02
I1112 19:58:05.374240 140264174335808 run_lib.py:166] step: 830000, eval_loss: 1.67439e+02
I1112 19:58:15.713946 140264174335808 run_lib.py:153] step: 830050, training_loss: 1.23675e+02
I1112 19:58:26.403490 140264174335808 run_lib.py:153] step: 830100, training_loss: 1.42634e+02
I1112 19:58:37.155895 140264174335808 run_lib.py:153] step: 830150, training_loss: 1.21431e+02
I1112 19:58:48.007362 140264174335808 run_lib.py:153] step: 830200, training_loss: 1.21419e+02
I1112 19:58:58.198944 140264174335808 run_lib.py:153] step: 830250, training_loss: 1.37253e+02
I1112 19:59:08.032204 140264174335808 run_lib.py:153] step: 830300, training_loss: 1.23154e+02
I1112 19:59:17.921165 140264174335808 run_lib.py:153] step: 830350, training_loss: 1.14776e+02
I1112 19:59:28.565330 140264174335808 run_lib.py:153] step: 830400, training_loss: 1.22397e+02
I1112 19:59:38.177571 140264174335808 run_lib.py:153] step: 830450, training_loss: 1.17259e+02
I1112 19:59:48.269499 140264174335808 run_lib.py:153] step: 830500, training_loss: 1.15040e+02
I1112 19:59:58.327331 140264174335808 run_lib.py:153] step: 830550, training_loss: 1.20571e+02
I1112 20:00:08.962775 140264174335808 run_lib.py:153] step: 830600, training_loss: 1.48901e+02
I1112 20:00:19.054525 140264174335808 run_lib.py:153] step: 830650, training_loss: 1.25872e+02
I1112 20:00:28.565033 140264174335808 run_lib.py:153] step: 830700, training_loss: 1.21763e+02
I1112 20:00:39.049741 140264174335808 run_lib.py:153] step: 830750, training_loss: 1.12792e+02
I1112 20:00:48.446149 140264174335808 run_lib.py:153] step: 830800, training_loss: 1.17549e+02
I1112 20:00:58.223833 140264174335808 run_lib.py:153] step: 830850, training_loss: 1.28965e+02
I1112 20:01:07.963706 140264174335808 run_lib.py:153] step: 830900, training_loss: 9.62723e+01
I1112 20:01:18.103946 140264174335808 run_lib.py:153] step: 830950, training_loss: 1.23286e+02
I1112 20:01:27.790315 140264174335808 run_lib.py:153] step: 831000, training_loss: 1.22476e+02
I1112 20:01:38.224519 140264174335808 run_lib.py:153] step: 831050, training_loss: 1.27819e+02
I1112 20:01:47.782499 140264174335808 run_lib.py:153] step: 831100, training_loss: 1.16946e+02
I1112 20:01:58.443062 140264174335808 run_lib.py:153] step: 831150, training_loss: 1.26283e+02
I1112 20:02:08.933519 140264174335808 run_lib.py:153] step: 831200, training_loss: 1.34456e+02
I1112 20:02:19.707442 140264174335808 run_lib.py:153] step: 831250, training_loss: 1.39255e+02
I1112 20:02:29.595523 140264174335808 run_lib.py:153] step: 831300, training_loss: 1.17285e+02
I1112 20:02:39.294646 140264174335808 run_lib.py:153] step: 831350, training_loss: 1.07906e+02
I1112 20:02:48.760346 140264174335808 run_lib.py:153] step: 831400, training_loss: 1.69167e+02
I1112 20:02:59.241306 140264174335808 run_lib.py:153] step: 831450, training_loss: 8.83557e+01
I1112 20:03:09.448395 140264174335808 run_lib.py:153] step: 831500, training_loss: 1.22386e+02
I1112 20:03:19.389678 140264174335808 run_lib.py:153] step: 831550, training_loss: 1.19933e+02
I1112 20:03:29.779181 140264174335808 run_lib.py:153] step: 831600, training_loss: 1.15211e+02
I1112 20:03:39.911866 140264174335808 run_lib.py:153] step: 831650, training_loss: 1.18082e+02
I1112 20:03:50.501542 140264174335808 run_lib.py:153] step: 831700, training_loss: 1.15015e+02
I1112 20:04:00.286356 140264174335808 run_lib.py:153] step: 831750, training_loss: 1.35278e+02
I1112 20:04:10.493169 140264174335808 run_lib.py:153] step: 831800, training_loss: 1.29677e+02
I1112 20:04:20.426636 140264174335808 run_lib.py:153] step: 831850, training_loss: 1.17072e+02
I1112 20:04:30.923616 140264174335808 run_lib.py:153] step: 831900, training_loss: 1.16886e+02
I1112 20:04:41.292594 140264174335808 run_lib.py:153] step: 831950, training_loss: 1.06551e+02
I1112 20:04:51.727441 140264174335808 run_lib.py:153] step: 832000, training_loss: 1.46040e+02
I1112 20:05:01.628726 140264174335808 run_lib.py:153] step: 832050, training_loss: 1.53044e+02
I1112 20:05:11.154775 140264174335808 run_lib.py:153] step: 832100, training_loss: 1.14866e+02
I1112 20:05:21.121606 140264174335808 run_lib.py:153] step: 832150, training_loss: 1.47143e+02
I1112 20:05:31.506909 140264174335808 run_lib.py:153] step: 832200, training_loss: 1.50344e+02
I1112 20:05:41.842170 140264174335808 run_lib.py:153] step: 832250, training_loss: 9.73488e+01
I1112 20:05:51.662184 140264174335808 run_lib.py:153] step: 832300, training_loss: 1.37475e+02
I1112 20:06:01.441575 140264174335808 run_lib.py:153] step: 832350, training_loss: 1.52011e+02
I1112 20:06:12.468983 140264174335808 run_lib.py:153] step: 832400, training_loss: 1.21729e+02
I1112 20:06:22.619866 140264174335808 run_lib.py:153] step: 832450, training_loss: 1.28984e+02
I1112 20:06:32.684114 140264174335808 run_lib.py:153] step: 832500, training_loss: 1.07894e+02
I1112 20:06:42.643838 140264174335808 run_lib.py:153] step: 832550, training_loss: 1.19711e+02
I1112 20:06:52.685224 140264174335808 run_lib.py:153] step: 832600, training_loss: 1.59555e+02
I1112 20:07:02.518768 140264174335808 run_lib.py:153] step: 832650, training_loss: 1.06618e+02
I1112 20:07:12.047455 140264174335808 run_lib.py:153] step: 832700, training_loss: 1.38307e+02
I1112 20:07:21.968713 140264174335808 run_lib.py:153] step: 832750, training_loss: 1.31721e+02
I1112 20:07:31.996571 140264174335808 run_lib.py:153] step: 832800, training_loss: 1.33142e+02
I1112 20:07:41.402767 140264174335808 run_lib.py:153] step: 832850, training_loss: 1.52458e+02
I1112 20:07:50.785469 140264174335808 run_lib.py:153] step: 832900, training_loss: 9.66401e+01
I1112 20:08:00.793110 140264174335808 run_lib.py:153] step: 832950, training_loss: 1.23256e+02
I1112 20:08:10.774528 140264174335808 run_lib.py:153] step: 833000, training_loss: 1.23483e+02
I1112 20:08:20.626881 140264174335808 run_lib.py:153] step: 833050, training_loss: 1.14457e+02
I1112 20:08:30.505659 140264174335808 run_lib.py:153] step: 833100, training_loss: 1.39054e+02
I1112 20:08:40.398178 140264174335808 run_lib.py:153] step: 833150, training_loss: 1.29696e+02
I1112 20:08:49.658417 140264174335808 run_lib.py:153] step: 833200, training_loss: 1.18452e+02
I1112 20:09:00.808993 140264174335808 run_lib.py:153] step: 833250, training_loss: 1.03934e+02
I1112 20:09:10.823770 140264174335808 run_lib.py:153] step: 833300, training_loss: 1.23031e+02
I1112 20:09:20.915434 140264174335808 run_lib.py:153] step: 833350, training_loss: 1.28673e+02
I1112 20:09:31.354800 140264174335808 run_lib.py:153] step: 833400, training_loss: 1.05335e+02
I1112 20:09:41.382065 140264174335808 run_lib.py:153] step: 833450, training_loss: 1.29835e+02
I1112 20:09:51.194411 140264174335808 run_lib.py:153] step: 833500, training_loss: 1.12869e+02
I1112 20:10:00.959133 140264174335808 run_lib.py:153] step: 833550, training_loss: 1.08897e+02
I1112 20:10:12.161300 140264174335808 run_lib.py:153] step: 833600, training_loss: 1.40281e+02
I1112 20:10:22.128371 140264174335808 run_lib.py:153] step: 833650, training_loss: 1.38276e+02
I1112 20:10:32.787593 140264174335808 run_lib.py:153] step: 833700, training_loss: 1.27155e+02
I1112 20:10:42.553550 140264174335808 run_lib.py:153] step: 833750, training_loss: 1.45856e+02
I1112 20:10:52.878957 140264174335808 run_lib.py:153] step: 833800, training_loss: 1.16725e+02
I1112 20:11:03.312457 140264174335808 run_lib.py:153] step: 833850, training_loss: 9.32146e+01
I1112 20:11:13.575894 140264174335808 run_lib.py:153] step: 833900, training_loss: 1.33609e+02
I1112 20:11:22.975519 140264174335808 run_lib.py:153] step: 833950, training_loss: 1.02289e+02
I1112 20:11:33.770599 140264174335808 run_lib.py:153] step: 834000, training_loss: 1.23101e+02
I1112 20:11:44.408409 140264174335808 run_lib.py:153] step: 834050, training_loss: 1.06368e+02
I1112 20:11:54.102191 140264174335808 run_lib.py:153] step: 834100, training_loss: 1.41384e+02
I1112 20:12:04.063277 140264174335808 run_lib.py:153] step: 834150, training_loss: 1.04462e+02
I1112 20:12:13.453443 140264174335808 run_lib.py:153] step: 834200, training_loss: 1.34536e+02
I1112 20:12:22.835594 140264174335808 run_lib.py:153] step: 834250, training_loss: 1.10653e+02
I1112 20:12:32.095416 140264174335808 run_lib.py:153] step: 834300, training_loss: 9.39045e+01
I1112 20:12:41.714126 140264174335808 run_lib.py:153] step: 834350, training_loss: 1.26100e+02
I1112 20:12:51.880194 140264174335808 run_lib.py:153] step: 834400, training_loss: 1.05779e+02
I1112 20:13:01.905800 140264174335808 run_lib.py:153] step: 834450, training_loss: 1.50843e+02
I1112 20:13:11.417732 140264174335808 run_lib.py:153] step: 834500, training_loss: 1.28218e+02
I1112 20:13:21.036107 140264174335808 run_lib.py:153] step: 834550, training_loss: 1.24630e+02
I1112 20:13:30.267436 140264174335808 run_lib.py:153] step: 834600, training_loss: 1.37489e+02
I1112 20:13:39.809911 140264174335808 run_lib.py:153] step: 834650, training_loss: 1.38783e+02
I1112 20:13:49.597253 140264174335808 run_lib.py:153] step: 834700, training_loss: 1.29132e+02
I1112 20:13:59.775735 140264174335808 run_lib.py:153] step: 834750, training_loss: 1.37667e+02
I1112 20:14:09.899724 140264174335808 run_lib.py:153] step: 834800, training_loss: 1.17218e+02
I1112 20:14:19.493330 140264174335808 run_lib.py:153] step: 834850, training_loss: 1.15558e+02
I1112 20:14:28.878443 140264174335808 run_lib.py:153] step: 834900, training_loss: 1.71556e+02
I1112 20:14:38.712852 140264174335808 run_lib.py:153] step: 834950, training_loss: 1.22255e+02
I1112 20:14:48.542031 140264174335808 run_lib.py:153] step: 835000, training_loss: 1.20087e+02
I1112 20:14:48.644812 140264174335808 run_lib.py:166] step: 835000, eval_loss: 1.22251e+02
I1112 20:14:58.855347 140264174335808 run_lib.py:153] step: 835050, training_loss: 1.29824e+02
I1112 20:15:09.453126 140264174335808 run_lib.py:153] step: 835100, training_loss: 1.41029e+02
I1112 20:15:19.479400 140264174335808 run_lib.py:153] step: 835150, training_loss: 1.22326e+02
I1112 20:15:30.359131 140264174335808 run_lib.py:153] step: 835200, training_loss: 1.39679e+02
I1112 20:15:41.015376 140264174335808 run_lib.py:153] step: 835250, training_loss: 1.28697e+02
I1112 20:15:51.384823 140264174335808 run_lib.py:153] step: 835300, training_loss: 1.03717e+02
I1112 20:16:02.223857 140264174335808 run_lib.py:153] step: 835350, training_loss: 1.28582e+02
I1112 20:16:12.285673 140264174335808 run_lib.py:153] step: 835400, training_loss: 1.33859e+02
I1112 20:16:21.902769 140264174335808 run_lib.py:153] step: 835450, training_loss: 1.28583e+02
I1112 20:16:31.740175 140264174335808 run_lib.py:153] step: 835500, training_loss: 1.27412e+02
I1112 20:16:42.429253 140264174335808 run_lib.py:153] step: 835550, training_loss: 1.19727e+02
I1112 20:16:53.262461 140264174335808 run_lib.py:153] step: 835600, training_loss: 1.51706e+02
I1112 20:17:03.126396 140264174335808 run_lib.py:153] step: 835650, training_loss: 1.43053e+02
I1112 20:17:12.957345 140264174335808 run_lib.py:153] step: 835700, training_loss: 1.27990e+02
I1112 20:17:22.707383 140264174335808 run_lib.py:153] step: 835750, training_loss: 1.25950e+02
I1112 20:17:33.472502 140264174335808 run_lib.py:153] step: 835800, training_loss: 1.48081e+02
I1112 20:17:43.676793 140264174335808 run_lib.py:153] step: 835850, training_loss: 1.56037e+02
I1112 20:17:53.576811 140264174335808 run_lib.py:153] step: 835900, training_loss: 1.20988e+02
I1112 20:18:03.987468 140264174335808 run_lib.py:153] step: 835950, training_loss: 1.55664e+02
I1112 20:18:14.343790 140264174335808 run_lib.py:153] step: 836000, training_loss: 1.23881e+02
I1112 20:18:24.157433 140264174335808 run_lib.py:153] step: 836050, training_loss: 1.30251e+02
I1112 20:18:33.597000 140264174335808 run_lib.py:153] step: 836100, training_loss: 1.43483e+02
I1112 20:18:43.635284 140264174335808 run_lib.py:153] step: 836150, training_loss: 1.07695e+02
I1112 20:18:53.836818 140264174335808 run_lib.py:153] step: 836200, training_loss: 1.16323e+02
I1112 20:19:03.182117 140264174335808 run_lib.py:153] step: 836250, training_loss: 1.23623e+02
I1112 20:19:13.142441 140264174335808 run_lib.py:153] step: 836300, training_loss: 1.36541e+02
I1112 20:19:23.415946 140264174335808 run_lib.py:153] step: 836350, training_loss: 1.26035e+02
I1112 20:19:33.637261 140264174335808 run_lib.py:153] step: 836400, training_loss: 1.35030e+02
I1112 20:19:44.515575 140264174335808 run_lib.py:153] step: 836450, training_loss: 1.12572e+02
I1112 20:19:55.328824 140264174335808 run_lib.py:153] step: 836500, training_loss: 1.58934e+02
I1112 20:20:05.534097 140264174335808 run_lib.py:153] step: 836550, training_loss: 1.44912e+02
I1112 20:20:15.884927 140264174335808 run_lib.py:153] step: 836600, training_loss: 1.38387e+02
I1112 20:20:25.743532 140264174335808 run_lib.py:153] step: 836650, training_loss: 1.07410e+02
I1112 20:20:35.679891 140264174335808 run_lib.py:153] step: 836700, training_loss: 1.10841e+02
I1112 20:20:46.324378 140264174335808 run_lib.py:153] step: 836750, training_loss: 1.29614e+02
I1112 20:20:57.020885 140264174335808 run_lib.py:153] step: 836800, training_loss: 1.40664e+02
I1112 20:21:07.149691 140264174335808 run_lib.py:153] step: 836850, training_loss: 1.08244e+02
I1112 20:21:17.035952 140264174335808 run_lib.py:153] step: 836900, training_loss: 1.39813e+02
I1112 20:21:26.868872 140264174335808 run_lib.py:153] step: 836950, training_loss: 1.03772e+02
I1112 20:21:36.875270 140264174335808 run_lib.py:153] step: 837000, training_loss: 1.15178e+02
I1112 20:21:46.844875 140264174335808 run_lib.py:153] step: 837050, training_loss: 1.20059e+02
I1112 20:21:56.734393 140264174335808 run_lib.py:153] step: 837100, training_loss: 1.46614e+02
I1112 20:22:06.649294 140264174335808 run_lib.py:153] step: 837150, training_loss: 1.39899e+02
I1112 20:22:16.384644 140264174335808 run_lib.py:153] step: 837200, training_loss: 1.35546e+02
I1112 20:22:26.498939 140264174335808 run_lib.py:153] step: 837250, training_loss: 1.31712e+02
I1112 20:22:36.255260 140264174335808 run_lib.py:153] step: 837300, training_loss: 1.46684e+02
I1112 20:22:46.032275 140264174335808 run_lib.py:153] step: 837350, training_loss: 1.28217e+02
I1112 20:22:56.289052 140264174335808 run_lib.py:153] step: 837400, training_loss: 1.43789e+02
I1112 20:23:06.432062 140264174335808 run_lib.py:153] step: 837450, training_loss: 1.25087e+02
I1112 20:23:16.824339 140264174335808 run_lib.py:153] step: 837500, training_loss: 1.29009e+02
I1112 20:23:27.093054 140264174335808 run_lib.py:153] step: 837550, training_loss: 1.29817e+02
I1112 20:23:38.092670 140264174335808 run_lib.py:153] step: 837600, training_loss: 1.04015e+02
I1112 20:23:48.459972 140264174335808 run_lib.py:153] step: 837650, training_loss: 1.36810e+02
I1112 20:23:57.827183 140264174335808 run_lib.py:153] step: 837700, training_loss: 1.19108e+02
I1112 20:24:07.842572 140264174335808 run_lib.py:153] step: 837750, training_loss: 1.48148e+02
I1112 20:24:17.863922 140264174335808 run_lib.py:153] step: 837800, training_loss: 1.41107e+02
I1112 20:24:27.811547 140264174335808 run_lib.py:153] step: 837850, training_loss: 1.25813e+02
I1112 20:24:37.777798 140264174335808 run_lib.py:153] step: 837900, training_loss: 1.62708e+02
I1112 20:24:48.136808 140264174335808 run_lib.py:153] step: 837950, training_loss: 9.13131e+01
I1112 20:24:57.873677 140264174335808 run_lib.py:153] step: 838000, training_loss: 1.39912e+02
I1112 20:25:08.012032 140264174335808 run_lib.py:153] step: 838050, training_loss: 1.12210e+02
I1112 20:25:17.793324 140264174335808 run_lib.py:153] step: 838100, training_loss: 1.54530e+02
I1112 20:25:27.947207 140264174335808 run_lib.py:153] step: 838150, training_loss: 1.23527e+02
I1112 20:25:38.091323 140264174335808 run_lib.py:153] step: 838200, training_loss: 1.48346e+02
I1112 20:25:48.238175 140264174335808 run_lib.py:153] step: 838250, training_loss: 1.11105e+02
I1112 20:25:58.600297 140264174335808 run_lib.py:153] step: 838300, training_loss: 1.30516e+02
I1112 20:26:09.052611 140264174335808 run_lib.py:153] step: 838350, training_loss: 1.23734e+02
I1112 20:26:18.924309 140264174335808 run_lib.py:153] step: 838400, training_loss: 1.26515e+02
I1112 20:26:28.785351 140264174335808 run_lib.py:153] step: 838450, training_loss: 1.10543e+02
I1112 20:26:39.122776 140264174335808 run_lib.py:153] step: 838500, training_loss: 1.20146e+02
I1112 20:26:49.545639 140264174335808 run_lib.py:153] step: 838550, training_loss: 1.37499e+02
I1112 20:26:59.290040 140264174335808 run_lib.py:153] step: 838600, training_loss: 1.20374e+02
I1112 20:27:09.320126 140264174335808 run_lib.py:153] step: 838650, training_loss: 1.19046e+02
I1112 20:27:19.736398 140264174335808 run_lib.py:153] step: 838700, training_loss: 1.46482e+02
I1112 20:27:29.820381 140264174335808 run_lib.py:153] step: 838750, training_loss: 1.12573e+02
I1112 20:27:39.599738 140264174335808 run_lib.py:153] step: 838800, training_loss: 1.12790e+02
I1112 20:27:48.995996 140264174335808 run_lib.py:153] step: 838850, training_loss: 1.29373e+02
I1112 20:27:58.825877 140264174335808 run_lib.py:153] step: 838900, training_loss: 1.40177e+02
I1112 20:28:08.883447 140264174335808 run_lib.py:153] step: 838950, training_loss: 1.08709e+02
I1112 20:28:18.510000 140264174335808 run_lib.py:153] step: 839000, training_loss: 1.35442e+02
I1112 20:28:29.212588 140264174335808 run_lib.py:153] step: 839050, training_loss: 1.21284e+02
I1112 20:28:39.737261 140264174335808 run_lib.py:153] step: 839100, training_loss: 1.14766e+02
I1112 20:28:49.456944 140264174335808 run_lib.py:153] step: 839150, training_loss: 1.47363e+02
I1112 20:28:59.216043 140264174335808 run_lib.py:153] step: 839200, training_loss: 1.40048e+02
I1112 20:29:09.298166 140264174335808 run_lib.py:153] step: 839250, training_loss: 1.26680e+02
I1112 20:29:19.586504 140264174335808 run_lib.py:153] step: 839300, training_loss: 1.35376e+02
I1112 20:29:29.786606 140264174335808 run_lib.py:153] step: 839350, training_loss: 1.03222e+02
I1112 20:29:40.862272 140264174335808 run_lib.py:153] step: 839400, training_loss: 1.57952e+02
I1112 20:29:51.240450 140264174335808 run_lib.py:153] step: 839450, training_loss: 1.30193e+02
I1112 20:30:01.736298 140264174335808 run_lib.py:153] step: 839500, training_loss: 1.23562e+02
I1112 20:30:11.859406 140264174335808 run_lib.py:153] step: 839550, training_loss: 1.58356e+02
I1112 20:30:21.404547 140264174335808 run_lib.py:153] step: 839600, training_loss: 1.35959e+02
I1112 20:30:31.934397 140264174335808 run_lib.py:153] step: 839650, training_loss: 1.66007e+02
I1112 20:30:42.094673 140264174335808 run_lib.py:153] step: 839700, training_loss: 9.63338e+01
I1112 20:30:52.656005 140264174335808 run_lib.py:153] step: 839750, training_loss: 1.20710e+02
I1112 20:31:03.400734 140264174335808 run_lib.py:153] step: 839800, training_loss: 1.15021e+02
I1112 20:31:13.415690 140264174335808 run_lib.py:153] step: 839850, training_loss: 1.08653e+02
I1112 20:31:23.487028 140264174335808 run_lib.py:153] step: 839900, training_loss: 1.22994e+02
I1112 20:31:33.226392 140264174335808 run_lib.py:153] step: 839950, training_loss: 1.11259e+02
I1112 20:31:42.726022 140264174335808 run_lib.py:153] step: 840000, training_loss: 1.57107e+02
I1112 20:31:43.260455 140264174335808 run_lib.py:166] step: 840000, eval_loss: 1.20482e+02
I1112 20:31:52.873850 140264174335808 run_lib.py:153] step: 840050, training_loss: 1.33285e+02
I1112 20:32:02.411485 140264174335808 run_lib.py:153] step: 840100, training_loss: 1.11126e+02
I1112 20:32:11.775286 140264174335808 run_lib.py:153] step: 840150, training_loss: 1.40784e+02
I1112 20:32:21.729161 140264174335808 run_lib.py:153] step: 840200, training_loss: 1.23739e+02
I1112 20:32:32.345056 140264174335808 run_lib.py:153] step: 840250, training_loss: 1.05802e+02
I1112 20:32:42.203733 140264174335808 run_lib.py:153] step: 840300, training_loss: 1.58923e+02
I1112 20:32:52.012087 140264174335808 run_lib.py:153] step: 840350, training_loss: 1.25052e+02
I1112 20:33:02.796190 140264174335808 run_lib.py:153] step: 840400, training_loss: 1.32739e+02
I1112 20:33:13.020860 140264174335808 run_lib.py:153] step: 840450, training_loss: 1.38030e+02
I1112 20:33:23.456797 140264174335808 run_lib.py:153] step: 840500, training_loss: 1.24646e+02
I1112 20:33:34.365894 140264174335808 run_lib.py:153] step: 840550, training_loss: 1.39402e+02
I1112 20:33:44.867879 140264174335808 run_lib.py:153] step: 840600, training_loss: 1.09741e+02
I1112 20:33:54.632489 140264174335808 run_lib.py:153] step: 840650, training_loss: 1.17093e+02
I1112 20:34:04.665874 140264174335808 run_lib.py:153] step: 840700, training_loss: 1.09169e+02
I1112 20:34:14.459850 140264174335808 run_lib.py:153] step: 840750, training_loss: 1.07097e+02
I1112 20:34:24.393651 140264174335808 run_lib.py:153] step: 840800, training_loss: 1.29156e+02
I1112 20:34:34.602609 140264174335808 run_lib.py:153] step: 840850, training_loss: 1.31889e+02
I1112 20:34:44.259883 140264174335808 run_lib.py:153] step: 840900, training_loss: 1.51904e+02
I1112 20:34:54.333234 140264174335808 run_lib.py:153] step: 840950, training_loss: 1.29117e+02
I1112 20:35:04.002176 140264174335808 run_lib.py:153] step: 841000, training_loss: 1.28931e+02
I1112 20:35:13.879346 140264174335808 run_lib.py:153] step: 841050, training_loss: 1.12299e+02
I1112 20:35:24.246670 140264174335808 run_lib.py:153] step: 841100, training_loss: 1.42522e+02
I1112 20:35:34.059802 140264174335808 run_lib.py:153] step: 841150, training_loss: 1.26399e+02
I1112 20:35:44.275522 140264174335808 run_lib.py:153] step: 841200, training_loss: 1.04270e+02
I1112 20:35:53.770536 140264174335808 run_lib.py:153] step: 841250, training_loss: 1.26162e+02
I1112 20:36:03.152022 140264174335808 run_lib.py:153] step: 841300, training_loss: 1.48522e+02
I1112 20:36:13.200600 140264174335808 run_lib.py:153] step: 841350, training_loss: 1.04273e+02
I1112 20:36:23.064176 140264174335808 run_lib.py:153] step: 841400, training_loss: 1.11016e+02
I1112 20:36:33.033890 140264174335808 run_lib.py:153] step: 841450, training_loss: 1.41840e+02
I1112 20:36:42.912436 140264174335808 run_lib.py:153] step: 841500, training_loss: 1.24924e+02
I1112 20:36:53.245845 140264174335808 run_lib.py:153] step: 841550, training_loss: 1.45938e+02
I1112 20:37:04.136060 140264174335808 run_lib.py:153] step: 841600, training_loss: 1.24076e+02
I1112 20:37:15.147785 140264174335808 run_lib.py:153] step: 841650, training_loss: 1.33312e+02
I1112 20:37:26.127951 140264174335808 run_lib.py:153] step: 841700, training_loss: 1.63794e+02
I1112 20:37:37.272583 140264174335808 run_lib.py:153] step: 841750, training_loss: 1.05235e+02
I1112 20:37:47.972773 140264174335808 run_lib.py:153] step: 841800, training_loss: 1.33168e+02
I1112 20:37:58.444177 140264174335808 run_lib.py:153] step: 841850, training_loss: 1.21896e+02
I1112 20:38:08.526890 140264174335808 run_lib.py:153] step: 841900, training_loss: 1.04548e+02
I1112 20:38:18.740240 140264174335808 run_lib.py:153] step: 841950, training_loss: 1.29372e+02
I1112 20:38:28.882657 140264174335808 run_lib.py:153] step: 842000, training_loss: 1.36334e+02
I1112 20:38:39.197584 140264174335808 run_lib.py:153] step: 842050, training_loss: 1.13144e+02
I1112 20:38:49.645076 140264174335808 run_lib.py:153] step: 842100, training_loss: 1.07331e+02
I1112 20:39:00.441895 140264174335808 run_lib.py:153] step: 842150, training_loss: 1.24008e+02
I1112 20:39:10.096586 140264174335808 run_lib.py:153] step: 842200, training_loss: 1.07891e+02
I1112 20:39:19.827936 140264174335808 run_lib.py:153] step: 842250, training_loss: 1.58411e+02
I1112 20:39:30.027097 140264174335808 run_lib.py:153] step: 842300, training_loss: 1.34094e+02
I1112 20:39:40.151729 140264174335808 run_lib.py:153] step: 842350, training_loss: 1.31405e+02
I1112 20:39:50.315558 140264174335808 run_lib.py:153] step: 842400, training_loss: 1.05606e+02
I1112 20:40:00.734239 140264174335808 run_lib.py:153] step: 842450, training_loss: 1.46029e+02
I1112 20:40:10.618719 140264174335808 run_lib.py:153] step: 842500, training_loss: 9.99640e+01
I1112 20:40:21.049258 140264174335808 run_lib.py:153] step: 842550, training_loss: 1.17048e+02
I1112 20:40:31.722749 140264174335808 run_lib.py:153] step: 842600, training_loss: 1.15904e+02
I1112 20:40:41.717568 140264174335808 run_lib.py:153] step: 842650, training_loss: 1.29130e+02
I1112 20:40:51.873851 140264174335808 run_lib.py:153] step: 842700, training_loss: 1.30788e+02
I1112 20:41:02.139775 140264174335808 run_lib.py:153] step: 842750, training_loss: 1.31600e+02
I1112 20:41:13.200022 140264174335808 run_lib.py:153] step: 842800, training_loss: 1.06404e+02
I1112 20:41:23.088097 140264174335808 run_lib.py:153] step: 842850, training_loss: 1.19863e+02
I1112 20:41:32.690240 140264174335808 run_lib.py:153] step: 842900, training_loss: 9.94408e+01
I1112 20:41:42.979852 140264174335808 run_lib.py:153] step: 842950, training_loss: 1.29563e+02
I1112 20:41:53.548251 140264174335808 run_lib.py:153] step: 843000, training_loss: 9.25416e+01
I1112 20:42:03.503081 140264174335808 run_lib.py:153] step: 843050, training_loss: 1.21803e+02
I1112 20:42:13.600449 140264174335808 run_lib.py:153] step: 843100, training_loss: 1.26265e+02
I1112 20:42:23.452454 140264174335808 run_lib.py:153] step: 843150, training_loss: 1.07546e+02
I1112 20:42:33.176408 140264174335808 run_lib.py:153] step: 843200, training_loss: 1.11893e+02
I1112 20:42:43.276077 140264174335808 run_lib.py:153] step: 843250, training_loss: 1.33700e+02
I1112 20:42:52.871603 140264174335808 run_lib.py:153] step: 843300, training_loss: 1.41515e+02
I1112 20:43:02.828719 140264174335808 run_lib.py:153] step: 843350, training_loss: 1.39666e+02
I1112 20:43:12.574894 140264174335808 run_lib.py:153] step: 843400, training_loss: 1.08423e+02
I1112 20:43:22.201335 140264174335808 run_lib.py:153] step: 843450, training_loss: 1.46079e+02
I1112 20:43:32.226429 140264174335808 run_lib.py:153] step: 843500, training_loss: 1.40914e+02
I1112 20:43:42.299110 140264174335808 run_lib.py:153] step: 843550, training_loss: 1.23640e+02
I1112 20:43:52.929981 140264174335808 run_lib.py:153] step: 843600, training_loss: 1.11763e+02
I1112 20:44:03.014410 140264174335808 run_lib.py:153] step: 843650, training_loss: 1.13954e+02
I1112 20:44:13.191587 140264174335808 run_lib.py:153] step: 843700, training_loss: 1.39446e+02
I1112 20:44:23.517267 140264174335808 run_lib.py:153] step: 843750, training_loss: 1.36194e+02
I1112 20:44:33.644351 140264174335808 run_lib.py:153] step: 843800, training_loss: 1.26372e+02
I1112 20:44:43.747023 140264174335808 run_lib.py:153] step: 843850, training_loss: 1.27869e+02
I1112 20:44:54.317693 140264174335808 run_lib.py:153] step: 843900, training_loss: 1.28170e+02
I1112 20:45:04.389417 140264174335808 run_lib.py:153] step: 843950, training_loss: 1.21317e+02
I1112 20:45:14.423016 140264174335808 run_lib.py:153] step: 844000, training_loss: 1.28008e+02
I1112 20:45:24.962791 140264174335808 run_lib.py:153] step: 844050, training_loss: 1.39451e+02
I1112 20:45:35.475505 140264174335808 run_lib.py:153] step: 844100, training_loss: 1.28014e+02
I1112 20:45:45.782520 140264174335808 run_lib.py:153] step: 844150, training_loss: 1.23705e+02
I1112 20:45:56.621486 140264174335808 run_lib.py:153] step: 844200, training_loss: 1.00899e+02
I1112 20:46:07.060094 140264174335808 run_lib.py:153] step: 844250, training_loss: 1.31784e+02
I1112 20:46:16.763180 140264174335808 run_lib.py:153] step: 844300, training_loss: 1.11474e+02
I1112 20:46:26.592899 140264174335808 run_lib.py:153] step: 844350, training_loss: 1.35153e+02
I1112 20:46:36.708439 140264174335808 run_lib.py:153] step: 844400, training_loss: 1.20639e+02
I1112 20:46:46.381946 140264174335808 run_lib.py:153] step: 844450, training_loss: 1.11791e+02
I1112 20:46:56.355855 140264174335808 run_lib.py:153] step: 844500, training_loss: 1.09704e+02
I1112 20:47:06.403456 140264174335808 run_lib.py:153] step: 844550, training_loss: 1.55311e+02
I1112 20:47:16.768986 140264174335808 run_lib.py:153] step: 844600, training_loss: 1.36848e+02
I1112 20:47:26.221463 140264174335808 run_lib.py:153] step: 844650, training_loss: 1.25914e+02
I1112 20:47:35.851378 140264174335808 run_lib.py:153] step: 844700, training_loss: 9.89204e+01
I1112 20:47:45.175307 140264174335808 run_lib.py:153] step: 844750, training_loss: 1.14626e+02
I1112 20:47:55.581106 140264174335808 run_lib.py:153] step: 844800, training_loss: 1.43794e+02
I1112 20:48:05.975353 140264174335808 run_lib.py:153] step: 844850, training_loss: 1.20378e+02
I1112 20:48:16.571864 140264174335808 run_lib.py:153] step: 844900, training_loss: 9.17469e+01
I1112 20:48:27.473140 140264174335808 run_lib.py:153] step: 844950, training_loss: 1.38211e+02
I1112 20:48:38.213311 140264174335808 run_lib.py:153] step: 845000, training_loss: 1.33154e+02
I1112 20:48:38.319066 140264174335808 run_lib.py:166] step: 845000, eval_loss: 1.29591e+02
I1112 20:48:48.060984 140264174335808 run_lib.py:153] step: 845050, training_loss: 1.21495e+02
I1112 20:48:58.500753 140264174335808 run_lib.py:153] step: 845100, training_loss: 1.00138e+02
I1112 20:49:07.939793 140264174335808 run_lib.py:153] step: 845150, training_loss: 1.21381e+02
I1112 20:49:18.346019 140264174335808 run_lib.py:153] step: 845200, training_loss: 1.36385e+02
I1112 20:49:28.778800 140264174335808 run_lib.py:153] step: 845250, training_loss: 1.39021e+02
I1112 20:49:39.419216 140264174335808 run_lib.py:153] step: 845300, training_loss: 1.17696e+02
I1112 20:49:49.853070 140264174335808 run_lib.py:153] step: 845350, training_loss: 1.28868e+02
I1112 20:50:00.653255 140264174335808 run_lib.py:153] step: 845400, training_loss: 1.23324e+02
I1112 20:50:10.770101 140264174335808 run_lib.py:153] step: 845450, training_loss: 1.14816e+02
I1112 20:50:20.757925 140264174335808 run_lib.py:153] step: 845500, training_loss: 1.51678e+02
I1112 20:50:31.348285 140264174335808 run_lib.py:153] step: 845550, training_loss: 1.24049e+02
I1112 20:50:41.243350 140264174335808 run_lib.py:153] step: 845600, training_loss: 1.34394e+02
I1112 20:50:51.576329 140264174335808 run_lib.py:153] step: 845650, training_loss: 1.45795e+02
I1112 20:51:01.068929 140264174335808 run_lib.py:153] step: 845700, training_loss: 1.32877e+02
I1112 20:51:10.519835 140264174335808 run_lib.py:153] step: 845750, training_loss: 1.56516e+02
I1112 20:51:21.114547 140264174335808 run_lib.py:153] step: 845800, training_loss: 1.21683e+02
I1112 20:51:31.511262 140264174335808 run_lib.py:153] step: 845850, training_loss: 1.27912e+02
I1112 20:51:42.063831 140264174335808 run_lib.py:153] step: 845900, training_loss: 1.40084e+02
I1112 20:51:52.877864 140264174335808 run_lib.py:153] step: 845950, training_loss: 1.08369e+02
I1112 20:52:03.264033 140264174335808 run_lib.py:153] step: 846000, training_loss: 1.27530e+02
I1112 20:52:12.700513 140264174335808 run_lib.py:153] step: 846050, training_loss: 1.30645e+02
I1112 20:52:22.967926 140264174335808 run_lib.py:153] step: 846100, training_loss: 7.87833e+01
I1112 20:52:33.943424 140264174335808 run_lib.py:153] step: 846150, training_loss: 1.24657e+02
I1112 20:52:44.128679 140264174335808 run_lib.py:153] step: 846200, training_loss: 9.99556e+01
I1112 20:52:53.870771 140264174335808 run_lib.py:153] step: 846250, training_loss: 1.14210e+02
I1112 20:53:04.610884 140264174335808 run_lib.py:153] step: 846300, training_loss: 1.19154e+02
I1112 20:53:15.220216 140264174335808 run_lib.py:153] step: 846350, training_loss: 1.07734e+02
I1112 20:53:24.759630 140264174335808 run_lib.py:153] step: 846400, training_loss: 1.49100e+02
I1112 20:53:35.044271 140264174335808 run_lib.py:153] step: 846450, training_loss: 1.43069e+02
I1112 20:53:44.791987 140264174335808 run_lib.py:153] step: 846500, training_loss: 1.48624e+02
I1112 20:53:55.295614 140264174335808 run_lib.py:153] step: 846550, training_loss: 1.15246e+02
I1112 20:54:06.188256 140264174335808 run_lib.py:153] step: 846600, training_loss: 1.15870e+02
I1112 20:54:15.861837 140264174335808 run_lib.py:153] step: 846650, training_loss: 1.45683e+02
I1112 20:54:25.601424 140264174335808 run_lib.py:153] step: 846700, training_loss: 1.43495e+02
I1112 20:54:35.898210 140264174335808 run_lib.py:153] step: 846750, training_loss: 1.02284e+02
I1112 20:54:46.326495 140264174335808 run_lib.py:153] step: 846800, training_loss: 1.00509e+02
I1112 20:54:57.181722 140264174335808 run_lib.py:153] step: 846850, training_loss: 1.06804e+02
I1112 20:55:07.680215 140264174335808 run_lib.py:153] step: 846900, training_loss: 1.14544e+02
I1112 20:55:17.855082 140264174335808 run_lib.py:153] step: 846950, training_loss: 1.17316e+02
I1112 20:55:28.287593 140264174335808 run_lib.py:153] step: 847000, training_loss: 1.36086e+02
I1112 20:55:38.366982 140264174335808 run_lib.py:153] step: 847050, training_loss: 1.18196e+02
I1112 20:55:48.094488 140264174335808 run_lib.py:153] step: 847100, training_loss: 1.27874e+02
I1112 20:55:58.437809 140264174335808 run_lib.py:153] step: 847150, training_loss: 1.20038e+02
I1112 20:56:08.427932 140264174335808 run_lib.py:153] step: 847200, training_loss: 1.31012e+02
I1112 20:56:18.433316 140264174335808 run_lib.py:153] step: 847250, training_loss: 1.24699e+02
I1112 20:56:28.783477 140264174335808 run_lib.py:153] step: 847300, training_loss: 1.21092e+02
I1112 20:56:38.368015 140264174335808 run_lib.py:153] step: 847350, training_loss: 1.13096e+02
I1112 20:56:48.273366 140264174335808 run_lib.py:153] step: 847400, training_loss: 8.04602e+01
I1112 20:56:59.342351 140264174335808 run_lib.py:153] step: 847450, training_loss: 1.36872e+02
I1112 20:57:09.502143 140264174335808 run_lib.py:153] step: 847500, training_loss: 1.24098e+02
I1112 20:57:19.553200 140264174335808 run_lib.py:153] step: 847550, training_loss: 1.13928e+02
I1112 20:57:29.142780 140264174335808 run_lib.py:153] step: 847600, training_loss: 1.40575e+02
I1112 20:57:38.977688 140264174335808 run_lib.py:153] step: 847650, training_loss: 1.13588e+02
I1112 20:57:49.162066 140264174335808 run_lib.py:153] step: 847700, training_loss: 1.38537e+02
I1112 20:57:59.231983 140264174335808 run_lib.py:153] step: 847750, training_loss: 1.16185e+02
I1112 20:58:08.931720 140264174335808 run_lib.py:153] step: 847800, training_loss: 1.12605e+02
I1112 20:58:18.540522 140264174335808 run_lib.py:153] step: 847850, training_loss: 1.43215e+02
I1112 20:58:27.913980 140264174335808 run_lib.py:153] step: 847900, training_loss: 1.37177e+02
I1112 20:58:37.403442 140264174335808 run_lib.py:153] step: 847950, training_loss: 1.31151e+02
I1112 20:58:47.111665 140264174335808 run_lib.py:153] step: 848000, training_loss: 1.34300e+02
I1112 20:58:56.873514 140264174335808 run_lib.py:153] step: 848050, training_loss: 1.30395e+02
I1112 20:59:07.195694 140264174335808 run_lib.py:153] step: 848100, training_loss: 1.26077e+02
I1112 20:59:17.471133 140264174335808 run_lib.py:153] step: 848150, training_loss: 1.29309e+02
I1112 20:59:27.294159 140264174335808 run_lib.py:153] step: 848200, training_loss: 1.32143e+02
I1112 20:59:37.164599 140264174335808 run_lib.py:153] step: 848250, training_loss: 1.09052e+02
I1112 20:59:47.024761 140264174335808 run_lib.py:153] step: 848300, training_loss: 1.44738e+02
I1112 20:59:56.985919 140264174335808 run_lib.py:153] step: 848350, training_loss: 1.70030e+02
I1112 21:00:07.284702 140264174335808 run_lib.py:153] step: 848400, training_loss: 1.34275e+02
I1112 21:00:17.254308 140264174335808 run_lib.py:153] step: 848450, training_loss: 1.18478e+02
I1112 21:00:26.896151 140264174335808 run_lib.py:153] step: 848500, training_loss: 1.22684e+02
I1112 21:00:36.856717 140264174335808 run_lib.py:153] step: 848550, training_loss: 1.50900e+02
I1112 21:00:47.113282 140264174335808 run_lib.py:153] step: 848600, training_loss: 1.45555e+02
I1112 21:00:57.336019 140264174335808 run_lib.py:153] step: 848650, training_loss: 1.22695e+02
I1112 21:01:07.164182 140264174335808 run_lib.py:153] step: 848700, training_loss: 1.24540e+02
I1112 21:01:16.959883 140264174335808 run_lib.py:153] step: 848750, training_loss: 1.07777e+02
I1112 21:01:26.439526 140264174335808 run_lib.py:153] step: 848800, training_loss: 1.15844e+02
I1112 21:01:36.116112 140264174335808 run_lib.py:153] step: 848850, training_loss: 1.44763e+02
I1112 21:01:46.164165 140264174335808 run_lib.py:153] step: 848900, training_loss: 1.12512e+02
I1112 21:01:55.970403 140264174335808 run_lib.py:153] step: 848950, training_loss: 1.13119e+02
I1112 21:02:05.422972 140264174335808 run_lib.py:153] step: 849000, training_loss: 1.51055e+02
I1112 21:02:14.927151 140264174335808 run_lib.py:153] step: 849050, training_loss: 1.24471e+02
I1112 21:02:24.763028 140264174335808 run_lib.py:153] step: 849100, training_loss: 1.12656e+02
I1112 21:02:34.659799 140264174335808 run_lib.py:153] step: 849150, training_loss: 1.17133e+02
I1112 21:02:44.892821 140264174335808 run_lib.py:153] step: 849200, training_loss: 1.27275e+02
I1112 21:02:55.082951 140264174335808 run_lib.py:153] step: 849250, training_loss: 1.03315e+02
I1112 21:03:05.599076 140264174335808 run_lib.py:153] step: 849300, training_loss: 1.32252e+02
I1112 21:03:15.887882 140264174335808 run_lib.py:153] step: 849350, training_loss: 1.29784e+02
I1112 21:03:25.722196 140264174335808 run_lib.py:153] step: 849400, training_loss: 1.09263e+02
I1112 21:03:35.589909 140264174335808 run_lib.py:153] step: 849450, training_loss: 1.23948e+02
I1112 21:03:45.783727 140264174335808 run_lib.py:153] step: 849500, training_loss: 1.32265e+02
I1112 21:03:55.762644 140264174335808 run_lib.py:153] step: 849550, training_loss: 1.15068e+02
I1112 21:04:06.134624 140264174335808 run_lib.py:153] step: 849600, training_loss: 1.20630e+02
I1112 21:04:16.564267 140264174335808 run_lib.py:153] step: 849650, training_loss: 1.39141e+02
I1112 21:04:26.564251 140264174335808 run_lib.py:153] step: 849700, training_loss: 1.46118e+02
I1112 21:04:36.981983 140264174335808 run_lib.py:153] step: 849750, training_loss: 1.28480e+02
I1112 21:04:46.677692 140264174335808 run_lib.py:153] step: 849800, training_loss: 9.63050e+01
I1112 21:04:56.433908 140264174335808 run_lib.py:153] step: 849850, training_loss: 1.23616e+02
I1112 21:05:05.734198 140264174335808 run_lib.py:153] step: 849900, training_loss: 1.42407e+02
I1112 21:05:15.563943 140264174335808 run_lib.py:153] step: 849950, training_loss: 1.15925e+02
I1112 21:05:25.160972 140264174335808 run_lib.py:153] step: 850000, training_loss: 1.32322e+02
I1112 21:05:25.705380 140264174335808 run_lib.py:166] step: 850000, eval_loss: 1.37321e+02
I1112 21:05:36.246474 140264174335808 run_lib.py:153] step: 850050, training_loss: 1.11981e+02
I1112 21:05:45.819645 140264174335808 run_lib.py:153] step: 850100, training_loss: 1.48640e+02
I1112 21:05:55.298424 140264174335808 run_lib.py:153] step: 850150, training_loss: 1.25963e+02
I1112 21:06:05.202025 140264174335808 run_lib.py:153] step: 850200, training_loss: 1.23977e+02
I1112 21:06:15.208777 140264174335808 run_lib.py:153] step: 850250, training_loss: 1.18935e+02
I1112 21:06:25.434290 140264174335808 run_lib.py:153] step: 850300, training_loss: 1.36298e+02
I1112 21:06:35.034976 140264174335808 run_lib.py:153] step: 850350, training_loss: 1.42096e+02
I1112 21:06:45.476521 140264174335808 run_lib.py:153] step: 850400, training_loss: 1.21890e+02
I1112 21:06:55.602146 140264174335808 run_lib.py:153] step: 850450, training_loss: 1.30131e+02
I1112 21:07:05.027866 140264174335808 run_lib.py:153] step: 850500, training_loss: 1.12488e+02
I1112 21:07:14.896202 140264174335808 run_lib.py:153] step: 850550, training_loss: 1.15917e+02
I1112 21:07:24.427836 140264174335808 run_lib.py:153] step: 850600, training_loss: 1.01006e+02
I1112 21:07:34.901831 140264174335808 run_lib.py:153] step: 850650, training_loss: 1.41296e+02
I1112 21:07:45.322976 140264174335808 run_lib.py:153] step: 850700, training_loss: 1.06037e+02
I1112 21:07:55.338491 140264174335808 run_lib.py:153] step: 850750, training_loss: 1.09748e+02
I1112 21:08:05.019415 140264174335808 run_lib.py:153] step: 850800, training_loss: 1.27967e+02
I1112 21:08:15.705669 140264174335808 run_lib.py:153] step: 850850, training_loss: 1.33229e+02
I1112 21:08:25.589135 140264174335808 run_lib.py:153] step: 850900, training_loss: 1.10098e+02
I1112 21:08:36.389196 140264174335808 run_lib.py:153] step: 850950, training_loss: 1.03456e+02
I1112 21:08:46.607998 140264174335808 run_lib.py:153] step: 851000, training_loss: 1.40895e+02
I1112 21:08:57.229124 140264174335808 run_lib.py:153] step: 851050, training_loss: 1.35053e+02
I1112 21:09:07.377078 140264174335808 run_lib.py:153] step: 851100, training_loss: 1.26834e+02
I1112 21:09:17.067660 140264174335808 run_lib.py:153] step: 851150, training_loss: 1.45274e+02
I1112 21:09:26.771629 140264174335808 run_lib.py:153] step: 851200, training_loss: 1.27437e+02
I1112 21:09:36.773658 140264174335808 run_lib.py:153] step: 851250, training_loss: 9.62766e+01
I1112 21:09:46.200784 140264174335808 run_lib.py:153] step: 851300, training_loss: 1.39081e+02
I1112 21:09:55.604813 140264174335808 run_lib.py:153] step: 851350, training_loss: 1.28830e+02
I1112 21:10:04.986926 140264174335808 run_lib.py:153] step: 851400, training_loss: 1.22707e+02
I1112 21:10:14.550576 140264174335808 run_lib.py:153] step: 851450, training_loss: 1.30428e+02
I1112 21:10:24.639759 140264174335808 run_lib.py:153] step: 851500, training_loss: 1.40467e+02
I1112 21:10:34.588823 140264174335808 run_lib.py:153] step: 851550, training_loss: 1.46242e+02
I1112 21:10:44.141627 140264174335808 run_lib.py:153] step: 851600, training_loss: 1.37996e+02
I1112 21:10:54.018344 140264174335808 run_lib.py:153] step: 851650, training_loss: 1.26374e+02
I1112 21:11:04.548295 140264174335808 run_lib.py:153] step: 851700, training_loss: 1.00767e+02
I1112 21:11:14.722299 140264174335808 run_lib.py:153] step: 851750, training_loss: 1.63554e+02
I1112 21:11:24.558688 140264174335808 run_lib.py:153] step: 851800, training_loss: 1.53678e+02
I1112 21:11:34.939564 140264174335808 run_lib.py:153] step: 851850, training_loss: 1.16987e+02
I1112 21:11:44.481969 140264174335808 run_lib.py:153] step: 851900, training_loss: 1.31957e+02
I1112 21:11:53.988134 140264174335808 run_lib.py:153] step: 851950, training_loss: 1.39328e+02
I1112 21:12:03.937104 140264174335808 run_lib.py:153] step: 852000, training_loss: 1.00787e+02
I1112 21:12:13.928780 140264174335808 run_lib.py:153] step: 852050, training_loss: 1.38498e+02
I1112 21:12:23.759215 140264174335808 run_lib.py:153] step: 852100, training_loss: 1.21047e+02
I1112 21:12:34.041236 140264174335808 run_lib.py:153] step: 852150, training_loss: 1.51233e+02
I1112 21:12:43.763031 140264174335808 run_lib.py:153] step: 852200, training_loss: 1.29969e+02
I1112 21:12:53.495302 140264174335808 run_lib.py:153] step: 852250, training_loss: 1.24098e+02
I1112 21:13:03.684320 140264174335808 run_lib.py:153] step: 852300, training_loss: 1.49534e+02
I1112 21:13:13.732586 140264174335808 run_lib.py:153] step: 852350, training_loss: 1.28750e+02
I1112 21:13:23.846802 140264174335808 run_lib.py:153] step: 852400, training_loss: 1.34957e+02
I1112 21:13:33.319565 140264174335808 run_lib.py:153] step: 852450, training_loss: 1.10158e+02
I1112 21:13:42.994579 140264174335808 run_lib.py:153] step: 852500, training_loss: 1.15782e+02
I1112 21:13:52.798739 140264174335808 run_lib.py:153] step: 852550, training_loss: 9.18025e+01
I1112 21:14:03.511859 140264174335808 run_lib.py:153] step: 852600, training_loss: 1.25806e+02
I1112 21:14:13.399432 140264174335808 run_lib.py:153] step: 852650, training_loss: 1.42309e+02
I1112 21:14:23.022078 140264174335808 run_lib.py:153] step: 852700, training_loss: 1.62245e+02
I1112 21:14:32.592929 140264174335808 run_lib.py:153] step: 852750, training_loss: 1.25471e+02
I1112 21:14:42.295429 140264174335808 run_lib.py:153] step: 852800, training_loss: 1.35303e+02
I1112 21:14:52.400216 140264174335808 run_lib.py:153] step: 852850, training_loss: 1.09620e+02
I1112 21:15:03.232838 140264174335808 run_lib.py:153] step: 852900, training_loss: 1.16453e+02
I1112 21:15:13.772034 140264174335808 run_lib.py:153] step: 852950, training_loss: 1.39764e+02
I1112 21:15:24.588260 140264174335808 run_lib.py:153] step: 853000, training_loss: 1.52193e+02
I1112 21:15:35.294577 140264174335808 run_lib.py:153] step: 853050, training_loss: 1.31541e+02
I1112 21:15:45.361929 140264174335808 run_lib.py:153] step: 853100, training_loss: 1.17929e+02
I1112 21:15:55.525575 140264174335808 run_lib.py:153] step: 853150, training_loss: 1.27707e+02
I1112 21:16:05.736480 140264174335808 run_lib.py:153] step: 853200, training_loss: 1.36813e+02
I1112 21:16:16.543366 140264174335808 run_lib.py:153] step: 853250, training_loss: 1.44106e+02
I1112 21:16:27.045228 140264174335808 run_lib.py:153] step: 853300, training_loss: 1.32481e+02
I1112 21:16:37.042604 140264174335808 run_lib.py:153] step: 853350, training_loss: 1.14696e+02
I1112 21:16:48.140097 140264174335808 run_lib.py:153] step: 853400, training_loss: 1.25132e+02
I1112 21:16:58.809315 140264174335808 run_lib.py:153] step: 853450, training_loss: 1.08586e+02
I1112 21:17:09.357595 140264174335808 run_lib.py:153] step: 853500, training_loss: 1.15347e+02
I1112 21:17:18.801201 140264174335808 run_lib.py:153] step: 853550, training_loss: 1.01900e+02
I1112 21:17:29.570022 140264174335808 run_lib.py:153] step: 853600, training_loss: 1.10115e+02
I1112 21:17:40.017302 140264174335808 run_lib.py:153] step: 853650, training_loss: 1.22858e+02
I1112 21:17:49.504231 140264174335808 run_lib.py:153] step: 853700, training_loss: 1.25144e+02
I1112 21:17:59.458185 140264174335808 run_lib.py:153] step: 853750, training_loss: 1.04534e+02
I1112 21:18:09.381094 140264174335808 run_lib.py:153] step: 853800, training_loss: 9.93290e+01
I1112 21:18:19.652762 140264174335808 run_lib.py:153] step: 853850, training_loss: 1.33899e+02
I1112 21:18:29.730319 140264174335808 run_lib.py:153] step: 853900, training_loss: 1.23094e+02
I1112 21:18:39.442066 140264174335808 run_lib.py:153] step: 853950, training_loss: 1.16178e+02
I1112 21:18:49.334176 140264174335808 run_lib.py:153] step: 854000, training_loss: 1.28145e+02
I1112 21:18:59.766430 140264174335808 run_lib.py:153] step: 854050, training_loss: 1.35683e+02
I1112 21:19:09.806268 140264174335808 run_lib.py:153] step: 854100, training_loss: 9.63997e+01
I1112 21:19:19.965223 140264174335808 run_lib.py:153] step: 854150, training_loss: 1.48197e+02
I1112 21:19:30.106513 140264174335808 run_lib.py:153] step: 854200, training_loss: 1.05596e+02
I1112 21:19:40.377259 140264174335808 run_lib.py:153] step: 854250, training_loss: 1.01149e+02
I1112 21:19:50.431145 140264174335808 run_lib.py:153] step: 854300, training_loss: 1.35697e+02
I1112 21:20:00.439706 140264174335808 run_lib.py:153] step: 854350, training_loss: 1.21725e+02
I1112 21:20:10.497659 140264174335808 run_lib.py:153] step: 854400, training_loss: 1.10374e+02
I1112 21:20:20.441950 140264174335808 run_lib.py:153] step: 854450, training_loss: 1.21724e+02
I1112 21:20:30.097459 140264174335808 run_lib.py:153] step: 854500, training_loss: 1.38871e+02
I1112 21:20:39.987491 140264174335808 run_lib.py:153] step: 854550, training_loss: 9.95342e+01
I1112 21:20:49.861143 140264174335808 run_lib.py:153] step: 854600, training_loss: 1.25832e+02
I1112 21:20:59.822819 140264174335808 run_lib.py:153] step: 854650, training_loss: 1.27905e+02
I1112 21:21:09.222546 140264174335808 run_lib.py:153] step: 854700, training_loss: 1.61433e+02
I1112 21:21:19.663544 140264174335808 run_lib.py:153] step: 854750, training_loss: 1.05562e+02
I1112 21:21:30.309826 140264174335808 run_lib.py:153] step: 854800, training_loss: 1.49986e+02
I1112 21:21:41.108696 140264174335808 run_lib.py:153] step: 854850, training_loss: 1.37857e+02
I1112 21:21:50.963618 140264174335808 run_lib.py:153] step: 854900, training_loss: 1.20690e+02
I1112 21:22:01.355879 140264174335808 run_lib.py:153] step: 854950, training_loss: 1.23898e+02
I1112 21:22:11.037120 140264174335808 run_lib.py:153] step: 855000, training_loss: 1.10643e+02
I1112 21:22:11.138937 140264174335808 run_lib.py:166] step: 855000, eval_loss: 1.20000e+02
I1112 21:22:20.735978 140264174335808 run_lib.py:153] step: 855050, training_loss: 1.17676e+02
I1112 21:22:30.324815 140264174335808 run_lib.py:153] step: 855100, training_loss: 1.22666e+02
I1112 21:22:40.829004 140264174335808 run_lib.py:153] step: 855150, training_loss: 1.19068e+02
I1112 21:22:50.876489 140264174335808 run_lib.py:153] step: 855200, training_loss: 8.28601e+01
I1112 21:23:01.238178 140264174335808 run_lib.py:153] step: 855250, training_loss: 1.09568e+02
I1112 21:23:11.316977 140264174335808 run_lib.py:153] step: 855300, training_loss: 1.13759e+02
I1112 21:23:21.512231 140264174335808 run_lib.py:153] step: 855350, training_loss: 1.10456e+02
I1112 21:23:31.861730 140264174335808 run_lib.py:153] step: 855400, training_loss: 1.01698e+02
I1112 21:23:42.804707 140264174335808 run_lib.py:153] step: 855450, training_loss: 1.50663e+02
I1112 21:23:53.368119 140264174335808 run_lib.py:153] step: 855500, training_loss: 1.15076e+02
I1112 21:24:03.038137 140264174335808 run_lib.py:153] step: 855550, training_loss: 1.59249e+02
I1112 21:24:14.038514 140264174335808 run_lib.py:153] step: 855600, training_loss: 1.34465e+02
I1112 21:24:24.633311 140264174335808 run_lib.py:153] step: 855650, training_loss: 1.14491e+02
I1112 21:24:34.638579 140264174335808 run_lib.py:153] step: 855700, training_loss: 1.01791e+02
I1112 21:24:44.699761 140264174335808 run_lib.py:153] step: 855750, training_loss: 1.36516e+02
I1112 21:24:55.280938 140264174335808 run_lib.py:153] step: 855800, training_loss: 1.28088e+02
I1112 21:25:05.970252 140264174335808 run_lib.py:153] step: 855850, training_loss: 1.59825e+02
I1112 21:25:16.063697 140264174335808 run_lib.py:153] step: 855900, training_loss: 1.25563e+02
I1112 21:25:26.452235 140264174335808 run_lib.py:153] step: 855950, training_loss: 1.47540e+02
I1112 21:25:37.091784 140264174335808 run_lib.py:153] step: 856000, training_loss: 1.44392e+02
I1112 21:25:47.071484 140264174335808 run_lib.py:153] step: 856050, training_loss: 1.22426e+02
I1112 21:25:57.881918 140264174335808 run_lib.py:153] step: 856100, training_loss: 1.17357e+02
I1112 21:26:07.836257 140264174335808 run_lib.py:153] step: 856150, training_loss: 1.43884e+02
I1112 21:26:17.859620 140264174335808 run_lib.py:153] step: 856200, training_loss: 1.20720e+02
I1112 21:26:27.939920 140264174335808 run_lib.py:153] step: 856250, training_loss: 1.37837e+02
I1112 21:26:37.648736 140264174335808 run_lib.py:153] step: 856300, training_loss: 1.26776e+02
I1112 21:26:47.149281 140264174335808 run_lib.py:153] step: 856350, training_loss: 1.18220e+02
I1112 21:26:56.834568 140264174335808 run_lib.py:153] step: 856400, training_loss: 1.10032e+02
I1112 21:27:07.430628 140264174335808 run_lib.py:153] step: 856450, training_loss: 8.44879e+01
I1112 21:27:16.975069 140264174335808 run_lib.py:153] step: 856500, training_loss: 1.62970e+02
I1112 21:27:26.995507 140264174335808 run_lib.py:153] step: 856550, training_loss: 1.24806e+02
I1112 21:27:37.078730 140264174335808 run_lib.py:153] step: 856600, training_loss: 1.11540e+02
I1112 21:27:47.291090 140264174335808 run_lib.py:153] step: 856650, training_loss: 1.41267e+02
I1112 21:27:57.164206 140264174335808 run_lib.py:153] step: 856700, training_loss: 1.21805e+02
I1112 21:28:07.378036 140264174335808 run_lib.py:153] step: 856750, training_loss: 1.26883e+02
I1112 21:28:17.341548 140264174335808 run_lib.py:153] step: 856800, training_loss: 1.34483e+02
I1112 21:28:27.252249 140264174335808 run_lib.py:153] step: 856850, training_loss: 1.28742e+02
I1112 21:28:38.324949 140264174335808 run_lib.py:153] step: 856900, training_loss: 1.22190e+02
I1112 21:28:48.287298 140264174335808 run_lib.py:153] step: 856950, training_loss: 1.56649e+02
I1112 21:28:58.455027 140264174335808 run_lib.py:153] step: 857000, training_loss: 1.15192e+02
I1112 21:29:08.327781 140264174335808 run_lib.py:153] step: 857050, training_loss: 1.27848e+02
I1112 21:29:18.399816 140264174335808 run_lib.py:153] step: 857100, training_loss: 9.20161e+01
I1112 21:29:28.755749 140264174335808 run_lib.py:153] step: 857150, training_loss: 1.10095e+02
I1112 21:29:39.412633 140264174335808 run_lib.py:153] step: 857200, training_loss: 1.23061e+02
